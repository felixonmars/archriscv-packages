From: Zenithal <i@zenithal.me>

Some changes would be rejected/ignored when patching firefox 92 src,
so I commented out some blocks, see these #-leading block below

From d994f662c2060f7a328d69603aff242ef4f679ff Mon Sep 17 00:00:00 2001
From: Makoto Kato <m_kato@ga2.so-net.ne.jp>
Date: Wed, 9 Jun 2021 10:04:18 +0000
Subject: [PATCH] hashglobe fix

---
 servo/components/hashglobe/src/alloc.rs | 1 +
 1 file changed, 1 insertion(+)

#diff --git a/servo/components/hashglobe/src/alloc.rs b/servo/components/hashglobe/src/alloc.rs
#index b1c7a6eca5ee6..6c447107cce86 100644
#--- a/servo/components/hashglobe/src/alloc.rs
#+++ b/servo/components/hashglobe/src/alloc.rs
#@@ -18,6 +18,7 @@ const MIN_ALIGN: usize = 8;
#     target_arch = "x86_64",
#     target_arch = "aarch64",
#     target_arch = "mips64",
#+    target_arch = "riscv64",
#     target_arch = "s390x",
#     target_arch = "sparc64"
# )))]
From c8fb2a3779d78de22ead674a5d435d64fa0a3ced Mon Sep 17 00:00:00 2001
From: Makoto Kato <m_kato@ga2.so-net.ne.jp>
Date: Wed, 9 Jun 2021 10:13:10 +0000
Subject: [PATCH] Add ARCH_CPU_* for riscv

---
 ipc/chromium/src/build/build_config.h             |  6 ++++++
 security/sandbox/chromium/build/build_config.h    | 12 ++++++++++++
 third_party/libwebrtc/webrtc/build/build_config.h |  3 +++
 3 files changed, 21 insertions(+)

diff --git a/ipc/chromium/src/build/build_config.h b/ipc/chromium/src/build/build_config.h
index f57319089c3dd..7d0de63d6f6b9 100644
--- a/ipc/chromium/src/build/build_config.h
+++ b/ipc/chromium/src/build/build_config.h
@@ -122,6 +122,12 @@
 #  define ARCH_CPU_ARM_FAMILY 1
 #  define ARCH_CPU_ARM64 1
 #  define ARCH_CPU_64_BITS 1
+#elif defined(__riscv) && __riscv_xlen == 32
+#  define ARCH_CPU_RISCV 1
+#  define ARCH_CPU_32_BITS 1
+#elif defined(__riscv) && __riscv_xlen == 64
+#  define ARCH_CPU_RISCV 1
+#  define ARCH_CPU_64_BITS 1
 #else
 #  error Please add support for your architecture in build/build_config.h
 #endif
#diff --git a/security/sandbox/chromium/build/build_config.h b/security/sandbox/chromium/build/build_config.h
#index d3cdd2db4a697..ca9461e240d23 100644
#--- a/security/sandbox/chromium/build/build_config.h
#+++ b/security/sandbox/chromium/build/build_config.h
#@@ -166,6 +166,18 @@
# #define ARCH_CPU_32_BITS 1
# #define ARCH_CPU_BIG_ENDIAN 1
# #endif
#+#elif defined(__riscv)
#+#if __riscv_xlen == 32
#+#  define ARCH_CPU_RISCV_FAMILY 1
#+#  define ARCH_CPU_RISCV32 1
#+#  define ARCH_CPU_32_BITS 1
#+#  define ARCH_CPU_LITTLE_ENDIAN 1
#+#elif __riscv_xlen == 64
#+#  define ARCH_CPU_RISCV_FAMILY 1
#+#  define ARCH_CPU_RISCV64 1
#+#  define ARCH_CPU_64_BITS 1
#+#  define ARCH_CPU_LITTLE_ENDIAN 1
#+#endif
# #else
# #error Please add support for your architecture in build/build_config.h
# #endif
#diff --git a/third_party/libwebrtc/webrtc/build/build_config.h b/third_party/libwebrtc/webrtc/build/build_config.h
#index 229d1f411cfd2..a9bc015c48fcd 100644
#--- a/third_party/libwebrtc/webrtc/build/build_config.h
#+++ b/third_party/libwebrtc/webrtc/build/build_config.h
#@@ -171,6 +171,9 @@
# #define ARCH_CPU_ARM_FAMILY 1
# #define ARCH_CPU_ARM64 1
# #define ARCH_CPU_64_BITS 1
#+#elif defined(__riscv) && __riscv_xlen == 64
#+#define ARCH_CPU_RISCV 1
#+#define ARCH_CPU_64_BITS 1
# #else
# #error Please add support for your architecture in build/build_config.h
# #endif
From be9cbd86b4c121dbdb626f8c373fd809f25bc23e Mon Sep 17 00:00:00 2001
From: Makoto Kato <m_kato@ga2.so-net.ne.jp>
Date: Sun, 13 Jun 2021 04:06:39 +0000
Subject: [PATCH] Update authenticator-rs

---
 .cargo/config.in                              |    5 +
 Cargo.lock                                    |    3 +-
 .../rust/authenticator/.cargo-checksum.json   |    2 +-
 third_party/rust/authenticator/.clippy.toml   |    2 +
 third_party/rust/authenticator/.flake8        |    4 +
 .../authenticator/.pre-commit-config.yaml     |   42 +
 third_party/rust/authenticator/.travis.yml    |   42 +
 third_party/rust/authenticator/Cargo.lock     | 1603 -----------------
 third_party/rust/authenticator/Cargo.toml     |  131 +-
 third_party/rust/authenticator/build.rs       |    2 +
 .../authenticator/src/linux/hidwrapper.rs     |    3 +
 .../authenticator/src/linux/ioctl_riscv64.rs  |    5 +
 toolkit/library/rust/shared/Cargo.toml        |    2 +-
 13 files changed, 154 insertions(+), 1692 deletions(-)
 create mode 100644 third_party/rust/authenticator/.clippy.toml
 create mode 100644 third_party/rust/authenticator/.flake8
 create mode 100644 third_party/rust/authenticator/.pre-commit-config.yaml
 create mode 100644 third_party/rust/authenticator/.travis.yml
 delete mode 100644 third_party/rust/authenticator/Cargo.lock
 create mode 100644 third_party/rust/authenticator/src/linux/ioctl_riscv64.rs

diff --git a/.cargo/config.in b/.cargo/config.in
index ad853d744f13a..e4a922765889b 100644
--- a/.cargo/config.in
+++ b/.cargo/config.in
@@ -47,6 +47,11 @@ git = "https://github.com/mozilla-spidermonkey/jsparagus"
 replace-with = "vendored-sources"
 rev = "d5d8c00ebd3281d12e0be5dfddbb69f791f836f1"
 
+[source."https://github.com/makotokato/authenticator-rs"]
+git = "https://github.com/makotokato/authenticator-rs"
+replace-with = "vendored-sources"
+rev = "eed8919d50559f4959e2d7d2af7b4d48869b5366"
+
 [source."https://github.com/kvark/spirv_cross"]
 branch = "wgpu5"
 git = "https://github.com/kvark/spirv_cross"
diff --git a/Cargo.lock b/Cargo.lock
index 7e17939fad48b..8519d3d0e95a6 100644
--- a/Cargo.lock
+++ b/Cargo.lock
@@ -203,8 +203,7 @@ dependencies = [
 [[package]]
 name = "authenticator"
 version = "0.3.1"
-source = "registry+https://github.com/rust-lang/crates.io-index"
-checksum = "08cee7a0952628fde958e149507c2bb321ab4fccfafd225da0b20adc956ef88a"
+source = "git+https://github.com/makotokato/authenticator-rs?rev=eed8919d50559f4959e2d7d2af7b4d48869b5366#eed8919d50559f4959e2d7d2af7b4d48869b5366"
 dependencies = [
  "bitflags",
  "core-foundation",
diff --git a/third_party/rust/authenticator/.cargo-checksum.json b/third_party/rust/authenticator/.cargo-checksum.json
index ce451ad09df4f..9791345c9de54 100644
--- a/third_party/rust/authenticator/.cargo-checksum.json
+++ b/third_party/rust/authenticator/.cargo-checksum.json
@@ -1 +1 @@
-{"files":{"Cargo.lock":"abaed4932db2206e5fdb7cb73a8c100f6c91fc84a8f33e8763677040ae8ea9bf","Cargo.toml":"9b56d5495021e7cd8ab7e019cceda45e906a2a3629a68e9019c6e5cb682dbc43","Cross.toml":"8d132da818d48492aa9f4b78a348f0df3adfae45d988d42ebd6be8a5adadb6c3","LICENSE":"e866c8f5864d4cacfe403820e722e9dc03fe3c7565efa5e4dad9051d827bb92a","README.md":"c87d9c7cc44f1dd4ef861a3a9f8cd2eb68aedd3814768871f5fb63c2070806cd","build.rs":"bc308b771ae9741d775370e3efe45e9cca166fd1d0335f4214b00497042ccc55","examples/main.rs":"d899646fa396776d0bb66efb86099ffb195566ecdb6fc4c1765ae3d54d696a8d","rustfmt.toml":"ceb6615363d6fff16426eb56f5727f98a7f7ed459ba9af735b1d8b672e2c3b9b","src/authenticatorservice.rs":"9fc5bcdd1e4f32e58ae920f96f40619a870b0a1b8d05db650803b2402a37fbf9","src/capi.rs":"1d3145ce81293bec697b0d385357fb1b0b495b0c356e2da5e6f15d028d328c70","src/consts.rs":"3dbcdfced6241822062e1aa2e6c8628af5f539ea18ee41edab51a3d33ebb77c6","src/errors.rs":"de89e57435ed1f9ff10f1f2d997a5b29d61cb215551e0ab40861a08ca52d1447","src/freebsd/device.rs":"595df4b3f66b90dd73f8df67e1a2ba9a20c0b5fd893afbadbec564aa34f89981","src/freebsd/mod.rs":"42dcb57fbeb00140003a8ad39acac9b547062b8f281a3fa5deb5f92a6169dde6","src/freebsd/monitor.rs":"c10b154632fbedc3dca27197f7fc890c3d50ac1744b927e9f1e44a9e8a13506e","src/freebsd/transaction.rs":"bfb92dcf2edeb5d620a019907fff1025eb36ef322055e78649a3055b074fa851","src/freebsd/uhid.rs":"84f564d337637c1cd107ccc536b8fce2230628e144e4031e8db4d7163c9c0cb3","src/hidproto.rs":"362fc8e24b94ba431aad5ee0002f5a3364badd937c706c0ae119a5a7a2abc7c2","src/lib.rs":"12f62285a3d33347f95236b71341462a76ea1ded67651fc96ba25d7bd1dd8298","src/linux/device.rs":"d27c5f877cf96b97668579ac5db0f2685f7c969e7a5d0ddc68043eb16bfcddb8","src/linux/hidraw.rs":"ed55caa40fd518d67bb67d5af08f9adcab34f89e0ca591142d45b87f172926dd","src/linux/hidwrapper.h":"72785db3a9b27ea72b6cf13a958fee032af54304522d002f56322473978a20f9","src/linux/hidwrapper.rs":"4be65676cf3220929700bf4906938dcbd1538ba53d40c60b08f9ba8890c910f6","src/linux/ioctl_aarch64le.rs":"2d8b265cd39a9f46816f83d5a5df0701c13eb842bc609325bad42ce50add3bf0","src/linux/ioctl_armle.rs":"2d8b265cd39a9f46816f83d5a5df0701c13eb842bc609325bad42ce50add3bf0","src/linux/ioctl_mips64le.rs":"fbda309934ad8bda689cd4fb5c0ca696fe26dedb493fe9d5a5322c3047d474fd","src/linux/ioctl_mipsbe.rs":"fbda309934ad8bda689cd4fb5c0ca696fe26dedb493fe9d5a5322c3047d474fd","src/linux/ioctl_mipsle.rs":"fbda309934ad8bda689cd4fb5c0ca696fe26dedb493fe9d5a5322c3047d474fd","src/linux/ioctl_powerpc64be.rs":"fbda309934ad8bda689cd4fb5c0ca696fe26dedb493fe9d5a5322c3047d474fd","src/linux/ioctl_powerpc64le.rs":"fbda309934ad8bda689cd4fb5c0ca696fe26dedb493fe9d5a5322c3047d474fd","src/linux/ioctl_powerpcbe.rs":"fbda309934ad8bda689cd4fb5c0ca696fe26dedb493fe9d5a5322c3047d474fd","src/linux/ioctl_s390xbe.rs":"2d8b265cd39a9f46816f83d5a5df0701c13eb842bc609325bad42ce50add3bf0","src/linux/ioctl_x86.rs":"2d8b265cd39a9f46816f83d5a5df0701c13eb842bc609325bad42ce50add3bf0","src/linux/ioctl_x86_64.rs":"2d8b265cd39a9f46816f83d5a5df0701c13eb842bc609325bad42ce50add3bf0","src/linux/mod.rs":"446e435126d2a58f167f648dd95cba28e8ac9c17f1f799e1eaeab80ea800fc57","src/linux/monitor.rs":"9ef4e22fdcf005dd5201b42595d958ea462998c75dbfc68c8a403e7be64328e4","src/linux/transaction.rs":"bfb92dcf2edeb5d620a019907fff1025eb36ef322055e78649a3055b074fa851","src/macos/device.rs":"cc97b773254a89526164987e4b8e4181910fc3decb32acf51ca86c596ad0147b","src/macos/iokit.rs":"7dc4e7bbf8e42e2fcde0cee8e48d14d6234a5a910bd5d3c4e966d8ba6b73992f","src/macos/mod.rs":"333e561554fc901d4f6092f6e4c85823e2b0c4ff31c9188d0e6d542b71a0a07c","src/macos/monitor.rs":"d059861b4739c9272fa305b6dd91ebeb08530bd0e70a013dd999565d6f06fb30","src/macos/transaction.rs":"935b4bc79b0e50a984604a1ada96a7ef723cc283b7d33ca07f3150b1752b99f7","src/manager.rs":"5a4cdc26b9fde20e1a3dc2389f15d38d9153109bfee5119c092fbfdbd19bad8d","src/netbsd/device.rs":"3a99a989a7a8411ddb9893c371644076662a3b488d40b436601c27fd92fdf159","src/netbsd/fd.rs":"260f1a8ae04896c0eb35ab0914e11ca9291e7317a086c94328aa219c0e1fc1d2","src/netbsd/mod.rs":"b1c52aa29537330cebe67427062d6c94871cab2a9b0c04b2305d686f07e88fd5","src/netbsd/monitor.rs":"dfd68e026c52271b68a3a9263837c793127e9d54ed19b748ef6d13ab4c44e09a","src/netbsd/transaction.rs":"9334a832a57e717a981c13c364ed4ee80ce9798460fc6c8954723d2fcf20585a","src/netbsd/uhid.rs":"154a4587767f151e3f846cc0b79f615d5137de67afed84f19176f27ac9097908","src/openbsd/device.rs":"ae1c8de90bb515a12d571372a30322fadb5122bc69ab71caf154452caa8a644f","src/openbsd/mod.rs":"514274d414042ff84b3667a41a736e78581e22fda87ccc97c2bc05617e381a30","src/openbsd/monitor.rs":"5eb071dd3719ea305eac21ec20596463f63790f8cd1f908a59e3f9cb0b71b5ad","src/openbsd/transaction.rs":"2380c9430f4c95a1fefaaab729d8ece0d149674708d705a71dd5d2513d9e1a4c","src/statecallback.rs":"6b16f97176db1ae3fc3851fe8394e4ffc324bc6fe59313845ac3a88132fd52f1","src/statemachine.rs":"27e2655411ebc1077c200f0aa2ba429ca656fc7dd6f90e08b51492b59ec72e61","src/stub/device.rs":"5e378147e113e20160a45d395b717bd3deecb327247c24b6735035f7d50861b7","src/stub/mod.rs":"6a7fec504a52d403b0241b18cd8b95088a31807571f4c0a67e4055afc74f4453","src/stub/transaction.rs":"4a2ccb2d72070a8bc61442254e063278c68212d5565ba5bfe4d47cacebf5bd1c","src/u2fhid-capi.h":"10f2658df774bb7f7f197a9f217b9e20d67b232b60a554e8ee3c3f71480ea1f6","src/u2fprotocol.rs":"72120773a948ffd667b5976c26ae27a4327769d97b0eef7a3b1e6b2b4bbb46a9","src/u2ftypes.rs":"a02d2c29790c5edfec9af320b1d4bcb93be0bbf02b881fa5aa403cfb687a25ae","src/util.rs":"d2042b2db4864f2b1192606c3251709361de7fb7521e1519190ef26a77de8e64","src/virtualdevices/mod.rs":"2c7df7691d5c150757304241351612aed4260d65b70ab0f483edbc1a5cfb5674","src/virtualdevices/software_u2f.rs":"1b86b94c6eadec6a22dffdd2b003c5324247c6412eeddb28a6094feb1c523f8e","src/virtualdevices/webdriver/mod.rs":"4a36e6dfa9f45f941d863b4039bfbcfa8eaca660bd6ed78aeb1a2962db64be5a","src/virtualdevices/webdriver/testtoken.rs":"7146e02f1a5dad2c8827dd11c12ee408c0e42a0706ac65f139998feffd42570f","src/virtualdevices/webdriver/virtualmanager.rs":"a55a28995c81b5affb0a74207b6dd556d272086a554676df2e675fe991d730a9","src/virtualdevices/webdriver/web_api.rs":"27206ee09c83fe25b34cad62174e42383defd8c8a5e917d30691412aacdae08f","src/windows/device.rs":"bc3f9587677c185a624c0aae7537baf9f780484ab8337929db994800b9064ba9","src/windows/mod.rs":"218e7f2fe91ecb390c12bba5a5ffdad2c1f0b22861c937f4d386262e5b3dd617","src/windows/monitor.rs":"3804dc67de46a1a6b7925c83e0df95d94ddfa1aa53a88fc845f4ff26aede57f8","src/windows/transaction.rs":"ee639f28b2dcdb7e00c922d8762fe6aa33def8c7aaeb46ec93e3a772407a9d86","src/windows/winapi.rs":"de92afb17df26216161138f18eb3b9162f3fb2cdeb74aa78173afe804ba02e00","testing/cross/powerpc64le-unknown-linux-gnu.Dockerfile":"d7463ff4376e3e0ca3fed879fab4aa975c4c0a3e7924c5b88aef9381a5d013de","testing/cross/x86_64-unknown-linux-gnu.Dockerfile":"11c79c04b07a171b0c9b63ef75fa75f33263ce76e3c1eda0879a3e723ebd0c24","testing/run_cross.sh":"cc2a7e0359f210eba2e7121f81eb8ab0125cea6e0d0f2698177b0fe2ad0c33d8","webdriver-tools/requirements.txt":"8236aa3dedad886f213c9b778fec80b037212d30e640b458984110211d546005","webdriver-tools/webdriver-driver.py":"82327c26ba271d1689acc87b612ab8436cb5475f0a3c0dba7baa06e7f6f5e19c"},"package":"08cee7a0952628fde958e149507c2bb321ab4fccfafd225da0b20adc956ef88a"}
\ No newline at end of file
+{"files":{".clippy.toml":"86011295a6e2cea043b8002238f9c96b39f17aa8241aa079f44bb6e71eb62421",".flake8":"04f55f4a3c02b50dfa568ce4f7c6a47a9374b6483256811f8be702d1382576cd",".pre-commit-config.yaml":"b7920a17d5a378c7702f9c39bf5156bb8c4ea15d8691217e0a5a8e8f571b4cf7",".travis.yml":"883be088379477e7fa6f3d06b1c8d59dc41da61b6c15d2675c62113341e7b2d5","Cargo.toml":"e7334212220a6d8ca01996888275cc0d11d098e36db1bf4c5b7429051897bf3f","Cross.toml":"8d132da818d48492aa9f4b78a348f0df3adfae45d988d42ebd6be8a5adadb6c3","LICENSE":"e866c8f5864d4cacfe403820e722e9dc03fe3c7565efa5e4dad9051d827bb92a","README.md":"c87d9c7cc44f1dd4ef861a3a9f8cd2eb68aedd3814768871f5fb63c2070806cd","build.rs":"a459ee1ace052f9692817b15c702cb6e5a6dac7c7dfe74fa075662dbcf808dbe","examples/main.rs":"d899646fa396776d0bb66efb86099ffb195566ecdb6fc4c1765ae3d54d696a8d","rustfmt.toml":"ceb6615363d6fff16426eb56f5727f98a7f7ed459ba9af735b1d8b672e2c3b9b","src/authenticatorservice.rs":"9fc5bcdd1e4f32e58ae920f96f40619a870b0a1b8d05db650803b2402a37fbf9","src/capi.rs":"1d3145ce81293bec697b0d385357fb1b0b495b0c356e2da5e6f15d028d328c70","src/consts.rs":"3dbcdfced6241822062e1aa2e6c8628af5f539ea18ee41edab51a3d33ebb77c6","src/errors.rs":"de89e57435ed1f9ff10f1f2d997a5b29d61cb215551e0ab40861a08ca52d1447","src/freebsd/device.rs":"595df4b3f66b90dd73f8df67e1a2ba9a20c0b5fd893afbadbec564aa34f89981","src/freebsd/mod.rs":"42dcb57fbeb00140003a8ad39acac9b547062b8f281a3fa5deb5f92a6169dde6","src/freebsd/monitor.rs":"c10b154632fbedc3dca27197f7fc890c3d50ac1744b927e9f1e44a9e8a13506e","src/freebsd/transaction.rs":"bfb92dcf2edeb5d620a019907fff1025eb36ef322055e78649a3055b074fa851","src/freebsd/uhid.rs":"84f564d337637c1cd107ccc536b8fce2230628e144e4031e8db4d7163c9c0cb3","src/hidproto.rs":"362fc8e24b94ba431aad5ee0002f5a3364badd937c706c0ae119a5a7a2abc7c2","src/lib.rs":"12f62285a3d33347f95236b71341462a76ea1ded67651fc96ba25d7bd1dd8298","src/linux/device.rs":"d27c5f877cf96b97668579ac5db0f2685f7c969e7a5d0ddc68043eb16bfcddb8","src/linux/hidraw.rs":"ed55caa40fd518d67bb67d5af08f9adcab34f89e0ca591142d45b87f172926dd","src/linux/hidwrapper.h":"72785db3a9b27ea72b6cf13a958fee032af54304522d002f56322473978a20f9","src/linux/hidwrapper.rs":"753c7459dbb73befdd186b6269ac33f7a4537b4c935928f50f2b2131756e787d","src/linux/ioctl_aarch64le.rs":"2d8b265cd39a9f46816f83d5a5df0701c13eb842bc609325bad42ce50add3bf0","src/linux/ioctl_armle.rs":"2d8b265cd39a9f46816f83d5a5df0701c13eb842bc609325bad42ce50add3bf0","src/linux/ioctl_mips64le.rs":"fbda309934ad8bda689cd4fb5c0ca696fe26dedb493fe9d5a5322c3047d474fd","src/linux/ioctl_mipsbe.rs":"fbda309934ad8bda689cd4fb5c0ca696fe26dedb493fe9d5a5322c3047d474fd","src/linux/ioctl_mipsle.rs":"fbda309934ad8bda689cd4fb5c0ca696fe26dedb493fe9d5a5322c3047d474fd","src/linux/ioctl_powerpc64be.rs":"fbda309934ad8bda689cd4fb5c0ca696fe26dedb493fe9d5a5322c3047d474fd","src/linux/ioctl_powerpc64le.rs":"fbda309934ad8bda689cd4fb5c0ca696fe26dedb493fe9d5a5322c3047d474fd","src/linux/ioctl_powerpcbe.rs":"fbda309934ad8bda689cd4fb5c0ca696fe26dedb493fe9d5a5322c3047d474fd","src/linux/ioctl_riscv64.rs":"2d8b265cd39a9f46816f83d5a5df0701c13eb842bc609325bad42ce50add3bf0","src/linux/ioctl_s390xbe.rs":"2d8b265cd39a9f46816f83d5a5df0701c13eb842bc609325bad42ce50add3bf0","src/linux/ioctl_x86.rs":"2d8b265cd39a9f46816f83d5a5df0701c13eb842bc609325bad42ce50add3bf0","src/linux/ioctl_x86_64.rs":"2d8b265cd39a9f46816f83d5a5df0701c13eb842bc609325bad42ce50add3bf0","src/linux/mod.rs":"446e435126d2a58f167f648dd95cba28e8ac9c17f1f799e1eaeab80ea800fc57","src/linux/monitor.rs":"9ef4e22fdcf005dd5201b42595d958ea462998c75dbfc68c8a403e7be64328e4","src/linux/transaction.rs":"bfb92dcf2edeb5d620a019907fff1025eb36ef322055e78649a3055b074fa851","src/macos/device.rs":"cc97b773254a89526164987e4b8e4181910fc3decb32acf51ca86c596ad0147b","src/macos/iokit.rs":"7dc4e7bbf8e42e2fcde0cee8e48d14d6234a5a910bd5d3c4e966d8ba6b73992f","src/macos/mod.rs":"333e561554fc901d4f6092f6e4c85823e2b0c4ff31c9188d0e6d542b71a0a07c","src/macos/monitor.rs":"d059861b4739c9272fa305b6dd91ebeb08530bd0e70a013dd999565d6f06fb30","src/macos/transaction.rs":"935b4bc79b0e50a984604a1ada96a7ef723cc283b7d33ca07f3150b1752b99f7","src/manager.rs":"5a4cdc26b9fde20e1a3dc2389f15d38d9153109bfee5119c092fbfdbd19bad8d","src/netbsd/device.rs":"3a99a989a7a8411ddb9893c371644076662a3b488d40b436601c27fd92fdf159","src/netbsd/fd.rs":"260f1a8ae04896c0eb35ab0914e11ca9291e7317a086c94328aa219c0e1fc1d2","src/netbsd/mod.rs":"b1c52aa29537330cebe67427062d6c94871cab2a9b0c04b2305d686f07e88fd5","src/netbsd/monitor.rs":"dfd68e026c52271b68a3a9263837c793127e9d54ed19b748ef6d13ab4c44e09a","src/netbsd/transaction.rs":"9334a832a57e717a981c13c364ed4ee80ce9798460fc6c8954723d2fcf20585a","src/netbsd/uhid.rs":"154a4587767f151e3f846cc0b79f615d5137de67afed84f19176f27ac9097908","src/openbsd/device.rs":"ae1c8de90bb515a12d571372a30322fadb5122bc69ab71caf154452caa8a644f","src/openbsd/mod.rs":"514274d414042ff84b3667a41a736e78581e22fda87ccc97c2bc05617e381a30","src/openbsd/monitor.rs":"5eb071dd3719ea305eac21ec20596463f63790f8cd1f908a59e3f9cb0b71b5ad","src/openbsd/transaction.rs":"2380c9430f4c95a1fefaaab729d8ece0d149674708d705a71dd5d2513d9e1a4c","src/statecallback.rs":"6b16f97176db1ae3fc3851fe8394e4ffc324bc6fe59313845ac3a88132fd52f1","src/statemachine.rs":"27e2655411ebc1077c200f0aa2ba429ca656fc7dd6f90e08b51492b59ec72e61","src/stub/device.rs":"5e378147e113e20160a45d395b717bd3deecb327247c24b6735035f7d50861b7","src/stub/mod.rs":"6a7fec504a52d403b0241b18cd8b95088a31807571f4c0a67e4055afc74f4453","src/stub/transaction.rs":"4a2ccb2d72070a8bc61442254e063278c68212d5565ba5bfe4d47cacebf5bd1c","src/u2fhid-capi.h":"10f2658df774bb7f7f197a9f217b9e20d67b232b60a554e8ee3c3f71480ea1f6","src/u2fprotocol.rs":"72120773a948ffd667b5976c26ae27a4327769d97b0eef7a3b1e6b2b4bbb46a9","src/u2ftypes.rs":"a02d2c29790c5edfec9af320b1d4bcb93be0bbf02b881fa5aa403cfb687a25ae","src/util.rs":"d2042b2db4864f2b1192606c3251709361de7fb7521e1519190ef26a77de8e64","src/virtualdevices/mod.rs":"2c7df7691d5c150757304241351612aed4260d65b70ab0f483edbc1a5cfb5674","src/virtualdevices/software_u2f.rs":"1b86b94c6eadec6a22dffdd2b003c5324247c6412eeddb28a6094feb1c523f8e","src/virtualdevices/webdriver/mod.rs":"4a36e6dfa9f45f941d863b4039bfbcfa8eaca660bd6ed78aeb1a2962db64be5a","src/virtualdevices/webdriver/testtoken.rs":"7146e02f1a5dad2c8827dd11c12ee408c0e42a0706ac65f139998feffd42570f","src/virtualdevices/webdriver/virtualmanager.rs":"a55a28995c81b5affb0a74207b6dd556d272086a554676df2e675fe991d730a9","src/virtualdevices/webdriver/web_api.rs":"27206ee09c83fe25b34cad62174e42383defd8c8a5e917d30691412aacdae08f","src/windows/device.rs":"bc3f9587677c185a624c0aae7537baf9f780484ab8337929db994800b9064ba9","src/windows/mod.rs":"218e7f2fe91ecb390c12bba5a5ffdad2c1f0b22861c937f4d386262e5b3dd617","src/windows/monitor.rs":"3804dc67de46a1a6b7925c83e0df95d94ddfa1aa53a88fc845f4ff26aede57f8","src/windows/transaction.rs":"ee639f28b2dcdb7e00c922d8762fe6aa33def8c7aaeb46ec93e3a772407a9d86","src/windows/winapi.rs":"de92afb17df26216161138f18eb3b9162f3fb2cdeb74aa78173afe804ba02e00","testing/cross/powerpc64le-unknown-linux-gnu.Dockerfile":"d7463ff4376e3e0ca3fed879fab4aa975c4c0a3e7924c5b88aef9381a5d013de","testing/cross/x86_64-unknown-linux-gnu.Dockerfile":"11c79c04b07a171b0c9b63ef75fa75f33263ce76e3c1eda0879a3e723ebd0c24","testing/run_cross.sh":"cc2a7e0359f210eba2e7121f81eb8ab0125cea6e0d0f2698177b0fe2ad0c33d8","webdriver-tools/requirements.txt":"8236aa3dedad886f213c9b778fec80b037212d30e640b458984110211d546005","webdriver-tools/webdriver-driver.py":"82327c26ba271d1689acc87b612ab8436cb5475f0a3c0dba7baa06e7f6f5e19c"},"package":null}
\ No newline at end of file
diff --git a/third_party/rust/authenticator/.clippy.toml b/third_party/rust/authenticator/.clippy.toml
new file mode 100644
index 0000000000000..844d0757e91f4
--- /dev/null
+++ b/third_party/rust/authenticator/.clippy.toml
@@ -0,0 +1,2 @@
+type-complexity-threshold = 384
+too-many-arguments-threshold = 8
diff --git a/third_party/rust/authenticator/.flake8 b/third_party/rust/authenticator/.flake8
new file mode 100644
index 0000000000000..5a725c9b4ce65
--- /dev/null
+++ b/third_party/rust/authenticator/.flake8
@@ -0,0 +1,4 @@
+[flake8]
+# See http://pep8.readthedocs.io/en/latest/intro.html#configuration
+ignore = E121, E123, E126, E129, E133, E203, E226, E241, E242, E704, W503, E402, E741
+max-line-length = 99
diff --git a/third_party/rust/authenticator/.pre-commit-config.yaml b/third_party/rust/authenticator/.pre-commit-config.yaml
new file mode 100644
index 0000000000000..e0ceb8ea5473c
--- /dev/null
+++ b/third_party/rust/authenticator/.pre-commit-config.yaml
@@ -0,0 +1,42 @@
+- repo: git://github.com/pre-commit/pre-commit-hooks
+  rev: HEAD
+  hooks:
+    - id: flake8
+    - id: check-ast
+    - id: detect-private-key
+    - id: detect-aws-credentials
+    - id: check-merge-conflict
+    - id: end-of-file-fixer
+    - id: requirements-txt-fixer
+    - id: trailing-whitespace
+- repo: local
+  hooks:
+    - id: rustfmt
+      name: Check rustfmt
+      language: system
+      entry: cargo fmt -- --check
+      pass_filenames: false
+      files: '.rs$'
+- repo: local
+  hooks:
+    - id: tests
+      name: Run tests
+      language: system
+      entry: cargo test --all-targets --all-features
+      pass_filenames: false
+      files: '.rs$'
+- repo: local
+  hooks:
+    - id: clippy
+      name: Check clippy
+      language: system
+      entry: cargo clippy --all-targets -- -A renamed_and_removed_lints -A clippy::new-ret-no-self -D warnings
+      pass_filenames: false
+      files: '.rs$'
+- repo: local
+  hooks:
+    - id: black
+      name: Check black
+      language: system
+      entry: black
+      files: '.py$'
diff --git a/third_party/rust/authenticator/.travis.yml b/third_party/rust/authenticator/.travis.yml
new file mode 100644
index 0000000000000..70ea5c5581af2
--- /dev/null
+++ b/third_party/rust/authenticator/.travis.yml
@@ -0,0 +1,42 @@
+os:
+  - linux
+  - windows
+
+language: rust
+rust:
+  - stable
+  - nightly
+cache: cargo
+
+jobs:
+  allow_failures:
+    - rust: nightly
+
+addons:
+  apt:
+    packages:
+      - build-essential
+      - libudev-dev
+
+install:
+  - rustup component add rustfmt
+  - rustup component add clippy
+
+script:
+- |
+  if [ "$TRAVIS_RUST_VERSION" == "nightly" ] && [ "$TRAVIS_OS_NAME" == "linux" ] ; then
+    export ASAN_OPTIONS="detect_odr_violation=1:leak_check_at_exit=0:detect_leaks=0"
+    export RUSTFLAGS="-Z sanitizer=address"
+  fi
+- |
+  if [ "$TRAVIS_RUST_VERSION" == "stable" ] && [ "$TRAVIS_OS_NAME" == "linux" ] ; then
+    echo "Running rustfmt"
+    cargo fmt --all -- --check
+    echo "Running clippy"
+    cargo clippy --all-targets --all-features -- -A renamed_and_removed_lints -A clippy::new-ret-no-self -D warnings
+
+    rustup install nightly
+    cargo install cargo-fuzz
+    cargo +nightly fuzz build
+  fi
+- cargo test --all-targets --all-features
diff --git a/third_party/rust/authenticator/Cargo.lock b/third_party/rust/authenticator/Cargo.lock
deleted file mode 100644
index 9f284b468deaa..0000000000000
--- a/third_party/rust/authenticator/Cargo.lock
+++ /dev/null
@@ -1,1603 +0,0 @@
-# This file is automatically @generated by Cargo.
-# It is not intended for manual editing.
-[[package]]
-name = "aho-corasick"
-version = "0.7.13"
-source = "registry+https://github.com/rust-lang/crates.io-index"
-dependencies = [
- "memchr 2.3.3 (registry+https://github.com/rust-lang/crates.io-index)",
-]
-
-[[package]]
-name = "ansi_term"
-version = "0.11.0"
-source = "registry+https://github.com/rust-lang/crates.io-index"
-dependencies = [
- "winapi 0.3.9 (registry+https://github.com/rust-lang/crates.io-index)",
-]
-
-[[package]]
-name = "assert_matches"
-version = "1.3.0"
-source = "registry+https://github.com/rust-lang/crates.io-index"
-
-[[package]]
-name = "atty"
-version = "0.2.14"
-source = "registry+https://github.com/rust-lang/crates.io-index"
-dependencies = [
- "hermit-abi 0.1.15 (registry+https://github.com/rust-lang/crates.io-index)",
- "libc 0.2.73 (registry+https://github.com/rust-lang/crates.io-index)",
- "winapi 0.3.9 (registry+https://github.com/rust-lang/crates.io-index)",
-]
-
-[[package]]
-name = "authenticator"
-version = "0.3.1"
-dependencies = [
- "assert_matches 1.3.0 (registry+https://github.com/rust-lang/crates.io-index)",
- "base64 0.10.1 (registry+https://github.com/rust-lang/crates.io-index)",
- "bindgen 0.51.1 (registry+https://github.com/rust-lang/crates.io-index)",
- "bitflags 1.2.1 (registry+https://github.com/rust-lang/crates.io-index)",
- "bytes 0.5.6 (registry+https://github.com/rust-lang/crates.io-index)",
- "core-foundation 0.9.0 (registry+https://github.com/rust-lang/crates.io-index)",
- "devd-rs 0.3.1 (registry+https://github.com/rust-lang/crates.io-index)",
- "env_logger 0.6.2 (registry+https://github.com/rust-lang/crates.io-index)",
- "getopts 0.2.21 (registry+https://github.com/rust-lang/crates.io-index)",
- "libc 0.2.73 (registry+https://github.com/rust-lang/crates.io-index)",
- "libudev 0.2.0 (registry+https://github.com/rust-lang/crates.io-index)",
- "log 0.4.11 (registry+https://github.com/rust-lang/crates.io-index)",
- "rand 0.7.3 (registry+https://github.com/rust-lang/crates.io-index)",
- "runloop 0.1.0 (registry+https://github.com/rust-lang/crates.io-index)",
- "serde 1.0.116 (registry+https://github.com/rust-lang/crates.io-index)",
- "serde_json 1.0.57 (registry+https://github.com/rust-lang/crates.io-index)",
- "sha2 0.8.2 (registry+https://github.com/rust-lang/crates.io-index)",
- "tokio 0.2.22 (registry+https://github.com/rust-lang/crates.io-index)",
- "warp 0.2.5 (registry+https://github.com/rust-lang/crates.io-index)",
- "winapi 0.3.9 (registry+https://github.com/rust-lang/crates.io-index)",
-]
-
-[[package]]
-name = "autocfg"
-version = "0.1.7"
-source = "registry+https://github.com/rust-lang/crates.io-index"
-
-[[package]]
-name = "autocfg"
-version = "1.0.1"
-source = "registry+https://github.com/rust-lang/crates.io-index"
-
-[[package]]
-name = "base64"
-version = "0.10.1"
-source = "registry+https://github.com/rust-lang/crates.io-index"
-dependencies = [
- "byteorder 1.3.4 (registry+https://github.com/rust-lang/crates.io-index)",
-]
-
-[[package]]
-name = "base64"
-version = "0.12.3"
-source = "registry+https://github.com/rust-lang/crates.io-index"
-
-[[package]]
-name = "bindgen"
-version = "0.51.1"
-source = "registry+https://github.com/rust-lang/crates.io-index"
-dependencies = [
- "bitflags 1.2.1 (registry+https://github.com/rust-lang/crates.io-index)",
- "cexpr 0.3.6 (registry+https://github.com/rust-lang/crates.io-index)",
- "cfg-if 0.1.10 (registry+https://github.com/rust-lang/crates.io-index)",
- "clang-sys 0.28.1 (registry+https://github.com/rust-lang/crates.io-index)",
- "clap 2.33.1 (registry+https://github.com/rust-lang/crates.io-index)",
- "env_logger 0.6.2 (registry+https://github.com/rust-lang/crates.io-index)",
- "lazy_static 1.4.0 (registry+https://github.com/rust-lang/crates.io-index)",
- "log 0.4.11 (registry+https://github.com/rust-lang/crates.io-index)",
- "peeking_take_while 0.1.2 (registry+https://github.com/rust-lang/crates.io-index)",
- "proc-macro2 1.0.19 (registry+https://github.com/rust-lang/crates.io-index)",
- "quote 1.0.7 (registry+https://github.com/rust-lang/crates.io-index)",
- "regex 1.3.9 (registry+https://github.com/rust-lang/crates.io-index)",
- "rustc-hash 1.1.0 (registry+https://github.com/rust-lang/crates.io-index)",
- "shlex 0.1.1 (registry+https://github.com/rust-lang/crates.io-index)",
- "which 3.1.1 (registry+https://github.com/rust-lang/crates.io-index)",
-]
-
-[[package]]
-name = "bitflags"
-version = "1.2.1"
-source = "registry+https://github.com/rust-lang/crates.io-index"
-
-[[package]]
-name = "block-buffer"
-version = "0.7.3"
-source = "registry+https://github.com/rust-lang/crates.io-index"
-dependencies = [
- "block-padding 0.1.5 (registry+https://github.com/rust-lang/crates.io-index)",
- "byte-tools 0.3.1 (registry+https://github.com/rust-lang/crates.io-index)",
- "byteorder 1.3.4 (registry+https://github.com/rust-lang/crates.io-index)",
- "generic-array 0.12.3 (registry+https://github.com/rust-lang/crates.io-index)",
-]
-
-[[package]]
-name = "block-buffer"
-version = "0.9.0"
-source = "registry+https://github.com/rust-lang/crates.io-index"
-dependencies = [
- "generic-array 0.14.4 (registry+https://github.com/rust-lang/crates.io-index)",
-]
-
-[[package]]
-name = "block-padding"
-version = "0.1.5"
-source = "registry+https://github.com/rust-lang/crates.io-index"
-dependencies = [
- "byte-tools 0.3.1 (registry+https://github.com/rust-lang/crates.io-index)",
-]
-
-[[package]]
-name = "buf_redux"
-version = "0.8.4"
-source = "registry+https://github.com/rust-lang/crates.io-index"
-dependencies = [
- "memchr 2.3.3 (registry+https://github.com/rust-lang/crates.io-index)",
- "safemem 0.3.3 (registry+https://github.com/rust-lang/crates.io-index)",
-]
-
-[[package]]
-name = "byte-tools"
-version = "0.3.1"
-source = "registry+https://github.com/rust-lang/crates.io-index"
-
-[[package]]
-name = "byteorder"
-version = "1.3.4"
-source = "registry+https://github.com/rust-lang/crates.io-index"
-
-[[package]]
-name = "bytes"
-version = "0.5.6"
-source = "registry+https://github.com/rust-lang/crates.io-index"
-dependencies = [
- "serde 1.0.116 (registry+https://github.com/rust-lang/crates.io-index)",
-]
-
-[[package]]
-name = "cc"
-version = "1.0.58"
-source = "registry+https://github.com/rust-lang/crates.io-index"
-
-[[package]]
-name = "cexpr"
-version = "0.3.6"
-source = "registry+https://github.com/rust-lang/crates.io-index"
-dependencies = [
- "nom 4.2.3 (registry+https://github.com/rust-lang/crates.io-index)",
-]
-
-[[package]]
-name = "cfg-if"
-version = "0.1.10"
-source = "registry+https://github.com/rust-lang/crates.io-index"
-
-[[package]]
-name = "clang-sys"
-version = "0.28.1"
-source = "registry+https://github.com/rust-lang/crates.io-index"
-dependencies = [
- "glob 0.3.0 (registry+https://github.com/rust-lang/crates.io-index)",
- "libc 0.2.73 (registry+https://github.com/rust-lang/crates.io-index)",
- "libloading 0.5.2 (registry+https://github.com/rust-lang/crates.io-index)",
-]
-
-[[package]]
-name = "clap"
-version = "2.33.1"
-source = "registry+https://github.com/rust-lang/crates.io-index"
-dependencies = [
- "ansi_term 0.11.0 (registry+https://github.com/rust-lang/crates.io-index)",
- "atty 0.2.14 (registry+https://github.com/rust-lang/crates.io-index)",
- "bitflags 1.2.1 (registry+https://github.com/rust-lang/crates.io-index)",
- "strsim 0.8.0 (registry+https://github.com/rust-lang/crates.io-index)",
- "textwrap 0.11.0 (registry+https://github.com/rust-lang/crates.io-index)",
- "unicode-width 0.1.8 (registry+https://github.com/rust-lang/crates.io-index)",
- "vec_map 0.8.2 (registry+https://github.com/rust-lang/crates.io-index)",
-]
-
-[[package]]
-name = "cloudabi"
-version = "0.0.3"
-source = "registry+https://github.com/rust-lang/crates.io-index"
-dependencies = [
- "bitflags 1.2.1 (registry+https://github.com/rust-lang/crates.io-index)",
-]
-
-[[package]]
-name = "core-foundation"
-version = "0.9.0"
-source = "registry+https://github.com/rust-lang/crates.io-index"
-dependencies = [
- "core-foundation-sys 0.8.0 (registry+https://github.com/rust-lang/crates.io-index)",
- "libc 0.2.73 (registry+https://github.com/rust-lang/crates.io-index)",
-]
-
-[[package]]
-name = "core-foundation-sys"
-version = "0.8.0"
-source = "registry+https://github.com/rust-lang/crates.io-index"
-
-[[package]]
-name = "cpuid-bool"
-version = "0.1.2"
-source = "registry+https://github.com/rust-lang/crates.io-index"
-
-[[package]]
-name = "devd-rs"
-version = "0.3.1"
-source = "registry+https://github.com/rust-lang/crates.io-index"
-dependencies = [
- "libc 0.2.73 (registry+https://github.com/rust-lang/crates.io-index)",
- "nom 5.1.2 (registry+https://github.com/rust-lang/crates.io-index)",
-]
-
-[[package]]
-name = "digest"
-version = "0.8.1"
-source = "registry+https://github.com/rust-lang/crates.io-index"
-dependencies = [
- "generic-array 0.12.3 (registry+https://github.com/rust-lang/crates.io-index)",
-]
-
-[[package]]
-name = "digest"
-version = "0.9.0"
-source = "registry+https://github.com/rust-lang/crates.io-index"
-dependencies = [
- "generic-array 0.14.4 (registry+https://github.com/rust-lang/crates.io-index)",
-]
-
-[[package]]
-name = "dtoa"
-version = "0.4.6"
-source = "registry+https://github.com/rust-lang/crates.io-index"
-
-[[package]]
-name = "env_logger"
-version = "0.6.2"
-source = "registry+https://github.com/rust-lang/crates.io-index"
-dependencies = [
- "atty 0.2.14 (registry+https://github.com/rust-lang/crates.io-index)",
- "humantime 1.3.0 (registry+https://github.com/rust-lang/crates.io-index)",
- "log 0.4.11 (registry+https://github.com/rust-lang/crates.io-index)",
- "regex 1.3.9 (registry+https://github.com/rust-lang/crates.io-index)",
- "termcolor 1.1.0 (registry+https://github.com/rust-lang/crates.io-index)",
-]
-
-[[package]]
-name = "fake-simd"
-version = "0.1.2"
-source = "registry+https://github.com/rust-lang/crates.io-index"
-
-[[package]]
-name = "fnv"
-version = "1.0.7"
-source = "registry+https://github.com/rust-lang/crates.io-index"
-
-[[package]]
-name = "fuchsia-cprng"
-version = "0.1.1"
-source = "registry+https://github.com/rust-lang/crates.io-index"
-
-[[package]]
-name = "fuchsia-zircon"
-version = "0.3.3"
-source = "registry+https://github.com/rust-lang/crates.io-index"
-dependencies = [
- "bitflags 1.2.1 (registry+https://github.com/rust-lang/crates.io-index)",
- "fuchsia-zircon-sys 0.3.3 (registry+https://github.com/rust-lang/crates.io-index)",
-]
-
-[[package]]
-name = "fuchsia-zircon-sys"
-version = "0.3.3"
-source = "registry+https://github.com/rust-lang/crates.io-index"
-
-[[package]]
-name = "futures"
-version = "0.3.5"
-source = "registry+https://github.com/rust-lang/crates.io-index"
-dependencies = [
- "futures-channel 0.3.5 (registry+https://github.com/rust-lang/crates.io-index)",
- "futures-core 0.3.5 (registry+https://github.com/rust-lang/crates.io-index)",
- "futures-io 0.3.5 (registry+https://github.com/rust-lang/crates.io-index)",
- "futures-sink 0.3.5 (registry+https://github.com/rust-lang/crates.io-index)",
- "futures-task 0.3.5 (registry+https://github.com/rust-lang/crates.io-index)",
- "futures-util 0.3.5 (registry+https://github.com/rust-lang/crates.io-index)",
-]
-
-[[package]]
-name = "futures-channel"
-version = "0.3.5"
-source = "registry+https://github.com/rust-lang/crates.io-index"
-dependencies = [
- "futures-core 0.3.5 (registry+https://github.com/rust-lang/crates.io-index)",
- "futures-sink 0.3.5 (registry+https://github.com/rust-lang/crates.io-index)",
-]
-
-[[package]]
-name = "futures-core"
-version = "0.3.5"
-source = "registry+https://github.com/rust-lang/crates.io-index"
-
-[[package]]
-name = "futures-io"
-version = "0.3.5"
-source = "registry+https://github.com/rust-lang/crates.io-index"
-
-[[package]]
-name = "futures-sink"
-version = "0.3.5"
-source = "registry+https://github.com/rust-lang/crates.io-index"
-
-[[package]]
-name = "futures-task"
-version = "0.3.5"
-source = "registry+https://github.com/rust-lang/crates.io-index"
-dependencies = [
- "once_cell 1.4.1 (registry+https://github.com/rust-lang/crates.io-index)",
-]
-
-[[package]]
-name = "futures-util"
-version = "0.3.5"
-source = "registry+https://github.com/rust-lang/crates.io-index"
-dependencies = [
- "futures-core 0.3.5 (registry+https://github.com/rust-lang/crates.io-index)",
- "futures-sink 0.3.5 (registry+https://github.com/rust-lang/crates.io-index)",
- "futures-task 0.3.5 (registry+https://github.com/rust-lang/crates.io-index)",
- "pin-project 0.4.23 (registry+https://github.com/rust-lang/crates.io-index)",
- "pin-utils 0.1.0 (registry+https://github.com/rust-lang/crates.io-index)",
- "slab 0.4.2 (registry+https://github.com/rust-lang/crates.io-index)",
-]
-
-[[package]]
-name = "generic-array"
-version = "0.12.3"
-source = "registry+https://github.com/rust-lang/crates.io-index"
-dependencies = [
- "typenum 1.12.0 (registry+https://github.com/rust-lang/crates.io-index)",
-]
-
-[[package]]
-name = "generic-array"
-version = "0.14.4"
-source = "registry+https://github.com/rust-lang/crates.io-index"
-dependencies = [
- "typenum 1.12.0 (registry+https://github.com/rust-lang/crates.io-index)",
- "version_check 0.9.2 (registry+https://github.com/rust-lang/crates.io-index)",
-]
-
-[[package]]
-name = "getopts"
-version = "0.2.21"
-source = "registry+https://github.com/rust-lang/crates.io-index"
-dependencies = [
- "unicode-width 0.1.8 (registry+https://github.com/rust-lang/crates.io-index)",
-]
-
-[[package]]
-name = "getrandom"
-version = "0.1.14"
-source = "registry+https://github.com/rust-lang/crates.io-index"
-dependencies = [
- "cfg-if 0.1.10 (registry+https://github.com/rust-lang/crates.io-index)",
- "libc 0.2.73 (registry+https://github.com/rust-lang/crates.io-index)",
- "wasi 0.9.0+wasi-snapshot-preview1 (registry+https://github.com/rust-lang/crates.io-index)",
-]
-
-[[package]]
-name = "glob"
-version = "0.3.0"
-source = "registry+https://github.com/rust-lang/crates.io-index"
-
-[[package]]
-name = "h2"
-version = "0.2.6"
-source = "registry+https://github.com/rust-lang/crates.io-index"
-dependencies = [
- "bytes 0.5.6 (registry+https://github.com/rust-lang/crates.io-index)",
- "fnv 1.0.7 (registry+https://github.com/rust-lang/crates.io-index)",
- "futures-core 0.3.5 (registry+https://github.com/rust-lang/crates.io-index)",
- "futures-sink 0.3.5 (registry+https://github.com/rust-lang/crates.io-index)",
- "futures-util 0.3.5 (registry+https://github.com/rust-lang/crates.io-index)",
- "http 0.2.1 (registry+https://github.com/rust-lang/crates.io-index)",
- "indexmap 1.6.0 (registry+https://github.com/rust-lang/crates.io-index)",
- "slab 0.4.2 (registry+https://github.com/rust-lang/crates.io-index)",
- "tokio 0.2.22 (registry+https://github.com/rust-lang/crates.io-index)",
- "tokio-util 0.3.1 (registry+https://github.com/rust-lang/crates.io-index)",
- "tracing 0.1.19 (registry+https://github.com/rust-lang/crates.io-index)",
-]
-
-[[package]]
-name = "hashbrown"
-version = "0.9.0"
-source = "registry+https://github.com/rust-lang/crates.io-index"
-
-[[package]]
-name = "headers"
-version = "0.3.2"
-source = "registry+https://github.com/rust-lang/crates.io-index"
-dependencies = [
- "base64 0.12.3 (registry+https://github.com/rust-lang/crates.io-index)",
- "bitflags 1.2.1 (registry+https://github.com/rust-lang/crates.io-index)",
- "bytes 0.5.6 (registry+https://github.com/rust-lang/crates.io-index)",
- "headers-core 0.2.0 (registry+https://github.com/rust-lang/crates.io-index)",
- "http 0.2.1 (registry+https://github.com/rust-lang/crates.io-index)",
- "mime 0.3.16 (registry+https://github.com/rust-lang/crates.io-index)",
- "sha-1 0.8.2 (registry+https://github.com/rust-lang/crates.io-index)",
- "time 0.1.44 (registry+https://github.com/rust-lang/crates.io-index)",
-]
-
-[[package]]
-name = "headers-core"
-version = "0.2.0"
-source = "registry+https://github.com/rust-lang/crates.io-index"
-dependencies = [
- "http 0.2.1 (registry+https://github.com/rust-lang/crates.io-index)",
-]
-
-[[package]]
-name = "hermit-abi"
-version = "0.1.15"
-source = "registry+https://github.com/rust-lang/crates.io-index"
-dependencies = [
- "libc 0.2.73 (registry+https://github.com/rust-lang/crates.io-index)",
-]
-
-[[package]]
-name = "http"
-version = "0.2.1"
-source = "registry+https://github.com/rust-lang/crates.io-index"
-dependencies = [
- "bytes 0.5.6 (registry+https://github.com/rust-lang/crates.io-index)",
- "fnv 1.0.7 (registry+https://github.com/rust-lang/crates.io-index)",
- "itoa 0.4.6 (registry+https://github.com/rust-lang/crates.io-index)",
-]
-
-[[package]]
-name = "http-body"
-version = "0.3.1"
-source = "registry+https://github.com/rust-lang/crates.io-index"
-dependencies = [
- "bytes 0.5.6 (registry+https://github.com/rust-lang/crates.io-index)",
- "http 0.2.1 (registry+https://github.com/rust-lang/crates.io-index)",
-]
-
-[[package]]
-name = "httparse"
-version = "1.3.4"
-source = "registry+https://github.com/rust-lang/crates.io-index"
-
-[[package]]
-name = "humantime"
-version = "1.3.0"
-source = "registry+https://github.com/rust-lang/crates.io-index"
-dependencies = [
- "quick-error 1.2.3 (registry+https://github.com/rust-lang/crates.io-index)",
-]
-
-[[package]]
-name = "hyper"
-version = "0.13.7"
-source = "registry+https://github.com/rust-lang/crates.io-index"
-dependencies = [
- "bytes 0.5.6 (registry+https://github.com/rust-lang/crates.io-index)",
- "futures-channel 0.3.5 (registry+https://github.com/rust-lang/crates.io-index)",
- "futures-core 0.3.5 (registry+https://github.com/rust-lang/crates.io-index)",
- "futures-util 0.3.5 (registry+https://github.com/rust-lang/crates.io-index)",
- "h2 0.2.6 (registry+https://github.com/rust-lang/crates.io-index)",
- "http 0.2.1 (registry+https://github.com/rust-lang/crates.io-index)",
- "http-body 0.3.1 (registry+https://github.com/rust-lang/crates.io-index)",
- "httparse 1.3.4 (registry+https://github.com/rust-lang/crates.io-index)",
- "itoa 0.4.6 (registry+https://github.com/rust-lang/crates.io-index)",
- "pin-project 0.4.23 (registry+https://github.com/rust-lang/crates.io-index)",
- "socket2 0.3.15 (registry+https://github.com/rust-lang/crates.io-index)",
- "time 0.1.44 (registry+https://github.com/rust-lang/crates.io-index)",
- "tokio 0.2.22 (registry+https://github.com/rust-lang/crates.io-index)",
- "tower-service 0.3.0 (registry+https://github.com/rust-lang/crates.io-index)",
- "tracing 0.1.19 (registry+https://github.com/rust-lang/crates.io-index)",
- "want 0.3.0 (registry+https://github.com/rust-lang/crates.io-index)",
-]
-
-[[package]]
-name = "idna"
-version = "0.2.0"
-source = "registry+https://github.com/rust-lang/crates.io-index"
-dependencies = [
- "matches 0.1.8 (registry+https://github.com/rust-lang/crates.io-index)",
- "unicode-bidi 0.3.4 (registry+https://github.com/rust-lang/crates.io-index)",
- "unicode-normalization 0.1.13 (registry+https://github.com/rust-lang/crates.io-index)",
-]
-
-[[package]]
-name = "indexmap"
-version = "1.6.0"
-source = "registry+https://github.com/rust-lang/crates.io-index"
-dependencies = [
- "autocfg 1.0.1 (registry+https://github.com/rust-lang/crates.io-index)",
- "hashbrown 0.9.0 (registry+https://github.com/rust-lang/crates.io-index)",
-]
-
-[[package]]
-name = "input_buffer"
-version = "0.3.1"
-source = "registry+https://github.com/rust-lang/crates.io-index"
-dependencies = [
- "bytes 0.5.6 (registry+https://github.com/rust-lang/crates.io-index)",
-]
-
-[[package]]
-name = "iovec"
-version = "0.1.4"
-source = "registry+https://github.com/rust-lang/crates.io-index"
-dependencies = [
- "libc 0.2.73 (registry+https://github.com/rust-lang/crates.io-index)",
-]
-
-[[package]]
-name = "itoa"
-version = "0.4.6"
-source = "registry+https://github.com/rust-lang/crates.io-index"
-
-[[package]]
-name = "kernel32-sys"
-version = "0.2.2"
-source = "registry+https://github.com/rust-lang/crates.io-index"
-dependencies = [
- "winapi 0.2.8 (registry+https://github.com/rust-lang/crates.io-index)",
- "winapi-build 0.1.1 (registry+https://github.com/rust-lang/crates.io-index)",
-]
-
-[[package]]
-name = "lazy_static"
-version = "1.4.0"
-source = "registry+https://github.com/rust-lang/crates.io-index"
-
-[[package]]
-name = "libc"
-version = "0.2.73"
-source = "registry+https://github.com/rust-lang/crates.io-index"
-
-[[package]]
-name = "libloading"
-version = "0.5.2"
-source = "registry+https://github.com/rust-lang/crates.io-index"
-dependencies = [
- "cc 1.0.58 (registry+https://github.com/rust-lang/crates.io-index)",
- "winapi 0.3.9 (registry+https://github.com/rust-lang/crates.io-index)",
-]
-
-[[package]]
-name = "libudev"
-version = "0.2.0"
-source = "registry+https://github.com/rust-lang/crates.io-index"
-dependencies = [
- "libc 0.2.73 (registry+https://github.com/rust-lang/crates.io-index)",
- "libudev-sys 0.1.4 (registry+https://github.com/rust-lang/crates.io-index)",
-]
-
-[[package]]
-name = "libudev-sys"
-version = "0.1.4"
-source = "registry+https://github.com/rust-lang/crates.io-index"
-dependencies = [
- "libc 0.2.73 (registry+https://github.com/rust-lang/crates.io-index)",
- "pkg-config 0.3.18 (registry+https://github.com/rust-lang/crates.io-index)",
-]
-
-[[package]]
-name = "log"
-version = "0.4.11"
-source = "registry+https://github.com/rust-lang/crates.io-index"
-dependencies = [
- "cfg-if 0.1.10 (registry+https://github.com/rust-lang/crates.io-index)",
-]
-
-[[package]]
-name = "matches"
-version = "0.1.8"
-source = "registry+https://github.com/rust-lang/crates.io-index"
-
-[[package]]
-name = "memchr"
-version = "2.3.3"
-source = "registry+https://github.com/rust-lang/crates.io-index"
-
-[[package]]
-name = "mime"
-version = "0.3.16"
-source = "registry+https://github.com/rust-lang/crates.io-index"
-
-[[package]]
-name = "mime_guess"
-version = "2.0.3"
-source = "registry+https://github.com/rust-lang/crates.io-index"
-dependencies = [
- "mime 0.3.16 (registry+https://github.com/rust-lang/crates.io-index)",
- "unicase 2.6.0 (registry+https://github.com/rust-lang/crates.io-index)",
-]
-
-[[package]]
-name = "mio"
-version = "0.6.22"
-source = "registry+https://github.com/rust-lang/crates.io-index"
-dependencies = [
- "cfg-if 0.1.10 (registry+https://github.com/rust-lang/crates.io-index)",
- "fuchsia-zircon 0.3.3 (registry+https://github.com/rust-lang/crates.io-index)",
- "fuchsia-zircon-sys 0.3.3 (registry+https://github.com/rust-lang/crates.io-index)",
- "iovec 0.1.4 (registry+https://github.com/rust-lang/crates.io-index)",
- "kernel32-sys 0.2.2 (registry+https://github.com/rust-lang/crates.io-index)",
- "libc 0.2.73 (registry+https://github.com/rust-lang/crates.io-index)",
- "log 0.4.11 (registry+https://github.com/rust-lang/crates.io-index)",
- "miow 0.2.1 (registry+https://github.com/rust-lang/crates.io-index)",
- "net2 0.2.35 (registry+https://github.com/rust-lang/crates.io-index)",
- "slab 0.4.2 (registry+https://github.com/rust-lang/crates.io-index)",
- "winapi 0.2.8 (registry+https://github.com/rust-lang/crates.io-index)",
-]
-
-[[package]]
-name = "miow"
-version = "0.2.1"
-source = "registry+https://github.com/rust-lang/crates.io-index"
-dependencies = [
- "kernel32-sys 0.2.2 (registry+https://github.com/rust-lang/crates.io-index)",
- "net2 0.2.35 (registry+https://github.com/rust-lang/crates.io-index)",
- "winapi 0.2.8 (registry+https://github.com/rust-lang/crates.io-index)",
- "ws2_32-sys 0.2.1 (registry+https://github.com/rust-lang/crates.io-index)",
-]
-
-[[package]]
-name = "multipart"
-version = "0.17.0"
-source = "registry+https://github.com/rust-lang/crates.io-index"
-dependencies = [
- "buf_redux 0.8.4 (registry+https://github.com/rust-lang/crates.io-index)",
- "httparse 1.3.4 (registry+https://github.com/rust-lang/crates.io-index)",
- "log 0.4.11 (registry+https://github.com/rust-lang/crates.io-index)",
- "mime 0.3.16 (registry+https://github.com/rust-lang/crates.io-index)",
- "mime_guess 2.0.3 (registry+https://github.com/rust-lang/crates.io-index)",
- "quick-error 1.2.3 (registry+https://github.com/rust-lang/crates.io-index)",
- "rand 0.6.5 (registry+https://github.com/rust-lang/crates.io-index)",
- "safemem 0.3.3 (registry+https://github.com/rust-lang/crates.io-index)",
- "tempfile 3.1.0 (registry+https://github.com/rust-lang/crates.io-index)",
- "twoway 0.1.8 (registry+https://github.com/rust-lang/crates.io-index)",
-]
-
-[[package]]
-name = "net2"
-version = "0.2.35"
-source = "registry+https://github.com/rust-lang/crates.io-index"
-dependencies = [
- "cfg-if 0.1.10 (registry+https://github.com/rust-lang/crates.io-index)",
- "libc 0.2.73 (registry+https://github.com/rust-lang/crates.io-index)",
- "winapi 0.3.9 (registry+https://github.com/rust-lang/crates.io-index)",
-]
-
-[[package]]
-name = "nom"
-version = "4.2.3"
-source = "registry+https://github.com/rust-lang/crates.io-index"
-dependencies = [
- "memchr 2.3.3 (registry+https://github.com/rust-lang/crates.io-index)",
- "version_check 0.1.5 (registry+https://github.com/rust-lang/crates.io-index)",
-]
-
-[[package]]
-name = "nom"
-version = "5.1.2"
-source = "registry+https://github.com/rust-lang/crates.io-index"
-dependencies = [
- "memchr 2.3.3 (registry+https://github.com/rust-lang/crates.io-index)",
- "version_check 0.9.2 (registry+https://github.com/rust-lang/crates.io-index)",
-]
-
-[[package]]
-name = "once_cell"
-version = "1.4.1"
-source = "registry+https://github.com/rust-lang/crates.io-index"
-
-[[package]]
-name = "opaque-debug"
-version = "0.2.3"
-source = "registry+https://github.com/rust-lang/crates.io-index"
-
-[[package]]
-name = "opaque-debug"
-version = "0.3.0"
-source = "registry+https://github.com/rust-lang/crates.io-index"
-
-[[package]]
-name = "peeking_take_while"
-version = "0.1.2"
-source = "registry+https://github.com/rust-lang/crates.io-index"
-
-[[package]]
-name = "percent-encoding"
-version = "2.1.0"
-source = "registry+https://github.com/rust-lang/crates.io-index"
-
-[[package]]
-name = "pin-project"
-version = "0.4.23"
-source = "registry+https://github.com/rust-lang/crates.io-index"
-dependencies = [
- "pin-project-internal 0.4.23 (registry+https://github.com/rust-lang/crates.io-index)",
-]
-
-[[package]]
-name = "pin-project-internal"
-version = "0.4.23"
-source = "registry+https://github.com/rust-lang/crates.io-index"
-dependencies = [
- "proc-macro2 1.0.19 (registry+https://github.com/rust-lang/crates.io-index)",
- "quote 1.0.7 (registry+https://github.com/rust-lang/crates.io-index)",
- "syn 1.0.41 (registry+https://github.com/rust-lang/crates.io-index)",
-]
-
-[[package]]
-name = "pin-project-lite"
-version = "0.1.7"
-source = "registry+https://github.com/rust-lang/crates.io-index"
-
-[[package]]
-name = "pin-utils"
-version = "0.1.0"
-source = "registry+https://github.com/rust-lang/crates.io-index"
-
-[[package]]
-name = "pkg-config"
-version = "0.3.18"
-source = "registry+https://github.com/rust-lang/crates.io-index"
-
-[[package]]
-name = "ppv-lite86"
-version = "0.2.8"
-source = "registry+https://github.com/rust-lang/crates.io-index"
-
-[[package]]
-name = "proc-macro2"
-version = "1.0.19"
-source = "registry+https://github.com/rust-lang/crates.io-index"
-dependencies = [
- "unicode-xid 0.2.1 (registry+https://github.com/rust-lang/crates.io-index)",
-]
-
-[[package]]
-name = "quick-error"
-version = "1.2.3"
-source = "registry+https://github.com/rust-lang/crates.io-index"
-
-[[package]]
-name = "quote"
-version = "1.0.7"
-source = "registry+https://github.com/rust-lang/crates.io-index"
-dependencies = [
- "proc-macro2 1.0.19 (registry+https://github.com/rust-lang/crates.io-index)",
-]
-
-[[package]]
-name = "rand"
-version = "0.6.5"
-source = "registry+https://github.com/rust-lang/crates.io-index"
-dependencies = [
- "autocfg 0.1.7 (registry+https://github.com/rust-lang/crates.io-index)",
- "libc 0.2.73 (registry+https://github.com/rust-lang/crates.io-index)",
- "rand_chacha 0.1.1 (registry+https://github.com/rust-lang/crates.io-index)",
- "rand_core 0.4.2 (registry+https://github.com/rust-lang/crates.io-index)",
- "rand_hc 0.1.0 (registry+https://github.com/rust-lang/crates.io-index)",
- "rand_isaac 0.1.1 (registry+https://github.com/rust-lang/crates.io-index)",
- "rand_jitter 0.1.4 (registry+https://github.com/rust-lang/crates.io-index)",
- "rand_os 0.1.3 (registry+https://github.com/rust-lang/crates.io-index)",
- "rand_pcg 0.1.2 (registry+https://github.com/rust-lang/crates.io-index)",
- "rand_xorshift 0.1.1 (registry+https://github.com/rust-lang/crates.io-index)",
- "winapi 0.3.9 (registry+https://github.com/rust-lang/crates.io-index)",
-]
-
-[[package]]
-name = "rand"
-version = "0.7.3"
-source = "registry+https://github.com/rust-lang/crates.io-index"
-dependencies = [
- "getrandom 0.1.14 (registry+https://github.com/rust-lang/crates.io-index)",
- "libc 0.2.73 (registry+https://github.com/rust-lang/crates.io-index)",
- "rand_chacha 0.2.2 (registry+https://github.com/rust-lang/crates.io-index)",
- "rand_core 0.5.1 (registry+https://github.com/rust-lang/crates.io-index)",
- "rand_hc 0.2.0 (registry+https://github.com/rust-lang/crates.io-index)",
-]
-
-[[package]]
-name = "rand_chacha"
-version = "0.1.1"
-source = "registry+https://github.com/rust-lang/crates.io-index"
-dependencies = [
- "autocfg 0.1.7 (registry+https://github.com/rust-lang/crates.io-index)",
- "rand_core 0.3.1 (registry+https://github.com/rust-lang/crates.io-index)",
-]
-
-[[package]]
-name = "rand_chacha"
-version = "0.2.2"
-source = "registry+https://github.com/rust-lang/crates.io-index"
-dependencies = [
- "ppv-lite86 0.2.8 (registry+https://github.com/rust-lang/crates.io-index)",
- "rand_core 0.5.1 (registry+https://github.com/rust-lang/crates.io-index)",
-]
-
-[[package]]
-name = "rand_core"
-version = "0.3.1"
-source = "registry+https://github.com/rust-lang/crates.io-index"
-dependencies = [
- "rand_core 0.4.2 (registry+https://github.com/rust-lang/crates.io-index)",
-]
-
-[[package]]
-name = "rand_core"
-version = "0.4.2"
-source = "registry+https://github.com/rust-lang/crates.io-index"
-
-[[package]]
-name = "rand_core"
-version = "0.5.1"
-source = "registry+https://github.com/rust-lang/crates.io-index"
-dependencies = [
- "getrandom 0.1.14 (registry+https://github.com/rust-lang/crates.io-index)",
-]
-
-[[package]]
-name = "rand_hc"
-version = "0.1.0"
-source = "registry+https://github.com/rust-lang/crates.io-index"
-dependencies = [
- "rand_core 0.3.1 (registry+https://github.com/rust-lang/crates.io-index)",
-]
-
-[[package]]
-name = "rand_hc"
-version = "0.2.0"
-source = "registry+https://github.com/rust-lang/crates.io-index"
-dependencies = [
- "rand_core 0.5.1 (registry+https://github.com/rust-lang/crates.io-index)",
-]
-
-[[package]]
-name = "rand_isaac"
-version = "0.1.1"
-source = "registry+https://github.com/rust-lang/crates.io-index"
-dependencies = [
- "rand_core 0.3.1 (registry+https://github.com/rust-lang/crates.io-index)",
-]
-
-[[package]]
-name = "rand_jitter"
-version = "0.1.4"
-source = "registry+https://github.com/rust-lang/crates.io-index"
-dependencies = [
- "libc 0.2.73 (registry+https://github.com/rust-lang/crates.io-index)",
- "rand_core 0.4.2 (registry+https://github.com/rust-lang/crates.io-index)",
- "winapi 0.3.9 (registry+https://github.com/rust-lang/crates.io-index)",
-]
-
-[[package]]
-name = "rand_os"
-version = "0.1.3"
-source = "registry+https://github.com/rust-lang/crates.io-index"
-dependencies = [
- "cloudabi 0.0.3 (registry+https://github.com/rust-lang/crates.io-index)",
- "fuchsia-cprng 0.1.1 (registry+https://github.com/rust-lang/crates.io-index)",
- "libc 0.2.73 (registry+https://github.com/rust-lang/crates.io-index)",
- "rand_core 0.4.2 (registry+https://github.com/rust-lang/crates.io-index)",
- "rdrand 0.4.0 (registry+https://github.com/rust-lang/crates.io-index)",
- "winapi 0.3.9 (registry+https://github.com/rust-lang/crates.io-index)",
-]
-
-[[package]]
-name = "rand_pcg"
-version = "0.1.2"
-source = "registry+https://github.com/rust-lang/crates.io-index"
-dependencies = [
- "autocfg 0.1.7 (registry+https://github.com/rust-lang/crates.io-index)",
- "rand_core 0.4.2 (registry+https://github.com/rust-lang/crates.io-index)",
-]
-
-[[package]]
-name = "rand_xorshift"
-version = "0.1.1"
-source = "registry+https://github.com/rust-lang/crates.io-index"
-dependencies = [
- "rand_core 0.3.1 (registry+https://github.com/rust-lang/crates.io-index)",
-]
-
-[[package]]
-name = "rdrand"
-version = "0.4.0"
-source = "registry+https://github.com/rust-lang/crates.io-index"
-dependencies = [
- "rand_core 0.3.1 (registry+https://github.com/rust-lang/crates.io-index)",
-]
-
-[[package]]
-name = "redox_syscall"
-version = "0.1.57"
-source = "registry+https://github.com/rust-lang/crates.io-index"
-
-[[package]]
-name = "regex"
-version = "1.3.9"
-source = "registry+https://github.com/rust-lang/crates.io-index"
-dependencies = [
- "aho-corasick 0.7.13 (registry+https://github.com/rust-lang/crates.io-index)",
- "memchr 2.3.3 (registry+https://github.com/rust-lang/crates.io-index)",
- "regex-syntax 0.6.18 (registry+https://github.com/rust-lang/crates.io-index)",
- "thread_local 1.0.1 (registry+https://github.com/rust-lang/crates.io-index)",
-]
-
-[[package]]
-name = "regex-syntax"
-version = "0.6.18"
-source = "registry+https://github.com/rust-lang/crates.io-index"
-
-[[package]]
-name = "remove_dir_all"
-version = "0.5.3"
-source = "registry+https://github.com/rust-lang/crates.io-index"
-dependencies = [
- "winapi 0.3.9 (registry+https://github.com/rust-lang/crates.io-index)",
-]
-
-[[package]]
-name = "runloop"
-version = "0.1.0"
-source = "registry+https://github.com/rust-lang/crates.io-index"
-
-[[package]]
-name = "rustc-hash"
-version = "1.1.0"
-source = "registry+https://github.com/rust-lang/crates.io-index"
-
-[[package]]
-name = "ryu"
-version = "1.0.5"
-source = "registry+https://github.com/rust-lang/crates.io-index"
-
-[[package]]
-name = "safemem"
-version = "0.3.3"
-source = "registry+https://github.com/rust-lang/crates.io-index"
-
-[[package]]
-name = "scoped-tls"
-version = "1.0.0"
-source = "registry+https://github.com/rust-lang/crates.io-index"
-
-[[package]]
-name = "serde"
-version = "1.0.116"
-source = "registry+https://github.com/rust-lang/crates.io-index"
-dependencies = [
- "serde_derive 1.0.116 (registry+https://github.com/rust-lang/crates.io-index)",
-]
-
-[[package]]
-name = "serde_derive"
-version = "1.0.116"
-source = "registry+https://github.com/rust-lang/crates.io-index"
-dependencies = [
- "proc-macro2 1.0.19 (registry+https://github.com/rust-lang/crates.io-index)",
- "quote 1.0.7 (registry+https://github.com/rust-lang/crates.io-index)",
- "syn 1.0.41 (registry+https://github.com/rust-lang/crates.io-index)",
-]
-
-[[package]]
-name = "serde_json"
-version = "1.0.57"
-source = "registry+https://github.com/rust-lang/crates.io-index"
-dependencies = [
- "itoa 0.4.6 (registry+https://github.com/rust-lang/crates.io-index)",
- "ryu 1.0.5 (registry+https://github.com/rust-lang/crates.io-index)",
- "serde 1.0.116 (registry+https://github.com/rust-lang/crates.io-index)",
-]
-
-[[package]]
-name = "serde_urlencoded"
-version = "0.6.1"
-source = "registry+https://github.com/rust-lang/crates.io-index"
-dependencies = [
- "dtoa 0.4.6 (registry+https://github.com/rust-lang/crates.io-index)",
- "itoa 0.4.6 (registry+https://github.com/rust-lang/crates.io-index)",
- "serde 1.0.116 (registry+https://github.com/rust-lang/crates.io-index)",
- "url 2.1.1 (registry+https://github.com/rust-lang/crates.io-index)",
-]
-
-[[package]]
-name = "sha-1"
-version = "0.8.2"
-source = "registry+https://github.com/rust-lang/crates.io-index"
-dependencies = [
- "block-buffer 0.7.3 (registry+https://github.com/rust-lang/crates.io-index)",
- "digest 0.8.1 (registry+https://github.com/rust-lang/crates.io-index)",
- "fake-simd 0.1.2 (registry+https://github.com/rust-lang/crates.io-index)",
- "opaque-debug 0.2.3 (registry+https://github.com/rust-lang/crates.io-index)",
-]
-
-[[package]]
-name = "sha-1"
-version = "0.9.1"
-source = "registry+https://github.com/rust-lang/crates.io-index"
-dependencies = [
- "block-buffer 0.9.0 (registry+https://github.com/rust-lang/crates.io-index)",
- "cfg-if 0.1.10 (registry+https://github.com/rust-lang/crates.io-index)",
- "cpuid-bool 0.1.2 (registry+https://github.com/rust-lang/crates.io-index)",
- "digest 0.9.0 (registry+https://github.com/rust-lang/crates.io-index)",
- "opaque-debug 0.3.0 (registry+https://github.com/rust-lang/crates.io-index)",
-]
-
-[[package]]
-name = "sha2"
-version = "0.8.2"
-source = "registry+https://github.com/rust-lang/crates.io-index"
-dependencies = [
- "block-buffer 0.7.3 (registry+https://github.com/rust-lang/crates.io-index)",
- "digest 0.8.1 (registry+https://github.com/rust-lang/crates.io-index)",
- "fake-simd 0.1.2 (registry+https://github.com/rust-lang/crates.io-index)",
- "opaque-debug 0.2.3 (registry+https://github.com/rust-lang/crates.io-index)",
-]
-
-[[package]]
-name = "shlex"
-version = "0.1.1"
-source = "registry+https://github.com/rust-lang/crates.io-index"
-
-[[package]]
-name = "slab"
-version = "0.4.2"
-source = "registry+https://github.com/rust-lang/crates.io-index"
-
-[[package]]
-name = "socket2"
-version = "0.3.15"
-source = "registry+https://github.com/rust-lang/crates.io-index"
-dependencies = [
- "cfg-if 0.1.10 (registry+https://github.com/rust-lang/crates.io-index)",
- "libc 0.2.73 (registry+https://github.com/rust-lang/crates.io-index)",
- "redox_syscall 0.1.57 (registry+https://github.com/rust-lang/crates.io-index)",
- "winapi 0.3.9 (registry+https://github.com/rust-lang/crates.io-index)",
-]
-
-[[package]]
-name = "strsim"
-version = "0.8.0"
-source = "registry+https://github.com/rust-lang/crates.io-index"
-
-[[package]]
-name = "syn"
-version = "1.0.41"
-source = "registry+https://github.com/rust-lang/crates.io-index"
-dependencies = [
- "proc-macro2 1.0.19 (registry+https://github.com/rust-lang/crates.io-index)",
- "quote 1.0.7 (registry+https://github.com/rust-lang/crates.io-index)",
- "unicode-xid 0.2.1 (registry+https://github.com/rust-lang/crates.io-index)",
-]
-
-[[package]]
-name = "tempfile"
-version = "3.1.0"
-source = "registry+https://github.com/rust-lang/crates.io-index"
-dependencies = [
- "cfg-if 0.1.10 (registry+https://github.com/rust-lang/crates.io-index)",
- "libc 0.2.73 (registry+https://github.com/rust-lang/crates.io-index)",
- "rand 0.7.3 (registry+https://github.com/rust-lang/crates.io-index)",
- "redox_syscall 0.1.57 (registry+https://github.com/rust-lang/crates.io-index)",
- "remove_dir_all 0.5.3 (registry+https://github.com/rust-lang/crates.io-index)",
- "winapi 0.3.9 (registry+https://github.com/rust-lang/crates.io-index)",
-]
-
-[[package]]
-name = "termcolor"
-version = "1.1.0"
-source = "registry+https://github.com/rust-lang/crates.io-index"
-dependencies = [
- "winapi-util 0.1.5 (registry+https://github.com/rust-lang/crates.io-index)",
-]
-
-[[package]]
-name = "textwrap"
-version = "0.11.0"
-source = "registry+https://github.com/rust-lang/crates.io-index"
-dependencies = [
- "unicode-width 0.1.8 (registry+https://github.com/rust-lang/crates.io-index)",
-]
-
-[[package]]
-name = "thread_local"
-version = "1.0.1"
-source = "registry+https://github.com/rust-lang/crates.io-index"
-dependencies = [
- "lazy_static 1.4.0 (registry+https://github.com/rust-lang/crates.io-index)",
-]
-
-[[package]]
-name = "time"
-version = "0.1.44"
-source = "registry+https://github.com/rust-lang/crates.io-index"
-dependencies = [
- "libc 0.2.73 (registry+https://github.com/rust-lang/crates.io-index)",
- "wasi 0.10.0+wasi-snapshot-preview1 (registry+https://github.com/rust-lang/crates.io-index)",
- "winapi 0.3.9 (registry+https://github.com/rust-lang/crates.io-index)",
-]
-
-[[package]]
-name = "tinyvec"
-version = "0.3.4"
-source = "registry+https://github.com/rust-lang/crates.io-index"
-
-[[package]]
-name = "tokio"
-version = "0.2.22"
-source = "registry+https://github.com/rust-lang/crates.io-index"
-dependencies = [
- "bytes 0.5.6 (registry+https://github.com/rust-lang/crates.io-index)",
- "fnv 1.0.7 (registry+https://github.com/rust-lang/crates.io-index)",
- "futures-core 0.3.5 (registry+https://github.com/rust-lang/crates.io-index)",
- "iovec 0.1.4 (registry+https://github.com/rust-lang/crates.io-index)",
- "lazy_static 1.4.0 (registry+https://github.com/rust-lang/crates.io-index)",
- "memchr 2.3.3 (registry+https://github.com/rust-lang/crates.io-index)",
- "mio 0.6.22 (registry+https://github.com/rust-lang/crates.io-index)",
- "pin-project-lite 0.1.7 (registry+https://github.com/rust-lang/crates.io-index)",
- "slab 0.4.2 (registry+https://github.com/rust-lang/crates.io-index)",
- "tokio-macros 0.2.5 (registry+https://github.com/rust-lang/crates.io-index)",
-]
-
-[[package]]
-name = "tokio-macros"
-version = "0.2.5"
-source = "registry+https://github.com/rust-lang/crates.io-index"
-dependencies = [
- "proc-macro2 1.0.19 (registry+https://github.com/rust-lang/crates.io-index)",
- "quote 1.0.7 (registry+https://github.com/rust-lang/crates.io-index)",
- "syn 1.0.41 (registry+https://github.com/rust-lang/crates.io-index)",
-]
-
-[[package]]
-name = "tokio-tungstenite"
-version = "0.11.0"
-source = "registry+https://github.com/rust-lang/crates.io-index"
-dependencies = [
- "futures-util 0.3.5 (registry+https://github.com/rust-lang/crates.io-index)",
- "log 0.4.11 (registry+https://github.com/rust-lang/crates.io-index)",
- "pin-project 0.4.23 (registry+https://github.com/rust-lang/crates.io-index)",
- "tokio 0.2.22 (registry+https://github.com/rust-lang/crates.io-index)",
- "tungstenite 0.11.1 (registry+https://github.com/rust-lang/crates.io-index)",
-]
-
-[[package]]
-name = "tokio-util"
-version = "0.3.1"
-source = "registry+https://github.com/rust-lang/crates.io-index"
-dependencies = [
- "bytes 0.5.6 (registry+https://github.com/rust-lang/crates.io-index)",
- "futures-core 0.3.5 (registry+https://github.com/rust-lang/crates.io-index)",
- "futures-sink 0.3.5 (registry+https://github.com/rust-lang/crates.io-index)",
- "log 0.4.11 (registry+https://github.com/rust-lang/crates.io-index)",
- "pin-project-lite 0.1.7 (registry+https://github.com/rust-lang/crates.io-index)",
- "tokio 0.2.22 (registry+https://github.com/rust-lang/crates.io-index)",
-]
-
-[[package]]
-name = "tower-service"
-version = "0.3.0"
-source = "registry+https://github.com/rust-lang/crates.io-index"
-
-[[package]]
-name = "tracing"
-version = "0.1.19"
-source = "registry+https://github.com/rust-lang/crates.io-index"
-dependencies = [
- "cfg-if 0.1.10 (registry+https://github.com/rust-lang/crates.io-index)",
- "log 0.4.11 (registry+https://github.com/rust-lang/crates.io-index)",
- "tracing-core 0.1.16 (registry+https://github.com/rust-lang/crates.io-index)",
-]
-
-[[package]]
-name = "tracing-core"
-version = "0.1.16"
-source = "registry+https://github.com/rust-lang/crates.io-index"
-dependencies = [
- "lazy_static 1.4.0 (registry+https://github.com/rust-lang/crates.io-index)",
-]
-
-[[package]]
-name = "tracing-futures"
-version = "0.2.4"
-source = "registry+https://github.com/rust-lang/crates.io-index"
-dependencies = [
- "pin-project 0.4.23 (registry+https://github.com/rust-lang/crates.io-index)",
- "tracing 0.1.19 (registry+https://github.com/rust-lang/crates.io-index)",
-]
-
-[[package]]
-name = "try-lock"
-version = "0.2.3"
-source = "registry+https://github.com/rust-lang/crates.io-index"
-
-[[package]]
-name = "tungstenite"
-version = "0.11.1"
-source = "registry+https://github.com/rust-lang/crates.io-index"
-dependencies = [
- "base64 0.12.3 (registry+https://github.com/rust-lang/crates.io-index)",
- "byteorder 1.3.4 (registry+https://github.com/rust-lang/crates.io-index)",
- "bytes 0.5.6 (registry+https://github.com/rust-lang/crates.io-index)",
- "http 0.2.1 (registry+https://github.com/rust-lang/crates.io-index)",
- "httparse 1.3.4 (registry+https://github.com/rust-lang/crates.io-index)",
- "input_buffer 0.3.1 (registry+https://github.com/rust-lang/crates.io-index)",
- "log 0.4.11 (registry+https://github.com/rust-lang/crates.io-index)",
- "rand 0.7.3 (registry+https://github.com/rust-lang/crates.io-index)",
- "sha-1 0.9.1 (registry+https://github.com/rust-lang/crates.io-index)",
- "url 2.1.1 (registry+https://github.com/rust-lang/crates.io-index)",
- "utf-8 0.7.5 (registry+https://github.com/rust-lang/crates.io-index)",
-]
-
-[[package]]
-name = "twoway"
-version = "0.1.8"
-source = "registry+https://github.com/rust-lang/crates.io-index"
-dependencies = [
- "memchr 2.3.3 (registry+https://github.com/rust-lang/crates.io-index)",
-]
-
-[[package]]
-name = "typenum"
-version = "1.12.0"
-source = "registry+https://github.com/rust-lang/crates.io-index"
-
-[[package]]
-name = "unicase"
-version = "2.6.0"
-source = "registry+https://github.com/rust-lang/crates.io-index"
-dependencies = [
- "version_check 0.9.2 (registry+https://github.com/rust-lang/crates.io-index)",
-]
-
-[[package]]
-name = "unicode-bidi"
-version = "0.3.4"
-source = "registry+https://github.com/rust-lang/crates.io-index"
-dependencies = [
- "matches 0.1.8 (registry+https://github.com/rust-lang/crates.io-index)",
-]
-
-[[package]]
-name = "unicode-normalization"
-version = "0.1.13"
-source = "registry+https://github.com/rust-lang/crates.io-index"
-dependencies = [
- "tinyvec 0.3.4 (registry+https://github.com/rust-lang/crates.io-index)",
-]
-
-[[package]]
-name = "unicode-width"
-version = "0.1.8"
-source = "registry+https://github.com/rust-lang/crates.io-index"
-
-[[package]]
-name = "unicode-xid"
-version = "0.2.1"
-source = "registry+https://github.com/rust-lang/crates.io-index"
-
-[[package]]
-name = "url"
-version = "2.1.1"
-source = "registry+https://github.com/rust-lang/crates.io-index"
-dependencies = [
- "idna 0.2.0 (registry+https://github.com/rust-lang/crates.io-index)",
- "matches 0.1.8 (registry+https://github.com/rust-lang/crates.io-index)",
- "percent-encoding 2.1.0 (registry+https://github.com/rust-lang/crates.io-index)",
-]
-
-[[package]]
-name = "urlencoding"
-version = "1.1.1"
-source = "registry+https://github.com/rust-lang/crates.io-index"
-
-[[package]]
-name = "utf-8"
-version = "0.7.5"
-source = "registry+https://github.com/rust-lang/crates.io-index"
-
-[[package]]
-name = "vec_map"
-version = "0.8.2"
-source = "registry+https://github.com/rust-lang/crates.io-index"
-
-[[package]]
-name = "version_check"
-version = "0.1.5"
-source = "registry+https://github.com/rust-lang/crates.io-index"
-
-[[package]]
-name = "version_check"
-version = "0.9.2"
-source = "registry+https://github.com/rust-lang/crates.io-index"
-
-[[package]]
-name = "want"
-version = "0.3.0"
-source = "registry+https://github.com/rust-lang/crates.io-index"
-dependencies = [
- "log 0.4.11 (registry+https://github.com/rust-lang/crates.io-index)",
- "try-lock 0.2.3 (registry+https://github.com/rust-lang/crates.io-index)",
-]
-
-[[package]]
-name = "warp"
-version = "0.2.5"
-source = "registry+https://github.com/rust-lang/crates.io-index"
-dependencies = [
- "bytes 0.5.6 (registry+https://github.com/rust-lang/crates.io-index)",
- "futures 0.3.5 (registry+https://github.com/rust-lang/crates.io-index)",
- "headers 0.3.2 (registry+https://github.com/rust-lang/crates.io-index)",
- "http 0.2.1 (registry+https://github.com/rust-lang/crates.io-index)",
- "hyper 0.13.7 (registry+https://github.com/rust-lang/crates.io-index)",
- "log 0.4.11 (registry+https://github.com/rust-lang/crates.io-index)",
- "mime 0.3.16 (registry+https://github.com/rust-lang/crates.io-index)",
- "mime_guess 2.0.3 (registry+https://github.com/rust-lang/crates.io-index)",
- "multipart 0.17.0 (registry+https://github.com/rust-lang/crates.io-index)",
- "pin-project 0.4.23 (registry+https://github.com/rust-lang/crates.io-index)",
- "scoped-tls 1.0.0 (registry+https://github.com/rust-lang/crates.io-index)",
- "serde 1.0.116 (registry+https://github.com/rust-lang/crates.io-index)",
- "serde_json 1.0.57 (registry+https://github.com/rust-lang/crates.io-index)",
- "serde_urlencoded 0.6.1 (registry+https://github.com/rust-lang/crates.io-index)",
- "tokio 0.2.22 (registry+https://github.com/rust-lang/crates.io-index)",
- "tokio-tungstenite 0.11.0 (registry+https://github.com/rust-lang/crates.io-index)",
- "tower-service 0.3.0 (registry+https://github.com/rust-lang/crates.io-index)",
- "tracing 0.1.19 (registry+https://github.com/rust-lang/crates.io-index)",
- "tracing-futures 0.2.4 (registry+https://github.com/rust-lang/crates.io-index)",
- "urlencoding 1.1.1 (registry+https://github.com/rust-lang/crates.io-index)",
-]
-
-[[package]]
-name = "wasi"
-version = "0.9.0+wasi-snapshot-preview1"
-source = "registry+https://github.com/rust-lang/crates.io-index"
-
-[[package]]
-name = "wasi"
-version = "0.10.0+wasi-snapshot-preview1"
-source = "registry+https://github.com/rust-lang/crates.io-index"
-
-[[package]]
-name = "which"
-version = "3.1.1"
-source = "registry+https://github.com/rust-lang/crates.io-index"
-dependencies = [
- "libc 0.2.73 (registry+https://github.com/rust-lang/crates.io-index)",
-]
-
-[[package]]
-name = "winapi"
-version = "0.2.8"
-source = "registry+https://github.com/rust-lang/crates.io-index"
-
-[[package]]
-name = "winapi"
-version = "0.3.9"
-source = "registry+https://github.com/rust-lang/crates.io-index"
-dependencies = [
- "winapi-i686-pc-windows-gnu 0.4.0 (registry+https://github.com/rust-lang/crates.io-index)",
- "winapi-x86_64-pc-windows-gnu 0.4.0 (registry+https://github.com/rust-lang/crates.io-index)",
-]
-
-[[package]]
-name = "winapi-build"
-version = "0.1.1"
-source = "registry+https://github.com/rust-lang/crates.io-index"
-
-[[package]]
-name = "winapi-i686-pc-windows-gnu"
-version = "0.4.0"
-source = "registry+https://github.com/rust-lang/crates.io-index"
-
-[[package]]
-name = "winapi-util"
-version = "0.1.5"
-source = "registry+https://github.com/rust-lang/crates.io-index"
-dependencies = [
- "winapi 0.3.9 (registry+https://github.com/rust-lang/crates.io-index)",
-]
-
-[[package]]
-name = "winapi-x86_64-pc-windows-gnu"
-version = "0.4.0"
-source = "registry+https://github.com/rust-lang/crates.io-index"
-
-[[package]]
-name = "ws2_32-sys"
-version = "0.2.1"
-source = "registry+https://github.com/rust-lang/crates.io-index"
-dependencies = [
- "winapi 0.2.8 (registry+https://github.com/rust-lang/crates.io-index)",
- "winapi-build 0.1.1 (registry+https://github.com/rust-lang/crates.io-index)",
-]
-
-[metadata]
-"checksum aho-corasick 0.7.13 (registry+https://github.com/rust-lang/crates.io-index)" = "043164d8ba5c4c3035fec9bbee8647c0261d788f3474306f93bb65901cae0e86"
-"checksum ansi_term 0.11.0 (registry+https://github.com/rust-lang/crates.io-index)" = "ee49baf6cb617b853aa8d93bf420db2383fab46d314482ca2803b40d5fde979b"
-"checksum assert_matches 1.3.0 (registry+https://github.com/rust-lang/crates.io-index)" = "7deb0a829ca7bcfaf5da70b073a8d128619259a7be8216a355e23f00763059e5"
-"checksum atty 0.2.14 (registry+https://github.com/rust-lang/crates.io-index)" = "d9b39be18770d11421cdb1b9947a45dd3f37e93092cbf377614828a319d5fee8"
-"checksum autocfg 0.1.7 (registry+https://github.com/rust-lang/crates.io-index)" = "1d49d90015b3c36167a20fe2810c5cd875ad504b39cff3d4eae7977e6b7c1cb2"
-"checksum autocfg 1.0.1 (registry+https://github.com/rust-lang/crates.io-index)" = "cdb031dd78e28731d87d56cc8ffef4a8f36ca26c38fe2de700543e627f8a464a"
-"checksum base64 0.10.1 (registry+https://github.com/rust-lang/crates.io-index)" = "0b25d992356d2eb0ed82172f5248873db5560c4721f564b13cb5193bda5e668e"
-"checksum base64 0.12.3 (registry+https://github.com/rust-lang/crates.io-index)" = "3441f0f7b02788e948e47f457ca01f1d7e6d92c693bc132c22b087d3141c03ff"
-"checksum bindgen 0.51.1 (registry+https://github.com/rust-lang/crates.io-index)" = "ebd71393f1ec0509b553aa012b9b58e81dadbdff7130bd3b8cba576e69b32f75"
-"checksum bitflags 1.2.1 (registry+https://github.com/rust-lang/crates.io-index)" = "cf1de2fe8c75bc145a2f577add951f8134889b4795d47466a54a5c846d691693"
-"checksum block-buffer 0.7.3 (registry+https://github.com/rust-lang/crates.io-index)" = "c0940dc441f31689269e10ac70eb1002a3a1d3ad1390e030043662eb7fe4688b"
-"checksum block-buffer 0.9.0 (registry+https://github.com/rust-lang/crates.io-index)" = "4152116fd6e9dadb291ae18fc1ec3575ed6d84c29642d97890f4b4a3417297e4"
-"checksum block-padding 0.1.5 (registry+https://github.com/rust-lang/crates.io-index)" = "fa79dedbb091f449f1f39e53edf88d5dbe95f895dae6135a8d7b881fb5af73f5"
-"checksum buf_redux 0.8.4 (registry+https://github.com/rust-lang/crates.io-index)" = "b953a6887648bb07a535631f2bc00fbdb2a2216f135552cb3f534ed136b9c07f"
-"checksum byte-tools 0.3.1 (registry+https://github.com/rust-lang/crates.io-index)" = "e3b5ca7a04898ad4bcd41c90c5285445ff5b791899bb1b0abdd2a2aa791211d7"
-"checksum byteorder 1.3.4 (registry+https://github.com/rust-lang/crates.io-index)" = "08c48aae112d48ed9f069b33538ea9e3e90aa263cfa3d1c24309612b1f7472de"
-"checksum bytes 0.5.6 (registry+https://github.com/rust-lang/crates.io-index)" = "0e4cec68f03f32e44924783795810fa50a7035d8c8ebe78580ad7e6c703fba38"
-"checksum cc 1.0.58 (registry+https://github.com/rust-lang/crates.io-index)" = "f9a06fb2e53271d7c279ec1efea6ab691c35a2ae67ec0d91d7acec0caf13b518"
-"checksum cexpr 0.3.6 (registry+https://github.com/rust-lang/crates.io-index)" = "fce5b5fb86b0c57c20c834c1b412fd09c77c8a59b9473f86272709e78874cd1d"
-"checksum cfg-if 0.1.10 (registry+https://github.com/rust-lang/crates.io-index)" = "4785bdd1c96b2a846b2bd7cc02e86b6b3dbf14e7e53446c4f54c92a361040822"
-"checksum clang-sys 0.28.1 (registry+https://github.com/rust-lang/crates.io-index)" = "81de550971c976f176130da4b2978d3b524eaa0fd9ac31f3ceb5ae1231fb4853"
-"checksum clap 2.33.1 (registry+https://github.com/rust-lang/crates.io-index)" = "bdfa80d47f954d53a35a64987ca1422f495b8d6483c0fe9f7117b36c2a792129"
-"checksum cloudabi 0.0.3 (registry+https://github.com/rust-lang/crates.io-index)" = "ddfc5b9aa5d4507acaf872de71051dfd0e309860e88966e1051e462a077aac4f"
-"checksum core-foundation 0.9.0 (registry+https://github.com/rust-lang/crates.io-index)" = "3b5ed8e7e76c45974e15e41bfa8d5b0483cd90191639e01d8f5f1e606299d3fb"
-"checksum core-foundation-sys 0.8.0 (registry+https://github.com/rust-lang/crates.io-index)" = "9a21fa21941700a3cd8fcb4091f361a6a712fac632f85d9f487cc892045d55c6"
-"checksum cpuid-bool 0.1.2 (registry+https://github.com/rust-lang/crates.io-index)" = "8aebca1129a03dc6dc2b127edd729435bbc4a37e1d5f4d7513165089ceb02634"
-"checksum devd-rs 0.3.1 (registry+https://github.com/rust-lang/crates.io-index)" = "1945ccb7caedabdfb9347766ead740fb1e0582b7425598325f546adbd832cce1"
-"checksum digest 0.8.1 (registry+https://github.com/rust-lang/crates.io-index)" = "f3d0c8c8752312f9713efd397ff63acb9f85585afbf179282e720e7704954dd5"
-"checksum digest 0.9.0 (registry+https://github.com/rust-lang/crates.io-index)" = "d3dd60d1080a57a05ab032377049e0591415d2b31afd7028356dbf3cc6dcb066"
-"checksum dtoa 0.4.6 (registry+https://github.com/rust-lang/crates.io-index)" = "134951f4028bdadb9b84baf4232681efbf277da25144b9b0ad65df75946c422b"
-"checksum env_logger 0.6.2 (registry+https://github.com/rust-lang/crates.io-index)" = "aafcde04e90a5226a6443b7aabdb016ba2f8307c847d524724bd9b346dd1a2d3"
-"checksum fake-simd 0.1.2 (registry+https://github.com/rust-lang/crates.io-index)" = "e88a8acf291dafb59c2d96e8f59828f3838bb1a70398823ade51a84de6a6deed"
-"checksum fnv 1.0.7 (registry+https://github.com/rust-lang/crates.io-index)" = "3f9eec918d3f24069decb9af1554cad7c880e2da24a9afd88aca000531ab82c1"
-"checksum fuchsia-cprng 0.1.1 (registry+https://github.com/rust-lang/crates.io-index)" = "a06f77d526c1a601b7c4cdd98f54b5eaabffc14d5f2f0296febdc7f357c6d3ba"
-"checksum fuchsia-zircon 0.3.3 (registry+https://github.com/rust-lang/crates.io-index)" = "2e9763c69ebaae630ba35f74888db465e49e259ba1bc0eda7d06f4a067615d82"
-"checksum fuchsia-zircon-sys 0.3.3 (registry+https://github.com/rust-lang/crates.io-index)" = "3dcaa9ae7725d12cdb85b3ad99a434db70b468c09ded17e012d86b5c1010f7a7"
-"checksum futures 0.3.5 (registry+https://github.com/rust-lang/crates.io-index)" = "1e05b85ec287aac0dc34db7d4a569323df697f9c55b99b15d6b4ef8cde49f613"
-"checksum futures-channel 0.3.5 (registry+https://github.com/rust-lang/crates.io-index)" = "f366ad74c28cca6ba456d95e6422883cfb4b252a83bed929c83abfdbbf2967d5"
-"checksum futures-core 0.3.5 (registry+https://github.com/rust-lang/crates.io-index)" = "59f5fff90fd5d971f936ad674802482ba441b6f09ba5e15fd8b39145582ca399"
-"checksum futures-io 0.3.5 (registry+https://github.com/rust-lang/crates.io-index)" = "de27142b013a8e869c14957e6d2edeef89e97c289e69d042ee3a49acd8b51789"
-"checksum futures-sink 0.3.5 (registry+https://github.com/rust-lang/crates.io-index)" = "3f2032893cb734c7a05d85ce0cc8b8c4075278e93b24b66f9de99d6eb0fa8acc"
-"checksum futures-task 0.3.5 (registry+https://github.com/rust-lang/crates.io-index)" = "bdb66b5f09e22019b1ab0830f7785bcea8e7a42148683f99214f73f8ec21a626"
-"checksum futures-util 0.3.5 (registry+https://github.com/rust-lang/crates.io-index)" = "8764574ff08b701a084482c3c7031349104b07ac897393010494beaa18ce32c6"
-"checksum generic-array 0.12.3 (registry+https://github.com/rust-lang/crates.io-index)" = "c68f0274ae0e023facc3c97b2e00f076be70e254bc851d972503b328db79b2ec"
-"checksum generic-array 0.14.4 (registry+https://github.com/rust-lang/crates.io-index)" = "501466ecc8a30d1d3b7fc9229b122b2ce8ed6e9d9223f1138d4babb253e51817"
-"checksum getopts 0.2.21 (registry+https://github.com/rust-lang/crates.io-index)" = "14dbbfd5c71d70241ecf9e6f13737f7b5ce823821063188d7e46c41d371eebd5"
-"checksum getrandom 0.1.14 (registry+https://github.com/rust-lang/crates.io-index)" = "7abc8dd8451921606d809ba32e95b6111925cd2906060d2dcc29c070220503eb"
-"checksum glob 0.3.0 (registry+https://github.com/rust-lang/crates.io-index)" = "9b919933a397b79c37e33b77bb2aa3dc8eb6e165ad809e58ff75bc7db2e34574"
-"checksum h2 0.2.6 (registry+https://github.com/rust-lang/crates.io-index)" = "993f9e0baeed60001cf565546b0d3dbe6a6ad23f2bd31644a133c641eccf6d53"
-"checksum hashbrown 0.9.0 (registry+https://github.com/rust-lang/crates.io-index)" = "00d63df3d41950fb462ed38308eea019113ad1508da725bbedcd0fa5a85ef5f7"
-"checksum headers 0.3.2 (registry+https://github.com/rust-lang/crates.io-index)" = "ed18eb2459bf1a09ad2d6b1547840c3e5e62882fa09b9a6a20b1de8e3228848f"
-"checksum headers-core 0.2.0 (registry+https://github.com/rust-lang/crates.io-index)" = "e7f66481bfee273957b1f20485a4ff3362987f85b2c236580d81b4eb7a326429"
-"checksum hermit-abi 0.1.15 (registry+https://github.com/rust-lang/crates.io-index)" = "3deed196b6e7f9e44a2ae8d94225d80302d81208b1bb673fd21fe634645c85a9"
-"checksum http 0.2.1 (registry+https://github.com/rust-lang/crates.io-index)" = "28d569972648b2c512421b5f2a405ad6ac9666547189d0c5477a3f200f3e02f9"
-"checksum http-body 0.3.1 (registry+https://github.com/rust-lang/crates.io-index)" = "13d5ff830006f7646652e057693569bfe0d51760c0085a071769d142a205111b"
-"checksum httparse 1.3.4 (registry+https://github.com/rust-lang/crates.io-index)" = "cd179ae861f0c2e53da70d892f5f3029f9594be0c41dc5269cd371691b1dc2f9"
-"checksum humantime 1.3.0 (registry+https://github.com/rust-lang/crates.io-index)" = "df004cfca50ef23c36850aaaa59ad52cc70d0e90243c3c7737a4dd32dc7a3c4f"
-"checksum hyper 0.13.7 (registry+https://github.com/rust-lang/crates.io-index)" = "3e68a8dd9716185d9e64ea473ea6ef63529252e3e27623295a0378a19665d5eb"
-"checksum idna 0.2.0 (registry+https://github.com/rust-lang/crates.io-index)" = "02e2673c30ee86b5b96a9cb52ad15718aa1f966f5ab9ad54a8b95d5ca33120a9"
-"checksum indexmap 1.6.0 (registry+https://github.com/rust-lang/crates.io-index)" = "55e2e4c765aa53a0424761bf9f41aa7a6ac1efa87238f59560640e27fca028f2"
-"checksum input_buffer 0.3.1 (registry+https://github.com/rust-lang/crates.io-index)" = "19a8a95243d5a0398cae618ec29477c6e3cb631152be5c19481f80bc71559754"
-"checksum iovec 0.1.4 (registry+https://github.com/rust-lang/crates.io-index)" = "b2b3ea6ff95e175473f8ffe6a7eb7c00d054240321b84c57051175fe3c1e075e"
-"checksum itoa 0.4.6 (registry+https://github.com/rust-lang/crates.io-index)" = "dc6f3ad7b9d11a0c00842ff8de1b60ee58661048eb8049ed33c73594f359d7e6"
-"checksum kernel32-sys 0.2.2 (registry+https://github.com/rust-lang/crates.io-index)" = "7507624b29483431c0ba2d82aece8ca6cdba9382bff4ddd0f7490560c056098d"
-"checksum lazy_static 1.4.0 (registry+https://github.com/rust-lang/crates.io-index)" = "e2abad23fbc42b3700f2f279844dc832adb2b2eb069b2df918f455c4e18cc646"
-"checksum libc 0.2.73 (registry+https://github.com/rust-lang/crates.io-index)" = "bd7d4bd64732af4bf3a67f367c27df8520ad7e230c5817b8ff485864d80242b9"
-"checksum libloading 0.5.2 (registry+https://github.com/rust-lang/crates.io-index)" = "f2b111a074963af1d37a139918ac6d49ad1d0d5e47f72fd55388619691a7d753"
-"checksum libudev 0.2.0 (registry+https://github.com/rust-lang/crates.io-index)" = "ea626d3bdf40a1c5aee3bcd4f40826970cae8d80a8fec934c82a63840094dcfe"
-"checksum libudev-sys 0.1.4 (registry+https://github.com/rust-lang/crates.io-index)" = "3c8469b4a23b962c1396b9b451dda50ef5b283e8dd309d69033475fa9b334324"
-"checksum log 0.4.11 (registry+https://github.com/rust-lang/crates.io-index)" = "4fabed175da42fed1fa0746b0ea71f412aa9d35e76e95e59b192c64b9dc2bf8b"
-"checksum matches 0.1.8 (registry+https://github.com/rust-lang/crates.io-index)" = "7ffc5c5338469d4d3ea17d269fa8ea3512ad247247c30bd2df69e68309ed0a08"
-"checksum memchr 2.3.3 (registry+https://github.com/rust-lang/crates.io-index)" = "3728d817d99e5ac407411fa471ff9800a778d88a24685968b36824eaf4bee400"
-"checksum mime 0.3.16 (registry+https://github.com/rust-lang/crates.io-index)" = "2a60c7ce501c71e03a9c9c0d35b861413ae925bd979cc7a4e30d060069aaac8d"
-"checksum mime_guess 2.0.3 (registry+https://github.com/rust-lang/crates.io-index)" = "2684d4c2e97d99848d30b324b00c8fcc7e5c897b7cbb5819b09e7c90e8baf212"
-"checksum mio 0.6.22 (registry+https://github.com/rust-lang/crates.io-index)" = "fce347092656428bc8eaf6201042cb551b8d67855af7374542a92a0fbfcac430"
-"checksum miow 0.2.1 (registry+https://github.com/rust-lang/crates.io-index)" = "8c1f2f3b1cf331de6896aabf6e9d55dca90356cc9960cca7eaaf408a355ae919"
-"checksum multipart 0.17.0 (registry+https://github.com/rust-lang/crates.io-index)" = "8209c33c951f07387a8497841122fc6f712165e3f9bda3e6be4645b58188f676"
-"checksum net2 0.2.35 (registry+https://github.com/rust-lang/crates.io-index)" = "3ebc3ec692ed7c9a255596c67808dee269f64655d8baf7b4f0638e51ba1d6853"
-"checksum nom 4.2.3 (registry+https://github.com/rust-lang/crates.io-index)" = "2ad2a91a8e869eeb30b9cb3119ae87773a8f4ae617f41b1eb9c154b2905f7bd6"
-"checksum nom 5.1.2 (registry+https://github.com/rust-lang/crates.io-index)" = "ffb4262d26ed83a1c0a33a38fe2bb15797329c85770da05e6b828ddb782627af"
-"checksum once_cell 1.4.1 (registry+https://github.com/rust-lang/crates.io-index)" = "260e51e7efe62b592207e9e13a68e43692a7a279171d6ba57abd208bf23645ad"
-"checksum opaque-debug 0.2.3 (registry+https://github.com/rust-lang/crates.io-index)" = "2839e79665f131bdb5782e51f2c6c9599c133c6098982a54c794358bf432529c"
-"checksum opaque-debug 0.3.0 (registry+https://github.com/rust-lang/crates.io-index)" = "624a8340c38c1b80fd549087862da4ba43e08858af025b236e509b6649fc13d5"
-"checksum peeking_take_while 0.1.2 (registry+https://github.com/rust-lang/crates.io-index)" = "19b17cddbe7ec3f8bc800887bab5e717348c95ea2ca0b1bf0837fb964dc67099"
-"checksum percent-encoding 2.1.0 (registry+https://github.com/rust-lang/crates.io-index)" = "d4fd5641d01c8f18a23da7b6fe29298ff4b55afcccdf78973b24cf3175fee32e"
-"checksum pin-project 0.4.23 (registry+https://github.com/rust-lang/crates.io-index)" = "ca4433fff2ae79342e497d9f8ee990d174071408f28f726d6d83af93e58e48aa"
-"checksum pin-project-internal 0.4.23 (registry+https://github.com/rust-lang/crates.io-index)" = "2c0e815c3ee9a031fdf5af21c10aa17c573c9c6a566328d99e3936c34e36461f"
-"checksum pin-project-lite 0.1.7 (registry+https://github.com/rust-lang/crates.io-index)" = "282adbf10f2698a7a77f8e983a74b2d18176c19a7fd32a45446139ae7b02b715"
-"checksum pin-utils 0.1.0 (registry+https://github.com/rust-lang/crates.io-index)" = "8b870d8c151b6f2fb93e84a13146138f05d02ed11c7e7c54f8826aaaf7c9f184"
-"checksum pkg-config 0.3.18 (registry+https://github.com/rust-lang/crates.io-index)" = "d36492546b6af1463394d46f0c834346f31548646f6ba10849802c9c9a27ac33"
-"checksum ppv-lite86 0.2.8 (registry+https://github.com/rust-lang/crates.io-index)" = "237a5ed80e274dbc66f86bd59c1e25edc039660be53194b5fe0a482e0f2612ea"
-"checksum proc-macro2 1.0.19 (registry+https://github.com/rust-lang/crates.io-index)" = "04f5f085b5d71e2188cb8271e5da0161ad52c3f227a661a3c135fdf28e258b12"
-"checksum quick-error 1.2.3 (registry+https://github.com/rust-lang/crates.io-index)" = "a1d01941d82fa2ab50be1e79e6714289dd7cde78eba4c074bc5a4374f650dfe0"
-"checksum quote 1.0.7 (registry+https://github.com/rust-lang/crates.io-index)" = "aa563d17ecb180e500da1cfd2b028310ac758de548efdd203e18f283af693f37"
-"checksum rand 0.6.5 (registry+https://github.com/rust-lang/crates.io-index)" = "6d71dacdc3c88c1fde3885a3be3fbab9f35724e6ce99467f7d9c5026132184ca"
-"checksum rand 0.7.3 (registry+https://github.com/rust-lang/crates.io-index)" = "6a6b1679d49b24bbfe0c803429aa1874472f50d9b363131f0e89fc356b544d03"
-"checksum rand_chacha 0.1.1 (registry+https://github.com/rust-lang/crates.io-index)" = "556d3a1ca6600bfcbab7c7c91ccb085ac7fbbcd70e008a98742e7847f4f7bcef"
-"checksum rand_chacha 0.2.2 (registry+https://github.com/rust-lang/crates.io-index)" = "f4c8ed856279c9737206bf725bf36935d8666ead7aa69b52be55af369d193402"
-"checksum rand_core 0.3.1 (registry+https://github.com/rust-lang/crates.io-index)" = "7a6fdeb83b075e8266dcc8762c22776f6877a63111121f5f8c7411e5be7eed4b"
-"checksum rand_core 0.4.2 (registry+https://github.com/rust-lang/crates.io-index)" = "9c33a3c44ca05fa6f1807d8e6743f3824e8509beca625669633be0acbdf509dc"
-"checksum rand_core 0.5.1 (registry+https://github.com/rust-lang/crates.io-index)" = "90bde5296fc891b0cef12a6d03ddccc162ce7b2aff54160af9338f8d40df6d19"
-"checksum rand_hc 0.1.0 (registry+https://github.com/rust-lang/crates.io-index)" = "7b40677c7be09ae76218dc623efbf7b18e34bced3f38883af07bb75630a21bc4"
-"checksum rand_hc 0.2.0 (registry+https://github.com/rust-lang/crates.io-index)" = "ca3129af7b92a17112d59ad498c6f81eaf463253766b90396d39ea7a39d6613c"
-"checksum rand_isaac 0.1.1 (registry+https://github.com/rust-lang/crates.io-index)" = "ded997c9d5f13925be2a6fd7e66bf1872597f759fd9dd93513dd7e92e5a5ee08"
-"checksum rand_jitter 0.1.4 (registry+https://github.com/rust-lang/crates.io-index)" = "1166d5c91dc97b88d1decc3285bb0a99ed84b05cfd0bc2341bdf2d43fc41e39b"
-"checksum rand_os 0.1.3 (registry+https://github.com/rust-lang/crates.io-index)" = "7b75f676a1e053fc562eafbb47838d67c84801e38fc1ba459e8f180deabd5071"
-"checksum rand_pcg 0.1.2 (registry+https://github.com/rust-lang/crates.io-index)" = "abf9b09b01790cfe0364f52bf32995ea3c39f4d2dd011eac241d2914146d0b44"
-"checksum rand_xorshift 0.1.1 (registry+https://github.com/rust-lang/crates.io-index)" = "cbf7e9e623549b0e21f6e97cf8ecf247c1a8fd2e8a992ae265314300b2455d5c"
-"checksum rdrand 0.4.0 (registry+https://github.com/rust-lang/crates.io-index)" = "678054eb77286b51581ba43620cc911abf02758c91f93f479767aed0f90458b2"
-"checksum redox_syscall 0.1.57 (registry+https://github.com/rust-lang/crates.io-index)" = "41cc0f7e4d5d4544e8861606a285bb08d3e70712ccc7d2b84d7c0ccfaf4b05ce"
-"checksum regex 1.3.9 (registry+https://github.com/rust-lang/crates.io-index)" = "9c3780fcf44b193bc4d09f36d2a3c87b251da4a046c87795a0d35f4f927ad8e6"
-"checksum regex-syntax 0.6.18 (registry+https://github.com/rust-lang/crates.io-index)" = "26412eb97c6b088a6997e05f69403a802a92d520de2f8e63c2b65f9e0f47c4e8"
-"checksum remove_dir_all 0.5.3 (registry+https://github.com/rust-lang/crates.io-index)" = "3acd125665422973a33ac9d3dd2df85edad0f4ae9b00dafb1a05e43a9f5ef8e7"
-"checksum runloop 0.1.0 (registry+https://github.com/rust-lang/crates.io-index)" = "5d79b4b604167921892e84afbbaad9d5ad74e091bf6c511d9dbfb0593f09fabd"
-"checksum rustc-hash 1.1.0 (registry+https://github.com/rust-lang/crates.io-index)" = "08d43f7aa6b08d49f382cde6a7982047c3426db949b1424bc4b7ec9ae12c6ce2"
-"checksum ryu 1.0.5 (registry+https://github.com/rust-lang/crates.io-index)" = "71d301d4193d031abdd79ff7e3dd721168a9572ef3fe51a1517aba235bd8f86e"
-"checksum safemem 0.3.3 (registry+https://github.com/rust-lang/crates.io-index)" = "ef703b7cb59335eae2eb93ceb664c0eb7ea6bf567079d843e09420219668e072"
-"checksum scoped-tls 1.0.0 (registry+https://github.com/rust-lang/crates.io-index)" = "ea6a9290e3c9cf0f18145ef7ffa62d68ee0bf5fcd651017e586dc7fd5da448c2"
-"checksum serde 1.0.116 (registry+https://github.com/rust-lang/crates.io-index)" = "96fe57af81d28386a513cbc6858332abc6117cfdb5999647c6444b8f43a370a5"
-"checksum serde_derive 1.0.116 (registry+https://github.com/rust-lang/crates.io-index)" = "f630a6370fd8e457873b4bd2ffdae75408bc291ba72be773772a4c2a065d9ae8"
-"checksum serde_json 1.0.57 (registry+https://github.com/rust-lang/crates.io-index)" = "164eacbdb13512ec2745fb09d51fd5b22b0d65ed294a1dcf7285a360c80a675c"
-"checksum serde_urlencoded 0.6.1 (registry+https://github.com/rust-lang/crates.io-index)" = "9ec5d77e2d4c73717816afac02670d5c4f534ea95ed430442cad02e7a6e32c97"
-"checksum sha-1 0.8.2 (registry+https://github.com/rust-lang/crates.io-index)" = "f7d94d0bede923b3cea61f3f1ff57ff8cdfd77b400fb8f9998949e0cf04163df"
-"checksum sha-1 0.9.1 (registry+https://github.com/rust-lang/crates.io-index)" = "170a36ea86c864a3f16dd2687712dd6646f7019f301e57537c7f4dc9f5916770"
-"checksum sha2 0.8.2 (registry+https://github.com/rust-lang/crates.io-index)" = "a256f46ea78a0c0d9ff00077504903ac881a1dafdc20da66545699e7776b3e69"
-"checksum shlex 0.1.1 (registry+https://github.com/rust-lang/crates.io-index)" = "7fdf1b9db47230893d76faad238fd6097fd6d6a9245cd7a4d90dbd639536bbd2"
-"checksum slab 0.4.2 (registry+https://github.com/rust-lang/crates.io-index)" = "c111b5bd5695e56cffe5129854aa230b39c93a305372fdbb2668ca2394eea9f8"
-"checksum socket2 0.3.15 (registry+https://github.com/rust-lang/crates.io-index)" = "b1fa70dc5c8104ec096f4fe7ede7a221d35ae13dcd19ba1ad9a81d2cab9a1c44"
-"checksum strsim 0.8.0 (registry+https://github.com/rust-lang/crates.io-index)" = "8ea5119cdb4c55b55d432abb513a0429384878c15dde60cc77b1c99de1a95a6a"
-"checksum syn 1.0.41 (registry+https://github.com/rust-lang/crates.io-index)" = "6690e3e9f692504b941dc6c3b188fd28df054f7fb8469ab40680df52fdcc842b"
-"checksum tempfile 3.1.0 (registry+https://github.com/rust-lang/crates.io-index)" = "7a6e24d9338a0a5be79593e2fa15a648add6138caa803e2d5bc782c371732ca9"
-"checksum termcolor 1.1.0 (registry+https://github.com/rust-lang/crates.io-index)" = "bb6bfa289a4d7c5766392812c0a1f4c1ba45afa1ad47803c11e1f407d846d75f"
-"checksum textwrap 0.11.0 (registry+https://github.com/rust-lang/crates.io-index)" = "d326610f408c7a4eb6f51c37c330e496b08506c9457c9d34287ecc38809fb060"
-"checksum thread_local 1.0.1 (registry+https://github.com/rust-lang/crates.io-index)" = "d40c6d1b69745a6ec6fb1ca717914848da4b44ae29d9b3080cbee91d72a69b14"
-"checksum time 0.1.44 (registry+https://github.com/rust-lang/crates.io-index)" = "6db9e6914ab8b1ae1c260a4ae7a49b6c5611b40328a735b21862567685e73255"
-"checksum tinyvec 0.3.4 (registry+https://github.com/rust-lang/crates.io-index)" = "238ce071d267c5710f9d31451efec16c5ee22de34df17cc05e56cbc92e967117"
-"checksum tokio 0.2.22 (registry+https://github.com/rust-lang/crates.io-index)" = "5d34ca54d84bf2b5b4d7d31e901a8464f7b60ac145a284fba25ceb801f2ddccd"
-"checksum tokio-macros 0.2.5 (registry+https://github.com/rust-lang/crates.io-index)" = "f0c3acc6aa564495a0f2e1d59fab677cd7f81a19994cfc7f3ad0e64301560389"
-"checksum tokio-tungstenite 0.11.0 (registry+https://github.com/rust-lang/crates.io-index)" = "6d9e878ad426ca286e4dcae09cbd4e1973a7f8987d97570e2469703dd7f5720c"
-"checksum tokio-util 0.3.1 (registry+https://github.com/rust-lang/crates.io-index)" = "be8242891f2b6cbef26a2d7e8605133c2c554cd35b3e4948ea892d6d68436499"
-"checksum tower-service 0.3.0 (registry+https://github.com/rust-lang/crates.io-index)" = "e987b6bf443f4b5b3b6f38704195592cca41c5bb7aedd3c3693c7081f8289860"
-"checksum tracing 0.1.19 (registry+https://github.com/rust-lang/crates.io-index)" = "6d79ca061b032d6ce30c660fded31189ca0b9922bf483cd70759f13a2d86786c"
-"checksum tracing-core 0.1.16 (registry+https://github.com/rust-lang/crates.io-index)" = "5bcf46c1f1f06aeea2d6b81f3c863d0930a596c86ad1920d4e5bad6dd1d7119a"
-"checksum tracing-futures 0.2.4 (registry+https://github.com/rust-lang/crates.io-index)" = "ab7bb6f14721aa00656086e9335d363c5c8747bae02ebe32ea2c7dece5689b4c"
-"checksum try-lock 0.2.3 (registry+https://github.com/rust-lang/crates.io-index)" = "59547bce71d9c38b83d9c0e92b6066c4253371f15005def0c30d9657f50c7642"
-"checksum tungstenite 0.11.1 (registry+https://github.com/rust-lang/crates.io-index)" = "f0308d80d86700c5878b9ef6321f020f29b1bb9d5ff3cab25e75e23f3a492a23"
-"checksum twoway 0.1.8 (registry+https://github.com/rust-lang/crates.io-index)" = "59b11b2b5241ba34be09c3cc85a36e56e48f9888862e19cedf23336d35316ed1"
-"checksum typenum 1.12.0 (registry+https://github.com/rust-lang/crates.io-index)" = "373c8a200f9e67a0c95e62a4f52fbf80c23b4381c05a17845531982fa99e6b33"
-"checksum unicase 2.6.0 (registry+https://github.com/rust-lang/crates.io-index)" = "50f37be617794602aabbeee0be4f259dc1778fabe05e2d67ee8f79326d5cb4f6"
-"checksum unicode-bidi 0.3.4 (registry+https://github.com/rust-lang/crates.io-index)" = "49f2bd0c6468a8230e1db229cff8029217cf623c767ea5d60bfbd42729ea54d5"
-"checksum unicode-normalization 0.1.13 (registry+https://github.com/rust-lang/crates.io-index)" = "6fb19cf769fa8c6a80a162df694621ebeb4dafb606470b2b2fce0be40a98a977"
-"checksum unicode-width 0.1.8 (registry+https://github.com/rust-lang/crates.io-index)" = "9337591893a19b88d8d87f2cec1e73fad5cdfd10e5a6f349f498ad6ea2ffb1e3"
-"checksum unicode-xid 0.2.1 (registry+https://github.com/rust-lang/crates.io-index)" = "f7fe0bb3479651439c9112f72b6c505038574c9fbb575ed1bf3b797fa39dd564"
-"checksum url 2.1.1 (registry+https://github.com/rust-lang/crates.io-index)" = "829d4a8476c35c9bf0bbce5a3b23f4106f79728039b726d292bb93bc106787cb"
-"checksum urlencoding 1.1.1 (registry+https://github.com/rust-lang/crates.io-index)" = "c9232eb53352b4442e40d7900465dfc534e8cb2dc8f18656fcb2ac16112b5593"
-"checksum utf-8 0.7.5 (registry+https://github.com/rust-lang/crates.io-index)" = "05e42f7c18b8f902290b009cde6d651262f956c98bc51bca4cd1d511c9cd85c7"
-"checksum vec_map 0.8.2 (registry+https://github.com/rust-lang/crates.io-index)" = "f1bddf1187be692e79c5ffeab891132dfb0f236ed36a43c7ed39f1165ee20191"
-"checksum version_check 0.1.5 (registry+https://github.com/rust-lang/crates.io-index)" = "914b1a6776c4c929a602fafd8bc742e06365d4bcbe48c30f9cca5824f70dc9dd"
-"checksum version_check 0.9.2 (registry+https://github.com/rust-lang/crates.io-index)" = "b5a972e5669d67ba988ce3dc826706fb0a8b01471c088cb0b6110b805cc36aed"
-"checksum want 0.3.0 (registry+https://github.com/rust-lang/crates.io-index)" = "1ce8a968cb1cd110d136ff8b819a556d6fb6d919363c61534f6860c7eb172ba0"
-"checksum warp 0.2.5 (registry+https://github.com/rust-lang/crates.io-index)" = "f41be6df54c97904af01aa23e613d4521eed7ab23537cede692d4058f6449407"
-"checksum wasi 0.10.0+wasi-snapshot-preview1 (registry+https://github.com/rust-lang/crates.io-index)" = "1a143597ca7c7793eff794def352d41792a93c481eb1042423ff7ff72ba2c31f"
-"checksum wasi 0.9.0+wasi-snapshot-preview1 (registry+https://github.com/rust-lang/crates.io-index)" = "cccddf32554fecc6acb585f82a32a72e28b48f8c4c1883ddfeeeaa96f7d8e519"
-"checksum which 3.1.1 (registry+https://github.com/rust-lang/crates.io-index)" = "d011071ae14a2f6671d0b74080ae0cd8ebf3a6f8c9589a2cd45f23126fe29724"
-"checksum winapi 0.2.8 (registry+https://github.com/rust-lang/crates.io-index)" = "167dc9d6949a9b857f3451275e911c3f44255842c1f7a76f33c55103a909087a"
-"checksum winapi 0.3.9 (registry+https://github.com/rust-lang/crates.io-index)" = "5c839a674fcd7a98952e593242ea400abe93992746761e38641405d28b00f419"
-"checksum winapi-build 0.1.1 (registry+https://github.com/rust-lang/crates.io-index)" = "2d315eee3b34aca4797b2da6b13ed88266e6d612562a0c46390af8299fc699bc"
-"checksum winapi-i686-pc-windows-gnu 0.4.0 (registry+https://github.com/rust-lang/crates.io-index)" = "ac3b87c63620426dd9b991e5ce0329eff545bccbbb34f3be09ff6fb6ab51b7b6"
-"checksum winapi-util 0.1.5 (registry+https://github.com/rust-lang/crates.io-index)" = "70ec6ce85bb158151cae5e5c87f95a8e97d2c0c4b001223f33a334e3ce5de178"
-"checksum winapi-x86_64-pc-windows-gnu 0.4.0 (registry+https://github.com/rust-lang/crates.io-index)" = "712e227841d057c1ee1cd2fb22fa7e5a5461ae8e48fa2ca79ec42cfc1931183f"
-"checksum ws2_32-sys 0.2.1 (registry+https://github.com/rust-lang/crates.io-index)" = "d59cefebd0c892fa2dd6de581e937301d8552cb44489cdff035c6187cb63fa5e"
diff --git a/third_party/rust/authenticator/Cargo.toml b/third_party/rust/authenticator/Cargo.toml
index 57d24bd66b948..c49befae2178c 100644
--- a/third_party/rust/authenticator/Cargo.toml
+++ b/third_party/rust/authenticator/Cargo.toml
@@ -1,99 +1,60 @@
-# THIS FILE IS AUTOMATICALLY GENERATED BY CARGO
-#
-# When uploading crates to the registry Cargo will automatically
-# "normalize" Cargo.toml files for maximal compatibility
-# with all versions of Cargo and also rewrite `path` dependencies
-# to registry (e.g., crates.io) dependencies
-#
-# If you believe there's an error in this file please file an
-# issue against the rust-lang/cargo repository. If you're
-# editing this file be aware that the upstream Cargo.toml
-# will likely look very different (and much more reasonable)
-
 [package]
-edition = "2018"
 name = "authenticator"
 version = "0.3.1"
 authors = ["J.C. Jones <jc@mozilla.com>", "Tim Taubert <ttaubert@mozilla.com>", "Kyle Machulis <kyle@nonpolynomial.com>"]
-description = "Library for interacting with CTAP1/2 security keys for Web Authentication. Used by Firefox."
 keywords = ["ctap2", "u2f", "fido", "webauthn"]
 categories = ["cryptography", "hardware-support", "os"]
-license = "MPL-2.0"
 repository = "https://github.com/mozilla/authenticator-rs/"
-[dependencies.base64]
-version = "^0.10"
-optional = true
-
-[dependencies.bitflags]
-version = "1.0"
-
-[dependencies.bytes]
-version = "0.5"
-features = ["serde"]
-optional = true
-
-[dependencies.libc]
-version = "0.2"
-
-[dependencies.log]
-version = "0.4"
-
-[dependencies.rand]
-version = "0.7"
-
-[dependencies.runloop]
-version = "0.1.0"
-
-[dependencies.serde]
-version = "1.0"
-features = ["derive"]
-optional = true
-
-[dependencies.serde_json]
-version = "1.0"
-optional = true
-
-[dependencies.tokio]
-version = "0.2"
-features = ["macros"]
-optional = true
+license = "MPL-2.0"
+description = "Library for interacting with CTAP1/2 security keys for Web Authentication. Used by Firefox."
+edition = "2018"
 
-[dependencies.warp]
-version = "0.2.4"
-optional = true
-[dev-dependencies.assert_matches]
-version = "1.2"
+[badges]
+travis-ci = { repository = "mozilla/authenticator-rs", branch = "master" }
+maintenance = { status = "actively-developed" }
 
-[dev-dependencies.base64]
-version = "^0.10"
+[features]
+binding-recompile = ["bindgen"]
+webdriver = ["base64", "bytes", "warp", "tokio", "serde", "serde_json"]
 
-[dev-dependencies.env_logger]
-version = "^0.6"
+[target.'cfg(target_os = "linux")'.dependencies]
+libudev = "^0.2"
 
-[dev-dependencies.getopts]
-version = "^0.2"
+[target.'cfg(target_os = "freebsd")'.dependencies]
+devd-rs = "0.3"
 
-[dev-dependencies.sha2]
-version = "^0.8.2"
-[build-dependencies.bindgen]
-version = "^0.51"
-optional = true
+[target.'cfg(target_os = "macos")'.dependencies]
+core-foundation = "0.9"
 
-[features]
-binding-recompile = ["bindgen"]
-webdriver = ["base64", "bytes", "warp", "tokio", "serde", "serde_json"]
-[target."cfg(target_os = \"freebsd\")".dependencies.devd-rs]
-version = "0.3"
-[target."cfg(target_os = \"linux\")".dependencies.libudev]
-version = "^0.2"
-[target."cfg(target_os = \"macos\")".dependencies.core-foundation]
-version = "0.9"
-[target."cfg(target_os = \"windows\")".dependencies.winapi]
+[target.'cfg(target_os = "windows")'.dependencies.winapi]
 version = "^0.3"
-features = ["handleapi", "hidclass", "hidpi", "hidusage", "setupapi"]
-[badges.maintenance]
-status = "actively-developed"
-
-[badges.travis-ci]
-branch = "master"
-repository = "mozilla/authenticator-rs"
+features = [
+    "handleapi",
+    "hidclass",
+    "hidpi",
+    "hidusage",
+    "setupapi",
+]
+
+[build-dependencies]
+bindgen = { version = "^0.58.1", optional = true }
+
+[dependencies]
+rand = "0.7"
+log = "0.4"
+libc = "0.2"
+runloop = "0.1.0"
+bitflags = "1.0"
+tokio = { version = "0.2", optional = true, features = ["macros"] }
+warp = { version = "0.2.4", optional = true }
+serde = { version = "1.0", optional = true, features = ["derive"] }
+serde_json = { version = "1.0", optional = true }
+bytes = { version = "0.5", optional = true, features = ["serde"] }
+base64 = { version = "^0.10", optional = true }
+
+[dev-dependencies]
+sha2 = "^0.8.2"
+base64 = "^0.10"
+env_logger = "^0.6"
+getopts = "^0.2"
+assert_matches = "1.2"
diff --git a/third_party/rust/authenticator/build.rs b/third_party/rust/authenticator/build.rs
index 299e4df6d7331..c972d85b898ea 100644
--- a/third_party/rust/authenticator/build.rs
+++ b/third_party/rust/authenticator/build.rs
@@ -45,6 +45,8 @@ fn main() {
         "ioctl_aarch64be.rs"
     } else if cfg!(all(target_arch = "s390x", target_endian = "big")) {
         "ioctl_s390xbe.rs"
+    } else if cfg!(all(target_arch = "riscv64", target_endian = "little")) {
+        "ioctl_riscv64.rs"
     } else {
         panic!("architecture not supported");
     };
diff --git a/third_party/rust/authenticator/src/linux/hidwrapper.rs b/third_party/rust/authenticator/src/linux/hidwrapper.rs
index ea1a39051b63a..82aabc6301017 100644
--- a/third_party/rust/authenticator/src/linux/hidwrapper.rs
+++ b/third_party/rust/authenticator/src/linux/hidwrapper.rs
@@ -46,3 +46,6 @@ include!("ioctl_aarch64be.rs");
 
 #[cfg(all(target_arch = "s390x", target_endian = "big"))]
 include!("ioctl_s390xbe.rs");
+
+#[cfg(all(target_arch = "riscv64", target_endian = "little"))]
+include!("ioctl_riscv64.rs");
diff --git a/third_party/rust/authenticator/src/linux/ioctl_riscv64.rs b/third_party/rust/authenticator/src/linux/ioctl_riscv64.rs
new file mode 100644
index 0000000000000..a784e9bf4600b
--- /dev/null
+++ b/third_party/rust/authenticator/src/linux/ioctl_riscv64.rs
@@ -0,0 +1,5 @@
+/* automatically generated by rust-bindgen */
+
+pub type __u32 = ::std::os::raw::c_uint;
+pub const _HIDIOCGRDESCSIZE: __u32 = 2147764225;
+pub const _HIDIOCGRDESC: __u32 = 2416199682;
diff --git a/toolkit/library/rust/shared/Cargo.toml b/toolkit/library/rust/shared/Cargo.toml
index 47ea0f7c4e00a..2c50e360158d7 100644
--- a/toolkit/library/rust/shared/Cargo.toml
+++ b/toolkit/library/rust/shared/Cargo.toml
@@ -25,7 +25,7 @@ cubeb-sys = { version = "0.9", optional = true, features=["gecko-in-tree"] }
 encoding_glue = { path = "../../../../intl/encoding_glue" }
 audioipc-client = { git = "https://github.com/mozilla/audioipc-2", rev = "7537bfadad2e981577eb75e4f13662fc517e1a09", optional = true }
 audioipc-server = { git = "https://github.com/mozilla/audioipc-2", rev = "7537bfadad2e981577eb75e4f13662fc517e1a09", optional = true }
-authenticator = "0.3.1"
+authenticator = { git = "https://github.com/makotokato/authenticator-rs", rev = "eed8919d50559f4959e2d7d2af7b4d48869b5366" }
 gkrust_utils = { path = "../../../../xpcom/rust/gkrust_utils" }
 gecko_logger = { path = "../../../../xpcom/rust/gecko_logger" }
 rsdparsa_capi = { path = "../../../../dom/media/webrtc/sdp/rsdparsa_capi" }
From 12413f122e2ea2a29663f5c7e38bef3896b02d9c Mon Sep 17 00:00:00 2001
From: Makoto Kato <m_kato@ga2.so-net.ne.jp>
Date: Mon, 14 Jun 2021 08:05:37 +0000
Subject: [PATCH] Add XPTCAll stub

---
 xpcom/reflect/xptcall/md/unix/moz.build       |   8 +
 .../xptcall/md/unix/xptcinvoke_asm_riscv64.S  |  72 ++++++++
 .../xptcall/md/unix/xptcinvoke_riscv64.cpp    |  89 ++++++++++
 .../xptcall/md/unix/xptcstubs_asm_riscv64.S   |  48 ++++++
 .../xptcall/md/unix/xptcstubs_riscv64.cpp     | 154 ++++++++++++++++++
 5 files changed, 371 insertions(+)
 create mode 100644 xpcom/reflect/xptcall/md/unix/xptcinvoke_asm_riscv64.S
 create mode 100644 xpcom/reflect/xptcall/md/unix/xptcinvoke_riscv64.cpp
 create mode 100644 xpcom/reflect/xptcall/md/unix/xptcstubs_asm_riscv64.S
 create mode 100644 xpcom/reflect/xptcall/md/unix/xptcstubs_riscv64.cpp

diff --git a/xpcom/reflect/xptcall/md/unix/moz.build b/xpcom/reflect/xptcall/md/unix/moz.build
index 1083e26a2fde1..4af0b577bca86 100644
--- a/xpcom/reflect/xptcall/md/unix/moz.build
+++ b/xpcom/reflect/xptcall/md/unix/moz.build
@@ -263,6 +263,14 @@ if CONFIG["OS_ARCH"] == "Linux":
                 "-fno-integrated-as",
             ]
 
+if CONFIG["OS_ARCH"] == "Linux" and  CONFIG["CPU_ARCH"] == "riscv64":
+    SOURCES += [
+        "xptcinvoke_asm_riscv64.S",
+        "xptcinvoke_riscv64.cpp",
+        "xptcstubs_asm_riscv64.S",
+        "xptcstubs_riscv64.cpp",
+    ]
+
 FINAL_LIBRARY = "xul"
 
 LOCAL_INCLUDES += [
diff --git a/xpcom/reflect/xptcall/md/unix/xptcinvoke_asm_riscv64.S b/xpcom/reflect/xptcall/md/unix/xptcinvoke_asm_riscv64.S
new file mode 100644
index 0000000000000..4b27dd77884bf
--- /dev/null
+++ b/xpcom/reflect/xptcall/md/unix/xptcinvoke_asm_riscv64.S
@@ -0,0 +1,72 @@
+/* This Source Code Form is subject to the terms of the Mozilla Public
+ * License, v. 2.0. If a copy of the MPL was not distributed with this
+ * file, You can obtain one at http://mozilla.org/MPL/2.0/. */
+
+    .set NGPREGS,8
+    .set NFPREGS,8
+
+    .text
+    .globl  _NS_InvokeByIndex
+    .type   _NS_InvokeByIndex, @function
+/*
+ * _NS_InvokeByIndex(nsISupports* that, uint32_t methodIndex,
+ *                   uint32_t paramCount, nsXPTCVariant* params)
+ */
+_NS_InvokeByIndex:
+    addi    sp, sp, -32
+    sd      s0, 0(sp)
+    sd      s1, 8(sp)
+    sd      ra, 16(sp)
+
+    mv      s0, a0 
+    mv      s1, a1 
+    mv      s2, sp 
+    mv      a4, sp
+
+    # 16-bytes alignment
+    addiw   a0, a0, 1
+    andi    a0, a0, -2
+    slli    a0, a0, 3
+    sub     sp, sp, a0
+
+    addi    sp, sp, -8*(NGPREGS+NFPREGS)
+    mv      a0, sp
+    addi    a1, sp, 8*NGPREGS
+
+    # extern "C" void invoke_copy_to_stack(uint64_t* gpregs, double* fpregs,
+    #                                 uint32_t paramCount, nsXPTCVariant* s,
+    #                                 uint64_t* d) {
+
+    call    invoke_copy_to_stack
+    addi    sp, sp, 8*(NGPREGS+NFPREGS)
+
+    ld      a1, 8(s0)
+    ld      a2, 16(s0)
+    ld      a3, 24(s0)
+    ld      a4, 32(s0)
+    ld      a5, 40(s0)
+    ld      a6, 48(s0)
+    ld      a7, 56(s0)
+
+    fld     fa0, 64(s0)
+    fld     fa1, 72(s0)
+    fld     fa2, 80(s0)
+    fld     fa3, 88(s0)
+    fld     fa4, 96(s0)
+    fld     fa5, 104(s0)
+    fld     fa6, 112(s0)
+    fld     fa7, 120(s0)
+
+    ld      t0, (s0)
+    slli    t1, s1, 3
+    add     t0, t0, t1
+    ld      t0, [t0]
+    call    t0
+
+    mv      sp, s2
+    ld      s0, 0(sp)
+    ld      s1, 8(sp)
+    addi    sp, sp, 32
+    ret
+    .size   _NS_InvokeByIndex, .-_NS_InvokeByIndex
+    .section        .note.GNU-stack,"",@progbits
diff --git a/xpcom/reflect/xptcall/md/unix/xptcinvoke_riscv64.cpp b/xpcom/reflect/xptcall/md/unix/xptcinvoke_riscv64.cpp
new file mode 100644
index 0000000000000..0d88718802314
--- /dev/null
+++ b/xpcom/reflect/xptcall/md/unix/xptcinvoke_riscv64.cpp
@@ -0,0 +1,89 @@
+/* This Source Code Form is subject to the terms of the Mozilla Public
+ * License, v. 2.0. If a copy of the MPL was not distributed with this
+ * file, You can obtain one at http://mozilla.org/MPL/2.0/. */
+
+// Platform specific code to invoke XPCOM methods on native objects
+
+#include "xptcprivate.h"
+
+extern "C" void invoke_copy_to_stack(uint64_t* gpregs, double* fpregs,
+                                     uint32_t paramCount, nsXPTCVariant* s,
+                                     uint64_t* d) {
+  static const uint32_t GPR_COUNT = 8;
+  static const uint32_t FPR_COUNT = 8;
+
+  uint32_t nr_gpr = 0;
+  uint32_t nr_fpr = 0;
+  uint64_t value;
+
+  for (uint32_t i = 0; i < paramCount; i++, s++) {
+    if (s->IsIndirect()) {
+      value = (uint64_t)&s->val;
+    } else {
+      switch (s->type) {
+        case nsXPTType::T_FLOAT:
+          break;
+        case nsXPTType::T_DOUBLE:
+          break;
+        case nsXPTType::T_I8:
+          value = s->val.i8;
+          break;
+        case nsXPTType::T_I16:
+          value = s->val.i16;
+          break;
+        case nsXPTType::T_I32:
+          value = s->val.i32;
+          break;
+        case nsXPTType::T_I64:
+          value = s->val.i64;
+          break;
+        case nsXPTType::T_U8:
+          value = s->val.u8;
+          break;
+        case nsXPTType::T_U16:
+          value = s->val.u16;
+          break;
+        case nsXPTType::T_U32:
+          value = s->val.u32;
+          break;
+        case nsXPTType::T_U64:
+          value = s->val.u64;
+          break;
+        case nsXPTType::T_BOOL:
+          value = s->val.b;
+          break;
+        case nsXPTType::T_CHAR:
+          value = s->val.c;
+          break;
+        case nsXPTType::T_WCHAR:
+          value = s->val.wc;
+          break;
+        default:
+          value = (uint64_t)s->val.p;
+          break;
+      }
+    }
+
+    if (!s->IsIndirect() && s->type == nsXPTType::T_DOUBLE) {
+      if (nr_fpr < FPR_COUNT) {
+        fpregs[nr_fpr++] = s->val.d;
+      } else {
+        *((double*)d) = s->val.d;
+        d++;
+      }
+    } else if (!s->IsIndirect() && s->type == nsXPTType::T_FLOAT) {
+      if (nr_fpr < FPR_COUNT) {
+        fpregs[nr_fpr++] = s->val.d;
+      } else {
+        *((float*)d) = s->val.f;
+        d++;
+      }
+    } else {
+      if (nr_gpr < GPR_COUNT) {
+        gpregs[nr_gpr++] = value;
+      } else {
+        *d++ = value;
+      }
+    }
+  }
+}
diff --git a/xpcom/reflect/xptcall/md/unix/xptcstubs_asm_riscv64.S b/xpcom/reflect/xptcall/md/unix/xptcstubs_asm_riscv64.S
new file mode 100644
index 0000000000000..cd15f9fa47280
--- /dev/null
+++ b/xpcom/reflect/xptcall/md/unix/xptcstubs_asm_riscv64.S
@@ -0,0 +1,48 @@
+# This Source Code Form is subject to the terms of the Mozilla Public
+# License, v. 2.0. If a copy of the MPL was not distributed with this
+# file, You can obtain one at http://mozilla.org/MPL/2.0/.
+
+    .set NGPREGS,8
+    .set NFPREGS,8
+
+    .text
+    .globl SharedStub
+    .hidden SharedStub
+    .type  SharedStub,@function
+
+SharedStub:
+    .cfi_startproc
+    addi    sp, sp, -8*(NGPREGS+NFPREGS)-16
+    .cfi_adjust_cfa_offset 8*(NGPREGS+NFPREGS)+16
+    sd      a0, 0(sp)
+    sd      a1, 8(sp)
+    sd      a2, 16(sp)
+    sd      a3, 24(sp)
+    sd      a4, 32(sp)
+    sd      a5, 40(sp)
+    sd      a6, 48(sp)
+    sd      a7, 56(sp)
+    fsd     fa0, 64(sp)
+    fsd     fa1, 72(sp)
+    fsd     fa2, 80(sp)
+    fsd     fa3, 88(sp)
+    fsd     fa4, 96(sp)
+    fsd     fa5, 104(sp)
+    fsd     fa6, 112(sp)
+    fsd     fa7, 128(sp)
+
+    # methodIndex passed from stub
+    mv      a1, t0
+    addi    a2, sp, 8*(NGPREGS+NFPREGS)+16+8
+    mv      a3, sp
+    addi    a4, sp, 64
+
+    call    _PrepareAndDispatch
+
+    addi    sp, sp, 8*(NGPREGS+NFPREGS)+16
+    .cfi_adjust_cfa_offset -8*(NGPREGS+NFPREGS)-16
+    ret
+    .cfi_endproc
+
+    .size SharedStub, . - SharedStub
+    .section .note.GNU-stack, "", @progbits
diff --git a/xpcom/reflect/xptcall/md/unix/xptcstubs_riscv64.cpp b/xpcom/reflect/xptcall/md/unix/xptcstubs_riscv64.cpp
new file mode 100644
index 0000000000000..3b2de5c934d09
--- /dev/null
+++ b/xpcom/reflect/xptcall/md/unix/xptcstubs_riscv64.cpp
@@ -0,0 +1,154 @@
+/* -*- Mode: C; tab-width: 8; indent-tabs-mode: nil; c-basic-offset: 2 -*- */
+/* This Source Code Form is subject to the terms of the Mozilla Public
+ * License, v. 2.0. If a copy of the MPL was not distributed with this
+ * file, You can obtain one at http://mozilla.org/MPL/2.0/. */
+
+#include "xptcprivate.h"
+
+extern "C" nsresult ATTRIBUTE_USED PrepareAndDispatch(nsXPTCStubBase* self,
+                                                      uint32_t methodIndex,
+                                                      uint64_t* args,
+                                                      uint64_t* gpregs,
+                                                      double* fpregs) {
+  static const uint32_t GPR_COUNT = 8;
+  static const uint32_t FPR_COUNT = 8;
+  nsXPTCMiniVariant paramBuffer[PARAM_BUFFER_COUNT];
+  const nsXPTMethodInfo* info;
+
+  self->mEntry->GetMethodInfo(uint16_t(methodIndex), &info);
+
+  uint32_t paramCount = info->GetParamCount();
+  const uint8_t indexOfJSContext = info->IndexOfJSContext();
+
+  uint64_t* ap = args;
+  uint32_t nr_gpr = 1;  // skip one GPR register for 'that'
+  uint32_t nr_fpr = 0;
+  uint64_t value;
+
+  for (uint32_t i = 0; i < paramCount; i++) {
+    const nsXPTParamInfo& param = info->GetParam(i);
+    const nsXPTType& type = param.GetType();
+    nsXPTCMiniVariant* dp = &paramBuffer[i];
+
+    if (i == indexOfJSContext) {
+      if (nr_gpr < GPR_COUNT)
+        nr_gpr++;
+      else
+        ap++;
+    }
+
+    if (!param.IsOut() && type == nsXPTType::T_DOUBLE) {
+      if (nr_fpr < FPR_COUNT) {
+        dp->val.d = fpregs[nr_fpr++];
+      } else {
+        dp->val.d = *(double*)ap++;
+      }
+      continue;
+    }
+
+    if (!param.IsOut() && type == nsXPTType::T_FLOAT) {
+      if (nr_fpr < FPR_COUNT) {
+        dp->val.d = fpregs[nr_fpr++];
+      } else {
+        dp->val.f = *(float*)ap++;
+      }
+      continue;
+    }
+
+    if (nr_gpr < GPR_COUNT) {
+      value = gpregs[nr_gpr++];
+    } else {
+      value = *ap++;
+    }
+
+    if (param.IsOut() || !type.IsArithmetic()) {
+      dp->val.p = (void*)value;
+      continue;
+    }
+
+    switch (type) {
+      case nsXPTType::T_I8:
+        dp->val.i8 = (int8_t)value;
+        break;
+      case nsXPTType::T_I16:
+        dp->val.i16 = (int16_t)value;
+        break;
+      case nsXPTType::T_I32:
+        dp->val.i32 = (int32_t)value;
+        break;
+      case nsXPTType::T_I64:
+        dp->val.i64 = (int64_t)value;
+        break;
+      case nsXPTType::T_U8:
+        dp->val.u8 = (uint8_t)value;
+        break;
+      case nsXPTType::T_U16:
+        dp->val.u16 = (uint16_t)value;
+        break;
+      case nsXPTType::T_U32:
+        dp->val.u32 = (uint32_t)value;
+        break;
+      case nsXPTType::T_U64:
+        dp->val.u64 = (uint64_t)value;
+        break;
+      case nsXPTType::T_BOOL:
+        dp->val.b = (bool)(uint8_t)value;
+        break;
+      case nsXPTType::T_CHAR:
+        dp->val.c = (char)value;
+        break;
+      case nsXPTType::T_WCHAR:
+        dp->val.wc = (wchar_t)value;
+        break;
+      default:
+        NS_ERROR("bad type");
+        break;
+    }
+  }
+
+  nsresult result =
+      self->mOuter->CallMethod((uint16_t)methodIndex, info, paramBuffer);
+
+  return result;
+}
+
+// Load t0 with the constant 'n' and branch to SharedStub().
+#define STUB_ENTRY(n)                                               \
+  __asm__(                                                          \
+      ".text\n\t"                                                   \
+      ".if "#n" < 10 \n\t"                                          \
+      ".globl  _ZN14nsXPTCStubBase5Stub"#n"Ev \n\t"                                 \
+      ".hidden _ZN14nsXPTCStubBase5Stub" #n "Ev \n\t"               \
+      ".type   _ZN14nsXPTCStubBase5Stub" #n "Ev,@function \n\n"     \
+      "_ZN14nsXPTCStubBase5Stub"#n"Ev: \n\t"                        \
+      ".elseif "#n" < 100 \n\t"                                                     \
+      ".globl  _ZN14nsXPTCStubBase6Stub"#n"Ev \n\t"                                 \
+      ".hidden _ZN14nsXPTCStubBase6Stub"#n"Ev \n\t"                                 \
+      ".type   _ZN14nsXPTCStubBase6Stub"#n"Ev,@function \n\n"                       \
+      "_ZN14nsXPTCStubBase6Stub"#n"Ev: \n\t"                                        \
+      ".elseif "#n" < 1000 \n\t"                                                    \
+      ".globl  _ZN14nsXPTCStubBase7Stub"#n"Ev \n\t"                                 \
+      ".hidden _ZN14nsXPTCStubBase7Stub"#n"Ev \n\t"                                 \
+      ".type   _ZN14nsXPTCStubBase7Stub"#n"Ev,@function \n\n"                       \
+      "_ZN14nsXPTCStubBase7Stub"#n"Ev: \n\t"                                        \
+      ".else  \n\t"                                                                 \
+      ".err   \"stub number "#n" >= 1000 not yet supported\"\n"                     \
+      ".endif \n\t"                                                                 \
+      "li      t0, "#n" \n\t"                                                       \
+      "j       SharedStub \n"                                                       \
+      ".if "#n" < 10\n\t"                                                           \
+      ".size   _ZN14nsXPTCStubBase5Stub"#n"Ev,.-_ZN14nsXPTCStubBase5Stub"#n"Ev\n\t" \
+      ".elseif "#n" < 100\n\t"                                                      \
+      ".size   _ZN14nsXPTCStubBase6Stub"#n"Ev,.-_ZN14nsXPTCStubBase6Stub"#n"Ev\n\t" \
+      ".else\n\t"                                                                   \
+      ".size   _ZN14nsXPTCStubBase7Stub"#n"Ev,.-_ZN14nsXPTCStubBase7Stub"#n"Ev\n\t" \
+      ".endif"                                                                      \
+);
+
+#define SENTINEL_ENTRY(n)                        \
+  nsresult nsXPTCStubBase::Sentinel##n() {       \
+    NS_ERROR("nsXPTCStubBase::Sentinel called"); \
+    return NS_ERROR_NOT_IMPLEMENTED;             \
+  }
+
+#include "xptcstubsdef.inc"
From c20ab426530a74731bb9ea9288a6bdb93309c817 Mon Sep 17 00:00:00 2001
From: Makoto Kato <m_kato@ga2.so-net.ne.jp>
Date: Mon, 14 Jun 2021 08:06:04 +0000
Subject: [PATCH] NSS fix

---
 security/nss/coreconf/config.gypi | 2 +-
 1 file changed, 1 insertion(+), 1 deletion(-)

#diff --git a/security/nss/coreconf/config.gypi b/security/nss/coreconf/config.gypi
#index eec50ddbb74cd..5800f87915389 100644
#--- a/security/nss/coreconf/config.gypi
#+++ b/security/nss/coreconf/config.gypi
#@@ -209,7 +209,7 @@
#           },
#         },
#       }],
#-      [ 'target_arch=="arm64" or target_arch=="aarch64" or target_arch=="sparc64" or target_arch=="ppc64" or target_arch=="ppc64le" or target_arch=="s390x" or target_arch=="mips64" or target_arch=="e2k"', {
#+      [ 'target_arch=="arm64" or target_arch=="aarch64" or target_arch=="sparc64" or target_arch=="ppc64" or target_arch=="ppc64le" or target_arch=="s390x" or target_arch=="mips64" or target_arch=="e2k" or target_arch=="riscv64"', {
#         'defines': [
#           'NSS_USE_64',
#         ],
From 6a8fbc856157600fbbb2a696b300899d54cf7b65 Mon Sep 17 00:00:00 2001
From: Makoto Kato <m_kato@ga2.so-net.ne.jp>
Date: Mon, 14 Jun 2021 14:44:26 +0000
Subject: [PATCH] Update xptcall

---
 .../xptcall/md/unix/xptcinvoke_asm_riscv64.S  | 30 +++++++++++--------
 .../xptcall/md/unix/xptcinvoke_riscv64.cpp    |  4 +++
 .../xptcall/md/unix/xptcstubs_asm_riscv64.S   |  2 +-
 .../xptcall/md/unix/xptcstubs_riscv64.cpp     | 20 ++++++++-----
 4 files changed, 36 insertions(+), 20 deletions(-)

diff --git a/xpcom/reflect/xptcall/md/unix/xptcinvoke_asm_riscv64.S b/xpcom/reflect/xptcall/md/unix/xptcinvoke_asm_riscv64.S
index 4b27dd77884bf..e7d35d3aa7a9c 100644
--- a/xpcom/reflect/xptcall/md/unix/xptcinvoke_asm_riscv64.S
+++ b/xpcom/reflect/xptcall/md/unix/xptcinvoke_asm_riscv64.S
@@ -6,17 +6,20 @@
     .set NFPREGS,8
 
     .text
-    .globl  _NS_InvokeByIndex
-    .type   _NS_InvokeByIndex, @function
+    .globl  NS_InvokeByIndex
+    .type   NS_InvokeByIndex, @function
 /*
- * _NS_InvokeByIndex(nsISupports* that, uint32_t methodIndex,
- *                   uint32_t paramCount, nsXPTCVariant* params)
+ * NS_InvokeByIndex(nsISupports* that, uint32_t methodIndex,
+ *                  uint32_t paramCount, nsXPTCVariant* params)
  */
-_NS_InvokeByIndex:
+NS_InvokeByIndex:
+    .cfi_startproc
     addi    sp, sp, -32
+    .cfi_adjust_cfa_offset 32
     sd      s0, 0(sp)
     sd      s1, 8(sp)
-    sd      ra, 16(sp)
+    sd      s2, 16(sp)
+    sd      ra, 24(sp)
 
     mv      s0, a0 
     mv      s1, a1 
@@ -57,16 +60,19 @@ _NS_InvokeByIndex:
     fld     fa6, 112(s0)
     fld     fa7, 120(s0)
 
-    ld      t0, (s0)
-    slli    t1, s1, 3
-    add     t0, t0, t1
-    ld      t0, [t0]
-    call    t0
+    ld      s0, (s0)
+    slli    s1, s1, 3
+    add     s0, s0, s1
+    ld      s0, [s0]
+    call    s0
 
     mv      sp, s2
     ld      s0, 0(sp)
     ld      s1, 8(sp)
+    ld      s2, 8(sp)
+    ld      ra, 24(sp)
     addi    sp, sp, 32
     ret
-    .size   _NS_InvokeByIndex, .-_NS_InvokeByIndex
+    .cfi_endproc
+    .size   NS_InvokeByIndex, .-NS_InvokeByIndex
     .section        .note.GNU-stack,"",@progbits
diff --git a/xpcom/reflect/xptcall/md/unix/xptcinvoke_riscv64.cpp b/xpcom/reflect/xptcall/md/unix/xptcinvoke_riscv64.cpp
index 0d88718802314..369511d33a40b 100644
--- a/xpcom/reflect/xptcall/md/unix/xptcinvoke_riscv64.cpp
+++ b/xpcom/reflect/xptcall/md/unix/xptcinvoke_riscv64.cpp
@@ -4,6 +4,10 @@
 
 // Platform specific code to invoke XPCOM methods on native objects
 
+#if defined(__riscv_float_abi_soft)
+#  error "Not support soft float ABI"
+#endif
+
 #include "xptcprivate.h"
 
 extern "C" void invoke_copy_to_stack(uint64_t* gpregs, double* fpregs,
diff --git a/xpcom/reflect/xptcall/md/unix/xptcstubs_asm_riscv64.S b/xpcom/reflect/xptcall/md/unix/xptcstubs_asm_riscv64.S
index cd15f9fa47280..870abd05fa7ba 100644
--- a/xpcom/reflect/xptcall/md/unix/xptcstubs_asm_riscv64.S
+++ b/xpcom/reflect/xptcall/md/unix/xptcstubs_asm_riscv64.S
@@ -37,7 +37,7 @@ SharedStub:
     mv      a3, sp
     addi    a4, sp, 64
 
-    call    _PrepareAndDispatch
+    call    PrepareAndDispatch
 
     addi    sp, sp, 8*(NGPREGS+NFPREGS)+16
     .cfi_adjust_cfa_offset -8*(NGPREGS+NFPREGS)-16
diff --git a/xpcom/reflect/xptcall/md/unix/xptcstubs_riscv64.cpp b/xpcom/reflect/xptcall/md/unix/xptcstubs_riscv64.cpp
index 3b2de5c934d09..94c7b4ab8d089 100644
--- a/xpcom/reflect/xptcall/md/unix/xptcstubs_riscv64.cpp
+++ b/xpcom/reflect/xptcall/md/unix/xptcstubs_riscv64.cpp
@@ -3,6 +3,10 @@
  * License, v. 2.0. If a copy of the MPL was not distributed with this
  * file, You can obtain one at http://mozilla.org/MPL/2.0/. */
 
+#if defined(__riscv_float_abi_soft)
+#  error "Not support soft float ABI"
+#endif
+
 #include "xptcprivate.h"
 
 extern "C" nsresult ATTRIBUTE_USED PrepareAndDispatch(nsXPTCStubBase* self,
@@ -113,14 +117,15 @@ extern "C" nsresult ATTRIBUTE_USED PrepareAndDispatch(nsXPTCStubBase* self,
 }
 
 // Load t0 with the constant 'n' and branch to SharedStub().
-#define STUB_ENTRY(n)                                               \
-  __asm__(                                                          \
-      ".text\n\t"                                                   \
-      ".if "#n" < 10 \n\t"                                          \
+// clang-format off
+#define STUB_ENTRY(n)                                                               \
+  __asm__(                                                                          \
+      ".text\n\t"                                                                   \
+      ".if "#n" < 10 \n\t"                                                          \
       ".globl  _ZN14nsXPTCStubBase5Stub"#n"Ev \n\t"                                 \
-      ".hidden _ZN14nsXPTCStubBase5Stub" #n "Ev \n\t"               \
-      ".type   _ZN14nsXPTCStubBase5Stub" #n "Ev,@function \n\n"     \
-      "_ZN14nsXPTCStubBase5Stub"#n"Ev: \n\t"                        \
+      ".hidden _ZN14nsXPTCStubBase5Stub"#n"Ev \n\t"                                 \
+      ".type   _ZN14nsXPTCStubBase5Stub"#n"Ev,@function \n\n"                       \
+      "_ZN14nsXPTCStubBase5Stub"#n"Ev: \n\t"                                        \
       ".elseif "#n" < 100 \n\t"                                                     \
       ".globl  _ZN14nsXPTCStubBase6Stub"#n"Ev \n\t"                                 \
       ".hidden _ZN14nsXPTCStubBase6Stub"#n"Ev \n\t"                                 \
@@ -144,6 +149,7 @@ extern "C" nsresult ATTRIBUTE_USED PrepareAndDispatch(nsXPTCStubBase* self,
       ".size   _ZN14nsXPTCStubBase7Stub"#n"Ev,.-_ZN14nsXPTCStubBase7Stub"#n"Ev\n\t" \
       ".endif"                                                                      \
 );
+// clang-format on
 
 #define SENTINEL_ENTRY(n)                        \
   nsresult nsXPTCStubBase::Sentinel##n() {       \
From 75685ab28c13c481a737b26ea95668461b060fac Mon Sep 17 00:00:00 2001
From: Makoto Kato <m_kato@ga2.so-net.ne.jp>
Date: Mon, 14 Jun 2021 16:00:08 +0000
Subject: [PATCH] Fix highway

---
 third_party/highway/hwy/base.h | 4 ++--
 1 file changed, 2 insertions(+), 2 deletions(-)

diff --git a/third_party/highway/hwy/base.h b/third_party/highway/hwy/base.h
index d87eb34b8eb5d..75fe585e4c296 100644
--- a/third_party/highway/hwy/base.h
+++ b/third_party/highway/hwy/base.h
@@ -316,7 +316,7 @@ namespace hwy {
 #if HWY_ARCH_X86
 static constexpr HWY_MAYBE_UNUSED size_t kMaxVectorSize = 64;  // AVX-512
 #define HWY_ALIGN_MAX alignas(64)
-#elif HWY_ARCH_RVV
+#elif HWY_ARCH_RVV && defined(__riscv_vector)
 // Not actually an upper bound on the size, but this value prevents crossing a
 // 4K boundary (relevant on Andes).
 static constexpr HWY_MAYBE_UNUSED size_t kMaxVectorSize = 4096;
@@ -333,7 +333,7 @@ static constexpr HWY_MAYBE_UNUSED size_t kMaxVectorSize = 16;
 // by concatenating base type and bits.
 
 // RVV already has a builtin type and the GCC intrinsics require it.
-#if HWY_ARCH_RVV && HWY_COMPILER_GCC
+#if HWY_ARCH_RVV && HWY_COMPILER_GCC && defined(__riscv_vector)
 #define HWY_NATIVE_FLOAT16 1
 #else
 #define HWY_NATIVE_FLOAT16 0
From 7d664f7f9c198322ceaf0ce22f3f42367584d52b Mon Sep 17 00:00:00 2001
From: Makoto Kato <m_kato@ga2.so-net.ne.jp>
Date: Tue, 15 Jun 2021 11:25:59 +0000
Subject: [PATCH] More webrtc fix. But this isn't built yet.

---
 third_party/libwebrtc/webrtc/build/build_config.h | 3 +++
 third_party/libwebrtc/webrtc/typedefs.h           | 8 ++++++++
 2 files changed, 11 insertions(+)

#diff --git a/third_party/libwebrtc/webrtc/build/build_config.h b/third_party/libwebrtc/webrtc/build/build_config.h
#index a9bc015c48fcd..272a840180605 100644
#--- a/third_party/libwebrtc/webrtc/build/build_config.h
#+++ b/third_party/libwebrtc/webrtc/build/build_config.h
#@@ -171,6 +171,9 @@
# #define ARCH_CPU_ARM_FAMILY 1
# #define ARCH_CPU_ARM64 1
# #define ARCH_CPU_64_BITS 1
#+#elif defined(__riscv) && __riscv_xlen == 32
#+#define ARCH_CPU_RISCV 1
#+#define ARCH_CPU_32_BITS 1
# #elif defined(__riscv) && __riscv_xlen == 64
# #define ARCH_CPU_RISCV 1
# #define ARCH_CPU_64_BITS 1
diff --git a/third_party/libwebrtc/webrtc/typedefs.h b/third_party/libwebrtc/webrtc/typedefs.h
index 60095a926faa2..2b9ca31d6e47c 100644
--- a/third_party/libwebrtc/webrtc/typedefs.h
+++ b/third_party/libwebrtc/webrtc/typedefs.h
@@ -120,6 +120,14 @@
 #define WEBRTC_ARCH_32_BITS 1
 #define WEBRTC_ARCH_BIG_ENDIAN
 #define WEBRTC_BIG_ENDIAN
+#elif defined(__riscv) && __riscv_xlen == 64
+#define WEBRTC_ARCH_64_BITS
+#define WEBRTC_ARCH_LITTLE_ENDIAN
+#define WEBRTC_LITTLE_ENDIAN
+#elif defined(__riscv) && __riscv_xlen == 32
+#define WEBRTC_ARCH_32_BITS
+#define WEBRTC_ARCH_LITTLE_ENDIAN
+#define WEBRTC_LITTLE_ENDIAN
 #elif defined(__pnacl__)
 #define WEBRTC_ARCH_32_BITS
 #define WEBRTC_ARCH_LITTLE_ENDIAN
From 1058e1e849b584a462fa65b9c5d0b539244dd2b3 Mon Sep 17 00:00:00 2001
From: Makoto Kato <m_kato@ga2.so-net.ne.jp>
Date: Tue, 15 Jun 2021 11:26:30 +0000
Subject: [PATCH] Update xptcall.

---
 .../xptcall/md/unix/xptcinvoke_asm_riscv64.S  | 75 +++++++++++--------
 .../xptcall/md/unix/xptcstubs_asm_riscv64.S   | 16 +++-
 2 files changed, 58 insertions(+), 33 deletions(-)

diff --git a/xpcom/reflect/xptcall/md/unix/xptcinvoke_asm_riscv64.S b/xpcom/reflect/xptcall/md/unix/xptcinvoke_asm_riscv64.S
index e7d35d3aa7a9c..a0b65c5d92032 100644
--- a/xpcom/reflect/xptcall/md/unix/xptcinvoke_asm_riscv64.S
+++ b/xpcom/reflect/xptcall/md/unix/xptcinvoke_asm_riscv64.S
@@ -16,62 +16,77 @@ NS_InvokeByIndex:
     .cfi_startproc
     addi    sp, sp, -32
     .cfi_adjust_cfa_offset 32
-    sd      s0, 0(sp)
+    sd      s0, 16(sp)
+    .cfi_rel_offset s0, 16
     sd      s1, 8(sp)
-    sd      s2, 16(sp)
+    .cfi_rel_offset s1, 8
+    sd      s2, 0(sp)
+    .cfi_rel_offset s0, 0
     sd      ra, 24(sp)
+    .cfi_rel_offset ra, 24
 
     mv      s0, a0 
     mv      s1, a1 
     mv      s2, sp 
-    mv      a4, sp
+    .cfi_def_cfa_register s2
 
-    # 16-bytes alignment
-    addiw   a0, a0, 1
+    /* 16-bytes alignment */
+    addiw   a0, a2, 1
     andi    a0, a0, -2
     slli    a0, a0, 3
     sub     sp, sp, a0
+    mv      a4, sp
 
     addi    sp, sp, -8*(NGPREGS+NFPREGS)
     mv      a0, sp
     addi    a1, sp, 8*NGPREGS
 
-    # extern "C" void invoke_copy_to_stack(uint64_t* gpregs, double* fpregs,
-    #                                 uint32_t paramCount, nsXPTCVariant* s,
-    #                                 uint64_t* d) {
+    /*
+     *  void invoke_copy_to_stack(uint64_t* gpregs, double* fpregs,
+     *                           uint32_t paramCount, nsXPTCVariant* s,
+     *                           uint64_t* d);
+     */
 
     call    invoke_copy_to_stack
-    addi    sp, sp, 8*(NGPREGS+NFPREGS)
 
-    ld      a1, 8(s0)
-    ld      a2, 16(s0)
-    ld      a3, 24(s0)
-    ld      a4, 32(s0)
-    ld      a5, 40(s0)
-    ld      a6, 48(s0)
-    ld      a7, 56(s0)
+    ld      a0, 0(sp)
+    ld      a1, 8(sp)
+    ld      a2, 16(sp)
+    ld      a3, 24(sp)
+    ld      a4, 32(sp)
+    ld      a5, 40(sp)
+    ld      a6, 48(sp)
+    ld      a7, 56(sp)
 
-    fld     fa0, 64(s0)
-    fld     fa1, 72(s0)
-    fld     fa2, 80(s0)
-    fld     fa3, 88(s0)
-    fld     fa4, 96(s0)
-    fld     fa5, 104(s0)
-    fld     fa6, 112(s0)
-    fld     fa7, 120(s0)
+    fld     fa0, 64(sp)
+    fld     fa1, 72(sp)
+    fld     fa2, 80(sp)
+    fld     fa3, 88(sp)
+    fld     fa4, 96(sp)
+    fld     fa5, 104(sp)
+    fld     fa6, 112(sp)
+    fld     fa7, 120(sp)
+
+    addi    sp, sp, 8*(NGPREGS+NFPREGS)
 
-    ld      s0, (s0)
-    slli    s1, s1, 3
+    ld      s0, 0(s0)
+    slliw   s1, s1, 3
     add     s0, s0, s1
-    ld      s0, [s0]
-    call    s0
+    ld      t0, 0(s0)
+    jalr    t0
 
     mv      sp, s2
-    ld      s0, 0(sp)
+    .cfi_def_cfa_register sp
+    ld      s0, 16(sp)
+    .cfi_restore s0
     ld      s1, 8(sp)
-    ld      s2, 8(sp)
+    .cfi_restore s1
+    ld      s2, 0(sp)
+    .cfi_restore s2
     ld      ra, 24(sp)
+    .cfi_restore ra
     addi    sp, sp, 32
+    .cfi_adjust_cfa_offset -32
     ret
     .cfi_endproc
     .size   NS_InvokeByIndex, .-NS_InvokeByIndex
diff --git a/xpcom/reflect/xptcall/md/unix/xptcstubs_asm_riscv64.S b/xpcom/reflect/xptcall/md/unix/xptcstubs_asm_riscv64.S
index 870abd05fa7ba..83fdfcb4397e9 100644
--- a/xpcom/reflect/xptcall/md/unix/xptcstubs_asm_riscv64.S
+++ b/xpcom/reflect/xptcall/md/unix/xptcstubs_asm_riscv64.S
@@ -29,16 +29,26 @@ SharedStub:
     fsd     fa4, 96(sp)
     fsd     fa5, 104(sp)
     fsd     fa6, 112(sp)
-    fsd     fa7, 128(sp)
+    fsd     fa7, 120(sp)
+    sd      ra, 136(sp)
+    .cfi_rel_offset ra, 136
 
     # methodIndex passed from stub
     mv      a1, t0
-    addi    a2, sp, 8*(NGPREGS+NFPREGS)+16+8
+    addi    a2, sp, 8*(NGPREGS+NFPREGS)+16
     mv      a3, sp
-    addi    a4, sp, 64
+    addi    a4, sp, 8*NGPREGS
+
+    # nsresult PrepareAndDispatch(nsXPTCStubBase* self,
+    #                             uint32_t methodIndex,
+    #                             uint64_t* args,
+    #                             uint64_t* gpregs,
+    #                             double* fpregs);
 
     call    PrepareAndDispatch
 
+    ld      ra, 136(sp)
+    .cfi_restore ra
     addi    sp, sp, 8*(NGPREGS+NFPREGS)+16
     .cfi_adjust_cfa_offset -8*(NGPREGS+NFPREGS)-16
     ret
From 735487e921dfb35d5a52eb2ff5222d70ad0aefa7 Mon Sep 17 00:00:00 2001
From: Makoto Kato <m_kato@ga2.so-net.ne.jp>
Date: Wed, 16 Jun 2021 23:19:59 +0900
Subject: [PATCH] Add toolchain prefix to strip.

---
 build/autoconf/toolchain.m4             |  1 -
 build/moz.configure/toolchain.configure | 14 ++++++++++++++
 js/src/old-configure.in                 |  2 --
 old-configure.in                        |  2 --
 4 files changed, 14 insertions(+), 5 deletions(-)

diff --git a/build/autoconf/toolchain.m4 b/build/autoconf/toolchain.m4
index 2a8744610c89c..407d0aa499088 100644
--- a/build/autoconf/toolchain.m4
+++ b/build/autoconf/toolchain.m4
@@ -82,7 +82,6 @@ AC_PROG_CXX
 AC_CHECK_PROGS(RANLIB, "${TOOLCHAIN_PREFIX}ranlib", :)
 AC_CHECK_PROGS(AS, "${TOOLCHAIN_PREFIX}as", :)
 AC_CHECK_PROGS(LIPO, "${TOOLCHAIN_PREFIX}lipo", :)
-AC_CHECK_PROGS(STRIP, "${TOOLCHAIN_PREFIX}strip", :)
 AC_CHECK_PROGS(OTOOL, "${TOOLCHAIN_PREFIX}otool", :)
 AC_CHECK_PROGS(INSTALL_NAME_TOOL, "${TOOLCHAIN_PREFIX}install_name_tool", :)
 AC_CHECK_PROGS(OBJCOPY, "${TOOLCHAIN_PREFIX}objcopy", :)
diff --git a/build/moz.configure/toolchain.configure b/build/moz.configure/toolchain.configure
index 218e4ec049d49..69e7c93a3d2fc 100755
--- a/build/moz.configure/toolchain.configure
+++ b/build/moz.configure/toolchain.configure
@@ -2690,6 +2690,20 @@ def nm_names(toolchain_prefix, c_compiler):
 check_prog("NM", nm_names, paths=clang_search_path, when=target_is_linux)
 
 
+@depends(toolchain_prefix)
+def strip_names(toolchain_prefix):
+    return tuple('%s%s' % (p, "strip") for p in (toolchain_prefix or ()) + ('',))
+
+
+option(env='STRIP', nargs=1, help='Path to the strip program', when=target_is_unix)
+
+strip = check_prog('STRIP', strip_names,
+                   input='STRIP', paths=clang_search_path,
+                   allow_missing=True, when=target_is_unix)
+
+add_old_configure_assignment('STRIP', strip)
+
+
 option("--enable-cpp-rtti", help="Enable C++ RTTI")
 
 add_old_configure_assignment("_MOZ_USE_RTTI", "1", when="--enable-cpp-rtti")
diff --git a/js/src/old-configure.in b/js/src/old-configure.in
index 77652f67529ca..b8ded24428578 100644
--- a/js/src/old-configure.in
+++ b/js/src/old-configure.in
@@ -92,7 +92,6 @@ else
     AC_PROG_CC
     AC_PROG_CXX
     MOZ_PATH_PROGS(AS, $AS as, $CC)
-    AC_CHECK_PROGS(STRIP, strip, :)
 fi
 
 MOZ_TOOL_VARIABLES
@@ -515,7 +514,6 @@ case "$target" in
         WIN32_GUI_EXE_LDFLAGS=-mwindows
     else
         TARGET_COMPILER_ABI=msvc
-        STRIP='echo not_strip'
         # aarch64 doesn't support subsystems below 6.02
         if test "$CPU_ARCH" = "aarch64"; then
             WIN32_SUBSYSTEM_VERSION=6.02
diff --git a/old-configure.in b/old-configure.in
index de2642f71d0fd..223230c480d86 100644
--- a/old-configure.in
+++ b/old-configure.in
@@ -104,7 +104,6 @@ else
     esac
     AC_PROG_CXX
     MOZ_PATH_PROGS(AS, $AS as, $CC)
-    AC_CHECK_PROGS(STRIP, strip, :)
     AC_CHECK_PROGS(OTOOL, otool, :)
 fi
 
@@ -608,7 +607,6 @@ case "$target" in
         LDFLAGS="$LDFLAGS -Wl,--no-insert-timestamp"
     else
         TARGET_COMPILER_ABI=msvc
-        STRIP='echo not_strip'
         # aarch64 doesn't support subsystems below 6.02
         if test "$CPU_ARCH" = "aarch64"; then
             WIN32_SUBSYSTEM_VERSION=6.02
From fc9ed124c89e626cb078f67331feba5d5a3f5563 Mon Sep 17 00:00:00 2001
From: Makoto Kato <m_kato@ga2.so-net.ne.jp>
Date: Thu, 17 Jun 2021 19:49:59 +0900
Subject: [PATCH] Fix xptcinvoke

---
 xpcom/reflect/xptcall/md/unix/xptcinvoke_asm_riscv64.S | 4 +++-
 xpcom/reflect/xptcall/md/unix/xptcinvoke_riscv64.cpp   | 4 ++--
 2 files changed, 5 insertions(+), 3 deletions(-)

diff --git a/xpcom/reflect/xptcall/md/unix/xptcinvoke_asm_riscv64.S b/xpcom/reflect/xptcall/md/unix/xptcinvoke_asm_riscv64.S
index a0b65c5d92032..6cd4edafdc2d1 100644
--- a/xpcom/reflect/xptcall/md/unix/xptcinvoke_asm_riscv64.S
+++ b/xpcom/reflect/xptcall/md/unix/xptcinvoke_asm_riscv64.S
@@ -49,7 +49,9 @@ NS_InvokeByIndex:
 
     call    invoke_copy_to_stack
 
-    ld      a0, 0(sp)
+    /* this */
+    mv      a0, s0
+
     ld      a1, 8(sp)
     ld      a2, 16(sp)
     ld      a3, 24(sp)
diff --git a/xpcom/reflect/xptcall/md/unix/xptcinvoke_riscv64.cpp b/xpcom/reflect/xptcall/md/unix/xptcinvoke_riscv64.cpp
index 369511d33a40b..b4fa8bc404e87 100644
--- a/xpcom/reflect/xptcall/md/unix/xptcinvoke_riscv64.cpp
+++ b/xpcom/reflect/xptcall/md/unix/xptcinvoke_riscv64.cpp
@@ -16,9 +16,9 @@ extern "C" void invoke_copy_to_stack(uint64_t* gpregs, double* fpregs,
   static const uint32_t GPR_COUNT = 8;
   static const uint32_t FPR_COUNT = 8;
 
-  uint32_t nr_gpr = 0;
+  uint32_t nr_gpr = 1;  // skip one GPR register for "this"
   uint32_t nr_fpr = 0;
-  uint64_t value;
+  uint64_t value = 0;
 
   for (uint32_t i = 0; i < paramCount; i++, s++) {
     if (s->IsIndirect()) {
From a418c651c88cd2682c4cfe61e9f57b5389078c09 Mon Sep 17 00:00:00 2001
From: Makoto Kato <m_kato@ga2.so-net.ne.jp>
Date: Thu, 17 Jun 2021 21:50:49 +0900
Subject: [PATCH] signal handler

---
 js/src/wasm/WasmSignalHandlers.cpp | 9 +++++++++
 1 file changed, 9 insertions(+)

diff --git a/js/src/wasm/WasmSignalHandlers.cpp b/js/src/wasm/WasmSignalHandlers.cpp
index 37bc5a9c19273..856368019235d 100644
--- a/js/src/wasm/WasmSignalHandlers.cpp
+++ b/js/src/wasm/WasmSignalHandlers.cpp
@@ -154,6 +154,11 @@ using mozilla::DebugOnly;
 #    define R01_sig(p) ((p)->uc_mcontext.gp_regs[1])
 #    define R32_sig(p) ((p)->uc_mcontext.gp_regs[32])
 #  endif
+#  if defined(__linux__) && defined(__riscv) && __riscv_xlen == 64
+#    define EPC_sig(p) ((p)->uc_mcontext.__gregs[0])
+#    define X02_sig(p) ((p)->uc_mcontext.__gregs[2])
+#    define X08_sig(p) ((p)->uc_mcontext.__gregs[8])
+#  endif
 #elif defined(__NetBSD__)
 #  define EIP_sig(p) ((p)->uc_mcontext.__gregs[_REG_EIP])
 #  define EBP_sig(p) ((p)->uc_mcontext.__gregs[_REG_EBP])
@@ -395,6 +400,10 @@ struct macos_aarch64_context {
 #  define PC_sig(p) R32_sig(p)
 #  define SP_sig(p) R01_sig(p)
 #  define FP_sig(p) R01_sig(p)
+#elif defined(__riscv) && __riscv_xlen == 64
+#  define PC_sig(p) EPC_sig(p)
+#  define SP_sig(p) X02_sig(p)
+#  define FP_sig(p) X08_sig(p)
 #endif
 
 static void SetContextPC(CONTEXT* context, uint8_t* pc) {
From 96b8edb87c25998af2c89a82e010c480244b8ab9 Mon Sep 17 00:00:00 2001
From: Makoto Kato <m_kato@ga2.so-net.ne.jp>
Date: Thu, 17 Jun 2021 23:37:12 +0900
Subject: [PATCH] Avoid _Unwind_Backtrace crash.

---
 .../xptcall/md/unix/xptcinvoke_asm_riscv64.S  | 32 +++++++++----------
 .../xptcall/md/unix/xptcinvoke_riscv64.cpp    | 10 ++++++
 2 files changed, 26 insertions(+), 16 deletions(-)

diff --git a/xpcom/reflect/xptcall/md/unix/xptcinvoke_asm_riscv64.S b/xpcom/reflect/xptcall/md/unix/xptcinvoke_asm_riscv64.S
index 6cd4edafdc2d1..b2ad16169b467 100644
--- a/xpcom/reflect/xptcall/md/unix/xptcinvoke_asm_riscv64.S
+++ b/xpcom/reflect/xptcall/md/unix/xptcinvoke_asm_riscv64.S
@@ -6,13 +6,13 @@
     .set NFPREGS,8
 
     .text
-    .globl  NS_InvokeByIndex
-    .type   NS_InvokeByIndex, @function
+    .globl  _NS_InvokeByIndex
+    .type   _NS_InvokeByIndex, @function
 /*
  * NS_InvokeByIndex(nsISupports* that, uint32_t methodIndex,
  *                  uint32_t paramCount, nsXPTCVariant* params)
  */
-NS_InvokeByIndex:
+_NS_InvokeByIndex:
     .cfi_startproc
     addi    sp, sp, -32
     .cfi_adjust_cfa_offset 32
@@ -21,14 +21,14 @@ NS_InvokeByIndex:
     sd      s1, 8(sp)
     .cfi_rel_offset s1, 8
     sd      s2, 0(sp)
-    .cfi_rel_offset s0, 0
+    .cfi_rel_offset s2, 0
     sd      ra, 24(sp)
     .cfi_rel_offset ra, 24
 
-    mv      s0, a0 
-    mv      s1, a1 
-    mv      s2, sp 
-    .cfi_def_cfa_register s2
+    mv      s2, a0
+    mv      s1, a1
+    mv      s0, sp
+    .cfi_def_cfa_register s0
 
     /* 16-bytes alignment */
     addiw   a0, a2, 1
@@ -49,8 +49,8 @@ NS_InvokeByIndex:
 
     call    invoke_copy_to_stack
 
-    /* this */
-    mv      a0, s0
+    /* 1st argument is this */
+    mv      a0, s2
 
     ld      a1, 8(sp)
     ld      a2, 16(sp)
@@ -71,13 +71,13 @@ NS_InvokeByIndex:
 
     addi    sp, sp, 8*(NGPREGS+NFPREGS)
 
-    ld      s0, 0(s0)
+    ld      s2, 0(s2)
     slliw   s1, s1, 3
-    add     s0, s0, s1
-    ld      t0, 0(s0)
+    add     s2, s2, s1
+    ld      t0, 0(s2)
     jalr    t0
 
-    mv      sp, s2
+    mv      sp, s0
     .cfi_def_cfa_register sp
     ld      s0, 16(sp)
     .cfi_restore s0
@@ -91,5 +91,5 @@ NS_InvokeByIndex:
     .cfi_adjust_cfa_offset -32
     ret
     .cfi_endproc
-    .size   NS_InvokeByIndex, .-NS_InvokeByIndex
-    .section        .note.GNU-stack,"",@progbits
+    .size   _NS_InvokeByIndex, . -_NS_InvokeByIndex
+    .section .note.GNU-stack, "", @progbits
diff --git a/xpcom/reflect/xptcall/md/unix/xptcinvoke_riscv64.cpp b/xpcom/reflect/xptcall/md/unix/xptcinvoke_riscv64.cpp
index b4fa8bc404e87..6ff8f28a274bf 100644
--- a/xpcom/reflect/xptcall/md/unix/xptcinvoke_riscv64.cpp
+++ b/xpcom/reflect/xptcall/md/unix/xptcinvoke_riscv64.cpp
@@ -91,3 +91,13 @@ extern "C" void invoke_copy_to_stack(uint64_t* gpregs, double* fpregs,
     }
   }
 }
+
+extern "C" nsresult _NS_InvokeByIndex(nsISupports* that, uint32_t methodIndex,
+                                      uint32_t paramCount,
+                                      nsXPTCVariant* params);
+
+EXPORT_XPCOM_API(nsresult)
+NS_InvokeByIndex(nsISupports* that, uint32_t methodIndex, uint32_t paramCount,
+                 nsXPTCVariant* params) {
+  return _NS_InvokeByIndex(that, methodIndex, paramCount, params);
+}
From 145f6968c46bf805a5213a606a1df98fcaabc2f4 Mon Sep 17 00:00:00 2001
From: Makoto Kato <m_kato@ga2.so-net.ne.jp>
Date: Sun, 20 Jun 2021 13:59:10 +0900
Subject: [PATCH] Don't use libwebrtc include path without MOZ_WEBRTC.

---
 dom/bindings/moz.build    | 8 ++++++--
 dom/media/gtest/moz.build | 8 ++++++--
 ipc/glue/moz.build        | 8 ++++++--
 3 files changed, 18 insertions(+), 6 deletions(-)

diff --git a/dom/bindings/moz.build b/dom/bindings/moz.build
index ca13276f98c86..8b0ef9d816117 100644
--- a/dom/bindings/moz.build
+++ b/dom/bindings/moz.build
@@ -94,12 +94,16 @@ LOCAL_INCLUDES += [
     "/layout/xul/tree",
     "/media/webrtc/",
     "/netwerk/base/",
-    "/third_party/libwebrtc",
-    "/third_party/libwebrtc/webrtc",
 ]
 
 LOCAL_INCLUDES += ["/third_party/msgpack/include"]
 
+if CONFIG["MOZ_WEBRTC"]:
+    LOCAL_INCLUDES += [
+        "/third_party/libwebrtc",
+        "/third_party/libwebrtc/webrtc",
+    ]
+
 DEFINES["GOOGLE_PROTOBUF_NO_RTTI"] = True
 DEFINES["GOOGLE_PROTOBUF_NO_STATIC_INITIALIZER"] = True
 
diff --git a/dom/media/gtest/moz.build b/dom/media/gtest/moz.build
index aaa5e45972eef..25c457c702636 100644
--- a/dom/media/gtest/moz.build
+++ b/dom/media/gtest/moz.build
@@ -11,10 +11,14 @@ DEFINES["ENABLE_SET_CUBEB_BACKEND"] = True
 LOCAL_INCLUDES += [
     "/dom/media/mediasink",
     "/dom/media/webrtc/common/",
-    "/third_party/libwebrtc",
-    "/third_party/libwebrtc/webrtc",
 ]
 
+if CONFIG["MOZ_WEBRTC"]:
+    LOCAL_INCLUDES += [
+        "/third_party/libwebrtc",
+        "/third_party/libwebrtc/webrtc",
+    ]
+
 UNIFIED_SOURCES += [
     "MockCubeb.cpp",
     "MockMediaResource.cpp",
diff --git a/ipc/glue/moz.build b/ipc/glue/moz.build
index 569ca15046bf1..2b3a1167267be 100644
--- a/ipc/glue/moz.build
+++ b/ipc/glue/moz.build
@@ -207,11 +207,15 @@ LOCAL_INCLUDES += [
     "/dom/indexedDB",
     "/dom/storage",
     "/netwerk/base",
-    "/third_party/libwebrtc",
-    "/third_party/libwebrtc/webrtc",
     "/xpcom/build",
 ]
 
+if CONFIG["MOZ_WEBRTC"]:
+    LOCAL_INCLUDES += [
+        "/third_party/libwebrtc",
+        "/third_party/libwebrtc/webrtc",
+    ]
+
 IPDL_SOURCES = [
     "InputStreamParams.ipdlh",
     "IPCStream.ipdlh",
From 575f97d84204fa8b2a5c3599ef6b0b6b85d73b3f Mon Sep 17 00:00:00 2001
From: Makoto Kato <m_kato@ga2.so-net.ne.jp>
Date: Sun, 20 Jun 2021 14:03:13 +0900
Subject: [PATCH] Update build_config.h

---
 ipc/chromium/src/build/build_config.h            |  5 +----
 security/sandbox/chromium/build/build_config.h   | 16 ++++------------
 .../libwebrtc/webrtc/build/build_config.h        |  5 +----
 3 files changed, 6 insertions(+), 20 deletions(-)

diff --git a/ipc/chromium/src/build/build_config.h b/ipc/chromium/src/build/build_config.h
index 7d0de63d6f6b9..de26701b2d716 100644
--- a/ipc/chromium/src/build/build_config.h
+++ b/ipc/chromium/src/build/build_config.h
@@ -122,11 +122,8 @@
 #  define ARCH_CPU_ARM_FAMILY 1
 #  define ARCH_CPU_ARM64 1
 #  define ARCH_CPU_64_BITS 1
-#elif defined(__riscv) && __riscv_xlen == 32
-#  define ARCH_CPU_RISCV 1
-#  define ARCH_CPU_32_BITS 1
 #elif defined(__riscv) && __riscv_xlen == 64
-#  define ARCH_CPU_RISCV 1
+#  define ARCH_CPU_RISCV64 1
 #  define ARCH_CPU_64_BITS 1
 #else
 #  error Please add support for your architecture in build/build_config.h
# diff --git a/security/sandbox/chromium/build/build_config.h b/security/sandbox/chromium/build/build_config.h
# index ca9461e240d23..48109c4d46af3 100644
# --- a/security/sandbox/chromium/build/build_config.h
# +++ b/security/sandbox/chromium/build/build_config.h
# @@ -166,18 +166,10 @@
#  #define ARCH_CPU_32_BITS 1
#  #define ARCH_CPU_BIG_ENDIAN 1
#  #endif
# -#elif defined(__riscv)
# -#if __riscv_xlen == 32
# -#  define ARCH_CPU_RISCV_FAMILY 1
# -#  define ARCH_CPU_RISCV32 1
# -#  define ARCH_CPU_32_BITS 1
# -#  define ARCH_CPU_LITTLE_ENDIAN 1
# -#elif __riscv_xlen == 64
# -#  define ARCH_CPU_RISCV_FAMILY 1
# -#  define ARCH_CPU_RISCV64 1
# -#  define ARCH_CPU_64_BITS 1
# -#  define ARCH_CPU_LITTLE_ENDIAN 1
# -#endif
# +#elif defined(__riscv) && __riscv_xlen == 64
# +#define ARCH_CPU_RISCV64 1
# +#define ARCH_CPU_64_BITS 1
# +#define ARCH_CPU_LITTLE_ENDIAN 1
#  #else
#  #error Please add support for your architecture in build/build_config.h
#  #endif
#diff --git a/third_party/libwebrtc/webrtc/build/build_config.h b/third_party/libwebrtc/webrtc/build/build_config.h
#index 272a840180605..afb82ecd602c1 100644
#--- a/third_party/libwebrtc/webrtc/build/build_config.h
#+++ b/third_party/libwebrtc/webrtc/build/build_config.h
#@@ -171,8 +171,8 @@
# #define ARCH_CPU_ARM_FAMILY 1
# #define ARCH_CPU_ARM64 1
# #define ARCH_CPU_64_BITS 1
# #elif defined(__riscv) && __riscv_xlen == 64
#-#define ARCH_CPU_RISCV 1
#+#define ARCH_CPU_RISCV64 1
# #define ARCH_CPU_64_BITS 1
# #else
# #error Please add support for your architecture in build/build_config.h
From 201a107ca4300a19a6d0a8e718ee1560e7d85731 Mon Sep 17 00:00:00 2001
From: Makoto Kato <m_kato@ga2.so-net.ne.jp>
Date: Sun, 20 Jun 2021 14:11:49 +0900
Subject: [PATCH] Update comment of xptcall

---
 .../xptcall/md/unix/xptcinvoke_asm_riscv64.S     | 16 +++++-----------
 .../xptcall/md/unix/xptcstubs_asm_riscv64.S      | 15 +++++----------
 2 files changed, 10 insertions(+), 21 deletions(-)

diff --git a/xpcom/reflect/xptcall/md/unix/xptcinvoke_asm_riscv64.S b/xpcom/reflect/xptcall/md/unix/xptcinvoke_asm_riscv64.S
index b2ad16169b467..46065232962cd 100644
--- a/xpcom/reflect/xptcall/md/unix/xptcinvoke_asm_riscv64.S
+++ b/xpcom/reflect/xptcall/md/unix/xptcinvoke_asm_riscv64.S
@@ -2,15 +2,15 @@
  * License, v. 2.0. If a copy of the MPL was not distributed with this
  * file, You can obtain one at http://mozilla.org/MPL/2.0/. */
 
-    .set NGPREGS,8
-    .set NFPREGS,8
+    .set NGPREGS, 8
+    .set NFPREGS, 8
 
     .text
     .globl  _NS_InvokeByIndex
     .type   _NS_InvokeByIndex, @function
 /*
- * NS_InvokeByIndex(nsISupports* that, uint32_t methodIndex,
- *                  uint32_t paramCount, nsXPTCVariant* params)
+ * _NS_InvokeByIndex(nsISupports* that, uint32_t methodIndex,
+ *                   uint32_t paramCount, nsXPTCVariant* params)
  */
 _NS_InvokeByIndex:
     .cfi_startproc
@@ -41,12 +41,6 @@ _NS_InvokeByIndex:
     mv      a0, sp
     addi    a1, sp, 8*NGPREGS
 
-    /*
-     *  void invoke_copy_to_stack(uint64_t* gpregs, double* fpregs,
-     *                           uint32_t paramCount, nsXPTCVariant* s,
-     *                           uint64_t* d);
-     */
-
     call    invoke_copy_to_stack
 
     /* 1st argument is this */
@@ -91,5 +85,5 @@ _NS_InvokeByIndex:
     .cfi_adjust_cfa_offset -32
     ret
     .cfi_endproc
-    .size   _NS_InvokeByIndex, . -_NS_InvokeByIndex
+    .size   _NS_InvokeByIndex, . - _NS_InvokeByIndex
     .section .note.GNU-stack, "", @progbits
diff --git a/xpcom/reflect/xptcall/md/unix/xptcstubs_asm_riscv64.S b/xpcom/reflect/xptcall/md/unix/xptcstubs_asm_riscv64.S
index 83fdfcb4397e9..02bb812d5997e 100644
--- a/xpcom/reflect/xptcall/md/unix/xptcstubs_asm_riscv64.S
+++ b/xpcom/reflect/xptcall/md/unix/xptcstubs_asm_riscv64.S
@@ -2,8 +2,8 @@
 # License, v. 2.0. If a copy of the MPL was not distributed with this
 # file, You can obtain one at http://mozilla.org/MPL/2.0/.
 
-    .set NGPREGS,8
-    .set NFPREGS,8
+    .set NGPREGS, 8
+    .set NFPREGS, 8
 
     .text
     .globl SharedStub
@@ -12,6 +12,7 @@
 
 SharedStub:
     .cfi_startproc
+    mv      t1, sp
     addi    sp, sp, -8*(NGPREGS+NFPREGS)-16
     .cfi_adjust_cfa_offset 8*(NGPREGS+NFPREGS)+16
     sd      a0, 0(sp)
@@ -33,18 +34,12 @@ SharedStub:
     sd      ra, 136(sp)
     .cfi_rel_offset ra, 136
 
-    # methodIndex passed from stub
+    /* methodIndex is passed from stub */
     mv      a1, t0
-    addi    a2, sp, 8*(NGPREGS+NFPREGS)+16
+    mv      a2, t1
     mv      a3, sp
     addi    a4, sp, 8*NGPREGS
 
-    # nsresult PrepareAndDispatch(nsXPTCStubBase* self,
-    #                             uint32_t methodIndex,
-    #                             uint64_t* args,
-    #                             uint64_t* gpregs,
-    #                             double* fpregs);
-
     call    PrepareAndDispatch
 
     ld      ra, 136(sp)
From 3add6a1ff92a51951e3dea8c7ce92af996336bcd Mon Sep 17 00:00:00 2001
From: Makoto Kato <m_kato@ga2.so-net.ne.jp>
Date: Tue, 10 Aug 2021 21:42:25 +0900
Subject: [PATCH] Update highway to 0.14.0

---
 media/highway/moz.yaml                        |   86 +-
 third_party/highway/BUILD                     |   38 +-
 third_party/highway/CMakeLists.txt            |   57 +-
 third_party/highway/CMakeLists.txt.in         |    2 +-
 third_party/highway/README.md                 |   87 +-
 third_party/highway/debian/changelog          |   16 +
 third_party/highway/debian/rules              |    0
 third_party/highway/hwy/aligned_allocator.cc  |   15 +-
 .../highway/hwy/aligned_allocator_test.cc     |   14 +-
 third_party/highway/hwy/base.h                |  293 +-
 third_party/highway/hwy/base_test.cc          |   20 +
 third_party/highway/hwy/cache_control.h       |   30 +-
 .../highway/hwy/contrib/image/image.cc        |    2 +-
 .../highway/hwy/contrib/image/image_test.cc   |   10 +-
 .../highway/hwy/contrib/math/math-inl.h       |  242 +-
 .../highway/hwy/contrib/math/math_test.cc     |   41 +-
 .../highway/hwy/detect_compiler_arch.h        |  188 ++
 third_party/highway/hwy/detect_targets.h      |  386 +++
 third_party/highway/hwy/examples/benchmark.cc |    2 +-
 third_party/highway/hwy/examples/skeleton.cc  |    2 +-
 .../highway/hwy/examples/skeleton_test.cc     |    8 +
 third_party/highway/hwy/foreach_target.h      |   24 +-
 third_party/highway/hwy/highway.h             |   90 +-
 third_party/highway/hwy/highway_test.cc       |   66 +-
 third_party/highway/hwy/nanobenchmark.cc      |    6 +-
 third_party/highway/hwy/nanobenchmark_test.cc |   17 +-
 third_party/highway/hwy/ops/arm_neon-inl.h    | 2262 +++++++++------
 third_party/highway/hwy/ops/arm_sve-inl.h     | 2312 ++++++++-------
 third_party/highway/hwy/ops/generic_ops-inl.h |  324 +++
 third_party/highway/hwy/ops/rvv-inl.h         |  742 +++--
 third_party/highway/hwy/ops/scalar-inl.h      |  441 +--
 third_party/highway/hwy/ops/set_macros-inl.h  |  109 +-
 third_party/highway/hwy/ops/shared-inl.h      |   11 +
 third_party/highway/hwy/ops/wasm_128-inl.h    | 1372 +++++----
 third_party/highway/hwy/ops/x86_128-inl.h     | 2485 +++++++++++------
 third_party/highway/hwy/ops/x86_256-inl.h     | 1031 ++++---
 third_party/highway/hwy/ops/x86_512-inl.h     | 1176 +++++---
 third_party/highway/hwy/targets.cc            |  134 +-
 third_party/highway/hwy/targets.h             |  317 +--
 third_party/highway/hwy/targets_test.cc       |   14 +
 .../highway/hwy/tests/arithmetic_test.cc      |  170 +-
 .../highway/hwy/tests/blockwise_test.cc       |  655 +++++
 third_party/highway/hwy/tests/combine_test.cc |  192 +-
 third_party/highway/hwy/tests/compare_test.cc |   46 +-
 third_party/highway/hwy/tests/convert_test.cc |  187 +-
 third_party/highway/hwy/tests/crypto_test.cc  |  549 ++++
 third_party/highway/hwy/tests/logical_test.cc |  516 +---
 third_party/highway/hwy/tests/mask_test.cc    |  435 +++
 third_party/highway/hwy/tests/memory_test.cc  |   25 +-
 third_party/highway/hwy/tests/swizzle_test.cc |  658 ++---
 third_party/highway/hwy/tests/test_util-inl.h |  238 +-
 .../highway/hwy/tests/test_util_test.cc       |    8 +
 third_party/highway/run_tests.sh              |    0
 third_party/highway/test.py                   |  131 -
 54 files changed, 11454 insertions(+), 6828 deletions(-)
 mode change 100644 => 100755 third_party/highway/debian/rules
 create mode 100644 third_party/highway/hwy/detect_compiler_arch.h
 create mode 100644 third_party/highway/hwy/detect_targets.h
 create mode 100644 third_party/highway/hwy/ops/generic_ops-inl.h
 create mode 100644 third_party/highway/hwy/tests/blockwise_test.cc
 create mode 100644 third_party/highway/hwy/tests/crypto_test.cc
 create mode 100644 third_party/highway/hwy/tests/mask_test.cc
 mode change 100644 => 100755 third_party/highway/run_tests.sh
 delete mode 100644 third_party/highway/test.py

diff --git a/media/highway/moz.yaml b/media/highway/moz.yaml
index 2381fb69b4f3a..a6d84aef91dbd 100644
--- a/media/highway/moz.yaml
+++ b/media/highway/moz.yaml
@@ -1,43 +1,43 @@
-# Version of this schema
-schema: 1
-
-bugzilla:
-  # Bugzilla product and component for this directory and subdirectories
-  product: Core
-  component: "ImageLib"
-
-# Document the source of externally hosted code
-origin:
-
-  # Short name of the package/library
-  name: highway
-
-  description: Performance-portable, length-agnostic SIMD with runtime dispatch
-
-  # Full URL for the package's homepage/etc
-  # Usually different from repository url
-  url: https://github.com/google/highway
-
-  # Human-readable identifier for this version/release
-  # Generally "version NNN", "tag SSS", "bookmark SSS"
-  release: commit d9882104caf9fb060d328d62cac9ce0a05a191db (2021-05-21T10:02:16Z).
-
-  # Revision to pull in
-  # Must be a long or short commit SHA (long preferred)
-  revision: d9882104caf9fb060d328d62cac9ce0a05a191db
-
-  # The package's license, where possible using the mnemonic from
-  # https://spdx.org/licenses/
-  # Multiple licenses can be specified (as a YAML list)
-  # A "LICENSE" file must exist containing the full license text
-  license: Apache-2.0
-
-  license-file: LICENSE
-
-vendoring:
-  url: https://github.com/google/highway.git
-  source-hosting: github
-  vendor-directory: third_party/highway
-
-  exclude:
-    - g3doc/
+# Version of this schema
+schema: 1
+
+bugzilla:
+  # Bugzilla product and component for this directory and subdirectories
+  product: Core
+  component: "ImageLib"
+
+# Document the source of externally hosted code
+origin:
+
+  # Short name of the package/library
+  name: highway
+
+  description: Performance-portable, length-agnostic SIMD with runtime dispatch
+
+  # Full URL for the package's homepage/etc
+  # Usually different from repository url
+  url: https://github.com/google/highway
+
+  # Human-readable identifier for this version/release
+  # Generally "version NNN", "tag SSS", "bookmark SSS"
+  release: commit 32e48580c1c82bd6233a10dce6021c0f583237d5 (2021-07-29T13:12:33Z).
+
+  # Revision to pull in
+  # Must be a long or short commit SHA (long preferred)
+  revision: 32e48580c1c82bd6233a10dce6021c0f583237d5
+
+  # The package's license, where possible using the mnemonic from
+  # https://spdx.org/licenses/
+  # Multiple licenses can be specified (as a YAML list)
+  # A "LICENSE" file must exist containing the full license text
+  license: Apache-2.0
+
+  license-file: LICENSE
+
+vendoring:
+  url: https://github.com/google/highway.git
+  source-hosting: github
+  vendor-directory: third_party/highway
+
+  exclude:
+    - g3doc/
diff --git a/third_party/highway/BUILD b/third_party/highway/BUILD
index e38101f41d661..a846a1d2d6561 100644
--- a/third_party/highway/BUILD
+++ b/third_party/highway/BUILD
@@ -41,6 +41,13 @@ selects.config_setting_group(
     ],
 )
 
+config_setting(
+    name = "emulate_sve",
+    values = {
+        "copt": "-DHWY_EMULATE_SVE",
+    },
+)
+
 # Additional warnings for Clang OR GCC (skip for MSVC)
 CLANG_GCC_COPTS = [
     "-Werror",
@@ -76,11 +83,10 @@ COPTS = select({
     "//conditions:default": CLANG_GCC_COPTS + CLANG_ONLY_COPTS,
 })
 
-# Unused on Bazel builds, where these are not defined/known; Copybara replaces
-# usages of this with an empty list.
+# Unused on Bazel builds, where this is not defined/known; Copybara replaces
+# usages with an empty list.
 COMPAT = [
-    "//buildenv/target:mobile",
-    "//buildenv/target:vendor",
+    "//buildenv/target:non_prod",  # includes mobile/vendor.
 ]
 
 # WARNING: changing flags such as HWY_DISABLED_TARGETS may break users without
@@ -94,19 +100,23 @@ cc_library(
         "hwy/aligned_allocator.cc",
         "hwy/targets.cc",
     ],
+    # Normal headers with include guards
     hdrs = [
         "hwy/aligned_allocator.h",
         "hwy/base.h",
         "hwy/cache_control.h",
-        "hwy/highway.h",
+        "hwy/detect_compiler_arch.h",  # private
+        "hwy/detect_targets.h",  # private
         "hwy/targets.h",
     ],
     compatible_with = [],
     copts = COPTS,
     textual_hdrs = [
+        "hwy/highway.h",  # public
         "hwy/foreach_target.h",  # public
         "hwy/ops/arm_neon-inl.h",
         "hwy/ops/arm_sve-inl.h",
+        "hwy/ops/generic_ops-inl.h",
         "hwy/ops/rvv-inl.h",
         "hwy/ops/scalar-inl.h",
         "hwy/ops/set_macros-inl.h",
@@ -116,6 +126,10 @@ cc_library(
         "hwy/ops/x86_256-inl.h",
         "hwy/ops/x86_512-inl.h",
     ],
+    deps = select({
+        ":emulate_sve": ["//third_party/farm_sve"],
+        "//conditions:default": [],
+    }),
 )
 
 cc_library(
@@ -186,10 +200,13 @@ HWY_TESTS = [
     ("hwy/", "highway_test"),
     ("hwy/", "targets_test"),
     ("hwy/tests/", "arithmetic_test"),
+    ("hwy/tests/", "blockwise_test"),
     ("hwy/tests/", "combine_test"),
     ("hwy/tests/", "compare_test"),
     ("hwy/tests/", "convert_test"),
+    ("hwy/tests/", "crypto_test"),
     ("hwy/tests/", "logical_test"),
+    ("hwy/tests/", "mask_test"),
     ("hwy/tests/", "memory_test"),
     ("hwy/tests/", "swizzle_test"),
     ("hwy/tests/", "test_util_test"),
@@ -204,9 +221,14 @@ HWY_TESTS = [
             srcs = [
                 subdir + test + ".cc",
             ],
-            copts = COPTS,
-            # for test_suite. math_test is not yet supported on RVV.
-            tags = ["hwy_ops_test"] if test != "math_test" else [],
+            copts = COPTS + [
+                # gTest triggers this warning (which is enabled by the
+                # extra-semi in COPTS), so we need to disable it here,
+                # but it's still enabled for :hwy.
+                "-Wno-c++98-compat-extra-semi",
+            ],
+            # for test_suite.
+            tags = ["hwy_ops_test"],
             deps = [
                 ":hwy",
                 ":hwy_test_util",
diff --git a/third_party/highway/CMakeLists.txt b/third_party/highway/CMakeLists.txt
index c6ec4012b1dd7..177c961fab708 100644
--- a/third_party/highway/CMakeLists.txt
+++ b/third_party/highway/CMakeLists.txt
@@ -19,7 +19,7 @@ if(POLICY CMP0083)
   cmake_policy(SET CMP0083 NEW)
 endif()
 
-project(hwy VERSION 0.12.1)  # Keep in sync with highway.h version
+project(hwy VERSION 0.14.0)  # Keep in sync with highway.h version
 
 set(CMAKE_CXX_STANDARD 11)
 set(CMAKE_CXX_EXTENSIONS OFF)
@@ -64,12 +64,15 @@ set(HWY_SOURCES
     hwy/aligned_allocator.h
     hwy/base.h
     hwy/cache_control.h
+    hwy/detect_compiler_arch.h  # private
+    hwy/detect_targets.h  # private
     hwy/foreach_target.h
     hwy/highway.h
     hwy/nanobenchmark.cc
     hwy/nanobenchmark.h
     hwy/ops/arm_neon-inl.h
     hwy/ops/arm_sve-inl.h
+    hwy/ops/generic_ops-inl.h
     hwy/ops/scalar-inl.h
     hwy/ops/set_macros-inl.h
     hwy/ops/shared-inl.h
@@ -107,7 +110,6 @@ else()
 
   if(${CMAKE_CXX_COMPILER_ID} MATCHES "Clang")
     list(APPEND HWY_FLAGS
-      -Wc++2a-extensions
       -Wfloat-overflow-conversion
       -Wfloat-zero-conversion
       -Wfor-loop-analysis
@@ -126,6 +128,9 @@ else()
       # Use color in messages
       -fdiagnostics-show-option -fcolor-diagnostics
     )
+    if (CMAKE_CXX_COMPILER_VERSION VERSION_GREATER_EQUAL 6.0)
+      list(APPEND HWY_FLAGS -Wc++2a-extensions)
+    endif()
   endif()
 
   if (WIN32)
@@ -174,6 +179,28 @@ target_compile_options(hwy_contrib PRIVATE ${HWY_FLAGS})
 set_property(TARGET hwy_contrib PROPERTY POSITION_INDEPENDENT_CODE ON)
 target_include_directories(hwy_contrib PUBLIC ${CMAKE_CURRENT_LIST_DIR})
 
+# -------------------------------------------------------- hwy_list_targets
+# Generate a tool to print the compiled-in targets as defined by the current
+# flags. This tool will print to stderr at build time, after building hwy.
+add_executable(hwy_list_targets hwy/tests/list_targets.cc)
+target_compile_options(hwy_list_targets PRIVATE ${HWY_FLAGS})
+target_include_directories(hwy_list_targets PRIVATE
+  $<TARGET_PROPERTY:hwy,INCLUDE_DIRECTORIES>)
+# TARGET_FILE always returns the path to executable
+# Naked target also not always could be run (due to the lack of '.\' prefix)
+# Thus effective command to run should contain the full path
+# and emulator prefix (if any).
+add_custom_command(TARGET hwy POST_BUILD
+    COMMAND ${CMAKE_CROSSCOMPILING_EMULATOR} $<TARGET_FILE:hwy_list_targets> || (exit 0))
+
+# --------------------------------------------------------
+# The following sections are skipped if this project is added via
+# add_subdirectory into parent project (i.e. used as library).
+# Those sections contain artifacts that are not required for building
+# parent project: tests, examples, benchmarks and installation.
+# --------------------------------------------------------
+if (CMAKE_SOURCE_DIR STREQUAL CMAKE_CURRENT_SOURCE_DIR)
+
 # -------------------------------------------------------- install library
 install(TARGETS hwy
   DESTINATION "${CMAKE_INSTALL_LIBDIR}")
@@ -207,20 +234,6 @@ foreach (pc libhwy.pc libhwy-contrib.pc libhwy-test.pc)
       DESTINATION "${CMAKE_INSTALL_LIBDIR}/pkgconfig")
 endforeach()
 
-# -------------------------------------------------------- hwy_list_targets
-# Generate a tool to print the compiled-in targets as defined by the current
-# flags. This tool will print to stderr at build time, after building hwy.
-add_executable(hwy_list_targets hwy/tests/list_targets.cc)
-target_compile_options(hwy_list_targets PRIVATE ${HWY_FLAGS})
-target_include_directories(hwy_list_targets PRIVATE
-  $<TARGET_PROPERTY:hwy,INCLUDE_DIRECTORIES>)
-# TARGET_FILE always returns the path to executable
-# Naked target also not always could be run (due to the lack of '.\' prefix)
-# Thus effective command to run should contain the full path
-# and emulator prefix (if any).
-add_custom_command(TARGET hwy POST_BUILD
-    COMMAND ${CMAKE_CROSSCOMPILING_EMULATOR} $<TARGET_FILE:hwy_list_targets> || (exit 0))
-
 # -------------------------------------------------------- Examples
 
 # Avoids mismatch between GTest's static CRT and our dynamic.
@@ -292,10 +305,13 @@ set(HWY_TEST_FILES
   hwy/targets_test.cc
   hwy/examples/skeleton_test.cc
   hwy/tests/arithmetic_test.cc
+  hwy/tests/blockwise_test.cc
   hwy/tests/combine_test.cc
   hwy/tests/compare_test.cc
   hwy/tests/convert_test.cc
+  hwy/tests/crypto_test.cc
   hwy/tests/logical_test.cc
+  hwy/tests/mask_test.cc
   hwy/tests/memory_test.cc
   hwy/tests/swizzle_test.cc
   hwy/tests/test_util_test.cc
@@ -307,8 +323,11 @@ foreach (TESTFILE IN LISTS HWY_TEST_FILES)
   get_filename_component(TESTNAME ${TESTFILE} NAME_WE)
   add_executable(${TESTNAME} ${TESTFILE})
   target_compile_options(${TESTNAME} PRIVATE ${HWY_FLAGS})
-  # Test all targets, not just the best/baseline.
-  target_compile_options(${TESTNAME} PRIVATE -DHWY_COMPILE_ALL_ATTAINABLE=1)
+  # Test all targets, not just the best/baseline. This changes the default
+  # policy to all-attainable; note that setting -DHWY_COMPILE_* directly can
+  # cause compile errors because only one may be set, and other CMakeLists.txt
+  # that include us may set them.
+  target_compile_options(${TESTNAME} PRIVATE -DHWY_IS_TEST=1)
 
   if(HWY_SYSTEM_GTEST)
     target_link_libraries(${TESTNAME} hwy hwy_contrib GTest::GTest GTest::Main)
@@ -333,3 +352,5 @@ endforeach ()
 target_sources(skeleton_test PRIVATE hwy/examples/skeleton.cc)
 
 endif() # BUILD_TESTING
+
+endif() # CMAKE_SOURCE_DIR STREQUAL CMAKE_CURRENT_SOURCE_DIR
diff --git a/third_party/highway/CMakeLists.txt.in b/third_party/highway/CMakeLists.txt.in
index f98ccb4ac9780..df401705ee991 100644
--- a/third_party/highway/CMakeLists.txt.in
+++ b/third_party/highway/CMakeLists.txt.in
@@ -1,4 +1,4 @@
-cmake_minimum_required(VERSION 2.8.2)
+cmake_minimum_required(VERSION 2.8.12)
 
 project(googletest-download NONE)
 
diff --git a/third_party/highway/README.md b/third_party/highway/README.md
index a66bdc5e14224..45256a6a1cab9 100644
--- a/third_party/highway/README.md
+++ b/third_party/highway/README.md
@@ -14,8 +14,12 @@ applying the same operation to 'lanes'.
 
 ## Current status
 
-Supported targets: scalar, SSE4, AVX2, AVX-512, NEON (ARMv7 and v8), WASM SIMD.
-Ports to RVV and SVE/SVE2 are in progress.
+Supported targets: scalar, S-SSE3, SSE4, AVX2, AVX-512, NEON (ARMv7 and v8),
+SVE, WASM SIMD.
+
+SVE is tested using farm_sve (see acknowledgments). SVE2 is implemented but not
+yet validated. A subset of RVV is implemented and tested with GCC and QEMU.
+Work is underway to compile using LLVM, which has different intrinsics with AVL.
 
 Version 0.11 is considered stable enough to use in other projects, and is
 expected to remain backwards compatible unless serious issues are discovered
@@ -61,11 +65,6 @@ make -j && make test
 
 Or you can run `run_tests.sh` (`run_tests.bat` on Windows).
 
-To test on all the attainable targets for your platform, use
-`cmake .. -DCMAKE_CXX_FLAGS="-DHWY_COMPILE_ALL_ATTAINABLE"`. Otherwise, the
-default configuration skips baseline targets (e.g. scalar) that are superseded
-by another baseline target.
-
 Bazel is also supported for building, but it is not as widely used/tested.
 
 ## Quick start
@@ -79,10 +78,11 @@ number of instructions per operation.
 We recommend using full SIMD vectors whenever possible for maximum performance
 portability. To obtain them, pass a `HWY_FULL(float)` tag to functions such as
 `Zero/Set/Load`. There is also the option of a vector of up to `N` (a power of
-two) lanes: `HWY_CAPPED(T, N)`. 128-bit vectors are guaranteed to be available
-for lanes of type `T` if `HWY_TARGET != HWY_SCALAR` and `N == 16 / sizeof(T)`.
+two <= 16/sizeof(T)) lanes of type `T`: `HWY_CAPPED(T, N)`. If `HWY_TARGET ==
+HWY_SCALAR`, the vector always has one lane. For all other targets, up to
+128-bit vectors are guaranteed to be available.
 
-Functions using Highway must be inside a namespace `namespace HWY_NAMESPACE {`
+Functions using Highway must be inside `namespace HWY_NAMESPACE {`
 (possibly nested in one or more other namespaces defined by the project), and
 additionally either prefixed with `HWY_ATTR`, or residing between
 `HWY_BEFORE_NAMESPACE()` and `HWY_AFTER_NAMESPACE()`.
@@ -97,7 +97,7 @@ additionally either prefixed with `HWY_ATTR`, or residing between
 
 *   For dynamic dispatch, a table of function pointers is generated via the
     `HWY_EXPORT` macro that is used by `HWY_DYNAMIC_DISPATCH(func)(args)` to
-    call the best function pointer for the current CPU supported targets. A
+    call the best function pointer for the current CPU's supported targets. A
     module is automatically compiled for each target in `HWY_TARGETS` (see
     [quick-reference](g3doc/quick_reference.md)) if `HWY_TARGET_INCLUDE` is
     defined and foreach_target.h is included.
@@ -139,10 +139,7 @@ Highway offers several ways to express loops where `N` need not divide `count`:
     The template parameter and second function arguments are again not needed.
 
     This avoids duplicating code, and is reasonable if `count` is large.
-    Otherwise, multiple iterations may be slower than one `LoopBody` variant
-    with masking, especially because the `HWY_SCALAR` target selected by
-    `HWY_CAPPED(T, 1)` is slower for some operations due to workarounds for
-    undefined behavior in C++.
+    If `count` is small, the second loop may be slower than the next option.
 
 *   Process whole vectors as above, followed by a single call to a modified
     `LoopBody` with masking:
@@ -157,9 +154,8 @@ Highway offers several ways to express loops where `N` need not divide `count`:
     }
     ```
     Now the template parameter and second function argument can be used inside
-    `LoopBody` to replace `Load/Store` of full aligned vectors with
-    `LoadN/StoreN(n)` that affect no more than `1 <= n <= N` aligned elements
-    (pending implementation).
+    `LoopBody` to 'blend' the new partial vector with previous memory contents:
+    `Store(IfThenElse(FirstN(d, N), partial, prev_full), d, aligned_pointer);`.
 
     This is a good default when it is infeasible to ensure vectors are padded.
     In contrast to the scalar loop, only a single final iteration is needed.
@@ -170,7 +166,9 @@ Highway offers several ways to express loops where `N` need not divide `count`:
     the trouble of using SIMD clearly cares about speed. However, portability,
     maintainability and readability also matter, otherwise we would write in
     assembly. We aim for performance within 10-20% of a hand-written assembly
-    implementation on the development platform.
+    implementation on the development platform. There is no performance gap vs.
+    intrinsics: Highway code can do anything they can. If necessary, you can use
+    platform-specific instructions inside `#if HWY_TARGET == HWY_NEON` etc.
 
 *   The guiding principles of C++ are "pay only for what you use" and "leave no
     room for a lower-level language below C++". We apply these by defining a
@@ -192,14 +190,12 @@ Highway offers several ways to express loops where `N` need not divide `count`:
     blocks) and AVX-512 added two kinds of predicates (writemask and zeromask).
     To ensure the API reflects hardware realities, we suggest a flexible
     approach that adds new operations as they become commonly available, with
-    scalar fallbacks where not supported.
+    fallback implementations where necessary.
 
-*   Masking is not yet widely supported on current CPUs. It is difficult to
-    define an interface that provides access to all platform features while
-    retaining performance portability. The P0214R5 proposal lacks support for
-    AVX-512/ARM SVE zeromasks. We suggest limiting usage of masks to the
-    `IfThen[Zero]Else[Zero]` functions until the community has gained more
-    experience with them.
+*   Masking/predication differs between platforms, and it is not clear how
+    important the use cases are beyond the ternary operator `IfThenElse`.
+    AVX-512/ARM SVE zeromasks are useful, but not supported by P0214R5.
+    We provide `IfThen[Zero]Else[Zero]` variants.
 
 *   "Width-agnostic" SIMD is more future-proof than user-specified fixed sizes.
     For example, valarray-like code can iterate over a 1D array with a
@@ -209,7 +205,7 @@ Highway offers several ways to express loops where `N` need not divide `count`:
     RiscV V as well as Agner Fog's
     [ForwardCom instruction set proposal](https://goo.gl/CFizWu). However, some
     applications may require fixed sizes, so we also guarantee support for
-    128-bit vectors in each instruction set.
+    <= 128-bit vectors in each instruction set.
 
 *   The API and its implementation should be usable and efficient with commonly
     used compilers, including MSVC. For example, we write `ShiftLeft<3>(v)`
@@ -227,23 +223,23 @@ Highway offers several ways to express loops where `N` need not divide `count`:
     Therefore, we provide code paths for multiple instruction sets and choose
     the most suitable at runtime. To reduce overhead, dispatch should be hoisted
     to higher layers instead of checking inside every low-level function.
-    Highway supports inlining functions in the same file or in *-inl.h headers.
-    We generate all code paths from the same source to reduce implementation-
-    and debugging cost.
+    Highway supports inlining functions in the same file or in `*-inl.h`
+    headers. We generate all code paths from the same source to reduce
+    implementation- and debugging cost.
 
 *   Not every CPU need be supported. For example, pre-SSE4.1 CPUs are
     increasingly rare and the AVX instruction set is limited to floating-point
     operations. To reduce code size and compile time, we provide specializations
-    for SSE4, AVX2 and AVX-512 instruction sets on x86, plus a scalar fallback.
+    for S-SSE3, SSE4, AVX2 and AVX-512 instruction sets on x86, plus a scalar
+    fallback.
 
 *   Access to platform-specific intrinsics is necessary for acceptance in
     performance-critical projects. We provide conversions to and from intrinsics
     to allow utilizing specialized platform-specific functionality, and simplify
     incremental porting of existing code.
 
-*   The core API should be compact and easy to learn. We provide only the few
-    dozen operations which are necessary and sufficient for most of the 150+
-    SIMD applications we examined.
+*   The core API should be compact and easy to learn; we provide a concise
+    summary in g3doc/quick_reference.md.
 
 ## Prior API designs
 
@@ -258,6 +254,10 @@ runtime dispatch.
 
 ## Differences versus [P0214R5 proposal](https://goo.gl/zKW4SA)
 
+1.  Allowing the use of built-in vector types by relying on non-member
+    functions. By contrast, P0214R5 requires a wrapper class, which does not
+    work for sizeless vector types currently used by ARM SVE and Risc-V.
+
 1.  Adding widely used and portable operations such as `AndNot`, `AverageRound`,
     bit-shift by immediates and `IfThenElse`.
 
@@ -286,13 +286,6 @@ runtime dispatch.
     static target selection and runtime dispatch for hotspots that may benefit
     from newer instruction sets if available.
 
-1.  Using built-in PPC vector types without a wrapper class. This leads to much
-    better code generation with GCC 6.3: https://godbolt.org/z/pd2PNP.
-    By contrast, P0214R5 requires a wrapper. We avoid this by using only the
-    member operators provided by the PPC vectors; all other functions and
-    typedefs are non-members. 2019-04 update: Clang power64le does not have
-    this issue, so we simplified get_part(d, v) to GetLane(v).
-
 1.  Omitting inefficient or non-performance-portable operations such as `hmax`,
     `operator[]`, and unsupported integer comparisons. Applications can often
     replace these operations at lower cost than emulating that exact behavior.
@@ -301,8 +294,8 @@ runtime dispatch.
 
 1.  Ensuring signed integer overflow has well-defined semantics (wraparound).
 
-1.  Simple header-only implementation and less than a tenth of the size of the
-    Vc library from which P0214 was derived (98,000 lines in
+1.  Simple header-only implementation and a fraction of the size of the
+    Vc library from which P0214 was derived (39K, vs. 92K lines in
     https://github.com/VcDevel/Vc according to the gloc Chrome extension).
 
 1.  Avoiding hidden performance costs. P0214R5 allows implicit conversions from
@@ -338,7 +331,7 @@ vectors cannot be default-constructed. We instead use a dedicated 'descriptor'
 type `Simd` for overloading, abbreviated to `D` for template arguments and
 `d` in lvalues.
 
-Note that generic function templates are possible (see highway.h).
+Note that generic function templates are possible (see generic_ops-inlz.h).
 
 ## Masks
 
@@ -362,5 +355,11 @@ set, we provide a special `ZeroIfNegative` function.
 [intro]: g3doc/highway_intro.pdf
 [instmtx]: g3doc/instruction_matrix.pdf
 
+## Acknowledgments
+
+We have used [farm-sve](https://gitlab.inria.fr/bramas/farm-sve) by Berenger
+Bramas; it has proved useful for checking the SVE port on an x86 development
+machine.
+
 This is not an officially supported Google product.
 Contact: janwas@google.com
diff --git a/third_party/highway/debian/changelog b/third_party/highway/debian/changelog
index 75768968fbea7..14b667c016c95 100644
--- a/third_party/highway/debian/changelog
+++ b/third_party/highway/debian/changelog
@@ -1,3 +1,19 @@
+highway (0.14.0-1) UNRELEASED; urgency=medium
+
+  * Add SVE, S-SSE3, AVX3_DL targets
+  * Support partial vectors in all ops
+  * Add PopulationCount, FindFirstTrue, Ne, TableLookupBytesOr0
+  * Add AESRound, CLMul, MulOdd, HWY_CAP_FLOAT16
+
+ -- Jan Wassenberg <janwas@google.com>  Thu, 29 Jul 2021 15:00:00 +0200
+
+highway (0.12.2-1) UNRELEASED; urgency=medium
+
+  * fix scalar-only test and Windows macro conflict with Load/StoreFence
+  * replace deprecated wasm intrinsics
+
+ -- Jan Wassenberg <janwas@google.com>  Mon, 31 May 2021 16:00:00 +0200
+
 highway (0.12.1-1) UNRELEASED; urgency=medium
 
   * doc updates, ARM GCC support, fix s390/ppc, complete partial vectors
diff --git a/third_party/highway/debian/rules b/third_party/highway/debian/rules
old mode 100644
new mode 100755
diff --git a/third_party/highway/hwy/aligned_allocator.cc b/third_party/highway/hwy/aligned_allocator.cc
index bec7c3bb1b701..4fcc364062e71 100644
--- a/third_party/highway/hwy/aligned_allocator.cc
+++ b/third_party/highway/hwy/aligned_allocator.cc
@@ -27,10 +27,21 @@
 namespace hwy {
 namespace {
 
-constexpr size_t kAlignment = HWY_MAX(HWY_ALIGNMENT, kMaxVectorSize);
+#if HWY_ARCH_RVV && defined(__riscv_vector)
+// Not actually an upper bound on the size, but this value prevents crossing a
+// 4K boundary (relevant on Andes).
+constexpr size_t kAlignment = HWY_MAX(HWY_ALIGNMENT, 4096);
+#else
+constexpr size_t kAlignment = HWY_ALIGNMENT;
+#endif
+
+#if HWY_ARCH_X86
 // On x86, aliasing can only occur at multiples of 2K, but that's too wasteful
 // if this is used for single-vector allocations. 256 is more reasonable.
 constexpr size_t kAlias = kAlignment * 4;
+#else
+constexpr size_t kAlias = kAlignment;
+#endif
 
 #pragma pack(push, 1)
 struct AllocationHeader {
@@ -94,7 +105,7 @@ void* AllocateAlignedBytes(const size_t payload_size, AllocPtr alloc_ptr,
   header->allocated = allocated;
   header->payload_size = payload_size;
 
-  return HWY_ASSUME_ALIGNED(reinterpret_cast<void*>(payload), kMaxVectorSize);
+  return HWY_ASSUME_ALIGNED(reinterpret_cast<void*>(payload), kAlignment);
 }
 
 void FreeAlignedBytes(const void* aligned_pointer, FreePtr free_ptr,
diff --git a/third_party/highway/hwy/aligned_allocator_test.cc b/third_party/highway/hwy/aligned_allocator_test.cc
index c11033b18c1e1..f729a2865fd10 100644
--- a/third_party/highway/hwy/aligned_allocator_test.cc
+++ b/third_party/highway/hwy/aligned_allocator_test.cc
@@ -120,7 +120,7 @@ TEST(AlignedAllocatorTest, AllocDefaultPointers) {
                                    /*opaque_ptr=*/nullptr);
   ASSERT_NE(nullptr, ptr);
   // Make sure the pointer is actually aligned.
-  EXPECT_EQ(0U, reinterpret_cast<uintptr_t>(ptr) % kMaxVectorSize);
+  EXPECT_EQ(0U, reinterpret_cast<uintptr_t>(ptr) % HWY_ALIGNMENT);
   char* p = static_cast<char*>(ptr);
   size_t ret = 0;
   for (size_t i = 0; i < kSize; i++) {
@@ -152,7 +152,7 @@ TEST(AlignedAllocatorTest, CustomAlloc) {
   // We should have only requested one alloc from the allocator.
   EXPECT_EQ(1U, fake_alloc.PendingAllocs());
   // Make sure the pointer is actually aligned.
-  EXPECT_EQ(0U, reinterpret_cast<uintptr_t>(ptr) % kMaxVectorSize);
+  EXPECT_EQ(0U, reinterpret_cast<uintptr_t>(ptr) % HWY_ALIGNMENT);
   FreeAlignedBytes(ptr, &FakeAllocator::StaticFree, &fake_alloc);
   EXPECT_EQ(0U, fake_alloc.PendingAllocs());
 }
@@ -197,7 +197,7 @@ TEST(AlignedAllocatorTest, MakeUniqueAlignedArray) {
 TEST(AlignedAllocatorTest, AllocSingleInt) {
   auto ptr = AllocateAligned<uint32_t>(1);
   ASSERT_NE(nullptr, ptr.get());
-  EXPECT_EQ(0U, reinterpret_cast<uintptr_t>(ptr.get()) % kMaxVectorSize);
+  EXPECT_EQ(0U, reinterpret_cast<uintptr_t>(ptr.get()) % HWY_ALIGNMENT);
   // Force delete of the unique_ptr now to check that it doesn't crash.
   ptr.reset(nullptr);
   EXPECT_EQ(nullptr, ptr.get());
@@ -207,7 +207,7 @@ TEST(AlignedAllocatorTest, AllocMultipleInt) {
   const size_t kSize = 7777;
   auto ptr = AllocateAligned<uint32_t>(kSize);
   ASSERT_NE(nullptr, ptr.get());
-  EXPECT_EQ(0U, reinterpret_cast<uintptr_t>(ptr.get()) % kMaxVectorSize);
+  EXPECT_EQ(0U, reinterpret_cast<uintptr_t>(ptr.get()) % HWY_ALIGNMENT);
   // ptr[i] is actually (*ptr.get())[i] which will use the operator[] of the
   // underlying type chosen by AllocateAligned() for the std::unique_ptr.
   EXPECT_EQ(&(ptr[0]) + 1, &(ptr[1]));
@@ -276,3 +276,9 @@ TEST(AlignedAllocatorTest, DefaultInit) {
 }
 
 }  // namespace hwy
+
+// Ought not to be necessary, but without this, no tests run on RVV.
+int main(int argc, char** argv) {
+  ::testing::InitGoogleTest(&argc, argv);
+  return RUN_ALL_TESTS();
+}
diff --git a/third_party/highway/hwy/base.h b/third_party/highway/hwy/base.h
index 75fe585e4c296..1ecd215d4103a 100644
--- a/third_party/highway/hwy/base.h
+++ b/third_party/highway/hwy/base.h
@@ -23,72 +23,7 @@
 #include <atomic>
 #include <cfloat>
 
-// Add to #if conditions to prevent IDE from graying out code.
-#if (defined __CDT_PARSER__) || (defined __INTELLISENSE__) || \
-    (defined Q_CREATOR_RUN) || (defined(__CLANGD__))
-#define HWY_IDE 1
-#else
-#define HWY_IDE 0
-#endif
-
-//------------------------------------------------------------------------------
-// Detect compiler using predefined macros
-
-// clang-cl defines _MSC_VER but doesn't behave like MSVC in other aspects like
-// used in HWY_DIAGNOSTICS(). We include a check that we are not clang for that
-// purpose.
-#if defined(_MSC_VER) && !defined(__clang__)
-#define HWY_COMPILER_MSVC _MSC_VER
-#else
-#define HWY_COMPILER_MSVC 0
-#endif
-
-#ifdef __INTEL_COMPILER
-#define HWY_COMPILER_ICC __INTEL_COMPILER
-#else
-#define HWY_COMPILER_ICC 0
-#endif
-
-#ifdef __GNUC__
-#define HWY_COMPILER_GCC (__GNUC__ * 100 + __GNUC_MINOR__)
-#else
-#define HWY_COMPILER_GCC 0
-#endif
-
-// Clang can masquerade as MSVC/GCC, in which case both are set.
-#ifdef __clang__
-#ifdef __APPLE__
-// Apple LLVM version is unrelated to the actual Clang version, which we need
-// for enabling workarounds. Use the presence of warning flags to deduce it.
-// Adapted from https://github.com/simd-everywhere/simde/ simde-detect-clang.h.
-#if __has_warning("-Wformat-insufficient-args")
-#define HWY_COMPILER_CLANG 1200
-#elif __has_warning("-Wimplicit-const-int-float-conversion")
-#define HWY_COMPILER_CLANG 1100
-#elif __has_warning("-Wmisleading-indentation")
-#define HWY_COMPILER_CLANG 1000
-#elif defined(__FILE_NAME__)
-#define HWY_COMPILER_CLANG 900
-#elif __has_warning("-Wextra-semi-stmt") || \
-    __has_builtin(__builtin_rotateleft32)
-#define HWY_COMPILER_CLANG 800
-#elif __has_warning("-Wc++98-compat-extra-semi")
-#define HWY_COMPILER_CLANG 700
-#else  // Anything older than 7.0 is not recommended for Highway.
-#define HWY_COMPILER_CLANG 600
-#endif  // __has_warning chain
-#else   // Non-Apple: normal version
-#define HWY_COMPILER_CLANG (__clang_major__ * 100 + __clang_minor__)
-#endif
-#else  // Not clang
-#define HWY_COMPILER_CLANG 0
-#endif
-
-// More than one may be nonzero, but we want at least one.
-#if !HWY_COMPILER_MSVC && !HWY_COMPILER_ICC && !HWY_COMPILER_GCC && \
-    !HWY_COMPILER_CLANG
-#error "Unsupported compiler"
-#endif
+#include "hwy/detect_compiler_arch.h"
 
 //------------------------------------------------------------------------------
 // Compiler-specific definitions
@@ -140,18 +75,6 @@
 //------------------------------------------------------------------------------
 // Builtin/attributes
 
-#ifdef __has_builtin
-#define HWY_HAS_BUILTIN(name) __has_builtin(name)
-#else
-#define HWY_HAS_BUILTIN(name) 0
-#endif
-
-#ifdef __has_attribute
-#define HWY_HAS_ATTRIBUTE(name) __has_attribute(name)
-#else
-#define HWY_HAS_ATTRIBUTE(name) 0
-#endif
-
 // Enables error-checking of format strings.
 #if HWY_HAS_ATTRIBUTE(__format__)
 #define HWY_FORMAT(idx_fmt, idx_arg) \
@@ -175,9 +98,9 @@
 // are inlined. Support both per-function annotation (HWY_ATTR) for lambdas and
 // automatic annotation via pragmas.
 #if HWY_COMPILER_CLANG
-#define HWY_PUSH_ATTRIBUTES(targets_str)                                     \
+#define HWY_PUSH_ATTRIBUTES(targets_str)                                \
   HWY_PRAGMA(clang attribute push(__attribute__((target(targets_str))), \
-                                       apply_to = function))
+                                  apply_to = function))
 #define HWY_POP_ATTRIBUTES HWY_PRAGMA(clang attribute pop)
 #elif HWY_COMPILER_GCC
 #define HWY_PUSH_ATTRIBUTES(targets_str) \
@@ -188,78 +111,6 @@
 #define HWY_POP_ATTRIBUTES
 #endif
 
-//------------------------------------------------------------------------------
-// Detect architecture using predefined macros
-
-#if defined(__i386__) || defined(_M_IX86)
-#define HWY_ARCH_X86_32 1
-#else
-#define HWY_ARCH_X86_32 0
-#endif
-
-#if defined(__x86_64__) || defined(_M_X64)
-#define HWY_ARCH_X86_64 1
-#else
-#define HWY_ARCH_X86_64 0
-#endif
-
-#if HWY_ARCH_X86_32 && HWY_ARCH_X86_64
-#error "Cannot have both x86-32 and x86-64"
-#endif
-
-#if HWY_ARCH_X86_32 || HWY_ARCH_X86_64
-#define HWY_ARCH_X86 1
-#else
-#define HWY_ARCH_X86 0
-#endif
-
-#if defined(__powerpc64__) || defined(_M_PPC)
-#define HWY_ARCH_PPC 1
-#else
-#define HWY_ARCH_PPC 0
-#endif
-
-#if defined(__ARM_ARCH_ISA_A64) || defined(__aarch64__) || defined(_M_ARM64)
-#define HWY_ARCH_ARM_A64 1
-#else
-#define HWY_ARCH_ARM_A64 0
-#endif
-
-#if defined(__arm__) || defined(_M_ARM)
-#define HWY_ARCH_ARM_V7 1
-#else
-#define HWY_ARCH_ARM_V7 0
-#endif
-
-#if HWY_ARCH_ARM_A64 && HWY_ARCH_ARM_V7
-#error "Cannot have both A64 and V7"
-#endif
-
-#if HWY_ARCH_ARM_A64 || HWY_ARCH_ARM_V7
-#define HWY_ARCH_ARM 1
-#else
-#define HWY_ARCH_ARM 0
-#endif
-
-#if defined(__EMSCRIPTEN__) || defined(__wasm__) || defined(__WASM__)
-#define HWY_ARCH_WASM 1
-#else
-#define HWY_ARCH_WASM 0
-#endif
-
-#ifdef __riscv
-#define HWY_ARCH_RVV 1
-#else
-#define HWY_ARCH_RVV 0
-#endif
-
-// It is an error to detect multiple architectures at the same time, but OK to
-// detect none of the above.
-#if (HWY_ARCH_X86 + HWY_ARCH_PPC + HWY_ARCH_ARM + HWY_ARCH_WASM + \
-     HWY_ARCH_RVV) > 1
-#error "Must not detect more than one architecture"
-#endif
-
 //------------------------------------------------------------------------------
 // Macros
 
@@ -305,24 +156,33 @@
   } while (0)
 #endif
 
+#if defined(HWY_EMULATE_SVE)
+class FarmFloat16;
+#endif
 
 namespace hwy {
 
 //------------------------------------------------------------------------------
-// Alignment
+// kMaxVectorSize (undocumented, pending removal)
 
-// Not guaranteed to be an upper bound, but the alignment established by
-// aligned_allocator is HWY_MAX(HWY_ALIGNMENT, kMaxVectorSize).
 #if HWY_ARCH_X86
 static constexpr HWY_MAYBE_UNUSED size_t kMaxVectorSize = 64;  // AVX-512
-#define HWY_ALIGN_MAX alignas(64)
 #elif HWY_ARCH_RVV && defined(__riscv_vector)
-// Not actually an upper bound on the size, but this value prevents crossing a
-// 4K boundary (relevant on Andes).
+// Not actually an upper bound on the size.
 static constexpr HWY_MAYBE_UNUSED size_t kMaxVectorSize = 4096;
-#define HWY_ALIGN_MAX alignas(8)  // only elements need be aligned
 #else
 static constexpr HWY_MAYBE_UNUSED size_t kMaxVectorSize = 16;
+#endif
+
+//------------------------------------------------------------------------------
+// Alignment
+
+// For stack-allocated partial arrays or LoadDup128.
+#if HWY_ARCH_X86
+#define HWY_ALIGN_MAX alignas(64)
+#elif HWY_ARCH_RVV && defined(__riscv_vector)
+#define HWY_ALIGN_MAX alignas(8)  // only elements need be aligned
+#else
 #define HWY_ALIGN_MAX alignas(16)
 #endif
 
@@ -333,13 +193,16 @@ static constexpr HWY_MAYBE_UNUSED size_t kMaxVectorSize = 16;
 // by concatenating base type and bits.
 
 // RVV already has a builtin type and the GCC intrinsics require it.
-#if HWY_ARCH_RVV && HWY_COMPILER_GCC && defined(__riscv_vector)
+#if (HWY_ARCH_RVV && HWY_COMPILER_GCC && defined(__riscv_vector)) || \
+    (HWY_ARCH_ARM && (__ARM_FP & 2))
 #define HWY_NATIVE_FLOAT16 1
 #else
 #define HWY_NATIVE_FLOAT16 0
 #endif
 
-#if HWY_NATIVE_FLOAT16
+#if defined(HWY_EMULATE_SVE)
+using float16_t = FarmFloat16;
+#elif HWY_NATIVE_FLOAT16
 using float16_t = __fp16;
 // Clang does not allow __fp16 arguments, but scalar.h requires LaneType
 // arguments, so use a wrapper.
@@ -368,6 +231,21 @@ struct EnableIfT<true, T> {
 template <bool Condition, class T = void>
 using EnableIf = typename EnableIfT<Condition, T>::type;
 
+template <typename T, typename U>
+struct IsSameT {
+  enum { value = 0 };
+};
+
+template <typename T>
+struct IsSameT<T, T> {
+  enum { value = 1 };
+};
+
+template <typename T, typename U>
+HWY_API constexpr bool IsSame() {
+  return IsSameT<T, U>::value;
+}
+
 // Insert into template/function arguments to enable this overload only for
 // vectors of AT MOST this many bits.
 //
@@ -377,6 +255,9 @@ using EnableIf = typename EnableIfT<Condition, T>::type;
 #define HWY_IF_LE128(T, N) hwy::EnableIf<N * sizeof(T) <= 16>* = nullptr
 #define HWY_IF_LE64(T, N) hwy::EnableIf<N * sizeof(T) <= 8>* = nullptr
 #define HWY_IF_LE32(T, N) hwy::EnableIf<N * sizeof(T) <= 4>* = nullptr
+#define HWY_IF_GE64(T, N) hwy::EnableIf<N * sizeof(T) >= 8>* = nullptr
+#define HWY_IF_GE128(T, N) hwy::EnableIf<N * sizeof(T) >= 16>* = nullptr
+#define HWY_IF_GT128(T, N) hwy::EnableIf<(N * sizeof(T) > 16)>* = nullptr
 
 #define HWY_IF_UNSIGNED(T) hwy::EnableIf<!IsSigned<T>()>* = nullptr
 #define HWY_IF_SIGNED(T) \
@@ -393,18 +274,36 @@ using EnableIf = typename EnableIfT<Condition, T>::type;
 template <size_t N>
 struct SizeTag {};
 
+template <class T>
+struct RemoveConstT {
+  using type = T;
+};
+template <class T>
+struct RemoveConstT<const T> {
+  using type = T;
+};
+
+template <class T>
+using RemoveConst = typename RemoveConstT<T>::type;
+
 //------------------------------------------------------------------------------
 // Type traits
 
 template <typename T>
 constexpr bool IsFloat() {
-  return T(1.25) != T(1);
+  // Cannot use T(1.25) != T(1) for float16_t, which can only be converted to or
+  // from a float, not compared.
+  return IsSame<T, float>() || IsSame<T, double>();
 }
 
 template <typename T>
 constexpr bool IsSigned() {
   return T(0) > T(-1);
 }
+template <>
+constexpr bool IsSigned<float16_t>() {
+  return true;
+}
 
 // Largest/smallest representable integer values.
 template <typename T>
@@ -555,6 +454,7 @@ struct Relations<float> {
   using Signed = int32_t;
   using Float = float;
   using Wide = double;
+  using Narrow = float16_t;
 };
 template <>
 struct Relations<double> {
@@ -564,6 +464,31 @@ struct Relations<double> {
   using Narrow = float;
 };
 
+template <size_t N>
+struct TypeFromSize;
+template <>
+struct TypeFromSize<1> {
+  using Unsigned = uint8_t;
+  using Signed = int8_t;
+};
+template <>
+struct TypeFromSize<2> {
+  using Unsigned = uint16_t;
+  using Signed = int16_t;
+};
+template <>
+struct TypeFromSize<4> {
+  using Unsigned = uint32_t;
+  using Signed = int32_t;
+  using Float = float;
+};
+template <>
+struct TypeFromSize<8> {
+  using Unsigned = uint64_t;
+  using Signed = int64_t;
+  using Float = double;
+};
+
 }  // namespace detail
 
 // Aliases for types of a different category, but the same size.
@@ -580,6 +505,14 @@ using MakeWide = typename detail::Relations<T>::Wide;
 template <typename T>
 using MakeNarrow = typename detail::Relations<T>::Narrow;
 
+// Obtain type from its size [bytes].
+template <size_t N>
+using UnsignedFromSize = typename detail::TypeFromSize<N>::Unsigned;
+template <size_t N>
+using SignedFromSize = typename detail::TypeFromSize<N>::Signed;
+template <size_t N>
+using FloatFromSize = typename detail::TypeFromSize<N>::Float;
+
 //------------------------------------------------------------------------------
 // Helper functions
 
@@ -599,11 +532,21 @@ HWY_API size_t Num0BitsBelowLS1Bit_Nonzero32(const uint32_t x) {
   unsigned long index;  // NOLINT
   _BitScanForward(&index, x);
   return index;
-#else  // HWY_COMPILER_MSVC
+#else   // HWY_COMPILER_MSVC
   return static_cast<size_t>(__builtin_ctz(x));
 #endif  // HWY_COMPILER_MSVC
 }
 
+HWY_API size_t Num0BitsBelowLS1Bit_Nonzero64(const uint64_t x) {
+#if HWY_COMPILER_MSVC
+  unsigned long index;  // NOLINT
+  _BitScanForward64(&index, x);
+  return index;
+#else   // HWY_COMPILER_MSVC
+  return static_cast<size_t>(__builtin_ctzll(x));
+#endif  // HWY_COMPILER_MSVC
+}
+
 HWY_API size_t PopCount(uint64_t x) {
 #if HWY_COMPILER_CLANG || HWY_COMPILER_GCC
   return static_cast<size_t>(__builtin_popcountll(x));
@@ -623,6 +566,30 @@ HWY_API size_t PopCount(uint64_t x) {
 #endif
 }
 
+#if HWY_COMPILER_MSVC && HWY_ARCH_X86_64
+#pragma intrinsic(_umul128)
+#endif
+
+// 64 x 64 = 128 bit multiplication
+HWY_API uint64_t Mul128(uint64_t a, uint64_t b, uint64_t* HWY_RESTRICT upper) {
+#if defined(__SIZEOF_INT128__)
+  __uint128_t product = (__uint128_t)a * (__uint128_t)b;
+  *upper = (uint64_t)(product >> 64);
+  return (uint64_t)(product & 0xFFFFFFFFFFFFFFFFULL);
+#elif HWY_COMPILER_MSVC && HWY_ARCH_X86_64
+  return _umul128(a, b, upper);
+#else
+  constexpr uint64_t kLo32 = 0xFFFFFFFFU;
+  const uint64_t lo_lo = (a & kLo32) * (b & kLo32);
+  const uint64_t hi_lo = (a >> 32) * (b & kLo32);
+  const uint64_t lo_hi = (a & kLo32) * (b >> 32);
+  const uint64_t hi_hi = (a >> 32) * (b >> 32);
+  const uint64_t t = (lo_lo >> 32) + (hi_lo & kLo32) + lo_hi;
+  *upper = (hi_lo >> 32) + (t >> 32) + hi_hi;
+  return (t << 32) | (lo_lo & kLo32);
+#endif
+}
+
 // The source/destination must not overlap/alias.
 template <size_t kBytes, typename From, typename To>
 HWY_API void CopyBytes(const From* from, To* to) {
diff --git a/third_party/highway/hwy/base_test.cc b/third_party/highway/hwy/base_test.cc
index 19e0b6f5445c4..df04f68f0aecf 100644
--- a/third_party/highway/hwy/base_test.cc
+++ b/third_party/highway/hwy/base_test.cc
@@ -90,6 +90,17 @@ HWY_NOINLINE void TestAllType() {
   ForFloatTypes(TestIsFloat());
 }
 
+struct TestIsSame {
+  template <class T>
+  HWY_NOINLINE void operator()(T /*unused*/) const {
+    static_assert(IsSame<T, T>(), "T == T");
+    static_assert(!IsSame<MakeSigned<T>, MakeUnsigned<T>>(), "S != U");
+    static_assert(!IsSame<MakeUnsigned<T>, MakeSigned<T>>(), "U != S");
+  }
+};
+
+HWY_NOINLINE void TestAllIsSame() { ForAllTypes(TestIsSame()); }
+
 HWY_NOINLINE void TestAllPopCount() {
   HWY_ASSERT_EQ(size_t(0), PopCount(0u));
   HWY_ASSERT_EQ(size_t(1), PopCount(1u));
@@ -113,11 +124,20 @@ HWY_NOINLINE void TestAllPopCount() {
 HWY_AFTER_NAMESPACE();
 
 #if HWY_ONCE
+
 namespace hwy {
 HWY_BEFORE_TEST(BaseTest);
 HWY_EXPORT_AND_TEST_P(BaseTest, TestAllLimits);
 HWY_EXPORT_AND_TEST_P(BaseTest, TestAllLowestHighest);
 HWY_EXPORT_AND_TEST_P(BaseTest, TestAllType);
+HWY_EXPORT_AND_TEST_P(BaseTest, TestAllIsSame);
 HWY_EXPORT_AND_TEST_P(BaseTest, TestAllPopCount);
 }  // namespace hwy
+
+// Ought not to be necessary, but without this, no tests run on RVV.
+int main(int argc, char **argv) {
+  ::testing::InitGoogleTest(&argc, argv);
+  return RUN_ALL_TESTS();
+}
+
 #endif
diff --git a/third_party/highway/hwy/cache_control.h b/third_party/highway/hwy/cache_control.h
index ab0a2347982b9..65f326a5f5f1f 100644
--- a/third_party/highway/hwy/cache_control.h
+++ b/third_party/highway/hwy/cache_control.h
@@ -32,6 +32,14 @@
 #include <emmintrin.h>  // SSE2
 #endif
 
+// Windows.h #defines these, which causes infinite recursion. Temporarily
+// undefine them in this header; these functions are anyway deprecated.
+// TODO(janwas): remove when these functions are removed.
+#pragma push_macro("LoadFence")
+#pragma push_macro("StoreFence")
+#undef LoadFence
+#undef StoreFence
+
 namespace hwy {
 
 // Even if N*sizeof(T) is smaller, Stream may write a multiple of this size.
@@ -47,20 +55,28 @@ namespace hwy {
 // Delays subsequent loads until prior loads are visible. On Intel CPUs, also
 // serves as a full fence (waits for all prior instructions to complete).
 // No effect on non-x86.
+// DEPRECATED due to differing behavior across architectures AND vendors.
 HWY_INLINE HWY_ATTR_CACHE void LoadFence() {
 #if HWY_ARCH_X86 && !defined(HWY_DISABLE_CACHE_CONTROL)
   _mm_lfence();
 #endif
 }
 
-// Ensures previous weakly-ordered stores are visible. No effect on non-x86.
-HWY_INLINE HWY_ATTR_CACHE void StoreFence() {
+// Ensures values written by previous `Stream` calls are visible on the current
+// core. This is NOT sufficient for synchronizing across cores; when `Stream`
+// outputs are to be consumed by other core(s), the producer must publish
+// availability (e.g. via mutex or atomic_flag) after `FlushStream`.
+HWY_INLINE HWY_ATTR_CACHE void FlushStream() {
 #if HWY_ARCH_X86 && !defined(HWY_DISABLE_CACHE_CONTROL)
   _mm_sfence();
 #endif
 }
 
-// Begins loading the cache line containing "p".
+// DEPRECATED, replace with `FlushStream`.
+HWY_INLINE HWY_ATTR_CACHE void StoreFence() { FlushStream(); }
+
+// Optionally begins loading the cache line containing "p" to reduce latency of
+// subsequent actual loads.
 template <typename T>
 HWY_INLINE HWY_ATTR_CACHE void Prefetch(const T* p) {
 #if HWY_ARCH_X86 && !defined(HWY_DISABLE_CACHE_CONTROL)
@@ -74,7 +90,7 @@ HWY_INLINE HWY_ATTR_CACHE void Prefetch(const T* p) {
 #endif
 }
 
-// Invalidates and flushes the cache line containing "p". No effect on non-x86.
+// Invalidates and flushes the cache line containing "p", if possible.
 HWY_INLINE HWY_ATTR_CACHE void FlushCacheline(const void* p) {
 #if HWY_ARCH_X86 && !defined(HWY_DISABLE_CACHE_CONTROL)
   _mm_clflush(p);
@@ -83,7 +99,7 @@ HWY_INLINE HWY_ATTR_CACHE void FlushCacheline(const void* p) {
 #endif
 }
 
-// Reduces power consumption in spin-loops. No effect on non-x86.
+// When called inside a spin-loop, may reduce power consumption.
 HWY_INLINE HWY_ATTR_CACHE void Pause() {
 #if HWY_ARCH_X86 && !defined(HWY_DISABLE_CACHE_CONTROL)
   _mm_pause();
@@ -92,4 +108,8 @@ HWY_INLINE HWY_ATTR_CACHE void Pause() {
 
 }  // namespace hwy
 
+// TODO(janwas): remove when these functions are removed. (See above.)
+#pragma pop_macro("StoreFence")
+#pragma pop_macro("LoadFence")
+
 #endif  // HIGHWAY_HWY_CACHE_CONTROL_H_
diff --git a/third_party/highway/hwy/contrib/image/image.cc b/third_party/highway/hwy/contrib/image/image.cc
index 0dfe739a4914c..0dc0108b1d19e 100644
--- a/third_party/highway/hwy/contrib/image/image.cc
+++ b/third_party/highway/hwy/contrib/image/image.cc
@@ -58,7 +58,7 @@ size_t ImageBase::BytesPerRow(const size_t xsize, const size_t sizeof_t) {
   }
 
   // Round up to vector and cache line size.
-  const size_t align = std::max<size_t>(vec_size, HWY_ALIGNMENT);
+  const size_t align = HWY_MAX(vec_size, HWY_ALIGNMENT);
   size_t bytes_per_row = RoundUpTo(valid_bytes, align);
 
   // During the lengthy window before writes are committed to memory, CPUs
diff --git a/third_party/highway/hwy/contrib/image/image_test.cc b/third_party/highway/hwy/contrib/image/image_test.cc
index c27e52a195773..4a4e54f76f1cf 100644
--- a/third_party/highway/hwy/contrib/image/image_test.cc
+++ b/third_party/highway/hwy/contrib/image/image_test.cc
@@ -96,7 +96,7 @@ struct TestUnalignedT {
         for (size_t y = 0; y < ysize; ++y) {
           T* HWY_RESTRICT row = img.MutableRow(y);
           for (size_t x = 0; x < xsize; ++x) {
-            accum |= LoadU(d, row + x);
+            accum = Or(accum, LoadU(d, row + x));
           }
         }
 
@@ -143,9 +143,17 @@ void TestUnaligned() { ForUnsignedTypes(TestUnalignedT()); }
 HWY_AFTER_NAMESPACE();
 
 #if HWY_ONCE
+
 namespace hwy {
 HWY_BEFORE_TEST(ImageTest);
 HWY_EXPORT_AND_TEST_P(ImageTest, TestAligned);
 HWY_EXPORT_AND_TEST_P(ImageTest, TestUnaligned);
 }  // namespace hwy
+
+// Ought not to be necessary, but without this, no tests run on RVV.
+int main(int argc, char** argv) {
+  ::testing::InitGoogleTest(&argc, argv);
+  return RUN_ALL_TESTS();
+}
+
 #endif
diff --git a/third_party/highway/hwy/contrib/math/math-inl.h b/third_party/highway/hwy/contrib/math/math-inl.h
index 15b80d63bafa2..dd8794be73a97 100644
--- a/third_party/highway/hwy/contrib/math/math-inl.h
+++ b/third_party/highway/hwy/contrib/math/math-inl.h
@@ -282,43 +282,49 @@ HWY_INLINE HWY_MAYBE_UNUSED T Estrin(T x, T c0, T c1) {
 }
 template <class T>
 HWY_INLINE HWY_MAYBE_UNUSED T Estrin(T x, T c0, T c1, T c2) {
-  T x2(x * x);
+  T x2 = Mul(x, x);
   return MulAdd(x2, c2, MulAdd(c1, x, c0));
 }
 template <class T>
 HWY_INLINE HWY_MAYBE_UNUSED T Estrin(T x, T c0, T c1, T c2, T c3) {
-  T x2(x * x);
+  T x2 = Mul(x, x);
   return MulAdd(x2, MulAdd(c3, x, c2), MulAdd(c1, x, c0));
 }
 template <class T>
 HWY_INLINE HWY_MAYBE_UNUSED T Estrin(T x, T c0, T c1, T c2, T c3, T c4) {
-  T x2(x * x), x4(x2 * x2);
+  T x2 = Mul(x, x);
+  T x4 = Mul(x2, x2);
   return MulAdd(x4, c4, MulAdd(x2, MulAdd(c3, x, c2), MulAdd(c1, x, c0)));
 }
 template <class T>
 HWY_INLINE HWY_MAYBE_UNUSED T Estrin(T x, T c0, T c1, T c2, T c3, T c4, T c5) {
-  T x2(x * x), x4(x2 * x2);
+  T x2 = Mul(x, x);
+  T x4 = Mul(x2, x2);
   return MulAdd(x4, MulAdd(c5, x, c4),
                 MulAdd(x2, MulAdd(c3, x, c2), MulAdd(c1, x, c0)));
 }
 template <class T>
 HWY_INLINE HWY_MAYBE_UNUSED T Estrin(T x, T c0, T c1, T c2, T c3, T c4, T c5,
                                      T c6) {
-  T x2(x * x), x4(x2 * x2);
+  T x2 = Mul(x, x);
+  T x4 = Mul(x2, x2);
   return MulAdd(x4, MulAdd(x2, c6, MulAdd(c5, x, c4)),
                 MulAdd(x2, MulAdd(c3, x, c2), MulAdd(c1, x, c0)));
 }
 template <class T>
 HWY_INLINE HWY_MAYBE_UNUSED T Estrin(T x, T c0, T c1, T c2, T c3, T c4, T c5,
                                      T c6, T c7) {
-  T x2(x * x), x4(x2 * x2);
+  T x2 = Mul(x, x);
+  T x4 = Mul(x2, x2);
   return MulAdd(x4, MulAdd(x2, MulAdd(c7, x, c6), MulAdd(c5, x, c4)),
                 MulAdd(x2, MulAdd(c3, x, c2), MulAdd(c1, x, c0)));
 }
 template <class T>
 HWY_INLINE HWY_MAYBE_UNUSED T Estrin(T x, T c0, T c1, T c2, T c3, T c4, T c5,
                                      T c6, T c7, T c8) {
-  T x2(x * x), x4(x2 * x2), x8(x4 * x4);
+  T x2 = Mul(x, x);
+  T x4 = Mul(x2, x2);
+  T x8 = Mul(x4, x4);
   return MulAdd(x8, c8,
                 MulAdd(x4, MulAdd(x2, MulAdd(c7, x, c6), MulAdd(c5, x, c4)),
                        MulAdd(x2, MulAdd(c3, x, c2), MulAdd(c1, x, c0))));
@@ -326,7 +332,9 @@ HWY_INLINE HWY_MAYBE_UNUSED T Estrin(T x, T c0, T c1, T c2, T c3, T c4, T c5,
 template <class T>
 HWY_INLINE HWY_MAYBE_UNUSED T Estrin(T x, T c0, T c1, T c2, T c3, T c4, T c5,
                                      T c6, T c7, T c8, T c9) {
-  T x2(x * x), x4(x2 * x2), x8(x4 * x4);
+  T x2 = Mul(x, x);
+  T x4 = Mul(x2, x2);
+  T x8 = Mul(x4, x4);
   return MulAdd(x8, MulAdd(c9, x, c8),
                 MulAdd(x4, MulAdd(x2, MulAdd(c7, x, c6), MulAdd(c5, x, c4)),
                        MulAdd(x2, MulAdd(c3, x, c2), MulAdd(c1, x, c0))));
@@ -334,7 +342,9 @@ HWY_INLINE HWY_MAYBE_UNUSED T Estrin(T x, T c0, T c1, T c2, T c3, T c4, T c5,
 template <class T>
 HWY_INLINE HWY_MAYBE_UNUSED T Estrin(T x, T c0, T c1, T c2, T c3, T c4, T c5,
                                      T c6, T c7, T c8, T c9, T c10) {
-  T x2(x * x), x4(x2 * x2), x8(x4 * x4);
+  T x2 = Mul(x, x);
+  T x4 = Mul(x2, x2);
+  T x8 = Mul(x4, x4);
   return MulAdd(x8, MulAdd(x2, c10, MulAdd(c9, x, c8)),
                 MulAdd(x4, MulAdd(x2, MulAdd(c7, x, c6), MulAdd(c5, x, c4)),
                        MulAdd(x2, MulAdd(c3, x, c2), MulAdd(c1, x, c0))));
@@ -342,7 +352,9 @@ HWY_INLINE HWY_MAYBE_UNUSED T Estrin(T x, T c0, T c1, T c2, T c3, T c4, T c5,
 template <class T>
 HWY_INLINE HWY_MAYBE_UNUSED T Estrin(T x, T c0, T c1, T c2, T c3, T c4, T c5,
                                      T c6, T c7, T c8, T c9, T c10, T c11) {
-  T x2(x * x), x4(x2 * x2), x8(x4 * x4);
+  T x2 = Mul(x, x);
+  T x4 = Mul(x2, x2);
+  T x8 = Mul(x4, x4);
   return MulAdd(x8, MulAdd(x2, MulAdd(c11, x, c10), MulAdd(c9, x, c8)),
                 MulAdd(x4, MulAdd(x2, MulAdd(c7, x, c6), MulAdd(c5, x, c4)),
                        MulAdd(x2, MulAdd(c3, x, c2), MulAdd(c1, x, c0))));
@@ -351,7 +363,9 @@ template <class T>
 HWY_INLINE HWY_MAYBE_UNUSED T Estrin(T x, T c0, T c1, T c2, T c3, T c4, T c5,
                                      T c6, T c7, T c8, T c9, T c10, T c11,
                                      T c12) {
-  T x2(x * x), x4(x2 * x2), x8(x4 * x4);
+  T x2 = Mul(x, x);
+  T x4 = Mul(x2, x2);
+  T x8 = Mul(x4, x4);
   return MulAdd(
       x8, MulAdd(x4, c12, MulAdd(x2, MulAdd(c11, x, c10), MulAdd(c9, x, c8))),
       MulAdd(x4, MulAdd(x2, MulAdd(c7, x, c6), MulAdd(c5, x, c4)),
@@ -361,7 +375,9 @@ template <class T>
 HWY_INLINE HWY_MAYBE_UNUSED T Estrin(T x, T c0, T c1, T c2, T c3, T c4, T c5,
                                      T c6, T c7, T c8, T c9, T c10, T c11,
                                      T c12, T c13) {
-  T x2(x * x), x4(x2 * x2), x8(x4 * x4);
+  T x2 = Mul(x, x);
+  T x4 = Mul(x2, x2);
+  T x8 = Mul(x4, x4);
   return MulAdd(x8,
                 MulAdd(x4, MulAdd(c13, x, c12),
                        MulAdd(x2, MulAdd(c11, x, c10), MulAdd(c9, x, c8))),
@@ -372,7 +388,9 @@ template <class T>
 HWY_INLINE HWY_MAYBE_UNUSED T Estrin(T x, T c0, T c1, T c2, T c3, T c4, T c5,
                                      T c6, T c7, T c8, T c9, T c10, T c11,
                                      T c12, T c13, T c14) {
-  T x2(x * x), x4(x2 * x2), x8(x4 * x4);
+  T x2 = Mul(x, x);
+  T x4 = Mul(x2, x2);
+  T x8 = Mul(x4, x4);
   return MulAdd(x8,
                 MulAdd(x4, MulAdd(x2, c14, MulAdd(c13, x, c12)),
                        MulAdd(x2, MulAdd(c11, x, c10), MulAdd(c9, x, c8))),
@@ -383,7 +401,9 @@ template <class T>
 HWY_INLINE HWY_MAYBE_UNUSED T Estrin(T x, T c0, T c1, T c2, T c3, T c4, T c5,
                                      T c6, T c7, T c8, T c9, T c10, T c11,
                                      T c12, T c13, T c14, T c15) {
-  T x2(x * x), x4(x2 * x2), x8(x4 * x4);
+  T x2 = Mul(x, x);
+  T x4 = Mul(x2, x2);
+  T x8 = Mul(x4, x4);
   return MulAdd(x8,
                 MulAdd(x4, MulAdd(x2, MulAdd(c15, x, c14), MulAdd(c13, x, c12)),
                        MulAdd(x2, MulAdd(c11, x, c10), MulAdd(c9, x, c8))),
@@ -394,7 +414,10 @@ template <class T>
 HWY_INLINE HWY_MAYBE_UNUSED T Estrin(T x, T c0, T c1, T c2, T c3, T c4, T c5,
                                      T c6, T c7, T c8, T c9, T c10, T c11,
                                      T c12, T c13, T c14, T c15, T c16) {
-  T x2(x * x), x4(x2 * x2), x8(x4 * x4), x16(x8 * x8);
+  T x2 = Mul(x, x);
+  T x4 = Mul(x2, x2);
+  T x8 = Mul(x4, x4);
+  T x16 = Mul(x8, x8);
   return MulAdd(
       x16, c16,
       MulAdd(x8,
@@ -407,7 +430,10 @@ template <class T>
 HWY_INLINE HWY_MAYBE_UNUSED T Estrin(T x, T c0, T c1, T c2, T c3, T c4, T c5,
                                      T c6, T c7, T c8, T c9, T c10, T c11,
                                      T c12, T c13, T c14, T c15, T c16, T c17) {
-  T x2(x * x), x4(x2 * x2), x8(x4 * x4), x16(x8 * x8);
+  T x2 = Mul(x, x);
+  T x4 = Mul(x2, x2);
+  T x8 = Mul(x4, x4);
+  T x16 = Mul(x8, x8);
   return MulAdd(
       x16, MulAdd(c17, x, c16),
       MulAdd(x8,
@@ -421,7 +447,10 @@ HWY_INLINE HWY_MAYBE_UNUSED T Estrin(T x, T c0, T c1, T c2, T c3, T c4, T c5,
                                      T c6, T c7, T c8, T c9, T c10, T c11,
                                      T c12, T c13, T c14, T c15, T c16, T c17,
                                      T c18) {
-  T x2(x * x), x4(x2 * x2), x8(x4 * x4), x16(x8 * x8);
+  T x2 = Mul(x, x);
+  T x4 = Mul(x2, x2);
+  T x8 = Mul(x4, x4);
+  T x16 = Mul(x8, x8);
   return MulAdd(
       x16, MulAdd(x2, c18, MulAdd(c17, x, c16)),
       MulAdd(x8,
@@ -497,8 +526,8 @@ struct AtanImpl<float> {
     const auto k6 = Set(d, -0.0159569028764963150024414f);
     const auto k7 = Set(d, +0.00282363896258175373077393f);
 
-    const auto y = (x * x);
-    return MulAdd(Estrin(y, k0, k1, k2, k3, k4, k5, k6, k7), (y * x), x);
+    const auto y = Mul(x, x);
+    return MulAdd(Estrin(y, k0, k1, k2, k3, k4, k5, k6, k7), Mul(y, x), x);
   }
 };
 
@@ -529,10 +558,10 @@ struct AtanImpl<double> {
     const auto k17 = Set(d, +0.000209850076645816976906797);
     const auto k18 = Set(d, -1.88796008463073496563746e-5);
 
-    const auto y = (x * x);
+    const auto y = Mul(x, x);
     return MulAdd(Estrin(y, k0, k1, k2, k3, k4, k5, k6, k7, k8, k9, k10, k11,
                          k12, k13, k14, k15, k16, k17, k18),
-                  (y * x), x);
+                  Mul(y, x), x);
   }
 };
 
@@ -553,8 +582,8 @@ struct CosSinImpl<float> {
     const auto k2 = Set(d, -1.981069071916863322258e-4f);
     const auto k3 = Set(d, +2.6083159809786593541503e-6f);
 
-    const auto y(x * x);
-    return MulAdd(Estrin(y, k0, k1, k2, k3), (y * x), x);
+    const auto y = Mul(x, x);
+    return MulAdd(Estrin(y, k0, k1, k2, k3), Mul(y, x), x);
   }
 
   template <class D, class V, class VI32>
@@ -628,8 +657,8 @@ struct CosSinImpl<double> {
     const auto k7 = Set(d, +2.81009972710863200091251e-15);
     const auto k8 = Set(d, -7.97255955009037868891952e-18);
 
-    const auto y(x * x);
-    return MulAdd(Estrin(y, k0, k1, k2, k3, k4, k5, k6, k7, k8), (y * x), x);
+    const auto y = Mul(x, x);
+    return MulAdd(Estrin(y, k0, k1, k2, k3, k4, k5, k6, k7, k8), Mul(y, x), x);
   }
 
   template <class D, class V, class VI32>
@@ -702,7 +731,7 @@ struct ExpImpl<float> {
     const auto k4 = Set(d, +0.00139304355252534151077271f);
     const auto k5 = Set(d, +0.000198527617612853646278381f);
 
-    return MulAdd(Estrin(x, k0, k1, k2, k3, k4, k5), (x * x), x);
+    return MulAdd(Estrin(x, k0, k1, k2, k3, k4, k5), Mul(x, x), x);
   }
 
   // Computes 2^x, where x is an integer.
@@ -710,14 +739,14 @@ struct ExpImpl<float> {
   HWY_INLINE Vec<D> Pow2I(D d, VI32 x) {
     const Rebind<int32_t, D> di32;
     const VI32 kOffset = Set(di32, 0x7F);
-    return BitCast(d, ShiftLeft<23>(x + kOffset));
+    return BitCast(d, ShiftLeft<23>(Add(x, kOffset)));
   }
 
   // Sets the exponent of 'x' to 2^e.
   template <class D, class V, class VI32>
   HWY_INLINE V LoadExpShortRange(D d, V x, VI32 e) {
     const VI32 y = ShiftRight<1>(e);
-    return x * Pow2I(d, y) * Pow2I(d, e - y);
+    return Mul(Mul(x, Pow2I(d, y)), Pow2I(d, Sub(e, y)));
   }
 
   template <class D, class V, class VI32>
@@ -740,7 +769,8 @@ struct LogImpl<float> {
   HWY_INLINE Vec<Rebind<int32_t, D>> Log2p1NoSubnormal(D /*d*/, V x) {
     const Rebind<int32_t, D> di32;
     const Rebind<uint32_t, D> du32;
-    return BitCast(di32, ShiftRight<23>(BitCast(du32, x))) - Set(di32, 0x7F);
+    const auto kBias = Set(di32, 0x7F);
+    return Sub(BitCast(di32, ShiftRight<23>(BitCast(du32, x))), kBias);
   }
 
   // Approximates Log(x) over the range [sqrt(2) / 2, sqrt(2)].
@@ -751,9 +781,9 @@ struct LogImpl<float> {
     const V k2 = Set(d, 0.28498786688f);
     const V k3 = Set(d, 0.24279078841f);
 
-    const V x2 = (x * x);
-    const V x4 = (x2 * x2);
-    return MulAdd(MulAdd(k2, x4, k0), x2, (MulAdd(k3, x4, k1) * x4));
+    const V x2 = Mul(x, x);
+    const V x4 = Mul(x2, x2);
+    return MulAdd(MulAdd(k2, x4, k0), x2, Mul(MulAdd(k3, x4, k1), x4));
   }
 };
 
@@ -781,7 +811,7 @@ struct ExpImpl<double> {
     const auto k10 = Set(d, +2.08860621107283687536341e-9);
 
     return MulAdd(Estrin(x, k0, k1, k2, k3, k4, k5, k6, k7, k8, k9, k10),
-                  (x * x), x);
+                  Mul(x, x), x);
   }
 
   // Computes 2^x, where x is an integer.
@@ -790,14 +820,14 @@ struct ExpImpl<double> {
     const Rebind<int32_t, D> di32;
     const Rebind<int64_t, D> di64;
     const VI32 kOffset = Set(di32, 0x3FF);
-    return BitCast(d, ShiftLeft<52>(PromoteTo(di64, x + kOffset)));
+    return BitCast(d, ShiftLeft<52>(PromoteTo(di64, Add(x, kOffset))));
   }
 
   // Sets the exponent of 'x' to 2^e.
   template <class D, class V, class VI32>
   HWY_INLINE V LoadExpShortRange(D d, V x, VI32 e) {
     const VI32 y = ShiftRight<1>(e);
-    return (x * Pow2I(d, y) * Pow2I(d, e - y));
+    return Mul(Mul(x, Pow2I(d, y)), Pow2I(d, Sub(e, y)));
   }
 
   template <class D, class V, class VI32>
@@ -820,7 +850,8 @@ struct LogImpl<double> {
   HWY_INLINE Vec<Rebind<int64_t, D>> Log2p1NoSubnormal(D /*d*/, V x) {
     const Rebind<int64_t, D> di64;
     const Rebind<uint64_t, D> du64;
-    return BitCast(di64, ShiftRight<52>(BitCast(du64, x))) - Set(di64, 0x3FF);
+    return Sub(BitCast(di64, ShiftRight<52>(BitCast(du64, x))),
+               Set(di64, 0x3FF));
   }
 
   // Approximates Log(x) over the range [sqrt(2) / 2, sqrt(2)].
@@ -834,10 +865,10 @@ struct LogImpl<double> {
     const V k5 = Set(d, 0.1531383769920937332);
     const V k6 = Set(d, 0.1479819860511658591);
 
-    const V x2 = (x * x);
-    const V x4 = (x2 * x2);
+    const V x2 = Mul(x, x);
+    const V x4 = Mul(x2, x2);
     return MulAdd(MulAdd(MulAdd(MulAdd(k6, x4, k4), x4, k2), x4, k0), x2,
-                  (MulAdd(MulAdd(k5, x4, k3), x4, k1) * x4));
+                  (Mul(MulAdd(MulAdd(k5, x4, k3), x4, k1), x4)));
   }
 };
 
@@ -877,31 +908,32 @@ HWY_INLINE V Log(const D d, V x) {
   VI exp_bits;
   V exp;
   if (kAllowSubnormals == true) {
-    const auto is_denormal = (x < kMinNormal);
-    x = IfThenElse(is_denormal, (x * kScale), x);
+    const auto is_denormal = Lt(x, kMinNormal);
+    x = IfThenElse(is_denormal, Mul(x, kScale), x);
 
     // Compute the new exponent.
-    exp_bits = (BitCast(di, x) + (kExpMask - kMagic));
+    exp_bits = Add(BitCast(di, x), Sub(kExpMask, kMagic));
     const VI exp_scale =
         BitCast(di, IfThenElseZero(is_denormal, BitCast(d, kExpScale)));
     exp = ConvertTo(
-        d, exp_scale + impl.Log2p1NoSubnormal(d, BitCast(d, exp_bits)));
+        d, Add(exp_scale, impl.Log2p1NoSubnormal(d, BitCast(d, exp_bits))));
   } else {
     // Compute the new exponent.
-    exp_bits = (BitCast(di, x) + (kExpMask - kMagic));
+    exp_bits = Add(BitCast(di, x), Sub(kExpMask, kMagic));
     exp = ConvertTo(d, impl.Log2p1NoSubnormal(d, BitCast(d, exp_bits)));
   }
 
   // Renormalize.
   const V y = Or(And(x, BitCast(d, kLowerBits)),
-                 BitCast(d, ((exp_bits & kManMask) + kMagic)));
+                 BitCast(d, Add(And(exp_bits, kManMask), kMagic)));
 
   // Approximate and reconstruct.
-  const V ym1 = (y - kOne);
-  const V z = (ym1 / (y + kOne));
+  const V ym1 = Sub(y, kOne);
+  const V z = Div(ym1, Add(y, kOne));
 
-  return MulSub(exp, kLn2Hi,
-                (MulSub(z, (ym1 - impl.LogPoly(d, z)), (exp * kLn2Lo)) - ym1));
+  return MulSub(
+      exp, kLn2Hi,
+      Sub(MulSub(z, Sub(ym1, impl.LogPoly(d, z)), Mul(exp, kLn2Lo)), ym1));
 }
 
 }  // namespace impl
@@ -912,22 +944,24 @@ HWY_INLINE V Acos(const D d, V x) {
 
   const V kZero = Zero(d);
   const V kHalf = Set(d, +0.5);
-  const V kOne = Set(d, +1.0);
-  const V kTwo = Set(d, +2.0);
   const V kPi = Set(d, +3.14159265358979323846264);
   const V kPiOverTwo = Set(d, +1.57079632679489661923132169);
 
   const V sign_x = And(SignBit(d), x);
   const V abs_x = Xor(x, sign_x);
-  const auto mask = (abs_x < kHalf);
-  const V yy = IfThenElse(mask, (abs_x * abs_x), ((kOne - abs_x) * kHalf));
+  const auto mask = Lt(abs_x, kHalf);
+  const V yy =
+      IfThenElse(mask, Mul(abs_x, abs_x), NegMulAdd(abs_x, kHalf, kHalf));
   const V y = IfThenElse(mask, abs_x, Sqrt(yy));
 
   impl::AsinImpl<LaneType> impl;
-  const V t = (impl.AsinPoly(d, yy, y) * (y * yy));
-  const V z = IfThenElse(mask, (kPiOverTwo - (Xor(y, sign_x) + Xor(t, sign_x))),
-                         ((t + y) * kTwo));
-  return IfThenElse(Or(mask, (x >= kZero)), z, (kPi - z));
+  const V t = Mul(impl.AsinPoly(d, yy, y), Mul(y, yy));
+
+  const V t_plus_y = Add(t, y);
+  const V z =
+      IfThenElse(mask, Sub(kPiOverTwo, Add(Xor(y, sign_x), Xor(t, sign_x))),
+                 Add(t_plus_y, t_plus_y));
+  return IfThenElse(Or(mask, Ge(x, kZero)), z, Sub(kPi, z));
 }
 
 template <class D, class V>
@@ -937,21 +971,22 @@ HWY_INLINE V Acosh(const D d, V x) {
   const V kOne = Set(d, +1.0);
   const V kTwo = Set(d, +2.0);
 
-  const auto is_x_large = (x > kLarge);
-  const auto is_x_gt_2 = (x > kTwo);
+  const auto is_x_large = Gt(x, kLarge);
+  const auto is_x_gt_2 = Gt(x, kTwo);
 
-  const V x_minus_1 = (x - kOne);
-  const V y0 = MulSub(kTwo, x, (kOne / (Sqrt(MulSub(x, x, kOne)) + x)));
+  const V x_minus_1 = Sub(x, kOne);
+  const V y0 = MulSub(kTwo, x, Div(kOne, Add(Sqrt(MulSub(x, x, kOne)), x)));
   const V y1 =
-      (Sqrt(MulAdd(x_minus_1, kTwo, (x_minus_1 * x_minus_1))) + x_minus_1);
+      Add(Sqrt(MulAdd(x_minus_1, kTwo, Mul(x_minus_1, x_minus_1))), x_minus_1);
   const V y2 =
-      IfThenElse(is_x_gt_2, IfThenElse(is_x_large, x, y0), (y1 + kOne));
+      IfThenElse(is_x_gt_2, IfThenElse(is_x_large, x, y0), Add(y1, kOne));
   const V z = impl::Log<D, V, /*kAllowSubnormals=*/false>(d, y2);
 
-  const auto is_pole = y2 == kOne;
-  const auto divisor = IfThenZeroElse(is_pole, y2) - kOne;
-  return IfThenElse(is_x_gt_2, z, IfThenElse(is_pole, y1, z * y1 / divisor)) +
-         IfThenElseZero(is_x_large, kLog2);
+  const auto is_pole = Eq(y2, kOne);
+  const auto divisor = Sub(IfThenZeroElse(is_pole, y2), kOne);
+  return Add(IfThenElse(is_x_gt_2, z,
+                        IfThenElse(is_pole, y1, Div(Mul(z, y1), divisor))),
+             IfThenElseZero(is_x_large, kLog2));
 }
 
 template <class D, class V>
@@ -959,19 +994,19 @@ HWY_INLINE V Asin(const D d, V x) {
   using LaneType = LaneType<V>;
 
   const V kHalf = Set(d, +0.5);
-  const V kOne = Set(d, +1.0);
   const V kTwo = Set(d, +2.0);
   const V kPiOverTwo = Set(d, +1.57079632679489661923132169);
 
   const V sign_x = And(SignBit(d), x);
   const V abs_x = Xor(x, sign_x);
-  const auto mask = (abs_x < kHalf);
-  const V yy = IfThenElse(mask, (abs_x * abs_x), (kOne - abs_x) * kHalf);
+  const auto mask = Lt(abs_x, kHalf);
+  const V yy =
+      IfThenElse(mask, Mul(abs_x, abs_x), NegMulAdd(abs_x, kHalf, kHalf));
   const V y = IfThenElse(mask, abs_x, Sqrt(yy));
 
   impl::AsinImpl<LaneType> impl;
-  const V z0 = MulAdd(impl.AsinPoly(d, yy, y), (yy * y), y);
-  const V z1 = (kPiOverTwo - (z0 * kTwo));
+  const V z0 = MulAdd(impl.AsinPoly(d, yy, y), Mul(yy, y), y);
+  const V z1 = NegMulAdd(z0, kTwo, kPiOverTwo);
   return Or(IfThenElse(mask, z0, z1), sign_x);
 }
 
@@ -986,23 +1021,23 @@ HWY_INLINE V Asinh(const D d, V x) {
   const V sign_x = And(SignBit(d), x);  // Extract the sign bit
   const V abs_x = Xor(x, sign_x);
 
-  const auto is_x_large = (abs_x > kLarge);
-  const auto is_x_lt_2 = (abs_x < kTwo);
+  const auto is_x_large = Gt(abs_x, kLarge);
+  const auto is_x_lt_2 = Lt(abs_x, kTwo);
 
-  const V x2 = (x * x);
-  const V sqrt_x2_plus_1 = Sqrt(x2 + kOne);
+  const V x2 = Mul(x, x);
+  const V sqrt_x2_plus_1 = Sqrt(Add(x2, kOne));
 
-  const V y0 = MulAdd(abs_x, kTwo, (kOne / (sqrt_x2_plus_1 + abs_x)));
-  const V y1 = ((x2 / (sqrt_x2_plus_1 + kOne)) + abs_x);
+  const V y0 = MulAdd(abs_x, kTwo, Div(kOne, Add(sqrt_x2_plus_1, abs_x)));
+  const V y1 = Add(Div(x2, Add(sqrt_x2_plus_1, kOne)), abs_x);
   const V y2 =
-      IfThenElse(is_x_lt_2, (y1 + kOne), IfThenElse(is_x_large, abs_x, y0));
+      IfThenElse(is_x_lt_2, Add(y1, kOne), IfThenElse(is_x_large, abs_x, y0));
   const V z = impl::Log<D, V, /*kAllowSubnormals=*/false>(d, y2);
 
-  const auto is_pole = y2 == kOne;
-  const auto divisor = IfThenZeroElse(is_pole, y2) - kOne;
-  const auto large = IfThenElse(is_pole, y1, z * y1 / divisor);
-  const V y = IfThenElse(abs_x < kSmall, x, large);
-  return Or((IfThenElse(is_x_lt_2, y, z) + IfThenElseZero(is_x_large, kLog2)),
+  const auto is_pole = Eq(y2, kOne);
+  const auto divisor = Sub(IfThenZeroElse(is_pole, y2), kOne);
+  const auto large = IfThenElse(is_pole, y1, Div(Mul(z, y1), divisor));
+  const V y = IfThenElse(Lt(abs_x, kSmall), x, large);
+  return Or(Add(IfThenElse(is_x_lt_2, y, z), IfThenElseZero(is_x_large, kLog2)),
             sign_x);
 }
 
@@ -1015,12 +1050,12 @@ HWY_INLINE V Atan(const D d, V x) {
 
   const V sign = And(SignBit(d), x);
   const V abs_x = Xor(x, sign);
-  const auto mask = (abs_x > kOne);
+  const auto mask = Gt(abs_x, kOne);
 
   impl::AtanImpl<LaneType> impl;
   const auto divisor = IfThenElse(mask, abs_x, kOne);
-  const V y = impl.AtanPoly(d, IfThenElse(mask, kOne / divisor, abs_x));
-  return Or(IfThenElse(mask, (kPiOverTwo - y), y), sign);
+  const V y = impl.AtanPoly(d, IfThenElse(mask, Div(kOne, divisor), abs_x));
+  return Or(IfThenElse(mask, Sub(kPiOverTwo, y), y), sign);
 }
 
 template <class D, class V>
@@ -1030,7 +1065,8 @@ HWY_INLINE V Atanh(const D d, V x) {
 
   const V sign = And(SignBit(d), x);  // Extract the sign bit
   const V abs_x = Xor(x, sign);
-  return Log1p(d, ((abs_x + abs_x) / (kOne - abs_x))) * Xor(kHalf, sign);
+  return Mul(Log1p(d, Div(Add(abs_x, abs_x), Sub(kOne, abs_x))),
+             Xor(kHalf, sign));
 }
 
 template <class D, class V>
@@ -1049,7 +1085,7 @@ HWY_INLINE V Cos(const D d, V x) {
   const V y = Abs(x);  // cos(x) == cos(|x|)
 
   // Compute the quadrant, q = int(|x| / pi) * 2 + 1
-  const VI32 q = (ShiftLeft<1>(impl.ToInt32(d, y * kOneOverPi)) + kOne);
+  const VI32 q = Add(ShiftLeft<1>(impl.ToInt32(d, Mul(y, kOneOverPi))), kOne);
 
   // Reduce range, apply sign, and approximate.
   return impl.Poly(
@@ -1076,8 +1112,8 @@ HWY_INLINE V Exp(const D d, V x) {
 
   // Reduce, approximate, and then reconstruct.
   const V y = impl.LoadExpShortRange(
-      d, (impl.ExpPoly(d, impl.ExpReduce(d, x, q)) + kOne), q);
-  return IfThenElseZero(x >= kLowerBound, y);
+      d, Add(impl.ExpPoly(d, impl.ExpReduce(d, x, q)), kOne), q);
+  return IfThenElseZero(Ge(x, kLowerBound), y);
 }
 
 template <class D, class V>
@@ -1102,9 +1138,9 @@ HWY_INLINE V Expm1(const D d, V x) {
 
   // Reduce, approximate, and then reconstruct.
   const V y = impl.ExpPoly(d, impl.ExpReduce(d, x, q));
-  const V z = IfThenElse(Abs(x) < kLn2Over2, y,
-                         impl.LoadExpShortRange(d, (y + kOne), q) - kOne);
-  return IfThenElse(x < kLowerBound, kNegOne, z);
+  const V z = IfThenElse(Lt(Abs(x), kLn2Over2), y,
+                         Sub(impl.LoadExpShortRange(d, Add(y, kOne), q), kOne));
+  return IfThenElse(Lt(x, kLowerBound), kNegOne, z);
 }
 
 template <class D, class V>
@@ -1114,24 +1150,24 @@ HWY_INLINE V Log(const D d, V x) {
 
 template <class D, class V>
 HWY_INLINE V Log10(const D d, V x) {
-  return Log(d, x) * Set(d, 0.4342944819032518276511);
+  return Mul(Log(d, x), Set(d, 0.4342944819032518276511));
 }
 
 template <class D, class V>
 HWY_INLINE V Log1p(const D d, V x) {
   const V kOne = Set(d, +1.0);
 
-  const V y = x + kOne;
-  const auto is_pole = y == kOne;
-  const auto divisor = IfThenZeroElse(is_pole, y) - kOne;
+  const V y = Add(x, kOne);
+  const auto is_pole = Eq(y, kOne);
+  const auto divisor = Sub(IfThenZeroElse(is_pole, y), kOne);
   const auto non_pole =
-      impl::Log<D, V, /*kAllowSubnormals=*/false>(d, y) * (x / divisor);
+      Mul(impl::Log<D, V, /*kAllowSubnormals=*/false>(d, y), Div(x, divisor));
   return IfThenElse(is_pole, x, non_pole);
 }
 
 template <class D, class V>
 HWY_INLINE V Log2(const D d, V x) {
-  return Log(d, x) * Set(d, 1.44269504088896340735992);
+  return Mul(Log(d, x), Set(d, 1.44269504088896340735992));
 }
 
 template <class D, class V>
@@ -1167,7 +1203,7 @@ HWY_INLINE V Sinh(const D d, V x) {
   const V sign = And(SignBit(d), x);  // Extract the sign bit
   const V abs_x = Xor(x, sign);
   const V y = Expm1(d, abs_x);
-  const V z = ((y + kTwo) / (y + kOne) * (y * kHalf));
+  const V z = Mul(Div(Add(y, kTwo), Add(y, kOne)), Mul(y, kHalf));
   return Xor(z, sign);  // Reapply the sign bit
 }
 
@@ -1179,8 +1215,8 @@ HWY_INLINE V Tanh(const D d, V x) {
 
   const V sign = And(SignBit(d), x);  // Extract the sign bit
   const V abs_x = Xor(x, sign);
-  const V y = Expm1(d, abs_x * kTwo);
-  const V z = IfThenElse((abs_x > kLimit), kOne, (y / (y + kTwo)));
+  const V y = Expm1(d, Mul(abs_x, kTwo));
+  const V z = IfThenElse(Gt(abs_x, kLimit), kOne, Div(y, Add(y, kTwo)));
   return Xor(z, sign);  // Reapply the sign bit
 }
 
diff --git a/third_party/highway/hwy/contrib/math/math_test.cc b/third_party/highway/hwy/contrib/math/math_test.cc
index 368ecfe062f64..798f5d6d920c2 100644
--- a/third_party/highway/hwy/contrib/math/math_test.cc
+++ b/third_party/highway/hwy/contrib/math/math_test.cc
@@ -51,8 +51,10 @@ void TestMath(const std::string name, T (*fx1)(T), Vec<D> (*fxN)(D, Vec<D>),
   }
 
   uint64_t max_ulp = 0;
-#if HWY_ARCH_ARM
   // Emulation is slower, so cannot afford as many.
+#if HWY_ARCH_RVV
+  constexpr UintT kSamplesPerRange = 2500;
+#elif HWY_ARCH_ARM
   constexpr UintT kSamplesPerRange = 25000;
 #else
   constexpr UintT kSamplesPerRange = 100000;
@@ -60,9 +62,9 @@ void TestMath(const std::string name, T (*fx1)(T), Vec<D> (*fxN)(D, Vec<D>),
   for (int range_index = 0; range_index < range_count; ++range_index) {
     const UintT start = ranges[range_index][0];
     const UintT stop = ranges[range_index][1];
-    const UintT step = std::max<UintT>(1, ((stop - start) / kSamplesPerRange));
+    const UintT step = HWY_MAX(1, ((stop - start) / kSamplesPerRange));
     for (UintT value_bits = start; value_bits <= stop; value_bits += step) {
-      const T value = BitCast<T>(std::min(value_bits, stop));
+      const T value = BitCast<T>(HWY_MIN(value_bits, stop));
       const T actual = GetLane(fxN(d, Set(d, value)));
       const T expected = fx1(value);
 
@@ -74,7 +76,7 @@ void TestMath(const std::string name, T (*fx1)(T), Vec<D> (*fxN)(D, Vec<D>),
 #endif
 
       const auto ulp = ComputeUlpDelta(actual, expected);
-      max_ulp = std::max<uint64_t>(max_ulp, ulp);
+      max_ulp = HWY_MAX(max_ulp, ulp);
       if (ulp > max_error_ulp) {
         std::cout << name << "<" << (kIsF32 ? "F32x" : "F64x") << Lanes(d)
                   << ">(" << value << ") expected: " << expected
@@ -88,6 +90,25 @@ void TestMath(const std::string name, T (*fx1)(T), Vec<D> (*fxN)(D, Vec<D>),
             << ", Max ULP: " << max_ulp << std::endl;
 }
 
+// TODO(janwas): remove once RVV supports fractional LMUL
+#undef DEFINE_MATH_TEST_FUNC
+#if HWY_TARGET == HWY_RVV
+
+#define DEFINE_MATH_TEST_FUNC(NAME)                    \
+  HWY_NOINLINE void TestAll##NAME() {                  \
+    ForFloatTypes(ForShrinkableVectors<Test##NAME>()); \
+  }
+
+#else
+
+#define DEFINE_MATH_TEST_FUNC(NAME)                 \
+  HWY_NOINLINE void TestAll##NAME() {               \
+    ForFloatTypes(ForPartialVectors<Test##NAME>()); \
+  }
+
+#endif
+
+#undef DEFINE_MATH_TEST
 #define DEFINE_MATH_TEST(NAME, F32x1, F32xN, F32_MIN, F32_MAX, F32_ERROR, \
                          F64x1, F64xN, F64_MIN, F64_MAX, F64_ERROR)       \
   struct Test##NAME {                                                     \
@@ -102,9 +123,7 @@ void TestMath(const std::string name, T (*fx1)(T), Vec<D> (*fxN)(D, Vec<D>),
       }                                                                   \
     }                                                                     \
   };                                                                      \
-  HWY_NOINLINE void TestAll##NAME() {                                     \
-    ForFloatTypes(ForPartialVectors<Test##NAME>());                       \
-  }
+  DEFINE_MATH_TEST_FUNC(NAME)
 
 // Floating point values closest to but less than 1.0
 const float kNearOneF = BitCast<float>(0x3F7FFFFF);
@@ -167,6 +186,7 @@ DEFINE_MATH_TEST(Tanh,
 HWY_AFTER_NAMESPACE();
 
 #if HWY_ONCE
+
 namespace hwy {
 HWY_BEFORE_TEST(HwyMathTest);
 HWY_EXPORT_AND_TEST_P(HwyMathTest, TestAllAcos);
@@ -186,4 +206,11 @@ HWY_EXPORT_AND_TEST_P(HwyMathTest, TestAllSin);
 HWY_EXPORT_AND_TEST_P(HwyMathTest, TestAllSinh);
 HWY_EXPORT_AND_TEST_P(HwyMathTest, TestAllTanh);
 }  // namespace hwy
+
+// Ought not to be necessary, but without this, no tests run on RVV.
+int main(int argc, char **argv) {
+  ::testing::InitGoogleTest(&argc, argv);
+  return RUN_ALL_TESTS();
+}
+
 #endif
diff --git a/third_party/highway/hwy/detect_compiler_arch.h b/third_party/highway/hwy/detect_compiler_arch.h
new file mode 100644
index 0000000000000..8a1e3ac31e978
--- /dev/null
+++ b/third_party/highway/hwy/detect_compiler_arch.h
@@ -0,0 +1,188 @@
+// Copyright 2020 Google LLC
+//
+// Licensed under the Apache License, Version 2.0 (the "License");
+// you may not use this file except in compliance with the License.
+// You may obtain a copy of the License at
+//
+//      http://www.apache.org/licenses/LICENSE-2.0
+//
+// Unless required by applicable law or agreed to in writing, software
+// distributed under the License is distributed on an "AS IS" BASIS,
+// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+// See the License for the specific language governing permissions and
+// limitations under the License.
+
+#ifndef HIGHWAY_HWY_DETECT_COMPILER_ARCH_H_
+#define HIGHWAY_HWY_DETECT_COMPILER_ARCH_H_
+
+// Detects compiler and arch from predefined macros. Zero dependencies for
+// inclusion by foreach_target.h.
+
+// Add to #if conditions to prevent IDE from graying out code.
+#if (defined __CDT_PARSER__) || (defined __INTELLISENSE__) || \
+    (defined Q_CREATOR_RUN) || (defined(__CLANGD__))
+#define HWY_IDE 1
+#else
+#define HWY_IDE 0
+#endif
+
+//------------------------------------------------------------------------------
+// Compiler
+
+// clang-cl defines _MSC_VER but doesn't behave like MSVC in other aspects like
+// used in HWY_DIAGNOSTICS(). We include a check that we are not clang for that
+// purpose.
+#if defined(_MSC_VER) && !defined(__clang__)
+#define HWY_COMPILER_MSVC _MSC_VER
+#else
+#define HWY_COMPILER_MSVC 0
+#endif
+
+#ifdef __INTEL_COMPILER
+#define HWY_COMPILER_ICC __INTEL_COMPILER
+#else
+#define HWY_COMPILER_ICC 0
+#endif
+
+#ifdef __GNUC__
+#define HWY_COMPILER_GCC (__GNUC__ * 100 + __GNUC_MINOR__)
+#else
+#define HWY_COMPILER_GCC 0
+#endif
+
+// Clang can masquerade as MSVC/GCC, in which case both are set.
+#ifdef __clang__
+#ifdef __APPLE__
+// Apple LLVM version is unrelated to the actual Clang version, which we need
+// for enabling workarounds. Use the presence of warning flags to deduce it.
+// Adapted from https://github.com/simd-everywhere/simde/ simde-detect-clang.h.
+#if __has_warning("-Wformat-insufficient-args")
+#define HWY_COMPILER_CLANG 1200
+#elif __has_warning("-Wimplicit-const-int-float-conversion")
+#define HWY_COMPILER_CLANG 1100
+#elif __has_warning("-Wmisleading-indentation")
+#define HWY_COMPILER_CLANG 1000
+#elif defined(__FILE_NAME__)
+#define HWY_COMPILER_CLANG 900
+#elif __has_warning("-Wextra-semi-stmt") || \
+    __has_builtin(__builtin_rotateleft32)
+#define HWY_COMPILER_CLANG 800
+#elif __has_warning("-Wc++98-compat-extra-semi")
+#define HWY_COMPILER_CLANG 700
+#else  // Anything older than 7.0 is not recommended for Highway.
+#define HWY_COMPILER_CLANG 600
+#endif  // __has_warning chain
+#else   // Non-Apple: normal version
+#define HWY_COMPILER_CLANG (__clang_major__ * 100 + __clang_minor__)
+#endif
+#else  // Not clang
+#define HWY_COMPILER_CLANG 0
+#endif
+
+// More than one may be nonzero, but we want at least one.
+#if !HWY_COMPILER_MSVC && !HWY_COMPILER_ICC && !HWY_COMPILER_GCC && \
+    !HWY_COMPILER_CLANG
+#error "Unsupported compiler"
+#endif
+
+#ifdef __has_builtin
+#define HWY_HAS_BUILTIN(name) __has_builtin(name)
+#else
+#define HWY_HAS_BUILTIN(name) 0
+#endif
+
+#ifdef __has_attribute
+#define HWY_HAS_ATTRIBUTE(name) __has_attribute(name)
+#else
+#define HWY_HAS_ATTRIBUTE(name) 0
+#endif
+
+//------------------------------------------------------------------------------
+// Architecture
+
+#if defined(HWY_EMULATE_SVE)
+
+#define HWY_ARCH_X86_32 0
+#define HWY_ARCH_X86_64 0
+#define HWY_ARCH_X86 0
+#define HWY_ARCH_PPC 0
+#define HWY_ARCH_ARM_A64 1
+#define HWY_ARCH_ARM_V7 0
+#define HWY_ARCH_ARM 1
+#define HWY_ARCH_WASM 0
+#define HWY_ARCH_RVV 0
+
+#else
+
+#if defined(__i386__) || defined(_M_IX86)
+#define HWY_ARCH_X86_32 1
+#else
+#define HWY_ARCH_X86_32 0
+#endif
+
+#if defined(__x86_64__) || defined(_M_X64)
+#define HWY_ARCH_X86_64 1
+#else
+#define HWY_ARCH_X86_64 0
+#endif
+
+#if HWY_ARCH_X86_32 && HWY_ARCH_X86_64
+#error "Cannot have both x86-32 and x86-64"
+#endif
+
+#if HWY_ARCH_X86_32 || HWY_ARCH_X86_64
+#define HWY_ARCH_X86 1
+#else
+#define HWY_ARCH_X86 0
+#endif
+
+#if defined(__powerpc64__) || defined(_M_PPC)
+#define HWY_ARCH_PPC 1
+#else
+#define HWY_ARCH_PPC 0
+#endif
+
+#if defined(__ARM_ARCH_ISA_A64) || defined(__aarch64__) || defined(_M_ARM64)
+#define HWY_ARCH_ARM_A64 1
+#else
+#define HWY_ARCH_ARM_A64 0
+#endif
+
+#if defined(__arm__) || defined(_M_ARM)
+#define HWY_ARCH_ARM_V7 1
+#else
+#define HWY_ARCH_ARM_V7 0
+#endif
+
+#if HWY_ARCH_ARM_A64 && HWY_ARCH_ARM_V7
+#error "Cannot have both A64 and V7"
+#endif
+
+#if HWY_ARCH_ARM_A64 || HWY_ARCH_ARM_V7
+#define HWY_ARCH_ARM 1
+#else
+#define HWY_ARCH_ARM 0
+#endif
+
+#if defined(__EMSCRIPTEN__) || defined(__wasm__) || defined(__WASM__)
+#define HWY_ARCH_WASM 1
+#else
+#define HWY_ARCH_WASM 0
+#endif
+
+#ifdef __riscv
+#define HWY_ARCH_RVV 1
+#else
+#define HWY_ARCH_RVV 0
+#endif
+
+#endif // defined(HWY_EMULATE_SVE)
+
+// It is an error to detect multiple architectures at the same time, but OK to
+// detect none of the above.
+#if (HWY_ARCH_X86 + HWY_ARCH_PPC + HWY_ARCH_ARM + HWY_ARCH_WASM + \
+     HWY_ARCH_RVV) > 1
+#error "Must not detect more than one architecture"
+#endif
+
+#endif  // HIGHWAY_HWY_DETECT_COMPILER_ARCH_H_
diff --git a/third_party/highway/hwy/detect_targets.h b/third_party/highway/hwy/detect_targets.h
new file mode 100644
index 0000000000000..c3ef2e4f3ef4a
--- /dev/null
+++ b/third_party/highway/hwy/detect_targets.h
@@ -0,0 +1,386 @@
+// Copyright 2021 Google LLC
+//
+// Licensed under the Apache License, Version 2.0 (the "License");
+// you may not use this file except in compliance with the License.
+// You may obtain a copy of the License at
+//
+//      http://www.apache.org/licenses/LICENSE-2.0
+//
+// Unless required by applicable law or agreed to in writing, software
+// distributed under the License is distributed on an "AS IS" BASIS,
+// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+// See the License for the specific language governing permissions and
+// limitations under the License.
+
+#ifndef HIGHWAY_HWY_DETECT_TARGETS_H_
+#define HIGHWAY_HWY_DETECT_TARGETS_H_
+
+// Defines targets and chooses which to enable.
+
+#include "hwy/detect_compiler_arch.h"
+
+//------------------------------------------------------------------------------
+// Optional configuration
+
+// See ../quick_reference.md for documentation of these macros.
+
+// Uncomment to override the default baseline determined from predefined macros:
+// #define HWY_BASELINE_TARGETS (HWY_SSE4 | HWY_SCALAR)
+
+// Uncomment to override the default blocklist:
+// #define HWY_BROKEN_TARGETS HWY_AVX3
+
+// Uncomment to definitely avoid generating those target(s):
+// #define HWY_DISABLED_TARGETS HWY_SSE4
+
+// Uncomment to avoid emitting BMI/BMI2/FMA instructions (allows generating
+// AVX2 target for VMs which support AVX2 but not the other instruction sets)
+// #define HWY_DISABLE_BMI2_FMA
+
+//------------------------------------------------------------------------------
+// Targets
+
+// Unique bit value for each target. A lower value is "better" (e.g. more lanes)
+// than a higher value within the same group/platform - see HWY_STATIC_TARGET.
+//
+// All values are unconditionally defined so we can test HWY_TARGETS without
+// first checking the HWY_ARCH_*.
+//
+// The C99 preprocessor evaluates #if expressions using intmax_t types, so we
+// can use 32-bit literals.
+
+// 1,2: reserved
+
+// Currently satisfiable by Ice Lake (VNNI, VPCLMULQDQ, VAES). Later to be
+// added: BF16 (Cooper Lake). VP2INTERSECT is only in Tiger Lake? We do not yet
+// have uses for VBMI, VBMI2, VPOPCNTDQ, BITALG, GFNI.
+#define HWY_AVX3_DL 4  // see HWY_WANT_AVX3_DL below
+#define HWY_AVX3 8
+#define HWY_AVX2 16
+// 32: reserved for AVX
+#define HWY_SSE4 64
+#define HWY_SSSE3 128
+// 0x100, 0x200: reserved for SSE3, SSE2
+
+// The highest bit in the HWY_TARGETS mask that a x86 target can have. Used for
+// dynamic dispatch. All x86 target bits must be lower or equal to
+// (1 << HWY_HIGHEST_TARGET_BIT_X86) and they can only use
+// HWY_MAX_DYNAMIC_TARGETS in total.
+#define HWY_HIGHEST_TARGET_BIT_X86 9
+
+#define HWY_SVE2 0x400
+#define HWY_SVE 0x800
+// 0x1000 reserved for Helium
+#define HWY_NEON 0x2000
+
+#define HWY_HIGHEST_TARGET_BIT_ARM 13
+
+// 0x4000, 0x8000 reserved
+#define HWY_PPC8 0x10000  // v2.07 or 3
+// 0x20000, 0x40000 reserved for prior VSX/AltiVec
+
+#define HWY_HIGHEST_TARGET_BIT_PPC 18
+
+// 0x80000 reserved
+#define HWY_WASM 0x100000
+
+#define HWY_HIGHEST_TARGET_BIT_WASM 20
+
+// 0x200000, 0x400000, 0x800000 reserved
+
+#define HWY_RVV 0x1000000
+
+#define HWY_HIGHEST_TARGET_BIT_RVV 24
+
+// 0x2000000, 0x4000000, 0x8000000, 0x10000000 reserved
+
+#define HWY_SCALAR 0x20000000
+
+#define HWY_HIGHEST_TARGET_BIT_SCALAR 29
+
+// Cannot use higher values, otherwise HWY_TARGETS computation might overflow.
+
+//------------------------------------------------------------------------------
+// Set default blocklists
+
+// Disabled means excluded from enabled at user's request. A separate config
+// macro allows disabling without deactivating the blocklist below.
+#ifndef HWY_DISABLED_TARGETS
+#define HWY_DISABLED_TARGETS 0
+#endif
+
+// Broken means excluded from enabled due to known compiler issues. Allow the
+// user to override this blocklist without any guarantee of success.
+#ifndef HWY_BROKEN_TARGETS
+
+// x86 clang-6: we saw multiple AVX2/3 compile errors and in one case invalid
+// SSE4 codegen (possibly only for msan), so disable all those targets.
+#if HWY_ARCH_X86 && (HWY_COMPILER_CLANG != 0 && HWY_COMPILER_CLANG < 700)
+#define HWY_BROKEN_TARGETS (HWY_SSE4 | HWY_AVX2 | HWY_AVX3 | HWY_AVX3_DL)
+// This entails a major speed reduction, so warn unless the user explicitly
+// opts in to scalar-only.
+#if !defined(HWY_COMPILE_ONLY_SCALAR)
+#pragma message("x86 Clang <= 6: define HWY_COMPILE_ONLY_SCALAR or upgrade.")
+#endif
+
+// 32-bit may fail to compile AVX2/3.
+#elif HWY_ARCH_X86_32
+#define HWY_BROKEN_TARGETS (HWY_AVX2 | HWY_AVX3 | HWY_AVX3_DL)
+
+// MSVC AVX3 support is buggy: https://github.com/Mysticial/Flops/issues/16
+#elif HWY_COMPILER_MSVC != 0
+#define HWY_BROKEN_TARGETS (HWY_AVX3 | HWY_AVX3_DL)
+
+// armv7be has not been tested and is not yet supported.
+#elif HWY_ARCH_ARM_V7 && (defined(__ARM_BIG_ENDIAN) || defined(__BIG_ENDIAN))
+#define HWY_BROKEN_TARGETS (HWY_NEON)
+
+// SVE[2] require recent clang or gcc versions.
+#elif (HWY_COMPILER_CLANG && HWY_COMPILER_CLANG < 1100) ||\
+(!HWY_COMPILER_CLANG && HWY_COMPILER_GCC && HWY_COMPILER_GCC < 1000)
+#define HWY_BROKEN_TARGETS (HWY_SVE | HWY_SVE2)
+
+#else
+#define HWY_BROKEN_TARGETS 0
+#endif
+
+#endif  // HWY_BROKEN_TARGETS
+
+// Enabled means not disabled nor blocklisted.
+#define HWY_ENABLED(targets) \
+  ((targets) & ~((HWY_DISABLED_TARGETS) | (HWY_BROKEN_TARGETS)))
+
+//------------------------------------------------------------------------------
+// Detect baseline targets using predefined macros
+
+// Baseline means the targets for which the compiler is allowed to generate
+// instructions, implying the target CPU would have to support them. Do not use
+// this directly because it does not take the blocklist into account. Allow the
+// user to override this without any guarantee of success.
+#ifndef HWY_BASELINE_TARGETS
+
+#if defined(HWY_EMULATE_SVE)
+#define HWY_BASELINE_TARGETS HWY_SVE  // does not support SVE2
+#define HWY_BASELINE_AVX3_DL 0
+#else
+
+// Also check HWY_ARCH to ensure that simulating unknown platforms ends up with
+// HWY_TARGET == HWY_SCALAR.
+
+#if HWY_ARCH_WASM && defined(__wasm_simd128__)
+#define HWY_BASELINE_WASM HWY_WASM
+#else
+#define HWY_BASELINE_WASM 0
+#endif
+
+// Avoid choosing the PPC target until we have an implementation.
+#if HWY_ARCH_PPC && defined(__VSX__) && 0
+#define HWY_BASELINE_PPC8 HWY_PPC8
+#else
+#define HWY_BASELINE_PPC8 0
+#endif
+
+// SVE compiles, but is not yet tested.
+#if HWY_ARCH_ARM && defined(__ARM_FEATURE_SVE2)
+#define HWY_BASELINE_SVE2 HWY_SVE2
+#else
+#define HWY_BASELINE_SVE2 0
+#endif
+
+#if HWY_ARCH_ARM && defined(__ARM_FEATURE_SVE)
+#define HWY_BASELINE_SVE HWY_SVE
+#else
+#define HWY_BASELINE_SVE 0
+#endif
+
+// GCC 4.5.4 only defines __ARM_NEON__; 5.4 defines both.
+#if HWY_ARCH_ARM && (defined(__ARM_NEON__) || defined(__ARM_NEON))
+#define HWY_BASELINE_NEON HWY_NEON
+#else
+#define HWY_BASELINE_NEON 0
+#endif
+
+// Special handling for MSVC because it has fewer predefined macros
+#if HWY_COMPILER_MSVC && !HWY_COMPILER_CLANG
+
+// We can only be sure SSSE3/SSE4 are enabled if AVX is
+// (https://stackoverflow.com/questions/18563978/)
+#if defined(__AVX__)
+#define HWY_CHECK_SSSE3 1
+#define HWY_CHECK_SSE4 1
+#else
+#define HWY_CHECK_SSSE3 0
+#define HWY_CHECK_SSE4 0
+#endif
+
+// Cannot check for PCLMUL/AES and BMI2/FMA/F16C individually; we assume
+// PCLMUL/AES are available if SSE4 is, and BMI2/FMA/F16C if AVX2 is.
+#define HWY_CHECK_PCLMUL_AES 1
+#define HWY_CHECK_BMI2_FMA 1
+#define HWY_CHECK_F16C 1
+
+#else  // non-MSVC
+
+#if defined(__SSSE3__)
+#define HWY_CHECK_SSSE3 1
+#else
+#define HWY_CHECK_SSSE3 0
+#endif
+
+#if defined(__SSE4_1__) && defined(__SSE4_2__)
+#define HWY_CHECK_SSE4 1
+#else
+#define HWY_CHECK_SSE4 0
+#endif
+
+// If these are disabled, they should not gate the availability of SSE4/AVX2.
+#if defined(HWY_DISABLE_PCLMUL_AES) || (defined(__PCLMUL__) && defined(__AES__))
+#define HWY_CHECK_PCLMUL_AES 1
+#else
+#define HWY_CHECK_PCLMUL_AES 0
+#endif
+
+#if defined(HWY_DISABLE_BMI2_FMA) || (defined(__BMI2__) && defined(__FMA__))
+#define HWY_CHECK_BMI2_FMA 1
+#else
+#define HWY_CHECK_BMI2_FMA 0
+#endif
+
+#if defined(HWY_DISABLE_F16C) || defined(__F16C__)
+#define HWY_CHECK_F16C 1
+#else
+#define HWY_CHECK_F16C 0
+#endif
+
+#endif  // non-MSVC
+
+#if HWY_ARCH_X86 && HWY_CHECK_SSSE3
+#define HWY_BASELINE_SSSE3 HWY_SSSE3
+#else
+#define HWY_BASELINE_SSSE3 0
+#endif
+
+#if HWY_ARCH_X86 && HWY_CHECK_SSE4 && HWY_CHECK_PCLMUL_AES
+#define HWY_BASELINE_SSE4 HWY_SSE4
+#else
+#define HWY_BASELINE_SSE4 0
+#endif
+
+#if HWY_BASELINE_SSE4 != 0 && HWY_CHECK_BMI2_FMA && HWY_CHECK_F16C && \
+    defined(__AVX2__)
+#define HWY_BASELINE_AVX2 HWY_AVX2
+#else
+#define HWY_BASELINE_AVX2 0
+#endif
+
+// Require everything in AVX2 plus AVX-512 flags (also set by MSVC)
+#if HWY_BASELINE_AVX2 != 0 && defined(__AVX512F__) && defined(__AVX512BW__) && \
+    defined(__AVX512DQ__) && defined(__AVX512VL__)
+#define HWY_BASELINE_AVX3 HWY_AVX3
+#else
+#define HWY_BASELINE_AVX3 0
+#endif
+
+// TODO(janwas): not yet known whether these will be set by MSVC
+#if HWY_BASELINE_AVX3 != 0 && defined(__AVXVNNI__) && defined(__VAES__) && \
+    defined(__VPCLMULQDQ__)
+#define HWY_BASELINE_AVX3_DL HWY_AVX3_DL
+#else
+#define HWY_BASELINE_AVX3_DL 0
+#endif
+
+#if HWY_ARCH_RVV && defined(__riscv_vector)
+#define HWY_BASELINE_RVV HWY_RVV
+#else
+#define HWY_BASELINE_RVV 0
+#endif
+
+#define HWY_BASELINE_TARGETS                                                \
+  (HWY_SCALAR | HWY_BASELINE_WASM | HWY_BASELINE_PPC8 | HWY_BASELINE_SVE2 | \
+   HWY_BASELINE_SVE | HWY_BASELINE_NEON | HWY_BASELINE_SSSE3 |              \
+   HWY_BASELINE_SSE4 | HWY_BASELINE_AVX2 | HWY_BASELINE_AVX3 |              \
+   HWY_BASELINE_AVX3_DL | HWY_BASELINE_RVV)
+
+#endif  // HWY_EMULATE_SVE
+
+#else
+// User already defined HWY_BASELINE_TARGETS, but we still need to define
+// HWY_BASELINE_AVX3 (matching user's definition) for HWY_CHECK_AVX3_DL.
+#define HWY_BASELINE_AVX3_DL (HWY_BASELINE_TARGETS & HWY_AVX3_DL)
+#endif  // HWY_BASELINE_TARGETS
+
+//------------------------------------------------------------------------------
+// Choose target for static dispatch
+
+#define HWY_ENABLED_BASELINE HWY_ENABLED(HWY_BASELINE_TARGETS)
+#if HWY_ENABLED_BASELINE == 0
+#error "At least one baseline target must be defined and enabled"
+#endif
+
+// Best baseline, used for static dispatch. This is the least-significant 1-bit
+// within HWY_ENABLED_BASELINE and lower bit values imply "better".
+#define HWY_STATIC_TARGET (HWY_ENABLED_BASELINE & -HWY_ENABLED_BASELINE)
+
+// Start by assuming static dispatch. If we later use dynamic dispatch, this
+// will be defined to other targets during the multiple-inclusion, and finally
+// return to the initial value. Defining this outside begin/end_target ensures
+// inl headers successfully compile by themselves (required by Bazel).
+#define HWY_TARGET HWY_STATIC_TARGET
+
+//------------------------------------------------------------------------------
+// Choose targets for dynamic dispatch according to one of four policies
+
+#if (defined(HWY_COMPILE_ONLY_SCALAR) + defined(HWY_COMPILE_ONLY_STATIC) + \
+     defined(HWY_COMPILE_ALL_ATTAINABLE)) > 1
+#error "Invalid config: can only define a single policy for targets"
+#endif
+
+// Further to checking for disabled/broken targets, we only use AVX3_DL after
+// explicit opt-in (via this macro OR baseline compiler flags) to avoid
+// generating a codepath which is only helpful if the app uses AVX3_DL features.
+#if defined(HWY_WANT_AVX3_DL)
+#define HWY_CHECK_AVX3_DL HWY_AVX3_DL
+#else
+#define HWY_CHECK_AVX3_DL HWY_BASELINE_AVX3_DL
+#endif
+
+// Attainable means enabled and the compiler allows intrinsics (even when not
+// allowed to autovectorize). Used in 3 and 4.
+#if HWY_ARCH_X86
+#define HWY_ATTAINABLE_TARGETS                                          \
+  HWY_ENABLED(HWY_SCALAR | HWY_SSSE3 | HWY_SSE4 | HWY_AVX2 | HWY_AVX3 | \
+              HWY_CHECK_AVX3_DL)
+#else
+#define HWY_ATTAINABLE_TARGETS HWY_ENABLED_BASELINE
+#endif
+
+// 1) For older compilers: disable all SIMD (could also set HWY_DISABLED_TARGETS
+// to ~HWY_SCALAR, but this is more explicit).
+#if defined(HWY_COMPILE_ONLY_SCALAR)
+#undef HWY_STATIC_TARGET
+#define HWY_STATIC_TARGET HWY_SCALAR  // override baseline
+#define HWY_TARGETS HWY_SCALAR
+
+// 2) For forcing static dispatch without code changes (removing HWY_EXPORT)
+#elif defined(HWY_COMPILE_ONLY_STATIC)
+#define HWY_TARGETS HWY_STATIC_TARGET
+
+// 3) For tests: include all attainable targets (in particular: scalar)
+#elif defined(HWY_COMPILE_ALL_ATTAINABLE) || defined(HWY_IS_TEST)
+#define HWY_TARGETS HWY_ATTAINABLE_TARGETS
+
+// 4) Default: attainable WITHOUT non-best baseline. This reduces code size by
+// excluding superseded targets, in particular scalar.
+#else
+#define HWY_TARGETS (HWY_ATTAINABLE_TARGETS & (2 * HWY_STATIC_TARGET - 1))
+
+#endif  // target policy
+
+// HWY_ONCE and the multiple-inclusion mechanism rely on HWY_STATIC_TARGET being
+// one of the dynamic targets. This also implies HWY_TARGETS != 0 and
+// (HWY_TARGETS & HWY_ENABLED_BASELINE) != 0.
+#if (HWY_TARGETS & HWY_STATIC_TARGET) == 0
+#error "Logic error: best baseline should be included in dynamic targets"
+#endif
+
+#endif  // HIGHWAY_HWY_DETECT_TARGETS_H_
diff --git a/third_party/highway/hwy/examples/benchmark.cc b/third_party/highway/hwy/examples/benchmark.cc
index 0debfd7db5877..f9b60ad5a2241 100644
--- a/third_party/highway/hwy/examples/benchmark.cc
+++ b/third_party/highway/hwy/examples/benchmark.cc
@@ -131,7 +131,7 @@ class BenchmarkDot : public TwoArray {
         sum[i] += sum[i + power];
       }
     }
-    dot_ = GetLane(SumOfLanes(sum[0]));
+    dot_ = GetLane(SumOfLanes(d, sum[0]));
     return static_cast<FuncOutput>(dot_);
   }
   void Verify(size_t num_items) {
diff --git a/third_party/highway/hwy/examples/skeleton.cc b/third_party/highway/hwy/examples/skeleton.cc
index fc05eb371f85b..460bc8f5b436d 100644
--- a/third_party/highway/hwy/examples/skeleton.cc
+++ b/third_party/highway/hwy/examples/skeleton.cc
@@ -45,7 +45,7 @@ HWY_NOINLINE void OneFloorLog2(const DF df, const uint8_t* HWY_RESTRICT values,
 
   const auto u8 = Load(d8, values);
   const auto bits = BitCast(d32, ConvertTo(df, PromoteTo(d32, u8)));
-  const auto exponent = ShiftRight<23>(bits) - Set(d32, 127);
+  const auto exponent = Sub(ShiftRight<23>(bits), Set(d32, 127));
   Store(DemoteTo(d8, exponent), d8, log2);
 }
 
diff --git a/third_party/highway/hwy/examples/skeleton_test.cc b/third_party/highway/hwy/examples/skeleton_test.cc
index 4a6a8769b346c..7f79b189f6d2e 100644
--- a/third_party/highway/hwy/examples/skeleton_test.cc
+++ b/third_party/highway/hwy/examples/skeleton_test.cc
@@ -99,9 +99,17 @@ HWY_NOINLINE void TestAllSumMulAdd() {
 HWY_AFTER_NAMESPACE();
 
 #if HWY_ONCE
+
 namespace skeleton {
 HWY_BEFORE_TEST(SkeletonTest);
 HWY_EXPORT_AND_TEST_P(SkeletonTest, TestAllFloorLog2);
 HWY_EXPORT_AND_TEST_P(SkeletonTest, TestAllSumMulAdd);
 }  // namespace skeleton
+
+// Ought not to be necessary, but without this, no tests run on RVV.
+int main(int argc, char **argv) {
+  ::testing::InitGoogleTest(&argc, argv);
+  return RUN_ALL_TESTS();
+}
+
 #endif
diff --git a/third_party/highway/hwy/foreach_target.h b/third_party/highway/hwy/foreach_target.h
index a0c4198b17c32..63a90cbbd086a 100644
--- a/third_party/highway/hwy/foreach_target.h
+++ b/third_party/highway/hwy/foreach_target.h
@@ -19,7 +19,7 @@
 // targets except HWY_STATIC_TARGET. Defines unique HWY_TARGET each time so that
 // highway.h defines the corresponding macro/namespace.
 
-#include "hwy/targets.h"
+#include "hwy/detect_targets.h"
 
 // *_inl.h may include other headers, which requires include guards to prevent
 // repeated inclusion. The guards must be reset after compiling each target, so
@@ -74,6 +74,17 @@
 #endif
 #endif
 
+#if (HWY_TARGETS & HWY_SSSE3) && (HWY_STATIC_TARGET != HWY_SSSE3)
+#undef HWY_TARGET
+#define HWY_TARGET HWY_SSSE3
+#include HWY_TARGET_INCLUDE
+#ifdef HWY_TARGET_TOGGLE
+#undef HWY_TARGET_TOGGLE
+#else
+#define HWY_TARGET_TOGGLE
+#endif
+#endif
+
 #if (HWY_TARGETS & HWY_SSE4) && (HWY_STATIC_TARGET != HWY_SSE4)
 #undef HWY_TARGET
 #define HWY_TARGET HWY_SSE4
@@ -107,6 +118,17 @@
 #endif
 #endif
 
+#if (HWY_TARGETS & HWY_AVX3_DL) && (HWY_STATIC_TARGET != HWY_AVX3_DL)
+#undef HWY_TARGET
+#define HWY_TARGET HWY_AVX3_DL
+#include HWY_TARGET_INCLUDE
+#ifdef HWY_TARGET_TOGGLE
+#undef HWY_TARGET_TOGGLE
+#else
+#define HWY_TARGET_TOGGLE
+#endif
+#endif
+
 #if (HWY_TARGETS & HWY_WASM) && (HWY_STATIC_TARGET != HWY_WASM)
 #undef HWY_TARGET
 #define HWY_TARGET HWY_WASM
diff --git a/third_party/highway/hwy/highway.h b/third_party/highway/hwy/highway.h
index a34c60ebef486..9312b5ad36328 100644
--- a/third_party/highway/hwy/highway.h
+++ b/third_party/highway/hwy/highway.h
@@ -27,17 +27,12 @@ namespace hwy {
 
 // API version (https://semver.org/); keep in sync with CMakeLists.txt.
 #define HWY_MAJOR 0
-#define HWY_MINOR 12
-#define HWY_PATCH 1
+#define HWY_MINOR 14
+#define HWY_PATCH 0
 
 //------------------------------------------------------------------------------
 // Shorthand for descriptors (defined in shared-inl.h) used to select overloads.
 
-// Because Highway functions take descriptor and/or vector arguments, ADL finds
-// these functions without requiring users in project::HWY_NAMESPACE to
-// qualify Highway functions with hwy::HWY_NAMESPACE. However, ADL rules for
-// templates require `using hwy::HWY_NAMESPACE::ShiftLeft;` etc. declarations.
-
 // HWY_FULL(T[,LMUL=1]) is a native vector/group. LMUL is the number of
 // registers in the group, and is ignored on targets that do not support groups.
 #define HWY_FULL1(T) hwy::HWY_NAMESPACE::Simd<T, HWY_LANES(T)>
@@ -81,12 +76,16 @@ namespace hwy {
 #define HWY_STATIC_DISPATCH(FUNC_NAME) N_SVE2::FUNC_NAME
 #elif HWY_STATIC_TARGET == HWY_PPC8
 #define HWY_STATIC_DISPATCH(FUNC_NAME) N_PPC8::FUNC_NAME
+#elif HWY_STATIC_TARGET == HWY_SSSE3
+#define HWY_STATIC_DISPATCH(FUNC_NAME) N_SSSE3::FUNC_NAME
 #elif HWY_STATIC_TARGET == HWY_SSE4
 #define HWY_STATIC_DISPATCH(FUNC_NAME) N_SSE4::FUNC_NAME
 #elif HWY_STATIC_TARGET == HWY_AVX2
 #define HWY_STATIC_DISPATCH(FUNC_NAME) N_AVX2::FUNC_NAME
 #elif HWY_STATIC_TARGET == HWY_AVX3
 #define HWY_STATIC_DISPATCH(FUNC_NAME) N_AVX3::FUNC_NAME
+#elif HWY_STATIC_TARGET == HWY_AVX3_DL
+#define HWY_STATIC_DISPATCH(FUNC_NAME) N_AVX3_DL::FUNC_NAME
 #endif
 
 // Dynamic dispatch declarations.
@@ -165,6 +164,12 @@ FunctionCache<RetType, Args...> FunctionCacheFactory(RetType (*)(Args...)) {
 #define HWY_CHOOSE_PPC8(FUNC_NAME) nullptr
 #endif
 
+#if HWY_TARGETS & HWY_SSSE3
+#define HWY_CHOOSE_SSSE3(FUNC_NAME) &N_SSSE3::FUNC_NAME
+#else
+#define HWY_CHOOSE_SSSE3(FUNC_NAME) nullptr
+#endif
+
 #if HWY_TARGETS & HWY_SSE4
 #define HWY_CHOOSE_SSE4(FUNC_NAME) &N_SSE4::FUNC_NAME
 #else
@@ -183,6 +188,12 @@ FunctionCache<RetType, Args...> FunctionCacheFactory(RetType (*)(Args...)) {
 #define HWY_CHOOSE_AVX3(FUNC_NAME) nullptr
 #endif
 
+#if HWY_TARGETS & HWY_AVX3_DL
+#define HWY_CHOOSE_AVX3_DL(FUNC_NAME) &N_AVX3_DL::FUNC_NAME
+#else
+#define HWY_CHOOSE_AVX3_DL(FUNC_NAME) nullptr
+#endif
+
 #define HWY_DISPATCH_TABLE(FUNC_NAME) \
   HWY_CONCAT(FUNC_NAME, HighwayDispatchTable)
 
@@ -270,11 +281,11 @@ FunctionCache<RetType, Args...> FunctionCacheFactory(RetType (*)(Args...)) {
 #endif
 
 // These define ops inside namespace hwy::HWY_NAMESPACE.
-#if HWY_TARGET == HWY_SSE4
+#if HWY_TARGET == HWY_SSSE3 || HWY_TARGET == HWY_SSE4
 #include "hwy/ops/x86_128-inl.h"
 #elif HWY_TARGET == HWY_AVX2
 #include "hwy/ops/x86_256-inl.h"
-#elif HWY_TARGET == HWY_AVX3
+#elif HWY_TARGET == HWY_AVX3 || HWY_TARGET == HWY_AVX3_DL
 #include "hwy/ops/x86_512-inl.h"
 #elif HWY_TARGET == HWY_PPC8
 #error "PPC is not yet supported"
@@ -292,65 +303,6 @@ FunctionCache<RetType, Args...> FunctionCacheFactory(RetType (*)(Args...)) {
 #pragma message("HWY_TARGET does not match any known target")
 #endif  // HWY_TARGET
 
-// Commonly used functions/types that must come after ops are defined.
-HWY_BEFORE_NAMESPACE();
-namespace hwy {
-namespace HWY_NAMESPACE {
-
-// The lane type of a vector type, e.g. float for Vec<Simd<float, 4>>.
-template <class V>
-using LaneType = decltype(GetLane(V()));
-
-// Vector type, e.g. Vec128<float> for Simd<float, 4>. Useful as the return type
-// of functions that do not take a vector argument, or as an argument type if
-// the function only has a template argument for D, or for explicit type names
-// instead of auto. This may be a built-in type.
-template <class D>
-using Vec = decltype(Zero(D()));
-
-// Mask type. Useful as the return type of functions that do not take a mask
-// argument, or as an argument type if the function only has a template argument
-// for D, or for explicit type names instead of auto.
-template <class D>
-using Mask = decltype(MaskFromVec(Zero(D())));
-
-// Returns the closest value to v within [lo, hi].
-template <class V>
-HWY_API V Clamp(const V v, const V lo, const V hi) {
-  return Min(Max(lo, v), hi);
-}
-
-// CombineShiftRightBytes (and ..Lanes) are not available for the scalar target.
-// TODO(janwas): implement for RVV
-#if HWY_TARGET != HWY_SCALAR && HWY_TARGET != HWY_RVV
-
-template <size_t kLanes, class V>
-HWY_API V CombineShiftRightLanes(const V hi, const V lo) {
-  return CombineShiftRightBytes<kLanes * sizeof(LaneType<V>)>(hi, lo);
-}
-
-#endif
-
-// Returns lanes with the most significant bit set and all other bits zero.
-template <class D>
-HWY_API Vec<D> SignBit(D d) {
-  using Unsigned = MakeUnsigned<TFromD<D>>;
-  const Unsigned bit = Unsigned(1) << (sizeof(Unsigned) * 8 - 1);
-  return BitCast(d, Set(Rebind<Unsigned, D>(), bit));
-}
-
-// Returns quiet NaN.
-template <class D>
-HWY_API Vec<D> NaN(D d) {
-  const RebindToSigned<D> di;
-  // LimitsMax sets all exponent and mantissa bits to 1. The exponent plus
-  // mantissa MSB (to indicate quiet) would be sufficient.
-  return BitCast(d, Set(di, LimitsMax<TFromD<decltype(di)>>()));
-}
-
-// NOLINTNEXTLINE(google-readability-namespace-comments)
-}  // namespace HWY_NAMESPACE
-}  // namespace hwy
-HWY_AFTER_NAMESPACE();
+#include "hwy/ops/generic_ops-inl.h"
 
 #endif  // HWY_HIGHWAY_PER_TARGET
diff --git a/third_party/highway/hwy/highway_test.cc b/third_party/highway/hwy/highway_test.cc
index ebe57f0ed5da0..8fc57be074efe 100644
--- a/third_party/highway/hwy/highway_test.cc
+++ b/third_party/highway/hwy/highway_test.cc
@@ -66,9 +66,9 @@ struct TestOverflow {
     const auto vmax = Set(d, LimitsMax<T>());
     const auto vmin = Set(d, LimitsMin<T>());
     // Unsigned underflow / negative -> positive
-    HWY_ASSERT_VEC_EQ(d, vmax, vmin - v1);
+    HWY_ASSERT_VEC_EQ(d, vmax, Sub(vmin, v1));
     // Unsigned overflow / positive -> negative
-    HWY_ASSERT_VEC_EQ(d, vmin, vmax + v1);
+    HWY_ASSERT_VEC_EQ(d, vmin, Add(vmax, v1));
   }
 };
 
@@ -76,6 +76,22 @@ HWY_NOINLINE void TestAllOverflow() {
   ForIntegerTypes(ForPartialVectors<TestOverflow>());
 }
 
+struct TestClamp {
+  template <class T, class D>
+  HWY_NOINLINE void operator()(T /*unused*/, D d) {
+    const auto v0 = Zero(d);
+    const auto v1 = Set(d, 1);
+    const auto v2 = Set(d, 2);
+
+    HWY_ASSERT_VEC_EQ(d, v1, Clamp(v2, v0, v1));
+    HWY_ASSERT_VEC_EQ(d, v1, Clamp(v0, v1, v2));
+  }
+};
+
+HWY_NOINLINE void TestAllClamp() {
+  ForAllTypes(ForPartialVectors<TestClamp>());
+}
+
 struct TestSignBitInteger {
   template <class T, class D>
   HWY_NOINLINE void operator()(T /*unused*/, D d) {
@@ -187,18 +203,18 @@ struct TestNaN {
     HWY_ASSERT_NAN(d, Or(nan, v1));
 
     // Comparison
-    HWY_ASSERT(AllFalse(Eq(nan, v1)));
-    HWY_ASSERT(AllFalse(Gt(nan, v1)));
-    HWY_ASSERT(AllFalse(Lt(nan, v1)));
-    HWY_ASSERT(AllFalse(Ge(nan, v1)));
-    HWY_ASSERT(AllFalse(Le(nan, v1)));
+    HWY_ASSERT(AllFalse(d, Eq(nan, v1)));
+    HWY_ASSERT(AllFalse(d, Gt(nan, v1)));
+    HWY_ASSERT(AllFalse(d, Lt(nan, v1)));
+    HWY_ASSERT(AllFalse(d, Ge(nan, v1)));
+    HWY_ASSERT(AllFalse(d, Le(nan, v1)));
 
     // Reduction
-    HWY_ASSERT_NAN(d, SumOfLanes(nan));
+    HWY_ASSERT_NAN(d, SumOfLanes(d, nan));
 // TODO(janwas): re-enable after QEMU is fixed
 #if HWY_TARGET != HWY_RVV
-    HWY_ASSERT_NAN(d, MinOfLanes(nan));
-    HWY_ASSERT_NAN(d, MaxOfLanes(nan));
+    HWY_ASSERT_NAN(d, MinOfLanes(d, nan));
+    HWY_ASSERT_NAN(d, MaxOfLanes(d, nan));
 #endif
 
     // Min
@@ -227,13 +243,6 @@ struct TestNaN {
 #endif
     HWY_ASSERT_NAN(d, Min(nan, nan));
     HWY_ASSERT_NAN(d, Max(nan, nan));
-
-    // Comparison
-    HWY_ASSERT(AllFalse(Eq(nan, v1)));
-    HWY_ASSERT(AllFalse(Gt(nan, v1)));
-    HWY_ASSERT(AllFalse(Lt(nan, v1)));
-    HWY_ASSERT(AllFalse(Ge(nan, v1)));
-    HWY_ASSERT(AllFalse(Le(nan, v1)));
   }
 };
 
@@ -286,6 +295,19 @@ HWY_NOINLINE void TestAllGetLane() {
   ForAllTypes(ForPartialVectors<TestGetLane>());
 }
 
+struct TestDFromV {
+  template <class T, class D>
+  HWY_NOINLINE void operator()(T /*unused*/, D d) {
+    const auto v0 = Zero(d);
+    using D0 = DFromV<decltype(v0)>;         // not necessarily same as D
+    const auto v0b = And(v0, Set(D0(), 1));  // but vectors can interoperate
+    HWY_ASSERT_VEC_EQ(d, v0, v0b);
+  }
+};
+
+HWY_NOINLINE void TestAllDFromV() {
+  ForAllTypes(ForPartialVectors<TestDFromV>());
+}
 
 // NOLINTNEXTLINE(google-readability-namespace-comments)
 }  // namespace HWY_NAMESPACE
@@ -293,13 +315,23 @@ HWY_NOINLINE void TestAllGetLane() {
 HWY_AFTER_NAMESPACE();
 
 #if HWY_ONCE
+
 namespace hwy {
 HWY_BEFORE_TEST(HighwayTest);
 HWY_EXPORT_AND_TEST_P(HighwayTest, TestAllSet);
 HWY_EXPORT_AND_TEST_P(HighwayTest, TestAllOverflow);
+HWY_EXPORT_AND_TEST_P(HighwayTest, TestAllClamp);
 HWY_EXPORT_AND_TEST_P(HighwayTest, TestAllSignBit);
 HWY_EXPORT_AND_TEST_P(HighwayTest, TestAllNaN);
 HWY_EXPORT_AND_TEST_P(HighwayTest, TestAllCopyAndAssign);
 HWY_EXPORT_AND_TEST_P(HighwayTest, TestAllGetLane);
+HWY_EXPORT_AND_TEST_P(HighwayTest, TestAllDFromV);
 }  // namespace hwy
+
+// Ought not to be necessary, but without this, no tests run on RVV.
+int main(int argc, char** argv) {
+  ::testing::InitGoogleTest(&argc, argv);
+  return RUN_ALL_TESTS();
+}
+
 #endif
diff --git a/third_party/highway/hwy/nanobenchmark.cc b/third_party/highway/hwy/nanobenchmark.cc
index a31ca1b263c05..0fb189dff8c8b 100644
--- a/third_party/highway/hwy/nanobenchmark.cc
+++ b/third_party/highway/hwy/nanobenchmark.cc
@@ -459,7 +459,7 @@ timer::Ticks SampleUntilStable(const double max_rel_mad, double* rel_mad,
       static_cast<size_t>(ticks_per_second * p.seconds_per_eval);
   size_t samples_per_eval =
       est == 0 ? p.min_samples_per_eval : ticks_per_eval / est;
-  samples_per_eval = std::max(samples_per_eval, p.min_samples_per_eval);
+  samples_per_eval = HWY_MAX(samples_per_eval, p.min_samples_per_eval);
 
   std::vector<timer::Ticks> samples;
   samples.reserve(1 + samples_per_eval);
@@ -530,7 +530,7 @@ size_t NumSkip(const Func func, const uint8_t* arg, const InputVec& unique,
     const timer::Ticks total = SampleUntilStable(
         p.target_rel_mad, &rel_mad, p,
         [func, arg, input]() { platform::PreventElision(func(arg, input)); });
-    min_duration = std::min(min_duration, total - timer_resolution);
+    min_duration = HWY_MIN(min_duration, total - timer_resolution);
   }
 
   // Number of repetitions required to reach the target resolution.
@@ -615,7 +615,7 @@ timer::Ticks TotalDuration(const Func func, const uint8_t* arg,
           platform::PreventElision(func(arg, input));
         }
       });
-  *max_rel_mad = std::max(*max_rel_mad, rel_mad);
+  *max_rel_mad = HWY_MAX(*max_rel_mad, rel_mad);
   return duration;
 }
 
diff --git a/third_party/highway/hwy/nanobenchmark_test.cc b/third_party/highway/hwy/nanobenchmark_test.cc
index a42caf5230aa7..b5fa7620c5d9b 100644
--- a/third_party/highway/hwy/nanobenchmark_test.cc
+++ b/third_party/highway/hwy/nanobenchmark_test.cc
@@ -23,6 +23,13 @@
 namespace hwy {
 namespace {
 
+// Governs duration of test; avoid timeout in debug builds.
+#ifdef NDEBUG
+constexpr size_t kMaxEvals = 4;
+#else
+constexpr size_t kMaxEvals = 3;
+#endif
+
 FuncOutput Div(const void*, FuncInput in) {
   // Here we're measuring the throughput because benchmark invocations are
   // independent. Any dividend will do; the divisor is nonzero.
@@ -34,7 +41,7 @@ void MeasureDiv(const FuncInput (&inputs)[N]) {
   printf("Measuring integer division (output on final two lines)\n");
   Result results[N];
   Params params;
-  params.max_evals = 4;  // avoid test timeout
+  params.max_evals = kMaxEvals;
   const size_t num_results = Measure(&Div, nullptr, inputs, N, results, params);
   for (size_t i = 0; i < num_results; ++i) {
     printf("%5zu: %6.2f ticks; MAD=%4.2f%%\n", results[i].input,
@@ -59,7 +66,7 @@ template <size_t N>
 void MeasureRandom(const FuncInput (&inputs)[N]) {
   Result results[N];
   Params p;
-  p.max_evals = 4;  // avoid test timeout
+  p.max_evals = kMaxEvals;
   p.verbose = false;
   const size_t num_results = Measure(&Random, nullptr, inputs, N, results, p);
   for (size_t i = 0; i < num_results; ++i) {
@@ -78,3 +85,9 @@ TEST(NanobenchmarkTest, RunAll) {
 
 }  // namespace
 }  // namespace hwy
+
+// Ought not to be necessary, but without this, no tests run on RVV.
+int main(int argc, char** argv) {
+  ::testing::InitGoogleTest(&argc, argv);
+  return RUN_ALL_TESTS();
+}
diff --git a/third_party/highway/hwy/ops/arm_neon-inl.h b/third_party/highway/hwy/ops/arm_neon-inl.h
index f1ed0c742d8dd..db29560af3a27 100644
--- a/third_party/highway/hwy/ops/arm_neon-inl.h
+++ b/third_party/highway/hwy/ops/arm_neon-inl.h
@@ -26,6 +26,9 @@ HWY_BEFORE_NAMESPACE();
 namespace hwy {
 namespace HWY_NAMESPACE {
 
+template <typename T>
+using Full128 = Simd<T, 16 / sizeof(T)>;
+
 namespace detail {  // for code folding and Raw128
 
 // Macros used to define single and double function calls for multiple types
@@ -70,7 +73,7 @@ namespace detail {  // for code folding and Raw128
 // parameters passed here (see HWY_NEON_BUILD_* macros defined before).
 #define HWY_NEON_DEF_FUNCTION(type, size, name, prefix, infix, suffix, args) \
   HWY_CONCAT(HWY_NEON_BUILD_TPL_, args)                                      \
-  HWY_INLINE HWY_CONCAT(HWY_NEON_BUILD_RET_, args)(type, size)               \
+  HWY_API HWY_CONCAT(HWY_NEON_BUILD_RET_, args)(type, size)                  \
       name(HWY_CONCAT(HWY_NEON_BUILD_PARAM_, args)(type, size)) {            \
     return HWY_CONCAT(HWY_NEON_BUILD_RET_, args)(type, size)(                \
         HWY_NEON_EVAL(prefix##infix##suffix, HWY_NEON_BUILD_ARG_##args));    \
@@ -441,9 +444,6 @@ struct Raw128<int8_t, 1> {
 
 }  // namespace detail
 
-template <typename T>
-using Full128 = Simd<T, 16 / sizeof(T)>;
-
 template <typename T, size_t N = 16 / sizeof(T)>
 class Vec128 {
   using Raw = typename detail::Raw128<T, N>::type;
@@ -481,7 +481,7 @@ class Vec128 {
   Raw raw;
 };
 
-// FF..FF or 0, also for floating-point - see README.
+// FF..FF or 0.
 template <typename T, size_t N = 16 / sizeof(T)>
 class Mask128 {
   // ARM C Language Extensions return and expect unsigned type.
@@ -496,6 +496,21 @@ class Mask128 {
   Raw raw;
 };
 
+namespace detail {
+
+// Deduce Simd<T, N> from Vec128<T, N>
+struct DeduceD {
+  template <typename T, size_t N>
+  Simd<T, N> operator()(Vec128<T, N>) const {
+    return Simd<T, N>();
+  }
+};
+
+}  // namespace detail
+
+template <class V>
+using DFromV = decltype(detail::DeduceD()(V()));
+
 // ------------------------------ BitCast
 
 namespace detail {
@@ -637,8 +652,8 @@ HWY_INLINE Vec128<float16_t, N> BitCastFromByte(Simd<float16_t, N> /* tag */,
 }  // namespace detail
 
 template <typename T, size_t N, typename FromT>
-HWY_INLINE Vec128<T, N> BitCast(
-    Simd<T, N> d, Vec128<FromT, N * sizeof(T) / sizeof(FromT)> v) {
+HWY_API Vec128<T, N> BitCast(Simd<T, N> d,
+                             Vec128<FromT, N * sizeof(T) / sizeof(FromT)> v) {
   return detail::BitCastFromByte(d, detail::BitCastToByte(v));
 }
 
@@ -660,13 +675,16 @@ HWY_NEON_DEF_FUNCTION_ALL_TYPES(Set, vdup, _n_, HWY_SET1)
 
 // Returns an all-zero vector.
 template <typename T, size_t N>
-HWY_INLINE Vec128<T, N> Zero(Simd<T, N> d) {
+HWY_API Vec128<T, N> Zero(Simd<T, N> d) {
   return Set(d, 0);
 }
 
+template <class D>
+using VFromD = decltype(Zero(D()));
+
 // Returns a vector with uninitialized elements.
 template <typename T, size_t N>
-HWY_INLINE Vec128<T, N> Undefined(Simd<T, N> /*d*/) {
+HWY_API Vec128<T, N> Undefined(Simd<T, N> /*d*/) {
   HWY_DIAGNOSTICS(push)
   HWY_DIAGNOSTICS_OFF(disable : 4701, ignored "-Wuninitialized")
   typename detail::Raw128<T, N>::type a;
@@ -686,81 +704,81 @@ Vec128<T, N> Iota(const Simd<T, N> d, const T2 first) {
 
 // ------------------------------ GetLane
 
-HWY_INLINE uint8_t GetLane(const Vec128<uint8_t, 16> v) {
+HWY_API uint8_t GetLane(const Vec128<uint8_t, 16> v) {
   return vgetq_lane_u8(v.raw, 0);
 }
 template <size_t N>
-HWY_INLINE uint8_t GetLane(const Vec128<uint8_t, N> v) {
+HWY_API uint8_t GetLane(const Vec128<uint8_t, N> v) {
   return vget_lane_u8(v.raw, 0);
 }
 
-HWY_INLINE int8_t GetLane(const Vec128<int8_t, 16> v) {
+HWY_API int8_t GetLane(const Vec128<int8_t, 16> v) {
   return vgetq_lane_s8(v.raw, 0);
 }
 template <size_t N>
-HWY_INLINE int8_t GetLane(const Vec128<int8_t, N> v) {
+HWY_API int8_t GetLane(const Vec128<int8_t, N> v) {
   return vget_lane_s8(v.raw, 0);
 }
 
-HWY_INLINE uint16_t GetLane(const Vec128<uint16_t, 8> v) {
+HWY_API uint16_t GetLane(const Vec128<uint16_t, 8> v) {
   return vgetq_lane_u16(v.raw, 0);
 }
 template <size_t N>
-HWY_INLINE uint16_t GetLane(const Vec128<uint16_t, N> v) {
+HWY_API uint16_t GetLane(const Vec128<uint16_t, N> v) {
   return vget_lane_u16(v.raw, 0);
 }
 
-HWY_INLINE int16_t GetLane(const Vec128<int16_t, 8> v) {
+HWY_API int16_t GetLane(const Vec128<int16_t, 8> v) {
   return vgetq_lane_s16(v.raw, 0);
 }
 template <size_t N>
-HWY_INLINE int16_t GetLane(const Vec128<int16_t, N> v) {
+HWY_API int16_t GetLane(const Vec128<int16_t, N> v) {
   return vget_lane_s16(v.raw, 0);
 }
 
-HWY_INLINE uint32_t GetLane(const Vec128<uint32_t, 4> v) {
+HWY_API uint32_t GetLane(const Vec128<uint32_t, 4> v) {
   return vgetq_lane_u32(v.raw, 0);
 }
 template <size_t N>
-HWY_INLINE uint32_t GetLane(const Vec128<uint32_t, N> v) {
+HWY_API uint32_t GetLane(const Vec128<uint32_t, N> v) {
   return vget_lane_u32(v.raw, 0);
 }
 
-HWY_INLINE int32_t GetLane(const Vec128<int32_t, 4> v) {
+HWY_API int32_t GetLane(const Vec128<int32_t, 4> v) {
   return vgetq_lane_s32(v.raw, 0);
 }
 template <size_t N>
-HWY_INLINE int32_t GetLane(const Vec128<int32_t, N> v) {
+HWY_API int32_t GetLane(const Vec128<int32_t, N> v) {
   return vget_lane_s32(v.raw, 0);
 }
 
-HWY_INLINE uint64_t GetLane(const Vec128<uint64_t, 2> v) {
+HWY_API uint64_t GetLane(const Vec128<uint64_t, 2> v) {
   return vgetq_lane_u64(v.raw, 0);
 }
-HWY_INLINE uint64_t GetLane(const Vec128<uint64_t, 1> v) {
+HWY_API uint64_t GetLane(const Vec128<uint64_t, 1> v) {
   return vget_lane_u64(v.raw, 0);
 }
-HWY_INLINE int64_t GetLane(const Vec128<int64_t, 2> v) {
+HWY_API int64_t GetLane(const Vec128<int64_t, 2> v) {
   return vgetq_lane_s64(v.raw, 0);
 }
-HWY_INLINE int64_t GetLane(const Vec128<int64_t, 1> v) {
+HWY_API int64_t GetLane(const Vec128<int64_t, 1> v) {
   return vget_lane_s64(v.raw, 0);
 }
 
-HWY_INLINE float GetLane(const Vec128<float, 4> v) {
+HWY_API float GetLane(const Vec128<float, 4> v) {
   return vgetq_lane_f32(v.raw, 0);
 }
-HWY_INLINE float GetLane(const Vec128<float, 2> v) {
+HWY_API float GetLane(const Vec128<float, 2> v) {
   return vget_lane_f32(v.raw, 0);
 }
-HWY_INLINE float GetLane(const Vec128<float, 1> v) {
+HWY_API float GetLane(const Vec128<float, 1> v) {
   return vget_lane_f32(v.raw, 0);
 }
 #if HWY_ARCH_ARM_A64
-HWY_INLINE double GetLane(const Vec128<double, 2> v) {
+HWY_API double GetLane(const Vec128<double, 2> v) {
   return vgetq_lane_f64(v.raw, 0);
 }
-HWY_INLINE double GetLane(const Vec128<double, 1> v) {
+HWY_API double GetLane(const Vec128<double, 1> v) {
   return vget_lane_f64(v.raw, 0);
 }
 #endif
@@ -803,56 +821,12 @@ HWY_NEON_DEF_FUNCTION_INT_64(SaturatedSub, vqsub, _, 2)
 HWY_NEON_DEF_FUNCTION_UINT_8(AverageRound, vrhadd, _, 2)
 HWY_NEON_DEF_FUNCTION_UINT_16(AverageRound, vrhadd, _, 2)
 
-// ------------------------------ Absolute value
-
-// Returns absolute value, except that LimitsMin() maps to LimitsMax() + 1.
-HWY_INLINE Vec128<int8_t> Abs(const Vec128<int8_t> v) {
-  return Vec128<int8_t>(vabsq_s8(v.raw));
-}
-HWY_INLINE Vec128<int16_t> Abs(const Vec128<int16_t> v) {
-  return Vec128<int16_t>(vabsq_s16(v.raw));
-}
-HWY_INLINE Vec128<int32_t> Abs(const Vec128<int32_t> v) {
-  return Vec128<int32_t>(vabsq_s32(v.raw));
-}
-// i64 is implemented after BroadcastSignBit.
-HWY_INLINE Vec128<float> Abs(const Vec128<float> v) {
-  return Vec128<float>(vabsq_f32(v.raw));
-}
-
-template <size_t N, HWY_IF_LE64(int8_t, N)>
-HWY_INLINE Vec128<int8_t, N> Abs(const Vec128<int8_t, N> v) {
-  return Vec128<int8_t, N>(vabs_s8(v.raw));
-}
-template <size_t N, HWY_IF_LE64(int16_t, N)>
-HWY_INLINE Vec128<int16_t, N> Abs(const Vec128<int16_t, N> v) {
-  return Vec128<int16_t, N>(vabs_s16(v.raw));
-}
-template <size_t N, HWY_IF_LE64(int32_t, N)>
-HWY_INLINE Vec128<int32_t, N> Abs(const Vec128<int32_t, N> v) {
-  return Vec128<int32_t, N>(vabs_s32(v.raw));
-}
-template <size_t N, HWY_IF_LE64(float, N)>
-HWY_INLINE Vec128<float, N> Abs(const Vec128<float, N> v) {
-  return Vec128<float, N>(vabs_f32(v.raw));
-}
-
-#if HWY_ARCH_ARM_A64
-HWY_INLINE Vec128<double> Abs(const Vec128<double> v) {
-  return Vec128<double>(vabsq_f64(v.raw));
-}
-
-HWY_INLINE Vec128<double, 1> Abs(const Vec128<double, 1> v) {
-  return Vec128<double, 1>(vabs_f64(v.raw));
-}
-#endif
-
 // ------------------------------ Neg
 
 HWY_NEON_DEF_FUNCTION_ALL_FLOATS(Neg, vneg, _, 1)
 HWY_NEON_DEF_FUNCTION_INT_8_16_32(Neg, vneg, _, 1)  // i64 implemented below
 
-HWY_INLINE Vec128<int64_t, 1> Neg(const Vec128<int64_t, 1> v) {
+HWY_API Vec128<int64_t, 1> Neg(const Vec128<int64_t, 1> v) {
 #if HWY_ARCH_ARM_A64
   return Vec128<int64_t, 1>(vneg_s64(v.raw));
 #else
@@ -860,7 +834,7 @@ HWY_INLINE Vec128<int64_t, 1> Neg(const Vec128<int64_t, 1> v) {
 #endif
 }
 
-HWY_INLINE Vec128<int64_t> Neg(const Vec128<int64_t> v) {
+HWY_API Vec128<int64_t> Neg(const Vec128<int64_t> v) {
 #if HWY_ARCH_ARM_A64
   return Vec128<int64_t>(vnegq_s64(v.raw));
 #else
@@ -875,7 +849,7 @@ HWY_INLINE Vec128<int64_t> Neg(const Vec128<int64_t> v) {
 #undef HWY_NEON_DEF_FUNCTION
 #define HWY_NEON_DEF_FUNCTION(type, size, name, prefix, infix, suffix, args)   \
   template <int kBits>                                                         \
-  HWY_INLINE Vec128<type, size> name(const Vec128<type, size> v) {             \
+  HWY_API Vec128<type, size> name(const Vec128<type, size> v) {                \
     return kBits == 0 ? v                                                      \
                       : Vec128<type, size>(HWY_NEON_EVAL(                      \
                             prefix##infix##suffix, v.raw, HWY_MAX(1, kBits))); \
@@ -890,230 +864,230 @@ HWY_NEON_DEF_FUNCTION_INTS(ShiftRight, vshr, _n_, HWY_SHIFT)
 
 // ------------------------------ Shl
 
-HWY_INLINE Vec128<uint8_t> operator<<(const Vec128<uint8_t> v,
-                                      const Vec128<uint8_t> bits) {
+HWY_API Vec128<uint8_t> operator<<(const Vec128<uint8_t> v,
+                                   const Vec128<uint8_t> bits) {
   return Vec128<uint8_t>(vshlq_u8(v.raw, vreinterpretq_s8_u8(bits.raw)));
 }
 template <size_t N, HWY_IF_LE64(uint8_t, N)>
-HWY_INLINE Vec128<uint8_t, N> operator<<(const Vec128<uint8_t, N> v,
-                                         const Vec128<uint8_t, N> bits) {
+HWY_API Vec128<uint8_t, N> operator<<(const Vec128<uint8_t, N> v,
+                                      const Vec128<uint8_t, N> bits) {
   return Vec128<uint8_t, N>(vshl_u8(v.raw, vreinterpret_s8_u8(bits.raw)));
 }
 
-HWY_INLINE Vec128<uint16_t> operator<<(const Vec128<uint16_t> v,
-                                       const Vec128<uint16_t> bits) {
+HWY_API Vec128<uint16_t> operator<<(const Vec128<uint16_t> v,
+                                    const Vec128<uint16_t> bits) {
   return Vec128<uint16_t>(vshlq_u16(v.raw, vreinterpretq_s16_u16(bits.raw)));
 }
 template <size_t N, HWY_IF_LE64(uint16_t, N)>
-HWY_INLINE Vec128<uint16_t, N> operator<<(const Vec128<uint16_t, N> v,
-                                          const Vec128<uint16_t, N> bits) {
+HWY_API Vec128<uint16_t, N> operator<<(const Vec128<uint16_t, N> v,
+                                       const Vec128<uint16_t, N> bits) {
   return Vec128<uint16_t, N>(vshl_u16(v.raw, vreinterpret_s16_u16(bits.raw)));
 }
 
-HWY_INLINE Vec128<uint32_t> operator<<(const Vec128<uint32_t> v,
-                                       const Vec128<uint32_t> bits) {
+HWY_API Vec128<uint32_t> operator<<(const Vec128<uint32_t> v,
+                                    const Vec128<uint32_t> bits) {
   return Vec128<uint32_t>(vshlq_u32(v.raw, vreinterpretq_s32_u32(bits.raw)));
 }
 template <size_t N, HWY_IF_LE64(uint32_t, N)>
-HWY_INLINE Vec128<uint32_t, N> operator<<(const Vec128<uint32_t, N> v,
-                                          const Vec128<uint32_t, N> bits) {
+HWY_API Vec128<uint32_t, N> operator<<(const Vec128<uint32_t, N> v,
+                                       const Vec128<uint32_t, N> bits) {
   return Vec128<uint32_t, N>(vshl_u32(v.raw, vreinterpret_s32_u32(bits.raw)));
 }
 
-HWY_INLINE Vec128<uint64_t> operator<<(const Vec128<uint64_t> v,
-                                       const Vec128<uint64_t> bits) {
+HWY_API Vec128<uint64_t> operator<<(const Vec128<uint64_t> v,
+                                    const Vec128<uint64_t> bits) {
   return Vec128<uint64_t>(vshlq_u64(v.raw, vreinterpretq_s64_u64(bits.raw)));
 }
-HWY_INLINE Vec128<uint64_t, 1> operator<<(const Vec128<uint64_t, 1> v,
-                                          const Vec128<uint64_t, 1> bits) {
+HWY_API Vec128<uint64_t, 1> operator<<(const Vec128<uint64_t, 1> v,
+                                       const Vec128<uint64_t, 1> bits) {
   return Vec128<uint64_t, 1>(vshl_u64(v.raw, vreinterpret_s64_u64(bits.raw)));
 }
 
-HWY_INLINE Vec128<int8_t> operator<<(const Vec128<int8_t> v,
-                                     const Vec128<int8_t> bits) {
+HWY_API Vec128<int8_t> operator<<(const Vec128<int8_t> v,
+                                  const Vec128<int8_t> bits) {
   return Vec128<int8_t>(vshlq_s8(v.raw, bits.raw));
 }
 template <size_t N, HWY_IF_LE64(int8_t, N)>
-HWY_INLINE Vec128<int8_t, N> operator<<(const Vec128<int8_t, N> v,
-                                        const Vec128<int8_t, N> bits) {
+HWY_API Vec128<int8_t, N> operator<<(const Vec128<int8_t, N> v,
+                                     const Vec128<int8_t, N> bits) {
   return Vec128<int8_t, N>(vshl_s8(v.raw, bits.raw));
 }
 
-HWY_INLINE Vec128<int16_t> operator<<(const Vec128<int16_t> v,
-                                      const Vec128<int16_t> bits) {
+HWY_API Vec128<int16_t> operator<<(const Vec128<int16_t> v,
+                                   const Vec128<int16_t> bits) {
   return Vec128<int16_t>(vshlq_s16(v.raw, bits.raw));
 }
 template <size_t N, HWY_IF_LE64(int16_t, N)>
-HWY_INLINE Vec128<int16_t, N> operator<<(const Vec128<int16_t, N> v,
-                                         const Vec128<int16_t, N> bits) {
+HWY_API Vec128<int16_t, N> operator<<(const Vec128<int16_t, N> v,
+                                      const Vec128<int16_t, N> bits) {
   return Vec128<int16_t, N>(vshl_s16(v.raw, bits.raw));
 }
 
-HWY_INLINE Vec128<int32_t> operator<<(const Vec128<int32_t> v,
-                                      const Vec128<int32_t> bits) {
+HWY_API Vec128<int32_t> operator<<(const Vec128<int32_t> v,
+                                   const Vec128<int32_t> bits) {
   return Vec128<int32_t>(vshlq_s32(v.raw, bits.raw));
 }
 template <size_t N, HWY_IF_LE64(int32_t, N)>
-HWY_INLINE Vec128<int32_t, N> operator<<(const Vec128<int32_t, N> v,
-                                         const Vec128<int32_t, N> bits) {
+HWY_API Vec128<int32_t, N> operator<<(const Vec128<int32_t, N> v,
+                                      const Vec128<int32_t, N> bits) {
   return Vec128<int32_t, N>(vshl_s32(v.raw, bits.raw));
 }
 
-HWY_INLINE Vec128<int64_t> operator<<(const Vec128<int64_t> v,
-                                      const Vec128<int64_t> bits) {
+HWY_API Vec128<int64_t> operator<<(const Vec128<int64_t> v,
+                                   const Vec128<int64_t> bits) {
   return Vec128<int64_t>(vshlq_s64(v.raw, bits.raw));
 }
-HWY_INLINE Vec128<int64_t, 1> operator<<(const Vec128<int64_t, 1> v,
-                                         const Vec128<int64_t, 1> bits) {
+HWY_API Vec128<int64_t, 1> operator<<(const Vec128<int64_t, 1> v,
+                                      const Vec128<int64_t, 1> bits) {
   return Vec128<int64_t, 1>(vshl_s64(v.raw, bits.raw));
 }
 
 // ------------------------------ Shr (Neg)
 
-HWY_INLINE Vec128<uint8_t> operator>>(const Vec128<uint8_t> v,
-                                      const Vec128<uint8_t> bits) {
+HWY_API Vec128<uint8_t> operator>>(const Vec128<uint8_t> v,
+                                   const Vec128<uint8_t> bits) {
   const int8x16_t neg_bits = Neg(BitCast(Full128<int8_t>(), bits)).raw;
   return Vec128<uint8_t>(vshlq_u8(v.raw, neg_bits));
 }
 template <size_t N, HWY_IF_LE64(uint8_t, N)>
-HWY_INLINE Vec128<uint8_t, N> operator>>(const Vec128<uint8_t, N> v,
-                                         const Vec128<uint8_t, N> bits) {
+HWY_API Vec128<uint8_t, N> operator>>(const Vec128<uint8_t, N> v,
+                                      const Vec128<uint8_t, N> bits) {
   const int8x8_t neg_bits = Neg(BitCast(Simd<int8_t, N>(), bits)).raw;
   return Vec128<uint8_t, N>(vshl_u8(v.raw, neg_bits));
 }
 
-HWY_INLINE Vec128<uint16_t> operator>>(const Vec128<uint16_t> v,
-                                       const Vec128<uint16_t> bits) {
+HWY_API Vec128<uint16_t> operator>>(const Vec128<uint16_t> v,
+                                    const Vec128<uint16_t> bits) {
   const int16x8_t neg_bits = Neg(BitCast(Full128<int16_t>(), bits)).raw;
   return Vec128<uint16_t>(vshlq_u16(v.raw, neg_bits));
 }
 template <size_t N, HWY_IF_LE64(uint16_t, N)>
-HWY_INLINE Vec128<uint16_t, N> operator>>(const Vec128<uint16_t, N> v,
-                                          const Vec128<uint16_t, N> bits) {
+HWY_API Vec128<uint16_t, N> operator>>(const Vec128<uint16_t, N> v,
+                                       const Vec128<uint16_t, N> bits) {
   const int16x4_t neg_bits = Neg(BitCast(Simd<int16_t, N>(), bits)).raw;
   return Vec128<uint16_t, N>(vshl_u16(v.raw, neg_bits));
 }
 
-HWY_INLINE Vec128<uint32_t> operator>>(const Vec128<uint32_t> v,
-                                       const Vec128<uint32_t> bits) {
+HWY_API Vec128<uint32_t> operator>>(const Vec128<uint32_t> v,
+                                    const Vec128<uint32_t> bits) {
   const int32x4_t neg_bits = Neg(BitCast(Full128<int32_t>(), bits)).raw;
   return Vec128<uint32_t>(vshlq_u32(v.raw, neg_bits));
 }
 template <size_t N, HWY_IF_LE64(uint32_t, N)>
-HWY_INLINE Vec128<uint32_t, N> operator>>(const Vec128<uint32_t, N> v,
-                                          const Vec128<uint32_t, N> bits) {
+HWY_API Vec128<uint32_t, N> operator>>(const Vec128<uint32_t, N> v,
+                                       const Vec128<uint32_t, N> bits) {
   const int32x2_t neg_bits = Neg(BitCast(Simd<int32_t, N>(), bits)).raw;
   return Vec128<uint32_t, N>(vshl_u32(v.raw, neg_bits));
 }
 
-HWY_INLINE Vec128<uint64_t> operator>>(const Vec128<uint64_t> v,
-                                       const Vec128<uint64_t> bits) {
+HWY_API Vec128<uint64_t> operator>>(const Vec128<uint64_t> v,
+                                    const Vec128<uint64_t> bits) {
   const int64x2_t neg_bits = Neg(BitCast(Full128<int64_t>(), bits)).raw;
   return Vec128<uint64_t>(vshlq_u64(v.raw, neg_bits));
 }
-HWY_INLINE Vec128<uint64_t, 1> operator>>(const Vec128<uint64_t, 1> v,
-                                          const Vec128<uint64_t, 1> bits) {
+HWY_API Vec128<uint64_t, 1> operator>>(const Vec128<uint64_t, 1> v,
+                                       const Vec128<uint64_t, 1> bits) {
   const int64x1_t neg_bits = Neg(BitCast(Simd<int64_t, 1>(), bits)).raw;
   return Vec128<uint64_t, 1>(vshl_u64(v.raw, neg_bits));
 }
 
-HWY_INLINE Vec128<int8_t> operator>>(const Vec128<int8_t> v,
-                                     const Vec128<int8_t> bits) {
+HWY_API Vec128<int8_t> operator>>(const Vec128<int8_t> v,
+                                  const Vec128<int8_t> bits) {
   return Vec128<int8_t>(vshlq_s8(v.raw, Neg(bits).raw));
 }
 template <size_t N, HWY_IF_LE64(int8_t, N)>
-HWY_INLINE Vec128<int8_t, N> operator>>(const Vec128<int8_t, N> v,
-                                        const Vec128<int8_t, N> bits) {
+HWY_API Vec128<int8_t, N> operator>>(const Vec128<int8_t, N> v,
+                                     const Vec128<int8_t, N> bits) {
   return Vec128<int8_t, N>(vshl_s8(v.raw, Neg(bits).raw));
 }
 
-HWY_INLINE Vec128<int16_t> operator>>(const Vec128<int16_t> v,
-                                      const Vec128<int16_t> bits) {
+HWY_API Vec128<int16_t> operator>>(const Vec128<int16_t> v,
+                                   const Vec128<int16_t> bits) {
   return Vec128<int16_t>(vshlq_s16(v.raw, Neg(bits).raw));
 }
 template <size_t N, HWY_IF_LE64(int16_t, N)>
-HWY_INLINE Vec128<int16_t, N> operator>>(const Vec128<int16_t, N> v,
-                                         const Vec128<int16_t, N> bits) {
+HWY_API Vec128<int16_t, N> operator>>(const Vec128<int16_t, N> v,
+                                      const Vec128<int16_t, N> bits) {
   return Vec128<int16_t, N>(vshl_s16(v.raw, Neg(bits).raw));
 }
 
-HWY_INLINE Vec128<int32_t> operator>>(const Vec128<int32_t> v,
-                                      const Vec128<int32_t> bits) {
+HWY_API Vec128<int32_t> operator>>(const Vec128<int32_t> v,
+                                   const Vec128<int32_t> bits) {
   return Vec128<int32_t>(vshlq_s32(v.raw, Neg(bits).raw));
 }
 template <size_t N, HWY_IF_LE64(int32_t, N)>
-HWY_INLINE Vec128<int32_t, N> operator>>(const Vec128<int32_t, N> v,
-                                         const Vec128<int32_t, N> bits) {
+HWY_API Vec128<int32_t, N> operator>>(const Vec128<int32_t, N> v,
+                                      const Vec128<int32_t, N> bits) {
   return Vec128<int32_t, N>(vshl_s32(v.raw, Neg(bits).raw));
 }
 
-HWY_INLINE Vec128<int64_t> operator>>(const Vec128<int64_t> v,
-                                      const Vec128<int64_t> bits) {
+HWY_API Vec128<int64_t> operator>>(const Vec128<int64_t> v,
+                                   const Vec128<int64_t> bits) {
   return Vec128<int64_t>(vshlq_s64(v.raw, Neg(bits).raw));
 }
-HWY_INLINE Vec128<int64_t, 1> operator>>(const Vec128<int64_t, 1> v,
-                                         const Vec128<int64_t, 1> bits) {
+HWY_API Vec128<int64_t, 1> operator>>(const Vec128<int64_t, 1> v,
+                                      const Vec128<int64_t, 1> bits) {
   return Vec128<int64_t, 1>(vshl_s64(v.raw, Neg(bits).raw));
 }
 
 // ------------------------------ ShiftLeftSame (Shl)
 
 template <typename T, size_t N>
-HWY_INLINE Vec128<T, N> ShiftLeftSame(const Vec128<T, N> v, int bits) {
+HWY_API Vec128<T, N> ShiftLeftSame(const Vec128<T, N> v, int bits) {
   return v << Set(Simd<T, N>(), bits);
 }
 template <typename T, size_t N>
-HWY_INLINE Vec128<T, N> ShiftRightSame(const Vec128<T, N> v, int bits) {
+HWY_API Vec128<T, N> ShiftRightSame(const Vec128<T, N> v, int bits) {
   return v >> Set(Simd<T, N>(), bits);
 }
 
 // ------------------------------ Integer multiplication
 
 // Unsigned
-HWY_INLINE Vec128<uint16_t> operator*(const Vec128<uint16_t> a,
-                                      const Vec128<uint16_t> b) {
+HWY_API Vec128<uint16_t> operator*(const Vec128<uint16_t> a,
+                                   const Vec128<uint16_t> b) {
   return Vec128<uint16_t>(vmulq_u16(a.raw, b.raw));
 }
-HWY_INLINE Vec128<uint32_t> operator*(const Vec128<uint32_t> a,
-                                      const Vec128<uint32_t> b) {
+HWY_API Vec128<uint32_t> operator*(const Vec128<uint32_t> a,
+                                   const Vec128<uint32_t> b) {
   return Vec128<uint32_t>(vmulq_u32(a.raw, b.raw));
 }
 
 template <size_t N, HWY_IF_LE64(uint16_t, N)>
-HWY_INLINE Vec128<uint16_t, N> operator*(const Vec128<uint16_t, N> a,
-                                         const Vec128<uint16_t, N> b) {
+HWY_API Vec128<uint16_t, N> operator*(const Vec128<uint16_t, N> a,
+                                      const Vec128<uint16_t, N> b) {
   return Vec128<uint16_t, N>(vmul_u16(a.raw, b.raw));
 }
 template <size_t N, HWY_IF_LE64(uint32_t, N)>
-HWY_INLINE Vec128<uint32_t, N> operator*(const Vec128<uint32_t, N> a,
-                                         const Vec128<uint32_t, N> b) {
+HWY_API Vec128<uint32_t, N> operator*(const Vec128<uint32_t, N> a,
+                                      const Vec128<uint32_t, N> b) {
   return Vec128<uint32_t, N>(vmul_u32(a.raw, b.raw));
 }
 
 // Signed
-HWY_INLINE Vec128<int16_t> operator*(const Vec128<int16_t> a,
-                                     const Vec128<int16_t> b) {
+HWY_API Vec128<int16_t> operator*(const Vec128<int16_t> a,
+                                  const Vec128<int16_t> b) {
   return Vec128<int16_t>(vmulq_s16(a.raw, b.raw));
 }
-HWY_INLINE Vec128<int32_t> operator*(const Vec128<int32_t> a,
-                                     const Vec128<int32_t> b) {
+HWY_API Vec128<int32_t> operator*(const Vec128<int32_t> a,
+                                  const Vec128<int32_t> b) {
   return Vec128<int32_t>(vmulq_s32(a.raw, b.raw));
 }
 
 template <size_t N, HWY_IF_LE64(uint16_t, N)>
-HWY_INLINE Vec128<int16_t, N> operator*(const Vec128<int16_t, N> a,
-                                        const Vec128<int16_t, N> b) {
+HWY_API Vec128<int16_t, N> operator*(const Vec128<int16_t, N> a,
+                                     const Vec128<int16_t, N> b) {
   return Vec128<int16_t, N>(vmul_s16(a.raw, b.raw));
 }
 template <size_t N, HWY_IF_LE64(int32_t, N)>
-HWY_INLINE Vec128<int32_t, N> operator*(const Vec128<int32_t, N> a,
-                                        const Vec128<int32_t, N> b) {
+HWY_API Vec128<int32_t, N> operator*(const Vec128<int32_t, N> a,
+                                     const Vec128<int32_t, N> b) {
   return Vec128<int32_t, N>(vmul_s32(a.raw, b.raw));
 }
 
 // Returns the upper 16 bits of a * b in each lane.
-HWY_INLINE Vec128<int16_t> MulHigh(const Vec128<int16_t> a,
-                                   const Vec128<int16_t> b) {
+HWY_API Vec128<int16_t> MulHigh(const Vec128<int16_t> a,
+                                const Vec128<int16_t> b) {
   int32x4_t rlo = vmull_s16(vget_low_s16(a.raw), vget_low_s16(b.raw));
 #if HWY_ARCH_ARM_A64
   int32x4_t rhi = vmull_high_s16(a.raw, b.raw);
@@ -1123,8 +1097,8 @@ HWY_INLINE Vec128<int16_t> MulHigh(const Vec128<int16_t> a,
   return Vec128<int16_t>(
       vuzp2q_s16(vreinterpretq_s16_s32(rlo), vreinterpretq_s16_s32(rhi)));
 }
-HWY_INLINE Vec128<uint16_t> MulHigh(const Vec128<uint16_t> a,
-                                    const Vec128<uint16_t> b) {
+HWY_API Vec128<uint16_t> MulHigh(const Vec128<uint16_t> a,
+                                 const Vec128<uint16_t> b) {
   uint32x4_t rlo = vmull_u16(vget_low_u16(a.raw), vget_low_u16(b.raw));
 #if HWY_ARCH_ARM_A64
   uint32x4_t rhi = vmull_high_u16(a.raw, b.raw);
@@ -1136,29 +1110,29 @@ HWY_INLINE Vec128<uint16_t> MulHigh(const Vec128<uint16_t> a,
 }
 
 template <size_t N, HWY_IF_LE64(int16_t, N)>
-HWY_INLINE Vec128<int16_t, N> MulHigh(const Vec128<int16_t, N> a,
-                                      const Vec128<int16_t, N> b) {
+HWY_API Vec128<int16_t, N> MulHigh(const Vec128<int16_t, N> a,
+                                   const Vec128<int16_t, N> b) {
   int16x8_t hi_lo = vreinterpretq_s16_s32(vmull_s16(a.raw, b.raw));
   return Vec128<int16_t, N>(vget_low_s16(vuzp2q_s16(hi_lo, hi_lo)));
 }
 template <size_t N, HWY_IF_LE64(uint16_t, N)>
-HWY_INLINE Vec128<uint16_t, N> MulHigh(const Vec128<uint16_t, N> a,
-                                       const Vec128<uint16_t, N> b) {
+HWY_API Vec128<uint16_t, N> MulHigh(const Vec128<uint16_t, N> a,
+                                    const Vec128<uint16_t, N> b) {
   uint16x8_t hi_lo = vreinterpretq_u16_u32(vmull_u16(a.raw, b.raw));
   return Vec128<uint16_t, N>(vget_low_u16(vuzp2q_u16(hi_lo, hi_lo)));
 }
 
 // Multiplies even lanes (0, 2 ..) and places the double-wide result into
 // even and the upper half into its odd neighbor lane.
-HWY_INLINE Vec128<int64_t> MulEven(const Vec128<int32_t> a,
-                                   const Vec128<int32_t> b) {
+HWY_API Vec128<int64_t> MulEven(const Vec128<int32_t> a,
+                                const Vec128<int32_t> b) {
   int32x4_t a_packed = vuzp1q_s32(a.raw, a.raw);
   int32x4_t b_packed = vuzp1q_s32(b.raw, b.raw);
   return Vec128<int64_t>(
       vmull_s32(vget_low_s32(a_packed), vget_low_s32(b_packed)));
 }
-HWY_INLINE Vec128<uint64_t> MulEven(const Vec128<uint32_t> a,
-                                    const Vec128<uint32_t> b) {
+HWY_API Vec128<uint64_t> MulEven(const Vec128<uint32_t> a,
+                                 const Vec128<uint32_t> b) {
   uint32x4_t a_packed = vuzp1q_u32(a.raw, a.raw);
   uint32x4_t b_packed = vuzp1q_u32(b.raw, b.raw);
   return Vec128<uint64_t>(
@@ -1166,32 +1140,46 @@ HWY_INLINE Vec128<uint64_t> MulEven(const Vec128<uint32_t> a,
 }
 
 template <size_t N>
-HWY_INLINE Vec128<int64_t, (N + 1) / 2> MulEven(const Vec128<int32_t, N> a,
-                                                const Vec128<int32_t, N> b) {
+HWY_API Vec128<int64_t, (N + 1) / 2> MulEven(const Vec128<int32_t, N> a,
+                                             const Vec128<int32_t, N> b) {
   int32x2_t a_packed = vuzp1_s32(a.raw, a.raw);
   int32x2_t b_packed = vuzp1_s32(b.raw, b.raw);
   return Vec128<int64_t, (N + 1) / 2>(
       vget_low_s64(vmull_s32(a_packed, b_packed)));
 }
 template <size_t N>
-HWY_INLINE Vec128<uint64_t, (N + 1) / 2> MulEven(const Vec128<uint32_t, N> a,
-                                                 const Vec128<uint32_t, N> b) {
+HWY_API Vec128<uint64_t, (N + 1) / 2> MulEven(const Vec128<uint32_t, N> a,
+                                              const Vec128<uint32_t, N> b) {
   uint32x2_t a_packed = vuzp1_u32(a.raw, a.raw);
   uint32x2_t b_packed = vuzp1_u32(b.raw, b.raw);
   return Vec128<uint64_t, (N + 1) / 2>(
       vget_low_u64(vmull_u32(a_packed, b_packed)));
 }
 
+HWY_INLINE Vec128<uint64_t> MulEven(const Vec128<uint64_t> a,
+                                    const Vec128<uint64_t> b) {
+  uint64_t hi;
+  uint64_t lo = Mul128(vgetq_lane_u64(a.raw, 0), vgetq_lane_u64(b.raw, 0), &hi);
+  return Vec128<uint64_t>(vsetq_lane_u64(hi, vdupq_n_u64(lo), 1));
+}
+
+HWY_INLINE Vec128<uint64_t> MulOdd(const Vec128<uint64_t> a,
+                                   const Vec128<uint64_t> b) {
+  uint64_t hi;
+  uint64_t lo = Mul128(vgetq_lane_u64(a.raw, 1), vgetq_lane_u64(b.raw, 1), &hi);
+  return Vec128<uint64_t>(vsetq_lane_u64(hi, vdupq_n_u64(lo), 1));
+}
+
 // ------------------------------ Floating-point mul / div
 
 HWY_NEON_DEF_FUNCTION_ALL_FLOATS(operator*, vmul, _, 2)
 
 // Approximate reciprocal
-HWY_INLINE Vec128<float> ApproximateReciprocal(const Vec128<float> v) {
+HWY_API Vec128<float> ApproximateReciprocal(const Vec128<float> v) {
   return Vec128<float>(vrecpeq_f32(v.raw));
 }
 template <size_t N>
-HWY_INLINE Vec128<float, N> ApproximateReciprocal(const Vec128<float, N> v) {
+HWY_API Vec128<float, N> ApproximateReciprocal(const Vec128<float, N> v) {
   return Vec128<float, N>(vrecpe_f32(v.raw));
 }
 
@@ -1214,8 +1202,8 @@ HWY_INLINE Vec128<float, N> ReciprocalNewtonRaphsonStep(
 }  // namespace detail
 
 template <size_t N>
-HWY_INLINE Vec128<float, N> operator/(const Vec128<float, N> a,
-                                      const Vec128<float, N> b) {
+HWY_API Vec128<float, N> operator/(const Vec128<float, N> a,
+                                   const Vec128<float, N> b) {
   auto x = ApproximateReciprocal(b);
   x *= detail::ReciprocalNewtonRaphsonStep(x, b);
   x *= detail::ReciprocalNewtonRaphsonStep(x, b);
@@ -1226,12 +1214,12 @@ HWY_INLINE Vec128<float, N> operator/(const Vec128<float, N> a,
 
 // ------------------------------ Absolute value of difference.
 
-HWY_INLINE Vec128<float> AbsDiff(const Vec128<float> a, const Vec128<float> b) {
+HWY_API Vec128<float> AbsDiff(const Vec128<float> a, const Vec128<float> b) {
   return Vec128<float>(vabdq_f32(a.raw, b.raw));
 }
 template <size_t N, HWY_IF_LE64(float, N)>
-HWY_INLINE Vec128<float, N> AbsDiff(const Vec128<float, N> a,
-                                    const Vec128<float, N> b) {
+HWY_API Vec128<float, N> AbsDiff(const Vec128<float, N> a,
+                                 const Vec128<float, N> b) {
   return Vec128<float, N>(vabd_f32(a.raw, b.raw));
 }
 
@@ -1240,34 +1228,33 @@ HWY_INLINE Vec128<float, N> AbsDiff(const Vec128<float, N> a,
 // Returns add + mul * x
 #if defined(__ARM_VFPV4__) || HWY_ARCH_ARM_A64
 template <size_t N, HWY_IF_LE64(float, N)>
-HWY_INLINE Vec128<float, N> MulAdd(const Vec128<float, N> mul,
-                                   const Vec128<float, N> x,
-                                   const Vec128<float, N> add) {
+HWY_API Vec128<float, N> MulAdd(const Vec128<float, N> mul,
+                                const Vec128<float, N> x,
+                                const Vec128<float, N> add) {
   return Vec128<float, N>(vfma_f32(add.raw, mul.raw, x.raw));
 }
-HWY_INLINE Vec128<float> MulAdd(const Vec128<float> mul, const Vec128<float> x,
-                                const Vec128<float> add) {
+HWY_API Vec128<float> MulAdd(const Vec128<float> mul, const Vec128<float> x,
+                             const Vec128<float> add) {
   return Vec128<float>(vfmaq_f32(add.raw, mul.raw, x.raw));
 }
 #else
 // Emulate FMA for floats.
 template <size_t N>
-HWY_INLINE Vec128<float, N> MulAdd(const Vec128<float, N> mul,
-                                   const Vec128<float, N> x,
-                                   const Vec128<float, N> add) {
+HWY_API Vec128<float, N> MulAdd(const Vec128<float, N> mul,
+                                const Vec128<float, N> x,
+                                const Vec128<float, N> add) {
   return mul * x + add;
 }
 #endif
 
 #if HWY_ARCH_ARM_A64
-HWY_INLINE Vec128<double, 1> MulAdd(const Vec128<double, 1> mul,
-                                    const Vec128<double, 1> x,
-                                    const Vec128<double, 1> add) {
+HWY_API Vec128<double, 1> MulAdd(const Vec128<double, 1> mul,
+                                 const Vec128<double, 1> x,
+                                 const Vec128<double, 1> add) {
   return Vec128<double, 1>(vfma_f64(add.raw, mul.raw, x.raw));
 }
-HWY_INLINE Vec128<double> MulAdd(const Vec128<double> mul,
-                                 const Vec128<double> x,
-                                 const Vec128<double> add) {
+HWY_API Vec128<double> MulAdd(const Vec128<double> mul, const Vec128<double> x,
+                              const Vec128<double> add) {
   return Vec128<double>(vfmaq_f64(add.raw, mul.raw, x.raw));
 }
 #endif
@@ -1275,66 +1262,65 @@ HWY_INLINE Vec128<double> MulAdd(const Vec128<double> mul,
 // Returns add - mul * x
 #if defined(__ARM_VFPV4__) || HWY_ARCH_ARM_A64
 template <size_t N, HWY_IF_LE64(float, N)>
-HWY_INLINE Vec128<float, N> NegMulAdd(const Vec128<float, N> mul,
-                                      const Vec128<float, N> x,
-                                      const Vec128<float, N> add) {
+HWY_API Vec128<float, N> NegMulAdd(const Vec128<float, N> mul,
+                                   const Vec128<float, N> x,
+                                   const Vec128<float, N> add) {
   return Vec128<float, N>(vfms_f32(add.raw, mul.raw, x.raw));
 }
-HWY_INLINE Vec128<float> NegMulAdd(const Vec128<float> mul,
-                                   const Vec128<float> x,
-                                   const Vec128<float> add) {
+HWY_API Vec128<float> NegMulAdd(const Vec128<float> mul, const Vec128<float> x,
+                                const Vec128<float> add) {
   return Vec128<float>(vfmsq_f32(add.raw, mul.raw, x.raw));
 }
 #else
 // Emulate FMA for floats.
 template <size_t N>
-HWY_INLINE Vec128<float, N> NegMulAdd(const Vec128<float, N> mul,
-                                      const Vec128<float, N> x,
-                                      const Vec128<float, N> add) {
+HWY_API Vec128<float, N> NegMulAdd(const Vec128<float, N> mul,
+                                   const Vec128<float, N> x,
+                                   const Vec128<float, N> add) {
   return add - mul * x;
 }
 #endif
 
 #if HWY_ARCH_ARM_A64
-HWY_INLINE Vec128<double, 1> NegMulAdd(const Vec128<double, 1> mul,
-                                       const Vec128<double, 1> x,
-                                       const Vec128<double, 1> add) {
+HWY_API Vec128<double, 1> NegMulAdd(const Vec128<double, 1> mul,
+                                    const Vec128<double, 1> x,
+                                    const Vec128<double, 1> add) {
   return Vec128<double, 1>(vfms_f64(add.raw, mul.raw, x.raw));
 }
-HWY_INLINE Vec128<double> NegMulAdd(const Vec128<double> mul,
-                                    const Vec128<double> x,
-                                    const Vec128<double> add) {
+HWY_API Vec128<double> NegMulAdd(const Vec128<double> mul,
+                                 const Vec128<double> x,
+                                 const Vec128<double> add) {
   return Vec128<double>(vfmsq_f64(add.raw, mul.raw, x.raw));
 }
 #endif
 
 // Returns mul * x - sub
 template <size_t N>
-HWY_INLINE Vec128<float, N> MulSub(const Vec128<float, N> mul,
-                                   const Vec128<float, N> x,
-                                   const Vec128<float, N> sub) {
+HWY_API Vec128<float, N> MulSub(const Vec128<float, N> mul,
+                                const Vec128<float, N> x,
+                                const Vec128<float, N> sub) {
   return MulAdd(mul, x, Neg(sub));
 }
 
 // Returns -mul * x - sub
 template <size_t N>
-HWY_INLINE Vec128<float, N> NegMulSub(const Vec128<float, N> mul,
-                                      const Vec128<float, N> x,
-                                      const Vec128<float, N> sub) {
+HWY_API Vec128<float, N> NegMulSub(const Vec128<float, N> mul,
+                                   const Vec128<float, N> x,
+                                   const Vec128<float, N> sub) {
   return Neg(MulAdd(mul, x, sub));
 }
 
 #if HWY_ARCH_ARM_A64
 template <size_t N>
-HWY_INLINE Vec128<double, N> MulSub(const Vec128<double, N> mul,
-                                    const Vec128<double, N> x,
-                                    const Vec128<double, N> sub) {
+HWY_API Vec128<double, N> MulSub(const Vec128<double, N> mul,
+                                 const Vec128<double, N> x,
+                                 const Vec128<double, N> sub) {
   return MulAdd(mul, x, Neg(sub));
 }
 template <size_t N>
-HWY_INLINE Vec128<double, N> NegMulSub(const Vec128<double, N> mul,
-                                       const Vec128<double, N> x,
-                                       const Vec128<double, N> sub) {
+HWY_API Vec128<double, N> NegMulSub(const Vec128<double, N> mul,
+                                    const Vec128<double, N> x,
+                                    const Vec128<double, N> sub) {
   return Neg(MulAdd(mul, x, sub));
 }
 #endif
@@ -1342,12 +1328,11 @@ HWY_INLINE Vec128<double, N> NegMulSub(const Vec128<double, N> mul,
 // ------------------------------ Floating-point square root (IfThenZeroElse)
 
 // Approximate reciprocal square root
-HWY_INLINE Vec128<float> ApproximateReciprocalSqrt(const Vec128<float> v) {
+HWY_API Vec128<float> ApproximateReciprocalSqrt(const Vec128<float> v) {
   return Vec128<float>(vrsqrteq_f32(v.raw));
 }
 template <size_t N>
-HWY_INLINE Vec128<float, N> ApproximateReciprocalSqrt(
-    const Vec128<float, N> v) {
+HWY_API Vec128<float, N> ApproximateReciprocalSqrt(const Vec128<float, N> v) {
   return Vec128<float, N>(vrsqrte_f32(v.raw));
 }
 
@@ -1371,7 +1356,7 @@ HWY_INLINE Vec128<float, N> ReciprocalSqrtStep(const Vec128<float, N> root,
 
 // Not defined on armv7: approximate
 template <size_t N>
-HWY_INLINE Vec128<float, N> Sqrt(const Vec128<float, N> v) {
+HWY_API Vec128<float, N> Sqrt(const Vec128<float, N> v) {
   auto recip = ApproximateReciprocalSqrt(v);
 
   recip *= detail::ReciprocalSqrtStep(v * recip, recip);
@@ -1389,13 +1374,13 @@ HWY_INLINE Vec128<float, N> Sqrt(const Vec128<float, N> v) {
 
 // There is no 64-bit vmvn, so cast instead of using HWY_NEON_DEF_FUNCTION.
 template <typename T>
-HWY_INLINE Vec128<T> Not(const Vec128<T> v) {
+HWY_API Vec128<T> Not(const Vec128<T> v) {
   const Full128<T> d;
   const Repartition<uint8_t, decltype(d)> d8;
   return BitCast(d, Vec128<uint8_t>(vmvnq_u8(BitCast(d8, v).raw)));
 }
 template <typename T, size_t N, HWY_IF_LE64(T, N)>
-HWY_INLINE Vec128<T, N> Not(const Vec128<T, N> v) {
+HWY_API Vec128<T, N> Not(const Vec128<T, N> v) {
   const Simd<T, N> d;
   const Repartition<uint8_t, decltype(d)> d8;
   using V8 = decltype(Zero(d8));
@@ -1407,7 +1392,7 @@ HWY_NEON_DEF_FUNCTION_INTS_UINTS(And, vand, _, 2)
 
 // Uses the u32/64 defined above.
 template <typename T, size_t N, HWY_IF_FLOAT(T)>
-HWY_INLINE Vec128<T, N> And(const Vec128<T, N> a, const Vec128<T, N> b) {
+HWY_API Vec128<T, N> And(const Vec128<T, N> a, const Vec128<T, N> b) {
   const Simd<MakeUnsigned<T>, N> d;
   return BitCast(Simd<T, N>(), BitCast(d, a) & BitCast(d, b));
 }
@@ -1421,15 +1406,15 @@ HWY_NEON_DEF_FUNCTION_INTS_UINTS(reversed_andnot, vbic, _, 2)
 
 // Returns ~not_mask & mask.
 template <typename T, size_t N, HWY_IF_NOT_FLOAT(T)>
-HWY_INLINE Vec128<T, N> AndNot(const Vec128<T, N> not_mask,
-                               const Vec128<T, N> mask) {
+HWY_API Vec128<T, N> AndNot(const Vec128<T, N> not_mask,
+                            const Vec128<T, N> mask) {
   return internal::reversed_andnot(mask, not_mask);
 }
 
 // Uses the u32/64 defined above.
 template <typename T, size_t N, HWY_IF_FLOAT(T)>
-HWY_INLINE Vec128<T, N> AndNot(const Vec128<T, N> not_mask,
-                               const Vec128<T, N> mask) {
+HWY_API Vec128<T, N> AndNot(const Vec128<T, N> not_mask,
+                            const Vec128<T, N> mask) {
   const Simd<MakeUnsigned<T>, N> du;
   Vec128<MakeUnsigned<T>, N> ret =
       internal::reversed_andnot(BitCast(du, mask), BitCast(du, not_mask));
@@ -1442,7 +1427,7 @@ HWY_NEON_DEF_FUNCTION_INTS_UINTS(Or, vorr, _, 2)
 
 // Uses the u32/64 defined above.
 template <typename T, size_t N, HWY_IF_FLOAT(T)>
-HWY_INLINE Vec128<T, N> Or(const Vec128<T, N> a, const Vec128<T, N> b) {
+HWY_API Vec128<T, N> Or(const Vec128<T, N> a, const Vec128<T, N> b) {
   const Simd<MakeUnsigned<T>, N> d;
   return BitCast(Simd<T, N>(), BitCast(d, a) | BitCast(d, b));
 }
@@ -1453,7 +1438,7 @@ HWY_NEON_DEF_FUNCTION_INTS_UINTS(Xor, veor, _, 2)
 
 // Uses the u32/64 defined above.
 template <typename T, size_t N, HWY_IF_FLOAT(T)>
-HWY_INLINE Vec128<T, N> Xor(const Vec128<T, N> a, const Vec128<T, N> b) {
+HWY_API Vec128<T, N> Xor(const Vec128<T, N> a, const Vec128<T, N> b) {
   const Simd<MakeUnsigned<T>, N> d;
   return BitCast(Simd<T, N>(), BitCast(d, a) ^ BitCast(d, b));
 }
@@ -1461,20 +1446,138 @@ HWY_INLINE Vec128<T, N> Xor(const Vec128<T, N> a, const Vec128<T, N> b) {
 // ------------------------------ Operator overloads (internal-only if float)
 
 template <typename T, size_t N>
-HWY_INLINE Vec128<T, N> operator&(const Vec128<T, N> a, const Vec128<T, N> b) {
+HWY_API Vec128<T, N> operator&(const Vec128<T, N> a, const Vec128<T, N> b) {
   return And(a, b);
 }
 
 template <typename T, size_t N>
-HWY_INLINE Vec128<T, N> operator|(const Vec128<T, N> a, const Vec128<T, N> b) {
+HWY_API Vec128<T, N> operator|(const Vec128<T, N> a, const Vec128<T, N> b) {
   return Or(a, b);
 }
 
 template <typename T, size_t N>
-HWY_INLINE Vec128<T, N> operator^(const Vec128<T, N> a, const Vec128<T, N> b) {
+HWY_API Vec128<T, N> operator^(const Vec128<T, N> a, const Vec128<T, N> b) {
   return Xor(a, b);
 }
 
+// ------------------------------ PopulationCount
+
+#ifdef HWY_NATIVE_POPCNT
+#undef HWY_NATIVE_POPCNT
+#else
+#define HWY_NATIVE_POPCNT
+#endif
+
+namespace detail {
+
+template <typename T>
+HWY_INLINE Vec128<T> PopulationCount(hwy::SizeTag<1> /* tag */, Vec128<T> v) {
+  const Full128<uint8_t> d8;
+  return Vec128<T>(vcntq_u8(BitCast(d8, v).raw));
+}
+template <typename T, size_t N, HWY_IF_LE64(T, N)>
+HWY_INLINE Vec128<T, N> PopulationCount(hwy::SizeTag<1> /* tag */,
+                                        Vec128<T, N> v) {
+  const Simd<uint8_t, N> d8;
+  return Vec128<T, N>(vcnt_u8(BitCast(d8, v).raw));
+}
+
+// ARM lacks popcount for lane sizes > 1, so take pairwise sums of the bytes.
+template <typename T>
+HWY_INLINE Vec128<T> PopulationCount(hwy::SizeTag<2> /* tag */, Vec128<T> v) {
+  const Full128<uint8_t> d8;
+  const uint8x16_t bytes = vcntq_u8(BitCast(d8, v).raw);
+  return Vec128<T>(vpaddlq_u8(bytes));
+}
+template <typename T, size_t N, HWY_IF_LE64(T, N)>
+HWY_INLINE Vec128<T, N> PopulationCount(hwy::SizeTag<2> /* tag */,
+                                        Vec128<T, N> v) {
+  const Repartition<uint8_t, Simd<T, N>> d8;
+  const uint8x8_t bytes = vcnt_u8(BitCast(d8, v).raw);
+  return Vec128<T, N>(vpaddl_u8(bytes));
+}
+
+template <typename T>
+HWY_INLINE Vec128<T> PopulationCount(hwy::SizeTag<4> /* tag */, Vec128<T> v) {
+  const Full128<uint8_t> d8;
+  const uint8x16_t bytes = vcntq_u8(BitCast(d8, v).raw);
+  return Vec128<T>(vpaddlq_u16(vpaddlq_u8(bytes)));
+}
+template <typename T, size_t N, HWY_IF_LE64(T, N)>
+HWY_INLINE Vec128<T, N> PopulationCount(hwy::SizeTag<4> /* tag */,
+                                        Vec128<T, N> v) {
+  const Repartition<uint8_t, Simd<T, N>> d8;
+  const uint8x8_t bytes = vcnt_u8(BitCast(d8, v).raw);
+  return Vec128<T, N>(vpaddl_u16(vpaddl_u8(bytes)));
+}
+
+template <typename T>
+HWY_INLINE Vec128<T> PopulationCount(hwy::SizeTag<8> /* tag */, Vec128<T> v) {
+  const Full128<uint8_t> d8;
+  const uint8x16_t bytes = vcntq_u8(BitCast(d8, v).raw);
+  return Vec128<T>(vpaddlq_u32(vpaddlq_u16(vpaddlq_u8(bytes))));
+}
+template <typename T, size_t N, HWY_IF_LE64(T, N)>
+HWY_INLINE Vec128<T, N> PopulationCount(hwy::SizeTag<8> /* tag */,
+                                        Vec128<T, N> v) {
+  const Repartition<uint8_t, Simd<T, N>> d8;
+  const uint8x8_t bytes = vcnt_u8(BitCast(d8, v).raw);
+  return Vec128<T, N>(vpaddl_u32(vpaddl_u16(vpaddl_u8(bytes))));
+}
+
+}  // namespace detail
+
+template <typename T, size_t N, HWY_IF_NOT_FLOAT(T)>
+HWY_API Vec128<T, N> PopulationCount(Vec128<T, N> v) {
+  return detail::PopulationCount(hwy::SizeTag<sizeof(T)>(), v);
+}
+
+// ================================================== SIGN
+
+// ------------------------------ Abs
+
+// Returns absolute value, except that LimitsMin() maps to LimitsMax() + 1.
+HWY_API Vec128<int8_t> Abs(const Vec128<int8_t> v) {
+  return Vec128<int8_t>(vabsq_s8(v.raw));
+}
+HWY_API Vec128<int16_t> Abs(const Vec128<int16_t> v) {
+  return Vec128<int16_t>(vabsq_s16(v.raw));
+}
+HWY_API Vec128<int32_t> Abs(const Vec128<int32_t> v) {
+  return Vec128<int32_t>(vabsq_s32(v.raw));
+}
+// i64 is implemented after BroadcastSignBit.
+HWY_API Vec128<float> Abs(const Vec128<float> v) {
+  return Vec128<float>(vabsq_f32(v.raw));
+}
+
+template <size_t N, HWY_IF_LE64(int8_t, N)>
+HWY_API Vec128<int8_t, N> Abs(const Vec128<int8_t, N> v) {
+  return Vec128<int8_t, N>(vabs_s8(v.raw));
+}
+template <size_t N, HWY_IF_LE64(int16_t, N)>
+HWY_API Vec128<int16_t, N> Abs(const Vec128<int16_t, N> v) {
+  return Vec128<int16_t, N>(vabs_s16(v.raw));
+}
+template <size_t N, HWY_IF_LE64(int32_t, N)>
+HWY_API Vec128<int32_t, N> Abs(const Vec128<int32_t, N> v) {
+  return Vec128<int32_t, N>(vabs_s32(v.raw));
+}
+template <size_t N, HWY_IF_LE64(float, N)>
+HWY_API Vec128<float, N> Abs(const Vec128<float, N> v) {
+  return Vec128<float, N>(vabs_f32(v.raw));
+}
+
+#if HWY_ARCH_ARM_A64
+HWY_API Vec128<double> Abs(const Vec128<double> v) {
+  return Vec128<double>(vabsq_f64(v.raw));
+}
+
+HWY_API Vec128<double, 1> Abs(const Vec128<double, 1> v) {
+  return Vec128<double, 1>(vabs_f64(v.raw));
+}
+#endif
+
 // ------------------------------ CopySign
 
 template <typename T, size_t N>
@@ -1505,19 +1608,19 @@ HWY_API Vec128<T, N> BroadcastSignBit(const Vec128<T, N> v) {
 
 // Mask and Vec have the same representation (true = FF..FF).
 template <typename T, size_t N>
-HWY_INLINE Mask128<T, N> MaskFromVec(const Vec128<T, N> v) {
+HWY_API Mask128<T, N> MaskFromVec(const Vec128<T, N> v) {
   const Simd<MakeUnsigned<T>, N> du;
   return Mask128<T, N>(BitCast(du, v).raw);
 }
 
 // DEPRECATED
 template <typename T, size_t N>
-HWY_INLINE Vec128<T, N> VecFromMask(const Mask128<T, N> v) {
+HWY_API Vec128<T, N> VecFromMask(const Mask128<T, N> v) {
   return BitCast(Simd<T, N>(), Vec128<MakeUnsigned<T>, N>(v.raw));
 }
 
 template <typename T, size_t N>
-HWY_INLINE Vec128<T, N> VecFromMask(Simd<T, N> d, const Mask128<T, N> v) {
+HWY_API Vec128<T, N> VecFromMask(Simd<T, N> d, const Mask128<T, N> v) {
   return BitCast(d, Vec128<MakeUnsigned<T>, N>(v.raw));
 }
 
@@ -1547,20 +1650,20 @@ HWY_NEON_DEF_FUNCTION_ALL_TYPES(IfThenElse, vbsl, _, HWY_IF)
 
 // mask ? yes : 0
 template <typename T, size_t N>
-HWY_INLINE Vec128<T, N> IfThenElseZero(const Mask128<T, N> mask,
-                                       const Vec128<T, N> yes) {
+HWY_API Vec128<T, N> IfThenElseZero(const Mask128<T, N> mask,
+                                    const Vec128<T, N> yes) {
   return yes & VecFromMask(Simd<T, N>(), mask);
 }
 
 // mask ? 0 : no
 template <typename T, size_t N>
-HWY_INLINE Vec128<T, N> IfThenZeroElse(const Mask128<T, N> mask,
-                                       const Vec128<T, N> no) {
+HWY_API Vec128<T, N> IfThenZeroElse(const Mask128<T, N> mask,
+                                    const Vec128<T, N> no) {
   return AndNot(VecFromMask(Simd<T, N>(), mask), no);
 }
 
 template <typename T, size_t N>
-HWY_INLINE Vec128<T, N> ZeroIfNegative(Vec128<T, N> v) {
+HWY_API Vec128<T, N> ZeroIfNegative(Vec128<T, N> v) {
   const auto zero = Zero(Simd<T, N>());
   return Max(zero, v);
 }
@@ -1569,8 +1672,7 @@ HWY_INLINE Vec128<T, N> ZeroIfNegative(Vec128<T, N> v) {
 
 template <typename T, size_t N>
 HWY_API Mask128<T, N> Not(const Mask128<T, N> m) {
-  const Simd<T, N> d;
-  return MaskFromVec(Not(VecFromMask(d, m)));
+  return MaskFromVec(Not(VecFromMask(Simd<T, N>(), m)));
 }
 
 template <typename T, size_t N>
@@ -1604,22 +1706,22 @@ HWY_API Mask128<T, N> Xor(const Mask128<T, N> a, Mask128<T, N> b) {
 // ------------------------------ Shuffle2301 (for i64 compares)
 
 // Swap 32-bit halves in 64-bits
-HWY_INLINE Vec128<uint32_t, 2> Shuffle2301(const Vec128<uint32_t, 2> v) {
+HWY_API Vec128<uint32_t, 2> Shuffle2301(const Vec128<uint32_t, 2> v) {
   return Vec128<uint32_t, 2>(vrev64_u32(v.raw));
 }
-HWY_INLINE Vec128<int32_t, 2> Shuffle2301(const Vec128<int32_t, 2> v) {
+HWY_API Vec128<int32_t, 2> Shuffle2301(const Vec128<int32_t, 2> v) {
   return Vec128<int32_t, 2>(vrev64_s32(v.raw));
 }
-HWY_INLINE Vec128<float, 2> Shuffle2301(const Vec128<float, 2> v) {
+HWY_API Vec128<float, 2> Shuffle2301(const Vec128<float, 2> v) {
   return Vec128<float, 2>(vrev64_f32(v.raw));
 }
-HWY_INLINE Vec128<uint32_t> Shuffle2301(const Vec128<uint32_t> v) {
+HWY_API Vec128<uint32_t> Shuffle2301(const Vec128<uint32_t> v) {
   return Vec128<uint32_t>(vrev64q_u32(v.raw));
 }
-HWY_INLINE Vec128<int32_t> Shuffle2301(const Vec128<int32_t> v) {
+HWY_API Vec128<int32_t> Shuffle2301(const Vec128<int32_t> v) {
   return Vec128<int32_t>(vrev64q_s32(v.raw));
 }
-HWY_INLINE Vec128<float> Shuffle2301(const Vec128<float> v) {
+HWY_API Vec128<float> Shuffle2301(const Vec128<float> v) {
   return Vec128<float>(vrev64q_f32(v.raw));
 }
 
@@ -1639,6 +1741,12 @@ HWY_NEON_DEF_FUNCTION_INT_8_16_32(operator==, vceq, _, HWY_COMPARE)
 HWY_NEON_DEF_FUNCTION_UINT_8_16_32(operator==, vceq, _, HWY_COMPARE)
 #endif
 
+// ------------------------------ Inequality
+template <typename T, size_t N>
+HWY_API Mask128<T, N> operator!=(const Vec128<T, N> a, const Vec128<T, N> b) {
+  return Not(a == b);
+}
+
 // ------------------------------ Strict inequality (signed, float)
 #if HWY_ARCH_ARM_A64
 HWY_NEON_DEF_FUNCTION_INTS(operator<, vclt, _, HWY_COMPARE)
@@ -1660,8 +1768,8 @@ HWY_NEON_DEF_FUNCTION_ALL_FLOATS(operator<=, vcle, _, HWY_COMPARE)
 #if HWY_ARCH_ARM_V7
 
 template <size_t N>
-HWY_INLINE Mask128<int64_t, N> operator==(const Vec128<int64_t, N> a,
-                                          const Vec128<int64_t, N> b) {
+HWY_API Mask128<int64_t, N> operator==(const Vec128<int64_t, N> a,
+                                       const Vec128<int64_t, N> b) {
   const Simd<int32_t, N * 2> d32;
   const Simd<int64_t, N> d64;
   const auto cmp32 = VecFromMask(d32, Eq(BitCast(d32, a), BitCast(d32, b)));
@@ -1670,8 +1778,8 @@ HWY_INLINE Mask128<int64_t, N> operator==(const Vec128<int64_t, N> a,
 }
 
 template <size_t N>
-HWY_INLINE Mask128<uint64_t, N> operator==(const Vec128<uint64_t, N> a,
-                                           const Vec128<uint64_t, N> b) {
+HWY_API Mask128<uint64_t, N> operator==(const Vec128<uint64_t, N> a,
+                                        const Vec128<uint64_t, N> b) {
   const Simd<uint32_t, N * 2> d32;
   const Simd<uint64_t, N> d64;
   const auto cmp32 = VecFromMask(d32, Eq(BitCast(d32, a), BitCast(d32, b)));
@@ -1679,13 +1787,13 @@ HWY_INLINE Mask128<uint64_t, N> operator==(const Vec128<uint64_t, N> a,
   return MaskFromVec(BitCast(d64, cmp64));
 }
 
-HWY_INLINE Mask128<int64_t> operator<(const Vec128<int64_t> a,
-                                      const Vec128<int64_t> b) {
+HWY_API Mask128<int64_t> operator<(const Vec128<int64_t> a,
+                                   const Vec128<int64_t> b) {
   const int64x2_t sub = vqsubq_s64(a.raw, b.raw);
   return MaskFromVec(BroadcastSignBit(Vec128<int64_t>(sub)));
 }
-HWY_INLINE Mask128<int64_t, 1> operator<(const Vec128<int64_t, 1> a,
-                                         const Vec128<int64_t, 1> b) {
+HWY_API Mask128<int64_t, 1> operator<(const Vec128<int64_t, 1> a,
+                                      const Vec128<int64_t, 1> b) {
   const int64x1_t sub = vqsub_s64(a.raw, b.raw);
   return MaskFromVec(BroadcastSignBit(Vec128<int64_t, 1>(sub)));
 }
@@ -1727,13 +1835,13 @@ HWY_NEON_DEF_FUNCTION_UINT_8_16_32(TestBit, vtst, _, HWY_TESTBIT)
 HWY_NEON_DEF_FUNCTION_INT_8_16_32(TestBit, vtst, _, HWY_TESTBIT)
 
 template <size_t N>
-HWY_INLINE Mask128<uint64_t, N> TestBit(Vec128<uint64_t, N> v,
-                                        Vec128<uint64_t, N> bit) {
+HWY_API Mask128<uint64_t, N> TestBit(Vec128<uint64_t, N> v,
+                                     Vec128<uint64_t, N> bit) {
   return (v & bit) == bit;
 }
 template <size_t N>
-HWY_INLINE Mask128<int64_t, N> TestBit(Vec128<int64_t, N> v,
-                                       Vec128<int64_t, N> bit) {
+HWY_API Mask128<int64_t, N> TestBit(Vec128<int64_t, N> v,
+                                    Vec128<int64_t, N> bit) {
   return (v & bit) == bit;
 }
 
@@ -1744,7 +1852,7 @@ HWY_INLINE Mask128<int64_t, N> TestBit(Vec128<int64_t, N> v,
 #undef HWY_NEON_BUILD_ARG_HWY_TESTBIT
 
 // ------------------------------ Abs i64 (IfThenElse, BroadcastSignBit)
-HWY_INLINE Vec128<int64_t> Abs(const Vec128<int64_t> v) {
+HWY_API Vec128<int64_t> Abs(const Vec128<int64_t> v) {
 #if HWY_ARCH_ARM_A64
   return Vec128<int64_t>(vabsq_s64(v.raw));
 #else
@@ -1752,7 +1860,7 @@ HWY_INLINE Vec128<int64_t> Abs(const Vec128<int64_t> v) {
   return IfThenElse(MaskFromVec(BroadcastSignBit(v)), zero - v, v);
 #endif
 }
-HWY_INLINE Vec128<int64_t, 1> Abs(const Vec128<int64_t, 1> v) {
+HWY_API Vec128<int64_t, 1> Abs(const Vec128<int64_t, 1> v) {
 #if HWY_ARCH_ARM_A64
   return Vec128<int64_t, 1>(vabs_s64(v.raw));
 #else
@@ -1765,11 +1873,11 @@ HWY_INLINE Vec128<int64_t, 1> Abs(const Vec128<int64_t, 1> v) {
 
 #if HWY_ARCH_ARM_A64
 
-HWY_INLINE Mask128<uint64_t> operator<(Vec128<uint64_t> a, Vec128<uint64_t> b) {
+HWY_API Mask128<uint64_t> operator<(Vec128<uint64_t> a, Vec128<uint64_t> b) {
   return Mask128<uint64_t>(vcltq_u64(a.raw, b.raw));
 }
-HWY_INLINE Mask128<uint64_t, 1> operator<(Vec128<uint64_t, 1> a,
-                                          Vec128<uint64_t, 1> b) {
+HWY_API Mask128<uint64_t, 1> operator<(Vec128<uint64_t, 1> a,
+                                       Vec128<uint64_t, 1> b) {
   return Mask128<uint64_t, 1>(vclt_u64(a.raw, b.raw));
 }
 
@@ -1779,8 +1887,8 @@ HWY_INLINE Mask128<uint64_t, 1> operator<(Vec128<uint64_t, 1> a,
 HWY_NEON_DEF_FUNCTION_UINT_8_16_32(Min, vmin, _, 2)
 
 template <size_t N>
-HWY_INLINE Vec128<uint64_t, N> Min(const Vec128<uint64_t, N> a,
-                                   const Vec128<uint64_t, N> b) {
+HWY_API Vec128<uint64_t, N> Min(const Vec128<uint64_t, N> a,
+                                const Vec128<uint64_t, N> b) {
 #if HWY_ARCH_ARM_A64
   return IfThenElse(b < a, b, a);
 #else
@@ -1794,8 +1902,8 @@ HWY_INLINE Vec128<uint64_t, N> Min(const Vec128<uint64_t, N> a,
 HWY_NEON_DEF_FUNCTION_INT_8_16_32(Min, vmin, _, 2)
 
 template <size_t N>
-HWY_INLINE Vec128<int64_t, N> Min(const Vec128<int64_t, N> a,
-                                  const Vec128<int64_t, N> b) {
+HWY_API Vec128<int64_t, N> Min(const Vec128<int64_t, N> a,
+                               const Vec128<int64_t, N> b) {
 #if HWY_ARCH_ARM_A64
   return IfThenElse(b < a, b, a);
 #else
@@ -1817,8 +1925,8 @@ HWY_NEON_DEF_FUNCTION_ALL_FLOATS(Min, vmin, _, 2)
 HWY_NEON_DEF_FUNCTION_UINT_8_16_32(Max, vmax, _, 2)
 
 template <size_t N>
-HWY_INLINE Vec128<uint64_t, N> Max(const Vec128<uint64_t, N> a,
-                                   const Vec128<uint64_t, N> b) {
+HWY_API Vec128<uint64_t, N> Max(const Vec128<uint64_t, N> a,
+                                const Vec128<uint64_t, N> b) {
 #if HWY_ARCH_ARM_A64
   return IfThenElse(b < a, a, b);
 #else
@@ -1832,8 +1940,8 @@ HWY_INLINE Vec128<uint64_t, N> Max(const Vec128<uint64_t, N> a,
 HWY_NEON_DEF_FUNCTION_INT_8_16_32(Max, vmax, _, 2)
 
 template <size_t N>
-HWY_INLINE Vec128<int64_t, N> Max(const Vec128<int64_t, N> a,
-                                  const Vec128<int64_t, N> b) {
+HWY_API Vec128<int64_t, N> Max(const Vec128<int64_t, N> a,
+                               const Vec128<int64_t, N> b) {
 #if HWY_ARCH_ARM_A64
   return IfThenElse(b < a, a, b);
 #else
@@ -1853,90 +1961,90 @@ HWY_NEON_DEF_FUNCTION_ALL_FLOATS(Max, vmax, _, 2)
 
 // ------------------------------ Load 128
 
-HWY_INLINE Vec128<uint8_t> LoadU(Full128<uint8_t> /* tag */,
-                                 const uint8_t* HWY_RESTRICT aligned) {
+HWY_API Vec128<uint8_t> LoadU(Full128<uint8_t> /* tag */,
+                              const uint8_t* HWY_RESTRICT aligned) {
   return Vec128<uint8_t>(vld1q_u8(aligned));
 }
-HWY_INLINE Vec128<uint16_t> LoadU(Full128<uint16_t> /* tag */,
-                                  const uint16_t* HWY_RESTRICT aligned) {
+HWY_API Vec128<uint16_t> LoadU(Full128<uint16_t> /* tag */,
+                               const uint16_t* HWY_RESTRICT aligned) {
   return Vec128<uint16_t>(vld1q_u16(aligned));
 }
-HWY_INLINE Vec128<uint32_t> LoadU(Full128<uint32_t> /* tag */,
-                                  const uint32_t* HWY_RESTRICT aligned) {
+HWY_API Vec128<uint32_t> LoadU(Full128<uint32_t> /* tag */,
+                               const uint32_t* HWY_RESTRICT aligned) {
   return Vec128<uint32_t>(vld1q_u32(aligned));
 }
-HWY_INLINE Vec128<uint64_t> LoadU(Full128<uint64_t> /* tag */,
-                                  const uint64_t* HWY_RESTRICT aligned) {
+HWY_API Vec128<uint64_t> LoadU(Full128<uint64_t> /* tag */,
+                               const uint64_t* HWY_RESTRICT aligned) {
   return Vec128<uint64_t>(vld1q_u64(aligned));
 }
-HWY_INLINE Vec128<int8_t> LoadU(Full128<int8_t> /* tag */,
-                                const int8_t* HWY_RESTRICT aligned) {
+HWY_API Vec128<int8_t> LoadU(Full128<int8_t> /* tag */,
+                             const int8_t* HWY_RESTRICT aligned) {
   return Vec128<int8_t>(vld1q_s8(aligned));
 }
-HWY_INLINE Vec128<int16_t> LoadU(Full128<int16_t> /* tag */,
-                                 const int16_t* HWY_RESTRICT aligned) {
+HWY_API Vec128<int16_t> LoadU(Full128<int16_t> /* tag */,
+                              const int16_t* HWY_RESTRICT aligned) {
   return Vec128<int16_t>(vld1q_s16(aligned));
 }
-HWY_INLINE Vec128<int32_t> LoadU(Full128<int32_t> /* tag */,
-                                 const int32_t* HWY_RESTRICT aligned) {
+HWY_API Vec128<int32_t> LoadU(Full128<int32_t> /* tag */,
+                              const int32_t* HWY_RESTRICT aligned) {
   return Vec128<int32_t>(vld1q_s32(aligned));
 }
-HWY_INLINE Vec128<int64_t> LoadU(Full128<int64_t> /* tag */,
-                                 const int64_t* HWY_RESTRICT aligned) {
+HWY_API Vec128<int64_t> LoadU(Full128<int64_t> /* tag */,
+                              const int64_t* HWY_RESTRICT aligned) {
   return Vec128<int64_t>(vld1q_s64(aligned));
 }
-HWY_INLINE Vec128<float> LoadU(Full128<float> /* tag */,
-                               const float* HWY_RESTRICT aligned) {
+HWY_API Vec128<float> LoadU(Full128<float> /* tag */,
+                            const float* HWY_RESTRICT aligned) {
   return Vec128<float>(vld1q_f32(aligned));
 }
 #if HWY_ARCH_ARM_A64
-HWY_INLINE Vec128<double> LoadU(Full128<double> /* tag */,
-                                const double* HWY_RESTRICT aligned) {
+HWY_API Vec128<double> LoadU(Full128<double> /* tag */,
+                             const double* HWY_RESTRICT aligned) {
   return Vec128<double>(vld1q_f64(aligned));
 }
 #endif
 
 // ------------------------------ Load 64
 
-HWY_INLINE Vec128<uint8_t, 8> LoadU(Simd<uint8_t, 8> /* tag */,
-                                    const uint8_t* HWY_RESTRICT p) {
+HWY_API Vec128<uint8_t, 8> LoadU(Simd<uint8_t, 8> /* tag */,
+                                 const uint8_t* HWY_RESTRICT p) {
   return Vec128<uint8_t, 8>(vld1_u8(p));
 }
-HWY_INLINE Vec128<uint16_t, 4> LoadU(Simd<uint16_t, 4> /* tag */,
-                                     const uint16_t* HWY_RESTRICT p) {
+HWY_API Vec128<uint16_t, 4> LoadU(Simd<uint16_t, 4> /* tag */,
+                                  const uint16_t* HWY_RESTRICT p) {
   return Vec128<uint16_t, 4>(vld1_u16(p));
 }
-HWY_INLINE Vec128<uint32_t, 2> LoadU(Simd<uint32_t, 2> /* tag */,
-                                     const uint32_t* HWY_RESTRICT p) {
+HWY_API Vec128<uint32_t, 2> LoadU(Simd<uint32_t, 2> /* tag */,
+                                  const uint32_t* HWY_RESTRICT p) {
   return Vec128<uint32_t, 2>(vld1_u32(p));
 }
-HWY_INLINE Vec128<uint64_t, 1> LoadU(Simd<uint64_t, 1> /* tag */,
-                                     const uint64_t* HWY_RESTRICT p) {
+HWY_API Vec128<uint64_t, 1> LoadU(Simd<uint64_t, 1> /* tag */,
+                                  const uint64_t* HWY_RESTRICT p) {
   return Vec128<uint64_t, 1>(vld1_u64(p));
 }
-HWY_INLINE Vec128<int8_t, 8> LoadU(Simd<int8_t, 8> /* tag */,
-                                   const int8_t* HWY_RESTRICT p) {
+HWY_API Vec128<int8_t, 8> LoadU(Simd<int8_t, 8> /* tag */,
+                                const int8_t* HWY_RESTRICT p) {
   return Vec128<int8_t, 8>(vld1_s8(p));
 }
-HWY_INLINE Vec128<int16_t, 4> LoadU(Simd<int16_t, 4> /* tag */,
-                                    const int16_t* HWY_RESTRICT p) {
+HWY_API Vec128<int16_t, 4> LoadU(Simd<int16_t, 4> /* tag */,
+                                 const int16_t* HWY_RESTRICT p) {
   return Vec128<int16_t, 4>(vld1_s16(p));
 }
-HWY_INLINE Vec128<int32_t, 2> LoadU(Simd<int32_t, 2> /* tag */,
-                                    const int32_t* HWY_RESTRICT p) {
+HWY_API Vec128<int32_t, 2> LoadU(Simd<int32_t, 2> /* tag */,
+                                 const int32_t* HWY_RESTRICT p) {
   return Vec128<int32_t, 2>(vld1_s32(p));
 }
-HWY_INLINE Vec128<int64_t, 1> LoadU(Simd<int64_t, 1> /* tag */,
-                                    const int64_t* HWY_RESTRICT p) {
+HWY_API Vec128<int64_t, 1> LoadU(Simd<int64_t, 1> /* tag */,
+                                 const int64_t* HWY_RESTRICT p) {
   return Vec128<int64_t, 1>(vld1_s64(p));
 }
-HWY_INLINE Vec128<float, 2> LoadU(Simd<float, 2> /* tag */,
-                                  const float* HWY_RESTRICT p) {
+HWY_API Vec128<float, 2> LoadU(Simd<float, 2> /* tag */,
+                               const float* HWY_RESTRICT p) {
   return Vec128<float, 2>(vld1_f32(p));
 }
 #if HWY_ARCH_ARM_A64
-HWY_INLINE Vec128<double, 1> LoadU(Simd<double, 1> /* tag */,
-                                   const double* HWY_RESTRICT p) {
+HWY_API Vec128<double, 1> LoadU(Simd<double, 1> /* tag */,
+                                const double* HWY_RESTRICT p) {
   return Vec128<double, 1>(vld1_f64(p));
 }
 #endif
@@ -1948,44 +2056,44 @@ HWY_INLINE Vec128<double, 1> LoadU(Simd<double, 1> /* tag */,
 // we don't actually care what is in it, and we don't want
 // to introduce extra overhead by initializing it to something.
 
-HWY_INLINE Vec128<uint8_t, 4> LoadU(Simd<uint8_t, 4> /*tag*/,
-                                    const uint8_t* HWY_RESTRICT p) {
+HWY_API Vec128<uint8_t, 4> LoadU(Simd<uint8_t, 4> /*tag*/,
+                                 const uint8_t* HWY_RESTRICT p) {
   uint32x2_t a = Undefined(Simd<uint32_t, 2>()).raw;
   uint32x2_t b = vld1_lane_u32(reinterpret_cast<const uint32_t*>(p), a, 0);
   return Vec128<uint8_t, 4>(vreinterpret_u8_u32(b));
 }
-HWY_INLINE Vec128<uint16_t, 2> LoadU(Simd<uint16_t, 2> /*tag*/,
-                                     const uint16_t* HWY_RESTRICT p) {
+HWY_API Vec128<uint16_t, 2> LoadU(Simd<uint16_t, 2> /*tag*/,
+                                  const uint16_t* HWY_RESTRICT p) {
   uint32x2_t a = Undefined(Simd<uint32_t, 2>()).raw;
   uint32x2_t b = vld1_lane_u32(reinterpret_cast<const uint32_t*>(p), a, 0);
   return Vec128<uint16_t, 2>(vreinterpret_u16_u32(b));
 }
-HWY_INLINE Vec128<uint32_t, 1> LoadU(Simd<uint32_t, 1> /*tag*/,
-                                     const uint32_t* HWY_RESTRICT p) {
+HWY_API Vec128<uint32_t, 1> LoadU(Simd<uint32_t, 1> /*tag*/,
+                                  const uint32_t* HWY_RESTRICT p) {
   uint32x2_t a = Undefined(Simd<uint32_t, 2>()).raw;
   uint32x2_t b = vld1_lane_u32(p, a, 0);
   return Vec128<uint32_t, 1>(b);
 }
-HWY_INLINE Vec128<int8_t, 4> LoadU(Simd<int8_t, 4> /*tag*/,
-                                   const int8_t* HWY_RESTRICT p) {
+HWY_API Vec128<int8_t, 4> LoadU(Simd<int8_t, 4> /*tag*/,
+                                const int8_t* HWY_RESTRICT p) {
   int32x2_t a = Undefined(Simd<int32_t, 2>()).raw;
   int32x2_t b = vld1_lane_s32(reinterpret_cast<const int32_t*>(p), a, 0);
   return Vec128<int8_t, 4>(vreinterpret_s8_s32(b));
 }
-HWY_INLINE Vec128<int16_t, 2> LoadU(Simd<int16_t, 2> /*tag*/,
-                                    const int16_t* HWY_RESTRICT p) {
+HWY_API Vec128<int16_t, 2> LoadU(Simd<int16_t, 2> /*tag*/,
+                                 const int16_t* HWY_RESTRICT p) {
   int32x2_t a = Undefined(Simd<int32_t, 2>()).raw;
   int32x2_t b = vld1_lane_s32(reinterpret_cast<const int32_t*>(p), a, 0);
   return Vec128<int16_t, 2>(vreinterpret_s16_s32(b));
 }
-HWY_INLINE Vec128<int32_t, 1> LoadU(Simd<int32_t, 1> /*tag*/,
-                                    const int32_t* HWY_RESTRICT p) {
+HWY_API Vec128<int32_t, 1> LoadU(Simd<int32_t, 1> /*tag*/,
+                                 const int32_t* HWY_RESTRICT p) {
   int32x2_t a = Undefined(Simd<int32_t, 2>()).raw;
   int32x2_t b = vld1_lane_s32(p, a, 0);
   return Vec128<int32_t, 1>(b);
 }
-HWY_INLINE Vec128<float, 1> LoadU(Simd<float, 1> /*tag*/,
-                                  const float* HWY_RESTRICT p) {
+HWY_API Vec128<float, 1> LoadU(Simd<float, 1> /*tag*/,
+                               const float* HWY_RESTRICT p) {
   float32x2_t a = Undefined(Simd<float, 2>()).raw;
   float32x2_t b = vld1_lane_f32(p, a, 0);
   return Vec128<float, 1>(b);
@@ -1993,26 +2101,26 @@ HWY_INLINE Vec128<float, 1> LoadU(Simd<float, 1> /*tag*/,
 
 // ------------------------------ Load 16
 
-HWY_INLINE Vec128<uint8_t, 2> LoadU(Simd<uint8_t, 2> /*tag*/,
-                                    const uint8_t* HWY_RESTRICT p) {
+HWY_API Vec128<uint8_t, 2> LoadU(Simd<uint8_t, 2> /*tag*/,
+                                 const uint8_t* HWY_RESTRICT p) {
   uint16x4_t a = Undefined(Simd<uint16_t, 4>()).raw;
   uint16x4_t b = vld1_lane_u16(reinterpret_cast<const uint16_t*>(p), a, 0);
   return Vec128<uint8_t, 2>(vreinterpret_u8_u16(b));
 }
-HWY_INLINE Vec128<uint16_t, 1> LoadU(Simd<uint16_t, 1> /*tag*/,
-                                     const uint16_t* HWY_RESTRICT p) {
+HWY_API Vec128<uint16_t, 1> LoadU(Simd<uint16_t, 1> /*tag*/,
+                                  const uint16_t* HWY_RESTRICT p) {
   uint16x4_t a = Undefined(Simd<uint16_t, 4>()).raw;
   uint16x4_t b = vld1_lane_u16(p, a, 0);
   return Vec128<uint16_t, 1>(b);
 }
-HWY_INLINE Vec128<int8_t, 2> LoadU(Simd<int8_t, 2> /*tag*/,
-                                   const int8_t* HWY_RESTRICT p) {
+HWY_API Vec128<int8_t, 2> LoadU(Simd<int8_t, 2> /*tag*/,
+                                const int8_t* HWY_RESTRICT p) {
   int16x4_t a = Undefined(Simd<int16_t, 4>()).raw;
   int16x4_t b = vld1_lane_s16(reinterpret_cast<const int16_t*>(p), a, 0);
   return Vec128<int8_t, 2>(vreinterpret_s8_s16(b));
 }
-HWY_INLINE Vec128<int16_t, 1> LoadU(Simd<int16_t, 1> /*tag*/,
-                                    const int16_t* HWY_RESTRICT p) {
+HWY_API Vec128<int16_t, 1> LoadU(Simd<int16_t, 1> /*tag*/,
+                                 const int16_t* HWY_RESTRICT p) {
   int16x4_t a = Undefined(Simd<int16_t, 4>()).raw;
   int16x4_t b = vld1_lane_s16(p, a, 0);
   return Vec128<int16_t, 1>(b);
@@ -2020,15 +2128,15 @@ HWY_INLINE Vec128<int16_t, 1> LoadU(Simd<int16_t, 1> /*tag*/,
 
 // ------------------------------ Load 8
 
-HWY_INLINE Vec128<uint8_t, 1> LoadU(Simd<uint8_t, 1> d,
-                                    const uint8_t* HWY_RESTRICT p) {
+HWY_API Vec128<uint8_t, 1> LoadU(Simd<uint8_t, 1> d,
+                                 const uint8_t* HWY_RESTRICT p) {
   uint8x8_t a = Undefined(d).raw;
   uint8x8_t b = vld1_lane_u8(p, a, 0);
   return Vec128<uint8_t, 1>(b);
 }
 
-HWY_INLINE Vec128<int8_t, 1> LoadU(Simd<int8_t, 1> d,
-                                   const int8_t* HWY_RESTRICT p) {
+HWY_API Vec128<int8_t, 1> LoadU(Simd<int8_t, 1> d,
+                                const int8_t* HWY_RESTRICT p) {
   int8x8_t a = Undefined(d).raw;
   int8x8_t b = vld1_lane_s8(p, a, 0);
   return Vec128<int8_t, 1>(b);
@@ -2036,8 +2144,8 @@ HWY_INLINE Vec128<int8_t, 1> LoadU(Simd<int8_t, 1> d,
 
 // float16_t uses the same Raw as uint16_t, so forward to that.
 template <size_t N>
-HWY_INLINE Vec128<float16_t, N> LoadU(Simd<float16_t, N> /*d*/,
-                                      const float16_t* HWY_RESTRICT p) {
+HWY_API Vec128<float16_t, N> LoadU(Simd<float16_t, N> /*d*/,
+                                   const float16_t* HWY_RESTRICT p) {
   const Simd<uint16_t, N> du16;
   const auto pu16 = reinterpret_cast<const uint16_t*>(p);
   return Vec128<float16_t, N>(LoadU(du16, pu16).raw);
@@ -2045,171 +2153,170 @@ HWY_INLINE Vec128<float16_t, N> LoadU(Simd<float16_t, N> /*d*/,
 
 // On ARM, Load is the same as LoadU.
 template <typename T, size_t N>
-HWY_INLINE Vec128<T, N> Load(Simd<T, N> d, const T* HWY_RESTRICT p) {
+HWY_API Vec128<T, N> Load(Simd<T, N> d, const T* HWY_RESTRICT p) {
   return LoadU(d, p);
 }
 
 // 128-bit SIMD => nothing to duplicate, same as an unaligned load.
 template <typename T, size_t N, HWY_IF_LE128(T, N)>
-HWY_INLINE Vec128<T, N> LoadDup128(Simd<T, N> d,
-                                   const T* const HWY_RESTRICT p) {
+HWY_API Vec128<T, N> LoadDup128(Simd<T, N> d, const T* const HWY_RESTRICT p) {
   return LoadU(d, p);
 }
 
 // ------------------------------ Store 128
 
-HWY_INLINE void StoreU(const Vec128<uint8_t> v, Full128<uint8_t> /* tag */,
-                       uint8_t* HWY_RESTRICT aligned) {
+HWY_API void StoreU(const Vec128<uint8_t> v, Full128<uint8_t> /* tag */,
+                    uint8_t* HWY_RESTRICT aligned) {
   vst1q_u8(aligned, v.raw);
 }
-HWY_INLINE void StoreU(const Vec128<uint16_t> v, Full128<uint16_t> /* tag */,
-                       uint16_t* HWY_RESTRICT aligned) {
+HWY_API void StoreU(const Vec128<uint16_t> v, Full128<uint16_t> /* tag */,
+                    uint16_t* HWY_RESTRICT aligned) {
   vst1q_u16(aligned, v.raw);
 }
-HWY_INLINE void StoreU(const Vec128<uint32_t> v, Full128<uint32_t> /* tag */,
-                       uint32_t* HWY_RESTRICT aligned) {
+HWY_API void StoreU(const Vec128<uint32_t> v, Full128<uint32_t> /* tag */,
+                    uint32_t* HWY_RESTRICT aligned) {
   vst1q_u32(aligned, v.raw);
 }
-HWY_INLINE void StoreU(const Vec128<uint64_t> v, Full128<uint64_t> /* tag */,
-                       uint64_t* HWY_RESTRICT aligned) {
+HWY_API void StoreU(const Vec128<uint64_t> v, Full128<uint64_t> /* tag */,
+                    uint64_t* HWY_RESTRICT aligned) {
   vst1q_u64(aligned, v.raw);
 }
-HWY_INLINE void StoreU(const Vec128<int8_t> v, Full128<int8_t> /* tag */,
-                       int8_t* HWY_RESTRICT aligned) {
+HWY_API void StoreU(const Vec128<int8_t> v, Full128<int8_t> /* tag */,
+                    int8_t* HWY_RESTRICT aligned) {
   vst1q_s8(aligned, v.raw);
 }
-HWY_INLINE void StoreU(const Vec128<int16_t> v, Full128<int16_t> /* tag */,
-                       int16_t* HWY_RESTRICT aligned) {
+HWY_API void StoreU(const Vec128<int16_t> v, Full128<int16_t> /* tag */,
+                    int16_t* HWY_RESTRICT aligned) {
   vst1q_s16(aligned, v.raw);
 }
-HWY_INLINE void StoreU(const Vec128<int32_t> v, Full128<int32_t> /* tag */,
-                       int32_t* HWY_RESTRICT aligned) {
+HWY_API void StoreU(const Vec128<int32_t> v, Full128<int32_t> /* tag */,
+                    int32_t* HWY_RESTRICT aligned) {
   vst1q_s32(aligned, v.raw);
 }
-HWY_INLINE void StoreU(const Vec128<int64_t> v, Full128<int64_t> /* tag */,
-                       int64_t* HWY_RESTRICT aligned) {
+HWY_API void StoreU(const Vec128<int64_t> v, Full128<int64_t> /* tag */,
+                    int64_t* HWY_RESTRICT aligned) {
   vst1q_s64(aligned, v.raw);
 }
-HWY_INLINE void StoreU(const Vec128<float> v, Full128<float> /* tag */,
-                       float* HWY_RESTRICT aligned) {
+HWY_API void StoreU(const Vec128<float> v, Full128<float> /* tag */,
+                    float* HWY_RESTRICT aligned) {
   vst1q_f32(aligned, v.raw);
 }
 #if HWY_ARCH_ARM_A64
-HWY_INLINE void StoreU(const Vec128<double> v, Full128<double> /* tag */,
-                       double* HWY_RESTRICT aligned) {
+HWY_API void StoreU(const Vec128<double> v, Full128<double> /* tag */,
+                    double* HWY_RESTRICT aligned) {
   vst1q_f64(aligned, v.raw);
 }
 #endif
 
 // ------------------------------ Store 64
 
-HWY_INLINE void StoreU(const Vec128<uint8_t, 8> v, Simd<uint8_t, 8> /* tag */,
-                       uint8_t* HWY_RESTRICT p) {
+HWY_API void StoreU(const Vec128<uint8_t, 8> v, Simd<uint8_t, 8> /* tag */,
+                    uint8_t* HWY_RESTRICT p) {
   vst1_u8(p, v.raw);
 }
-HWY_INLINE void StoreU(const Vec128<uint16_t, 4> v, Simd<uint16_t, 4> /* tag */,
-                       uint16_t* HWY_RESTRICT p) {
+HWY_API void StoreU(const Vec128<uint16_t, 4> v, Simd<uint16_t, 4> /* tag */,
+                    uint16_t* HWY_RESTRICT p) {
   vst1_u16(p, v.raw);
 }
-HWY_INLINE void StoreU(const Vec128<uint32_t, 2> v, Simd<uint32_t, 2> /* tag */,
-                       uint32_t* HWY_RESTRICT p) {
+HWY_API void StoreU(const Vec128<uint32_t, 2> v, Simd<uint32_t, 2> /* tag */,
+                    uint32_t* HWY_RESTRICT p) {
   vst1_u32(p, v.raw);
 }
-HWY_INLINE void StoreU(const Vec128<uint64_t, 1> v, Simd<uint64_t, 1> /* tag */,
-                       uint64_t* HWY_RESTRICT p) {
+HWY_API void StoreU(const Vec128<uint64_t, 1> v, Simd<uint64_t, 1> /* tag */,
+                    uint64_t* HWY_RESTRICT p) {
   vst1_u64(p, v.raw);
 }
-HWY_INLINE void StoreU(const Vec128<int8_t, 8> v, Simd<int8_t, 8> /* tag */,
-                       int8_t* HWY_RESTRICT p) {
+HWY_API void StoreU(const Vec128<int8_t, 8> v, Simd<int8_t, 8> /* tag */,
+                    int8_t* HWY_RESTRICT p) {
   vst1_s8(p, v.raw);
 }
-HWY_INLINE void StoreU(const Vec128<int16_t, 4> v, Simd<int16_t, 4> /* tag */,
-                       int16_t* HWY_RESTRICT p) {
+HWY_API void StoreU(const Vec128<int16_t, 4> v, Simd<int16_t, 4> /* tag */,
+                    int16_t* HWY_RESTRICT p) {
   vst1_s16(p, v.raw);
 }
-HWY_INLINE void StoreU(const Vec128<int32_t, 2> v, Simd<int32_t, 2> /* tag */,
-                       int32_t* HWY_RESTRICT p) {
+HWY_API void StoreU(const Vec128<int32_t, 2> v, Simd<int32_t, 2> /* tag */,
+                    int32_t* HWY_RESTRICT p) {
   vst1_s32(p, v.raw);
 }
-HWY_INLINE void StoreU(const Vec128<int64_t, 1> v, Simd<int64_t, 1> /* tag */,
-                       int64_t* HWY_RESTRICT p) {
+HWY_API void StoreU(const Vec128<int64_t, 1> v, Simd<int64_t, 1> /* tag */,
+                    int64_t* HWY_RESTRICT p) {
   vst1_s64(p, v.raw);
 }
-HWY_INLINE void StoreU(const Vec128<float, 2> v, Simd<float, 2> /* tag */,
-                       float* HWY_RESTRICT p) {
+HWY_API void StoreU(const Vec128<float, 2> v, Simd<float, 2> /* tag */,
+                    float* HWY_RESTRICT p) {
   vst1_f32(p, v.raw);
 }
 #if HWY_ARCH_ARM_A64
-HWY_INLINE void StoreU(const Vec128<double, 1> v, Simd<double, 1> /* tag */,
-                       double* HWY_RESTRICT p) {
+HWY_API void StoreU(const Vec128<double, 1> v, Simd<double, 1> /* tag */,
+                    double* HWY_RESTRICT p) {
   vst1_f64(p, v.raw);
 }
 #endif
 
 // ------------------------------ Store 32
 
-HWY_INLINE void StoreU(const Vec128<uint8_t, 4> v, Simd<uint8_t, 4>,
-                       uint8_t* HWY_RESTRICT p) {
+HWY_API void StoreU(const Vec128<uint8_t, 4> v, Simd<uint8_t, 4>,
+                    uint8_t* HWY_RESTRICT p) {
   uint32x2_t a = vreinterpret_u32_u8(v.raw);
   vst1_lane_u32(reinterpret_cast<uint32_t*>(p), a, 0);
 }
-HWY_INLINE void StoreU(const Vec128<uint16_t, 2> v, Simd<uint16_t, 2>,
-                       uint16_t* HWY_RESTRICT p) {
+HWY_API void StoreU(const Vec128<uint16_t, 2> v, Simd<uint16_t, 2>,
+                    uint16_t* HWY_RESTRICT p) {
   uint32x2_t a = vreinterpret_u32_u16(v.raw);
   vst1_lane_u32(reinterpret_cast<uint32_t*>(p), a, 0);
 }
-HWY_INLINE void StoreU(const Vec128<uint32_t, 1> v, Simd<uint32_t, 1>,
-                       uint32_t* HWY_RESTRICT p) {
+HWY_API void StoreU(const Vec128<uint32_t, 1> v, Simd<uint32_t, 1>,
+                    uint32_t* HWY_RESTRICT p) {
   vst1_lane_u32(p, v.raw, 0);
 }
-HWY_INLINE void StoreU(const Vec128<int8_t, 4> v, Simd<int8_t, 4>,
-                       int8_t* HWY_RESTRICT p) {
+HWY_API void StoreU(const Vec128<int8_t, 4> v, Simd<int8_t, 4>,
+                    int8_t* HWY_RESTRICT p) {
   int32x2_t a = vreinterpret_s32_s8(v.raw);
   vst1_lane_s32(reinterpret_cast<int32_t*>(p), a, 0);
 }
-HWY_INLINE void StoreU(const Vec128<int16_t, 2> v, Simd<int16_t, 2>,
-                       int16_t* HWY_RESTRICT p) {
+HWY_API void StoreU(const Vec128<int16_t, 2> v, Simd<int16_t, 2>,
+                    int16_t* HWY_RESTRICT p) {
   int32x2_t a = vreinterpret_s32_s16(v.raw);
   vst1_lane_s32(reinterpret_cast<int32_t*>(p), a, 0);
 }
-HWY_INLINE void StoreU(const Vec128<int32_t, 1> v, Simd<int32_t, 1>,
-                       int32_t* HWY_RESTRICT p) {
+HWY_API void StoreU(const Vec128<int32_t, 1> v, Simd<int32_t, 1>,
+                    int32_t* HWY_RESTRICT p) {
   vst1_lane_s32(p, v.raw, 0);
 }
-HWY_INLINE void StoreU(const Vec128<float, 1> v, Simd<float, 1>,
-                       float* HWY_RESTRICT p) {
+HWY_API void StoreU(const Vec128<float, 1> v, Simd<float, 1>,
+                    float* HWY_RESTRICT p) {
   vst1_lane_f32(p, v.raw, 0);
 }
 
 // ------------------------------ Store 16
 
-HWY_INLINE void StoreU(const Vec128<uint8_t, 2> v, Simd<uint8_t, 2>,
-                       uint8_t* HWY_RESTRICT p) {
+HWY_API void StoreU(const Vec128<uint8_t, 2> v, Simd<uint8_t, 2>,
+                    uint8_t* HWY_RESTRICT p) {
   uint16x4_t a = vreinterpret_u16_u8(v.raw);
   vst1_lane_u16(reinterpret_cast<uint16_t*>(p), a, 0);
 }
-HWY_INLINE void StoreU(const Vec128<uint16_t, 1> v, Simd<uint16_t, 1>,
-                       uint16_t* HWY_RESTRICT p) {
+HWY_API void StoreU(const Vec128<uint16_t, 1> v, Simd<uint16_t, 1>,
+                    uint16_t* HWY_RESTRICT p) {
   vst1_lane_u16(p, v.raw, 0);
 }
-HWY_INLINE void StoreU(const Vec128<int8_t, 2> v, Simd<int8_t, 2>,
-                       int8_t* HWY_RESTRICT p) {
+HWY_API void StoreU(const Vec128<int8_t, 2> v, Simd<int8_t, 2>,
+                    int8_t* HWY_RESTRICT p) {
   int16x4_t a = vreinterpret_s16_s8(v.raw);
   vst1_lane_s16(reinterpret_cast<int16_t*>(p), a, 0);
 }
-HWY_INLINE void StoreU(const Vec128<int16_t, 1> v, Simd<int16_t, 1>,
-                       int16_t* HWY_RESTRICT p) {
+HWY_API void StoreU(const Vec128<int16_t, 1> v, Simd<int16_t, 1>,
+                    int16_t* HWY_RESTRICT p) {
   vst1_lane_s16(p, v.raw, 0);
 }
 
 // ------------------------------ Store 8
 
-HWY_INLINE void StoreU(const Vec128<uint8_t, 1> v, Simd<uint8_t, 1>,
-                       uint8_t* HWY_RESTRICT p) {
+HWY_API void StoreU(const Vec128<uint8_t, 1> v, Simd<uint8_t, 1>,
+                    uint8_t* HWY_RESTRICT p) {
   vst1_lane_u8(p, v.raw, 0);
 }
-HWY_INLINE void StoreU(const Vec128<int8_t, 1> v, Simd<int8_t, 1>,
-                       int8_t* HWY_RESTRICT p) {
+HWY_API void StoreU(const Vec128<int8_t, 1> v, Simd<int8_t, 1>,
+                    int8_t* HWY_RESTRICT p) {
   vst1_lane_s8(p, v.raw, 0);
 }
 
@@ -2224,7 +2331,7 @@ HWY_API void StoreU(Vec128<float16_t, N> v, Simd<float16_t, N> /* tag */,
 
 // On ARM, Store is the same as StoreU.
 template <typename T, size_t N>
-HWY_INLINE void Store(Vec128<T, N> v, Simd<T, N> d, T* HWY_RESTRICT p) {
+HWY_API void Store(Vec128<T, N> v, Simd<T, N> d, T* HWY_RESTRICT p) {
   StoreU(v, d, p);
 }
 
@@ -2233,8 +2340,8 @@ HWY_INLINE void Store(Vec128<T, N> v, Simd<T, N> d, T* HWY_RESTRICT p) {
 // Same as aligned stores on non-x86.
 
 template <typename T, size_t N>
-HWY_INLINE void Stream(const Vec128<T, N> v, Simd<T, N> d,
-                       T* HWY_RESTRICT aligned) {
+HWY_API void Stream(const Vec128<T, N> v, Simd<T, N> d,
+                    T* HWY_RESTRICT aligned) {
   Store(v, d, aligned);
 }
 
@@ -2243,131 +2350,131 @@ HWY_INLINE void Stream(const Vec128<T, N> v, Simd<T, N> d,
 // ------------------------------ Promotions (part w/ narrow lanes -> full)
 
 // Unsigned: zero-extend to full vector.
-HWY_INLINE Vec128<uint16_t> PromoteTo(Full128<uint16_t> /* tag */,
-                                      const Vec128<uint8_t, 8> v) {
+HWY_API Vec128<uint16_t> PromoteTo(Full128<uint16_t> /* tag */,
+                                   const Vec128<uint8_t, 8> v) {
   return Vec128<uint16_t>(vmovl_u8(v.raw));
 }
-HWY_INLINE Vec128<uint32_t> PromoteTo(Full128<uint32_t> /* tag */,
-                                      const Vec128<uint8_t, 4> v) {
+HWY_API Vec128<uint32_t> PromoteTo(Full128<uint32_t> /* tag */,
+                                   const Vec128<uint8_t, 4> v) {
   uint16x8_t a = vmovl_u8(v.raw);
   return Vec128<uint32_t>(vmovl_u16(vget_low_u16(a)));
 }
-HWY_INLINE Vec128<uint32_t> PromoteTo(Full128<uint32_t> /* tag */,
-                                      const Vec128<uint16_t, 4> v) {
+HWY_API Vec128<uint32_t> PromoteTo(Full128<uint32_t> /* tag */,
+                                   const Vec128<uint16_t, 4> v) {
   return Vec128<uint32_t>(vmovl_u16(v.raw));
 }
-HWY_INLINE Vec128<uint64_t> PromoteTo(Full128<uint64_t> /* tag */,
-                                      const Vec128<uint32_t, 2> v) {
+HWY_API Vec128<uint64_t> PromoteTo(Full128<uint64_t> /* tag */,
+                                   const Vec128<uint32_t, 2> v) {
   return Vec128<uint64_t>(vmovl_u32(v.raw));
 }
-HWY_INLINE Vec128<int16_t> PromoteTo(Full128<int16_t> d,
-                                     const Vec128<uint8_t, 8> v) {
+HWY_API Vec128<int16_t> PromoteTo(Full128<int16_t> d,
+                                  const Vec128<uint8_t, 8> v) {
   return BitCast(d, Vec128<uint16_t>(vmovl_u8(v.raw)));
 }
-HWY_INLINE Vec128<int32_t> PromoteTo(Full128<int32_t> d,
-                                     const Vec128<uint8_t, 4> v) {
+HWY_API Vec128<int32_t> PromoteTo(Full128<int32_t> d,
+                                  const Vec128<uint8_t, 4> v) {
   uint16x8_t a = vmovl_u8(v.raw);
   return BitCast(d, Vec128<uint32_t>(vmovl_u16(vget_low_u16(a))));
 }
-HWY_INLINE Vec128<int32_t> PromoteTo(Full128<int32_t> d,
-                                     const Vec128<uint16_t, 4> v) {
+HWY_API Vec128<int32_t> PromoteTo(Full128<int32_t> d,
+                                  const Vec128<uint16_t, 4> v) {
   return BitCast(d, Vec128<uint32_t>(vmovl_u16(v.raw)));
 }
 
 // Unsigned: zero-extend to half vector.
 template <size_t N, HWY_IF_LE64(uint16_t, N)>
-HWY_INLINE Vec128<uint16_t, N> PromoteTo(Simd<uint16_t, N> /* tag */,
-                                         const Vec128<uint8_t, N> v) {
+HWY_API Vec128<uint16_t, N> PromoteTo(Simd<uint16_t, N> /* tag */,
+                                      const Vec128<uint8_t, N> v) {
   return Vec128<uint16_t, N>(vget_low_u16(vmovl_u8(v.raw)));
 }
 template <size_t N, HWY_IF_LE64(uint32_t, N)>
-HWY_INLINE Vec128<uint32_t, N> PromoteTo(Simd<uint32_t, N> /* tag */,
-                                         const Vec128<uint8_t, N> v) {
+HWY_API Vec128<uint32_t, N> PromoteTo(Simd<uint32_t, N> /* tag */,
+                                      const Vec128<uint8_t, N> v) {
   uint16x8_t a = vmovl_u8(v.raw);
   return Vec128<uint32_t, N>(vget_low_u32(vmovl_u16(vget_low_u16(a))));
 }
 template <size_t N>
-HWY_INLINE Vec128<uint32_t, N> PromoteTo(Simd<uint32_t, N> /* tag */,
-                                         const Vec128<uint16_t, N> v) {
+HWY_API Vec128<uint32_t, N> PromoteTo(Simd<uint32_t, N> /* tag */,
+                                      const Vec128<uint16_t, N> v) {
   return Vec128<uint32_t, N>(vget_low_u32(vmovl_u16(v.raw)));
 }
 template <size_t N, HWY_IF_LE64(uint64_t, N)>
-HWY_INLINE Vec128<uint64_t, N> PromoteTo(Simd<uint64_t, N> /* tag */,
-                                         const Vec128<uint32_t, N> v) {
+HWY_API Vec128<uint64_t, N> PromoteTo(Simd<uint64_t, N> /* tag */,
+                                      const Vec128<uint32_t, N> v) {
   return Vec128<uint64_t, N>(vget_low_u64(vmovl_u32(v.raw)));
 }
 template <size_t N, HWY_IF_LE64(int16_t, N)>
-HWY_INLINE Vec128<int16_t, N> PromoteTo(Simd<int16_t, N> d,
-                                        const Vec128<uint8_t, N> v) {
+HWY_API Vec128<int16_t, N> PromoteTo(Simd<int16_t, N> d,
+                                     const Vec128<uint8_t, N> v) {
   return BitCast(d, Vec128<uint16_t, N>(vget_low_u16(vmovl_u8(v.raw))));
 }
 template <size_t N, HWY_IF_LE64(int32_t, N)>
-HWY_INLINE Vec128<int32_t, N> PromoteTo(Simd<int32_t, N> /* tag */,
-                                        const Vec128<uint8_t, N> v) {
+HWY_API Vec128<int32_t, N> PromoteTo(Simd<int32_t, N> /* tag */,
+                                     const Vec128<uint8_t, N> v) {
   uint16x8_t a = vmovl_u8(v.raw);
   uint32x4_t b = vmovl_u16(vget_low_u16(a));
   return Vec128<int32_t, N>(vget_low_s32(vreinterpretq_s32_u32(b)));
 }
 template <size_t N, HWY_IF_LE64(int32_t, N)>
-HWY_INLINE Vec128<int32_t, N> PromoteTo(Simd<int32_t, N> /* tag */,
-                                        const Vec128<uint16_t, N> v) {
+HWY_API Vec128<int32_t, N> PromoteTo(Simd<int32_t, N> /* tag */,
+                                     const Vec128<uint16_t, N> v) {
   uint32x4_t a = vmovl_u16(v.raw);
   return Vec128<int32_t, N>(vget_low_s32(vreinterpretq_s32_u32(a)));
 }
 
 // Signed: replicate sign bit to full vector.
-HWY_INLINE Vec128<int16_t> PromoteTo(Full128<int16_t> /* tag */,
-                                     const Vec128<int8_t, 8> v) {
+HWY_API Vec128<int16_t> PromoteTo(Full128<int16_t> /* tag */,
+                                  const Vec128<int8_t, 8> v) {
   return Vec128<int16_t>(vmovl_s8(v.raw));
 }
-HWY_INLINE Vec128<int32_t> PromoteTo(Full128<int32_t> /* tag */,
-                                     const Vec128<int8_t, 4> v) {
+HWY_API Vec128<int32_t> PromoteTo(Full128<int32_t> /* tag */,
+                                  const Vec128<int8_t, 4> v) {
   int16x8_t a = vmovl_s8(v.raw);
   return Vec128<int32_t>(vmovl_s16(vget_low_s16(a)));
 }
-HWY_INLINE Vec128<int32_t> PromoteTo(Full128<int32_t> /* tag */,
-                                     const Vec128<int16_t, 4> v) {
+HWY_API Vec128<int32_t> PromoteTo(Full128<int32_t> /* tag */,
+                                  const Vec128<int16_t, 4> v) {
   return Vec128<int32_t>(vmovl_s16(v.raw));
 }
-HWY_INLINE Vec128<int64_t> PromoteTo(Full128<int64_t> /* tag */,
-                                     const Vec128<int32_t, 2> v) {
+HWY_API Vec128<int64_t> PromoteTo(Full128<int64_t> /* tag */,
+                                  const Vec128<int32_t, 2> v) {
   return Vec128<int64_t>(vmovl_s32(v.raw));
 }
 
 // Signed: replicate sign bit to half vector.
 template <size_t N>
-HWY_INLINE Vec128<int16_t, N> PromoteTo(Simd<int16_t, N> /* tag */,
-                                        const Vec128<int8_t, N> v) {
+HWY_API Vec128<int16_t, N> PromoteTo(Simd<int16_t, N> /* tag */,
+                                     const Vec128<int8_t, N> v) {
   return Vec128<int16_t, N>(vget_low_s16(vmovl_s8(v.raw)));
 }
 template <size_t N>
-HWY_INLINE Vec128<int32_t, N> PromoteTo(Simd<int32_t, N> /* tag */,
-                                        const Vec128<int8_t, N> v) {
+HWY_API Vec128<int32_t, N> PromoteTo(Simd<int32_t, N> /* tag */,
+                                     const Vec128<int8_t, N> v) {
   int16x8_t a = vmovl_s8(v.raw);
   int32x4_t b = vmovl_s16(vget_low_s16(a));
   return Vec128<int32_t, N>(vget_low_s32(b));
 }
 template <size_t N>
-HWY_INLINE Vec128<int32_t, N> PromoteTo(Simd<int32_t, N> /* tag */,
-                                        const Vec128<int16_t, N> v) {
+HWY_API Vec128<int32_t, N> PromoteTo(Simd<int32_t, N> /* tag */,
+                                     const Vec128<int16_t, N> v) {
   return Vec128<int32_t, N>(vget_low_s32(vmovl_s16(v.raw)));
 }
 template <size_t N>
-HWY_INLINE Vec128<int64_t, N> PromoteTo(Simd<int64_t, N> /* tag */,
-                                        const Vec128<int32_t, N> v) {
+HWY_API Vec128<int64_t, N> PromoteTo(Simd<int64_t, N> /* tag */,
+                                     const Vec128<int32_t, N> v) {
   return Vec128<int64_t, N>(vget_low_s64(vmovl_s32(v.raw)));
 }
 
 #if __ARM_FP & 2
 
-HWY_INLINE Vec128<float> PromoteTo(Full128<float> /* tag */,
-                                   const Vec128<float16_t, 4> v) {
+HWY_API Vec128<float> PromoteTo(Full128<float> /* tag */,
+                                const Vec128<float16_t, 4> v) {
   const float32x4_t f32 = vcvt_f32_f16(vreinterpret_f16_u16(v.raw));
   return Vec128<float>(f32);
 }
 template <size_t N>
-HWY_INLINE Vec128<float, N> PromoteTo(Simd<float, N> /* tag */,
-                                      const Vec128<float16_t, N> v) {
+HWY_API Vec128<float, N> PromoteTo(Simd<float, N> /* tag */,
+                                   const Vec128<float16_t, N> v) {
   const float32x4_t f32 = vcvt_f32_f16(vreinterpret_f16_u16(v.raw));
   return Vec128<float, N>(vget_low_f32(f32));
 }
@@ -2375,8 +2482,8 @@ HWY_INLINE Vec128<float, N> PromoteTo(Simd<float, N> /* tag */,
 #else
 
 template <size_t N>
-HWY_INLINE Vec128<float, N> PromoteTo(Simd<float, N> /* tag */,
-                                      const Vec128<float16_t, N> v) {
+HWY_API Vec128<float, N> PromoteTo(Simd<float, N> /* tag */,
+                                   const Vec128<float16_t, N> v) {
   const Simd<int32_t, N> di32;
   const Simd<uint32_t, N> du32;
   const Simd<float, N> df32;
@@ -2400,24 +2507,24 @@ HWY_INLINE Vec128<float, N> PromoteTo(Simd<float, N> /* tag */,
 
 #if HWY_ARCH_ARM_A64
 
-HWY_INLINE Vec128<double> PromoteTo(Full128<double> /* tag */,
-                                    const Vec128<float, 2> v) {
+HWY_API Vec128<double> PromoteTo(Full128<double> /* tag */,
+                                 const Vec128<float, 2> v) {
   return Vec128<double>(vcvt_f64_f32(v.raw));
 }
 
-HWY_INLINE Vec128<double, 1> PromoteTo(Simd<double, 1> /* tag */,
-                                       const Vec128<float, 1> v) {
+HWY_API Vec128<double, 1> PromoteTo(Simd<double, 1> /* tag */,
+                                    const Vec128<float, 1> v) {
   return Vec128<double, 1>(vget_low_f64(vcvt_f64_f32(v.raw)));
 }
 
-HWY_INLINE Vec128<double> PromoteTo(Full128<double> /* tag */,
-                                    const Vec128<int32_t, 2> v) {
+HWY_API Vec128<double> PromoteTo(Full128<double> /* tag */,
+                                 const Vec128<int32_t, 2> v) {
   const int64x2_t i64 = vmovl_s32(v.raw);
   return Vec128<double>(vcvtq_f64_s64(i64));
 }
 
-HWY_INLINE Vec128<double, 1> PromoteTo(Simd<double, 1> /* tag */,
-                                       const Vec128<int32_t, 1> v) {
+HWY_API Vec128<double, 1> PromoteTo(Simd<double, 1> /* tag */,
+                                    const Vec128<int32_t, 1> v) {
   const int64x1_t i64 = vget_low_s64(vmovl_s32(v.raw));
   return Vec128<double, 1>(vcvt_f64_s64(i64));
 }
@@ -2427,76 +2534,76 @@ HWY_INLINE Vec128<double, 1> PromoteTo(Simd<double, 1> /* tag */,
 // ------------------------------ Demotions (full -> part w/ narrow lanes)
 
 // From full vector to half or quarter
-HWY_INLINE Vec128<uint16_t, 4> DemoteTo(Simd<uint16_t, 4> /* tag */,
-                                        const Vec128<int32_t> v) {
+HWY_API Vec128<uint16_t, 4> DemoteTo(Simd<uint16_t, 4> /* tag */,
+                                     const Vec128<int32_t> v) {
   return Vec128<uint16_t, 4>(vqmovun_s32(v.raw));
 }
-HWY_INLINE Vec128<int16_t, 4> DemoteTo(Simd<int16_t, 4> /* tag */,
-                                       const Vec128<int32_t> v) {
+HWY_API Vec128<int16_t, 4> DemoteTo(Simd<int16_t, 4> /* tag */,
+                                    const Vec128<int32_t> v) {
   return Vec128<int16_t, 4>(vqmovn_s32(v.raw));
 }
-HWY_INLINE Vec128<uint8_t, 4> DemoteTo(Simd<uint8_t, 4> /* tag */,
-                                       const Vec128<int32_t> v) {
+HWY_API Vec128<uint8_t, 4> DemoteTo(Simd<uint8_t, 4> /* tag */,
+                                    const Vec128<int32_t> v) {
   const uint16x4_t a = vqmovun_s32(v.raw);
   return Vec128<uint8_t, 4>(vqmovn_u16(vcombine_u16(a, a)));
 }
-HWY_INLINE Vec128<uint8_t, 8> DemoteTo(Simd<uint8_t, 8> /* tag */,
-                                       const Vec128<int16_t> v) {
+HWY_API Vec128<uint8_t, 8> DemoteTo(Simd<uint8_t, 8> /* tag */,
+                                    const Vec128<int16_t> v) {
   return Vec128<uint8_t, 8>(vqmovun_s16(v.raw));
 }
-HWY_INLINE Vec128<int8_t, 4> DemoteTo(Simd<int8_t, 4> /* tag */,
-                                      const Vec128<int32_t> v) {
+HWY_API Vec128<int8_t, 4> DemoteTo(Simd<int8_t, 4> /* tag */,
+                                   const Vec128<int32_t> v) {
   const int16x4_t a = vqmovn_s32(v.raw);
   return Vec128<int8_t, 4>(vqmovn_s16(vcombine_s16(a, a)));
 }
-HWY_INLINE Vec128<int8_t, 8> DemoteTo(Simd<int8_t, 8> /* tag */,
-                                      const Vec128<int16_t> v) {
+HWY_API Vec128<int8_t, 8> DemoteTo(Simd<int8_t, 8> /* tag */,
+                                   const Vec128<int16_t> v) {
   return Vec128<int8_t, 8>(vqmovn_s16(v.raw));
 }
 
 // From half vector to partial half
 template <size_t N, HWY_IF_LE64(int32_t, N)>
-HWY_INLINE Vec128<uint16_t, N> DemoteTo(Simd<uint16_t, N> /* tag */,
-                                        const Vec128<int32_t, N> v) {
+HWY_API Vec128<uint16_t, N> DemoteTo(Simd<uint16_t, N> /* tag */,
+                                     const Vec128<int32_t, N> v) {
   return Vec128<uint16_t, N>(vqmovun_s32(vcombine_s32(v.raw, v.raw)));
 }
 template <size_t N, HWY_IF_LE64(int32_t, N)>
-HWY_INLINE Vec128<int16_t, N> DemoteTo(Simd<int16_t, N> /* tag */,
-                                       const Vec128<int32_t, N> v) {
+HWY_API Vec128<int16_t, N> DemoteTo(Simd<int16_t, N> /* tag */,
+                                    const Vec128<int32_t, N> v) {
   return Vec128<int16_t, N>(vqmovn_s32(vcombine_s32(v.raw, v.raw)));
 }
 template <size_t N, HWY_IF_LE64(int32_t, N)>
-HWY_INLINE Vec128<uint8_t, N> DemoteTo(Simd<uint8_t, N> /* tag */,
-                                       const Vec128<int32_t, N> v) {
+HWY_API Vec128<uint8_t, N> DemoteTo(Simd<uint8_t, N> /* tag */,
+                                    const Vec128<int32_t, N> v) {
   const uint16x4_t a = vqmovun_s32(vcombine_s32(v.raw, v.raw));
   return Vec128<uint8_t, N>(vqmovn_u16(vcombine_u16(a, a)));
 }
 template <size_t N, HWY_IF_LE64(int16_t, N)>
-HWY_INLINE Vec128<uint8_t, N> DemoteTo(Simd<uint8_t, N> /* tag */,
-                                       const Vec128<int16_t, N> v) {
+HWY_API Vec128<uint8_t, N> DemoteTo(Simd<uint8_t, N> /* tag */,
+                                    const Vec128<int16_t, N> v) {
   return Vec128<uint8_t, N>(vqmovun_s16(vcombine_s16(v.raw, v.raw)));
 }
 template <size_t N, HWY_IF_LE64(int32_t, N)>
-HWY_INLINE Vec128<int8_t, N> DemoteTo(Simd<int8_t, N> /* tag */,
-                                      const Vec128<int32_t, N> v) {
+HWY_API Vec128<int8_t, N> DemoteTo(Simd<int8_t, N> /* tag */,
+                                   const Vec128<int32_t, N> v) {
   const int16x4_t a = vqmovn_s32(vcombine_s32(v.raw, v.raw));
   return Vec128<int8_t, N>(vqmovn_s16(vcombine_s16(a, a)));
 }
 template <size_t N, HWY_IF_LE64(int16_t, N)>
-HWY_INLINE Vec128<int8_t, N> DemoteTo(Simd<int8_t, N> /* tag */,
-                                      const Vec128<int16_t, N> v) {
+HWY_API Vec128<int8_t, N> DemoteTo(Simd<int8_t, N> /* tag */,
+                                   const Vec128<int16_t, N> v) {
   return Vec128<int8_t, N>(vqmovn_s16(vcombine_s16(v.raw, v.raw)));
 }
 
 #if __ARM_FP & 2
 
-HWY_INLINE Vec128<float16_t, 4> DemoteTo(Simd<float16_t, 4> /* tag */,
-                                         const Vec128<float> v) {
+HWY_API Vec128<float16_t, 4> DemoteTo(Simd<float16_t, 4> /* tag */,
+                                      const Vec128<float> v) {
   return Vec128<float16_t, 4>{vreinterpret_u16_f16(vcvt_f16_f32(v.raw))};
 }
 template <size_t N>
-HWY_INLINE Vec128<float16_t, N> DemoteTo(Simd<float16_t, N> /* tag */,
-                                         const Vec128<float, N> v) {
+HWY_API Vec128<float16_t, N> DemoteTo(Simd<float16_t, N> /* tag */,
+                                      const Vec128<float, N> v) {
   const float16x4_t f16 = vcvt_f16_f32(vcombine_f32(v.raw, v.raw));
   return Vec128<float16_t, N>(vreinterpret_u16_f16(f16));
 }
@@ -2504,8 +2611,8 @@ HWY_INLINE Vec128<float16_t, N> DemoteTo(Simd<float16_t, N> /* tag */,
 #else
 
 template <size_t N>
-HWY_INLINE Vec128<float16_t, N> DemoteTo(Simd<float16_t, N> /* tag */,
-                                         const Vec128<float, N> v) {
+HWY_API Vec128<float16_t, N> DemoteTo(Simd<float16_t, N> /* tag */,
+                                      const Vec128<float, N> v) {
   const Simd<int32_t, N> di;
   const Simd<uint32_t, N> du;
   const Simd<uint16_t, N> du16;
@@ -2536,22 +2643,22 @@ HWY_INLINE Vec128<float16_t, N> DemoteTo(Simd<float16_t, N> /* tag */,
 #endif
 #if HWY_ARCH_ARM_A64
 
-HWY_INLINE Vec128<float, 2> DemoteTo(Simd<float, 2> /* tag */,
-                                     const Vec128<double> v) {
+HWY_API Vec128<float, 2> DemoteTo(Simd<float, 2> /* tag */,
+                                  const Vec128<double> v) {
   return Vec128<float, 2>(vcvt_f32_f64(v.raw));
 }
-HWY_INLINE Vec128<float, 1> DemoteTo(Simd<float, 1> /* tag */,
-                                     const Vec128<double, 1> v) {
+HWY_API Vec128<float, 1> DemoteTo(Simd<float, 1> /* tag */,
+                                  const Vec128<double, 1> v) {
   return Vec128<float, 1>(vcvt_f32_f64(vcombine_f64(v.raw, v.raw)));
 }
 
-HWY_INLINE Vec128<int32_t, 2> DemoteTo(Simd<int32_t, 2> /* tag */,
-                                       const Vec128<double> v) {
+HWY_API Vec128<int32_t, 2> DemoteTo(Simd<int32_t, 2> /* tag */,
+                                    const Vec128<double> v) {
   const int64x2_t i64 = vcvtq_s64_f64(v.raw);
   return Vec128<int32_t, 2>(vqmovn_s64(i64));
 }
-HWY_INLINE Vec128<int32_t, 1> DemoteTo(Simd<int32_t, 1> /* tag */,
-                                       const Vec128<double, 1> v) {
+HWY_API Vec128<int32_t, 1> DemoteTo(Simd<int32_t, 1> /* tag */,
+                                    const Vec128<double, 1> v) {
   const int64x1_t i64 = vcvt_s64_f64(v.raw);
   // There is no i64x1 -> i32x1 narrow, so expand to int64x2_t first.
   const int64x2_t i64x2 = vcombine_s64(i64, i64);
@@ -2579,8 +2686,8 @@ HWY_DIAGNOSTICS(push)
 HWY_DIAGNOSTICS_OFF(disable : 4701, ignored "-Wuninitialized")
 
 template <size_t N>
-HWY_INLINE Vec128<uint8_t, N> DemoteTo(Simd<uint8_t, N> /* tag */,
-                                       const Vec128<int32_t> v) {
+HWY_API Vec128<uint8_t, N> DemoteTo(Simd<uint8_t, N> /* tag */,
+                                    const Vec128<int32_t> v) {
   Vec128<uint16_t, N> a = DemoteTo(Simd<uint16_t, N>(), v);
   Vec128<uint16_t, N> b;
   uint16x8_t c = vcombine_u16(a.raw, b.raw);
@@ -2588,8 +2695,8 @@ HWY_INLINE Vec128<uint8_t, N> DemoteTo(Simd<uint8_t, N> /* tag */,
 }
 
 template <size_t N>
-HWY_INLINE Vec128<int8_t, N> DemoteTo(Simd<int8_t, N> /* tag */,
-                                      const Vec128<int32_t> v) {
+HWY_API Vec128<int8_t, N> DemoteTo(Simd<int8_t, N> /* tag */,
+                                   const Vec128<int32_t> v) {
   Vec128<int16_t, N> a = DemoteTo(Simd<int16_t, N>(), v);
   Vec128<int16_t, N> b;
   int16x8_t c = vcombine_s16(a.raw, b.raw);
@@ -2600,45 +2707,45 @@ HWY_DIAGNOSTICS(pop)
 
 // ------------------------------ Convert integer <=> floating-point
 
-HWY_INLINE Vec128<float> ConvertTo(Full128<float> /* tag */,
-                                   const Vec128<int32_t> v) {
+HWY_API Vec128<float> ConvertTo(Full128<float> /* tag */,
+                                const Vec128<int32_t> v) {
   return Vec128<float>(vcvtq_f32_s32(v.raw));
 }
 template <size_t N, HWY_IF_LE64(int32_t, N)>
-HWY_INLINE Vec128<float, N> ConvertTo(Simd<float, N> /* tag */,
-                                      const Vec128<int32_t, N> v) {
+HWY_API Vec128<float, N> ConvertTo(Simd<float, N> /* tag */,
+                                   const Vec128<int32_t, N> v) {
   return Vec128<float, N>(vcvt_f32_s32(v.raw));
 }
 
 // Truncates (rounds toward zero).
-HWY_INLINE Vec128<int32_t> ConvertTo(Full128<int32_t> /* tag */,
-                                     const Vec128<float> v) {
+HWY_API Vec128<int32_t> ConvertTo(Full128<int32_t> /* tag */,
+                                  const Vec128<float> v) {
   return Vec128<int32_t>(vcvtq_s32_f32(v.raw));
 }
 template <size_t N, HWY_IF_LE64(float, N)>
-HWY_INLINE Vec128<int32_t, N> ConvertTo(Simd<int32_t, N> /* tag */,
-                                        const Vec128<float, N> v) {
+HWY_API Vec128<int32_t, N> ConvertTo(Simd<int32_t, N> /* tag */,
+                                     const Vec128<float, N> v) {
   return Vec128<int32_t, N>(vcvt_s32_f32(v.raw));
 }
 
 #if HWY_ARCH_ARM_A64
 
-HWY_INLINE Vec128<double> ConvertTo(Full128<double> /* tag */,
-                                    const Vec128<int64_t> v) {
+HWY_API Vec128<double> ConvertTo(Full128<double> /* tag */,
+                                 const Vec128<int64_t> v) {
   return Vec128<double>(vcvtq_f64_s64(v.raw));
 }
-HWY_INLINE Vec128<double, 1> ConvertTo(Simd<double, 1> /* tag */,
-                                       const Vec128<int64_t, 1> v) {
+HWY_API Vec128<double, 1> ConvertTo(Simd<double, 1> /* tag */,
+                                    const Vec128<int64_t, 1> v) {
   return Vec128<double, 1>(vcvt_f64_s64(v.raw));
 }
 
 // Truncates (rounds toward zero).
-HWY_INLINE Vec128<int64_t> ConvertTo(Full128<int64_t> /* tag */,
-                                     const Vec128<double> v) {
+HWY_API Vec128<int64_t> ConvertTo(Full128<int64_t> /* tag */,
+                                  const Vec128<double> v) {
   return Vec128<int64_t>(vcvtq_s64_f64(v.raw));
 }
-HWY_INLINE Vec128<int64_t, 1> ConvertTo(Simd<int64_t, 1> /* tag */,
-                                        const Vec128<double, 1> v) {
+HWY_API Vec128<int64_t, 1> ConvertTo(Simd<int64_t, 1> /* tag */,
+                                     const Vec128<double, 1> v) {
   return Vec128<int64_t, 1>(vcvt_s64_f64(v.raw));
 }
 
@@ -2672,14 +2779,14 @@ namespace detail {
 // The original value is already the desired result if NaN or the magnitude is
 // large (i.e. the value is already an integer).
 template <size_t N>
-HWY_API Mask128<float, N> UseInt(const Vec128<float, N> v) {
+HWY_INLINE Mask128<float, N> UseInt(const Vec128<float, N> v) {
   return Abs(v) < Set(Simd<float, N>(), MantissaEnd<float>());
 }
 
 }  // namespace detail
 
 template <size_t N>
-HWY_INLINE Vec128<float, N> Trunc(const Vec128<float, N> v) {
+HWY_API Vec128<float, N> Trunc(const Vec128<float, N> v) {
   const Simd<float, N> df;
   const RebindToSigned<decltype(df)> di;
 
@@ -2690,7 +2797,7 @@ HWY_INLINE Vec128<float, N> Trunc(const Vec128<float, N> v) {
 }
 
 template <size_t N>
-HWY_INLINE Vec128<float, N> Round(const Vec128<float, N> v) {
+HWY_API Vec128<float, N> Round(const Vec128<float, N> v) {
   const Simd<float, N> df;
 
   // ARMv7 also lacks a native NearestInt, but we can instead rely on rounding
@@ -2707,7 +2814,7 @@ HWY_INLINE Vec128<float, N> Round(const Vec128<float, N> v) {
 }
 
 template <size_t N>
-HWY_INLINE Vec128<float, N> Ceil(const Vec128<float, N> v) {
+HWY_API Vec128<float, N> Ceil(const Vec128<float, N> v) {
   const Simd<float, N> df;
   const RebindToSigned<decltype(df)> di;
 
@@ -2721,7 +2828,7 @@ HWY_INLINE Vec128<float, N> Ceil(const Vec128<float, N> v) {
 }
 
 template <size_t N>
-HWY_INLINE Vec128<float, N> Floor(const Vec128<float, N> v) {
+HWY_API Vec128<float, N> Floor(const Vec128<float, N> v) {
   const Simd<float, N> df;
   const Simd<int32_t, N> di;
 
@@ -2740,18 +2847,18 @@ HWY_INLINE Vec128<float, N> Floor(const Vec128<float, N> v) {
 
 #if HWY_ARCH_ARM_A64
 
-HWY_INLINE Vec128<int32_t> NearestInt(const Vec128<float> v) {
+HWY_API Vec128<int32_t> NearestInt(const Vec128<float> v) {
   return Vec128<int32_t>(vcvtnq_s32_f32(v.raw));
 }
 template <size_t N, HWY_IF_LE64(float, N)>
-HWY_INLINE Vec128<int32_t, N> NearestInt(const Vec128<float, N> v) {
+HWY_API Vec128<int32_t, N> NearestInt(const Vec128<float, N> v) {
   return Vec128<int32_t, N>(vcvtn_s32_f32(v.raw));
 }
 
 #else
 
 template <size_t N>
-HWY_INLINE Vec128<int32_t, N> NearestInt(const Vec128<float, N> v) {
+HWY_API Vec128<int32_t, N> NearestInt(const Vec128<float, N> v) {
   const Simd<int32_t, N> di;
   return ConvertTo(di, Round(v));
 }
@@ -2760,104 +2867,98 @@ HWY_INLINE Vec128<int32_t, N> NearestInt(const Vec128<float, N> v) {
 
 // ================================================== SWIZZLE
 
-// ------------------------------ Extract half
+// ------------------------------ LowerHalf
 
 // <= 64 bit: just return different type
 template <typename T, size_t N, HWY_IF_LE64(uint8_t, N)>
-HWY_INLINE Vec128<T, N / 2> LowerHalf(const Vec128<T, N> v) {
+HWY_API Vec128<T, N / 2> LowerHalf(const Vec128<T, N> v) {
   return Vec128<T, N / 2>(v.raw);
 }
 
-HWY_INLINE Vec128<uint8_t, 8> LowerHalf(const Vec128<uint8_t> v) {
+HWY_API Vec128<uint8_t, 8> LowerHalf(const Vec128<uint8_t> v) {
   return Vec128<uint8_t, 8>(vget_low_u8(v.raw));
 }
-HWY_INLINE Vec128<uint16_t, 4> LowerHalf(const Vec128<uint16_t> v) {
+HWY_API Vec128<uint16_t, 4> LowerHalf(const Vec128<uint16_t> v) {
   return Vec128<uint16_t, 4>(vget_low_u16(v.raw));
 }
-HWY_INLINE Vec128<uint32_t, 2> LowerHalf(const Vec128<uint32_t> v) {
+HWY_API Vec128<uint32_t, 2> LowerHalf(const Vec128<uint32_t> v) {
   return Vec128<uint32_t, 2>(vget_low_u32(v.raw));
 }
-HWY_INLINE Vec128<uint64_t, 1> LowerHalf(const Vec128<uint64_t> v) {
+HWY_API Vec128<uint64_t, 1> LowerHalf(const Vec128<uint64_t> v) {
   return Vec128<uint64_t, 1>(vget_low_u64(v.raw));
 }
-HWY_INLINE Vec128<int8_t, 8> LowerHalf(const Vec128<int8_t> v) {
+HWY_API Vec128<int8_t, 8> LowerHalf(const Vec128<int8_t> v) {
   return Vec128<int8_t, 8>(vget_low_s8(v.raw));
 }
-HWY_INLINE Vec128<int16_t, 4> LowerHalf(const Vec128<int16_t> v) {
+HWY_API Vec128<int16_t, 4> LowerHalf(const Vec128<int16_t> v) {
   return Vec128<int16_t, 4>(vget_low_s16(v.raw));
 }
-HWY_INLINE Vec128<int32_t, 2> LowerHalf(const Vec128<int32_t> v) {
+HWY_API Vec128<int32_t, 2> LowerHalf(const Vec128<int32_t> v) {
   return Vec128<int32_t, 2>(vget_low_s32(v.raw));
 }
-HWY_INLINE Vec128<int64_t, 1> LowerHalf(const Vec128<int64_t> v) {
+HWY_API Vec128<int64_t, 1> LowerHalf(const Vec128<int64_t> v) {
   return Vec128<int64_t, 1>(vget_low_s64(v.raw));
 }
-HWY_INLINE Vec128<float, 2> LowerHalf(const Vec128<float> v) {
+HWY_API Vec128<float, 2> LowerHalf(const Vec128<float> v) {
   return Vec128<float, 2>(vget_low_f32(v.raw));
 }
 #if HWY_ARCH_ARM_A64
-HWY_INLINE Vec128<double, 1> LowerHalf(const Vec128<double> v) {
+HWY_API Vec128<double, 1> LowerHalf(const Vec128<double> v) {
   return Vec128<double, 1>(vget_low_f64(v.raw));
 }
 #endif
 
-HWY_INLINE Vec128<uint8_t, 8> UpperHalf(const Vec128<uint8_t> v) {
-  return Vec128<uint8_t, 8>(vget_high_u8(v.raw));
-}
-HWY_INLINE Vec128<uint16_t, 4> UpperHalf(const Vec128<uint16_t> v) {
-  return Vec128<uint16_t, 4>(vget_high_u16(v.raw));
-}
-HWY_INLINE Vec128<uint32_t, 2> UpperHalf(const Vec128<uint32_t> v) {
-  return Vec128<uint32_t, 2>(vget_high_u32(v.raw));
-}
-HWY_INLINE Vec128<uint64_t, 1> UpperHalf(const Vec128<uint64_t> v) {
-  return Vec128<uint64_t, 1>(vget_high_u64(v.raw));
-}
-HWY_INLINE Vec128<int8_t, 8> UpperHalf(const Vec128<int8_t> v) {
-  return Vec128<int8_t, 8>(vget_high_s8(v.raw));
-}
-HWY_INLINE Vec128<int16_t, 4> UpperHalf(const Vec128<int16_t> v) {
-  return Vec128<int16_t, 4>(vget_high_s16(v.raw));
-}
-HWY_INLINE Vec128<int32_t, 2> UpperHalf(const Vec128<int32_t> v) {
-  return Vec128<int32_t, 2>(vget_high_s32(v.raw));
-}
-HWY_INLINE Vec128<int64_t, 1> UpperHalf(const Vec128<int64_t> v) {
-  return Vec128<int64_t, 1>(vget_high_s64(v.raw));
-}
-HWY_INLINE Vec128<float, 2> UpperHalf(const Vec128<float> v) {
-  return Vec128<float, 2>(vget_high_f32(v.raw));
-}
-#if HWY_ARCH_ARM_A64
-HWY_INLINE Vec128<double, 1> UpperHalf(const Vec128<double> v) {
-  return Vec128<double, 1>(vget_high_f64(v.raw));
+template <typename T, size_t N>
+HWY_API Vec128<T, N / 2> LowerHalf(Simd<T, N / 2> /* tag */, Vec128<T, N> v) {
+  return LowerHalf(v);
 }
-#endif
 
-// ------------------------------ Extract from 2x 128-bit at constant offset
+// ------------------------------ CombineShiftRightBytes
 
-// Extracts 128 bits from <hi, lo> by skipping the least-significant kBytes.
-template <int kBytes, typename T>
-HWY_INLINE Vec128<T> CombineShiftRightBytes(const Vec128<T> hi,
-                                            const Vec128<T> lo) {
+// 128-bit
+template <int kBytes, typename T, class V128 = Vec128<T>>
+HWY_API V128 CombineShiftRightBytes(Full128<T> d, V128 hi, V128 lo) {
   static_assert(0 < kBytes && kBytes < 16, "kBytes must be in [1, 15]");
-  const Full128<uint8_t> d8;
-  return BitCast(Full128<T>(),
-                 Vec128<uint8_t>(vextq_u8(BitCast(d8, lo).raw,
-                                          BitCast(d8, hi).raw, kBytes)));
+  const Repartition<uint8_t, decltype(d)> d8;
+  uint8x16_t v8 = vextq_u8(BitCast(d8, lo).raw, BitCast(d8, hi).raw, kBytes);
+  return BitCast(d, Vec128<uint8_t>(v8));
+}
+
+// 64-bit
+template <int kBytes, typename T, class V64 = Vec128<T, 8 / sizeof(T)>>
+HWY_API V64 CombineShiftRightBytes(Simd<T, 8 / sizeof(T)> d, V64 hi, V64 lo) {
+  static_assert(0 < kBytes && kBytes < 8, "kBytes must be in [1, 7]");
+  const Repartition<uint8_t, decltype(d)> d8;
+  uint8x8_t v8 = vext_u8(BitCast(d8, lo).raw, BitCast(d8, hi).raw, kBytes);
+  return BitCast(d, VFromD<decltype(d8)>(v8));
 }
 
+// <= 32-bit defined after ShiftLeftBytes.
+
 // ------------------------------ Shift vector by constant #bytes
 
 namespace detail {
 
-// Need to partially specialize because CombineShiftRightBytes<16> and <0> are
-// compile errors.
+// Partially specialize because kBytes = 0 and >= size are compile errors;
+// callers replace the latter with 0xFF for easier specialization.
 template <int kBytes>
 struct ShiftLeftBytesT {
-  template <class T, size_t N>
+  // Full
+  template <class T>
+  HWY_INLINE Vec128<T> operator()(const Vec128<T> v) {
+    const Full128<T> d;
+    return CombineShiftRightBytes<16 - kBytes>(d, v, Zero(d));
+  }
+
+  // Partial
+  template <class T, size_t N, HWY_IF_LE64(T, N)>
   HWY_INLINE Vec128<T, N> operator()(const Vec128<T, N> v) {
-    return CombineShiftRightBytes<16 - kBytes>(v, Zero(Full128<T>()));
+    // Expand to 64-bit so we only use the native EXT instruction.
+    const Simd<T, 8 / sizeof(T)> d64;
+    const auto zero64 = Zero(d64);
+    const decltype(zero64) v64(v.raw);
+    return Vec128<T, N>(
+        CombineShiftRightBytes<8 - kBytes>(d64, v64, zero64).raw);
   }
 };
 template <>
@@ -2867,12 +2968,27 @@ struct ShiftLeftBytesT<0> {
     return v;
   }
 };
+template <>
+struct ShiftLeftBytesT<0xFF> {
+  template <class T, size_t N>
+  HWY_INLINE Vec128<T, N> operator()(const Vec128<T, N> /* v */) {
+    return Zero(Simd<T, N>());
+  }
+};
 
 template <int kBytes>
 struct ShiftRightBytesT {
   template <class T, size_t N>
-  HWY_INLINE Vec128<T, N> operator()(const Vec128<T, N> v) {
-    return CombineShiftRightBytes<kBytes>(Zero(Full128<T>()), v);
+  HWY_INLINE Vec128<T, N> operator()(Vec128<T, N> v) {
+    const Simd<T, N> d;
+    // For < 64-bit vectors, zero undefined lanes so we shift in zeros.
+    if (N * sizeof(T) < 8) {
+      constexpr size_t kReg = N * sizeof(T) == 16 ? 16 : 8;
+      const Simd<T, kReg / sizeof(T)> dreg;
+      v = Vec128<T, N>(
+          IfThenElseZero(FirstN(dreg, N), VFromD<decltype(dreg)>(v.raw)).raw);
+    }
+    return CombineShiftRightBytes<kBytes>(d, Zero(d), v);
   }
 };
 template <>
@@ -2882,61 +2998,151 @@ struct ShiftRightBytesT<0> {
     return v;
   }
 };
+template <>
+struct ShiftRightBytesT<0xFF> {
+  template <class T, size_t N>
+  HWY_INLINE Vec128<T, N> operator()(const Vec128<T, N> /* v */) {
+    return Zero(Simd<T, N>());
+  }
+};
 
 }  // namespace detail
 
-// 0x01..0F, kBytes = 1 => 0x02..0F00
 template <int kBytes, typename T, size_t N>
-HWY_INLINE Vec128<T, N> ShiftLeftBytes(const Vec128<T, N> v) {
-  return detail::ShiftLeftBytesT<kBytes>()(v);
+HWY_API Vec128<T, N> ShiftLeftBytes(Simd<T, N> /* tag */, Vec128<T, N> v) {
+  return detail::ShiftLeftBytesT < kBytes >= N * sizeof(T) ? 0xFF
+                                                           : kBytes > ()(v);
+}
+
+template <int kBytes, typename T, size_t N>
+HWY_API Vec128<T, N> ShiftLeftBytes(const Vec128<T, N> v) {
+  return ShiftLeftBytes<kBytes>(Simd<T, N>(), v);
 }
 
 template <int kLanes, typename T, size_t N>
-HWY_INLINE Vec128<T, N> ShiftLeftLanes(const Vec128<T, N> v) {
-  const Simd<uint8_t, N * sizeof(T)> d8;
-  const Simd<T, N> d;
+HWY_API Vec128<T, N> ShiftLeftLanes(Simd<T, N> d, const Vec128<T, N> v) {
+  const Repartition<uint8_t, decltype(d)> d8;
   return BitCast(d, ShiftLeftBytes<kLanes * sizeof(T)>(BitCast(d8, v)));
 }
 
+template <int kLanes, typename T, size_t N>
+HWY_API Vec128<T, N> ShiftLeftLanes(const Vec128<T, N> v) {
+  return ShiftLeftLanes<kLanes>(Simd<T, N>(), v);
+}
+
 // 0x01..0F, kBytes = 1 => 0x0001..0E
 template <int kBytes, typename T, size_t N>
-HWY_INLINE Vec128<T, N> ShiftRightBytes(const Vec128<T, N> v) {
-  return detail::ShiftRightBytesT<kBytes>()(v);
+HWY_API Vec128<T, N> ShiftRightBytes(Simd<T, N> /* tag */, Vec128<T, N> v) {
+  return detail::ShiftRightBytesT < kBytes >= N * sizeof(T) ? 0xFF
+                                                            : kBytes > ()(v);
 }
 
 template <int kLanes, typename T, size_t N>
-HWY_INLINE Vec128<T, N> ShiftRightLanes(const Vec128<T, N> v) {
-  const Simd<uint8_t, N * sizeof(T)> d8;
-  const Simd<T, N> d;
+HWY_API Vec128<T, N> ShiftRightLanes(Simd<T, N> d, const Vec128<T, N> v) {
+  const Repartition<uint8_t, decltype(d)> d8;
   return BitCast(d, ShiftRightBytes<kLanes * sizeof(T)>(BitCast(d8, v)));
 }
 
+// Calls ShiftLeftBytes
+template <int kBytes, typename T, size_t N, HWY_IF_LE32(T, N)>
+HWY_API Vec128<T, N> CombineShiftRightBytes(Simd<T, N> d, Vec128<T, N> hi,
+                                            Vec128<T, N> lo) {
+  constexpr size_t kSize = N * sizeof(T);
+  static_assert(0 < kBytes && kBytes < kSize, "kBytes invalid");
+  const Repartition<uint8_t, decltype(d)> d8;
+  const Simd<uint8_t, 8> d_full8;
+  const Repartition<T, decltype(d_full8)> d_full;
+  using V64 = VFromD<decltype(d_full8)>;
+  const V64 hi64(BitCast(d8, hi).raw);
+  // Move into most-significant bytes
+  const V64 lo64 = ShiftLeftBytes<8 - kSize>(V64(BitCast(d8, lo).raw));
+  const V64 r = CombineShiftRightBytes<8 - kSize + kBytes>(d_full8, hi64, lo64);
+  // After casting to full 64-bit vector of correct type, shrink to 32-bit
+  return Vec128<T, N>(BitCast(d_full, r).raw);
+}
+
+// ------------------------------ UpperHalf (ShiftRightBytes)
+
+// Full input
+HWY_API Vec128<uint8_t, 8> UpperHalf(Simd<uint8_t, 8> /* tag */,
+                                     const Vec128<uint8_t> v) {
+  return Vec128<uint8_t, 8>(vget_high_u8(v.raw));
+}
+HWY_API Vec128<uint16_t, 4> UpperHalf(Simd<uint16_t, 4> /* tag */,
+                                      const Vec128<uint16_t> v) {
+  return Vec128<uint16_t, 4>(vget_high_u16(v.raw));
+}
+HWY_API Vec128<uint32_t, 2> UpperHalf(Simd<uint32_t, 2> /* tag */,
+                                      const Vec128<uint32_t> v) {
+  return Vec128<uint32_t, 2>(vget_high_u32(v.raw));
+}
+HWY_API Vec128<uint64_t, 1> UpperHalf(Simd<uint64_t, 1> /* tag */,
+                                      const Vec128<uint64_t> v) {
+  return Vec128<uint64_t, 1>(vget_high_u64(v.raw));
+}
+HWY_API Vec128<int8_t, 8> UpperHalf(Simd<int8_t, 8> /* tag */,
+                                    const Vec128<int8_t> v) {
+  return Vec128<int8_t, 8>(vget_high_s8(v.raw));
+}
+HWY_API Vec128<int16_t, 4> UpperHalf(Simd<int16_t, 4> /* tag */,
+                                     const Vec128<int16_t> v) {
+  return Vec128<int16_t, 4>(vget_high_s16(v.raw));
+}
+HWY_API Vec128<int32_t, 2> UpperHalf(Simd<int32_t, 2> /* tag */,
+                                     const Vec128<int32_t> v) {
+  return Vec128<int32_t, 2>(vget_high_s32(v.raw));
+}
+HWY_API Vec128<int64_t, 1> UpperHalf(Simd<int64_t, 1> /* tag */,
+                                     const Vec128<int64_t> v) {
+  return Vec128<int64_t, 1>(vget_high_s64(v.raw));
+}
+HWY_API Vec128<float, 2> UpperHalf(Simd<float, 2> /* tag */,
+                                   const Vec128<float> v) {
+  return Vec128<float, 2>(vget_high_f32(v.raw));
+}
+#if HWY_ARCH_ARM_A64
+HWY_API Vec128<double, 1> UpperHalf(Simd<double, 1> /* tag */,
+                                    const Vec128<double> v) {
+  return Vec128<double, 1>(vget_high_f64(v.raw));
+}
+#endif
+
+// Partial
+template <typename T, size_t N, HWY_IF_LE64(T, N)>
+HWY_API Vec128<T, (N + 1) / 2> UpperHalf(Half<Simd<T, N>> /* tag */,
+                                         Vec128<T, N> v) {
+  const Simd<T, N> d;
+  const auto vu = BitCast(RebindToUnsigned<decltype(d)>(), v);
+  const auto upper = BitCast(d, ShiftRightBytes<N * sizeof(T) / 2>(vu));
+  return Vec128<T, (N + 1) / 2>(upper.raw);
+}
+
 // ------------------------------ Broadcast/splat any lane
 
 #if HWY_ARCH_ARM_A64
 // Unsigned
 template <int kLane>
-HWY_INLINE Vec128<uint16_t> Broadcast(const Vec128<uint16_t> v) {
+HWY_API Vec128<uint16_t> Broadcast(const Vec128<uint16_t> v) {
   static_assert(0 <= kLane && kLane < 8, "Invalid lane");
   return Vec128<uint16_t>(vdupq_laneq_u16(v.raw, kLane));
 }
 template <int kLane, size_t N, HWY_IF_LE64(uint16_t, N)>
-HWY_INLINE Vec128<uint16_t, N> Broadcast(const Vec128<uint16_t, N> v) {
+HWY_API Vec128<uint16_t, N> Broadcast(const Vec128<uint16_t, N> v) {
   static_assert(0 <= kLane && kLane < N, "Invalid lane");
   return Vec128<uint16_t, N>(vdup_lane_u16(v.raw, kLane));
 }
 template <int kLane>
-HWY_INLINE Vec128<uint32_t> Broadcast(const Vec128<uint32_t> v) {
+HWY_API Vec128<uint32_t> Broadcast(const Vec128<uint32_t> v) {
   static_assert(0 <= kLane && kLane < 4, "Invalid lane");
   return Vec128<uint32_t>(vdupq_laneq_u32(v.raw, kLane));
 }
 template <int kLane, size_t N, HWY_IF_LE64(uint32_t, N)>
-HWY_INLINE Vec128<uint32_t, N> Broadcast(const Vec128<uint32_t, N> v) {
+HWY_API Vec128<uint32_t, N> Broadcast(const Vec128<uint32_t, N> v) {
   static_assert(0 <= kLane && kLane < N, "Invalid lane");
   return Vec128<uint32_t, N>(vdup_lane_u32(v.raw, kLane));
 }
 template <int kLane>
-HWY_INLINE Vec128<uint64_t> Broadcast(const Vec128<uint64_t> v) {
+HWY_API Vec128<uint64_t> Broadcast(const Vec128<uint64_t> v) {
   static_assert(0 <= kLane && kLane < 2, "Invalid lane");
   return Vec128<uint64_t>(vdupq_laneq_u64(v.raw, kLane));
 }
@@ -2944,27 +3150,27 @@ HWY_INLINE Vec128<uint64_t> Broadcast(const Vec128<uint64_t> v) {
 
 // Signed
 template <int kLane>
-HWY_INLINE Vec128<int16_t> Broadcast(const Vec128<int16_t> v) {
+HWY_API Vec128<int16_t> Broadcast(const Vec128<int16_t> v) {
   static_assert(0 <= kLane && kLane < 8, "Invalid lane");
   return Vec128<int16_t>(vdupq_laneq_s16(v.raw, kLane));
 }
 template <int kLane, size_t N, HWY_IF_LE64(int16_t, N)>
-HWY_INLINE Vec128<int16_t, N> Broadcast(const Vec128<int16_t, N> v) {
+HWY_API Vec128<int16_t, N> Broadcast(const Vec128<int16_t, N> v) {
   static_assert(0 <= kLane && kLane < N, "Invalid lane");
   return Vec128<int16_t, N>(vdup_lane_s16(v.raw, kLane));
 }
 template <int kLane>
-HWY_INLINE Vec128<int32_t> Broadcast(const Vec128<int32_t> v) {
+HWY_API Vec128<int32_t> Broadcast(const Vec128<int32_t> v) {
   static_assert(0 <= kLane && kLane < 4, "Invalid lane");
   return Vec128<int32_t>(vdupq_laneq_s32(v.raw, kLane));
 }
 template <int kLane, size_t N, HWY_IF_LE64(int32_t, N)>
-HWY_INLINE Vec128<int32_t, N> Broadcast(const Vec128<int32_t, N> v) {
+HWY_API Vec128<int32_t, N> Broadcast(const Vec128<int32_t, N> v) {
   static_assert(0 <= kLane && kLane < N, "Invalid lane");
   return Vec128<int32_t, N>(vdup_lane_s32(v.raw, kLane));
 }
 template <int kLane>
-HWY_INLINE Vec128<int64_t> Broadcast(const Vec128<int64_t> v) {
+HWY_API Vec128<int64_t> Broadcast(const Vec128<int64_t> v) {
   static_assert(0 <= kLane && kLane < 2, "Invalid lane");
   return Vec128<int64_t>(vdupq_laneq_s64(v.raw, kLane));
 }
@@ -2972,22 +3178,22 @@ HWY_INLINE Vec128<int64_t> Broadcast(const Vec128<int64_t> v) {
 
 // Float
 template <int kLane>
-HWY_INLINE Vec128<float> Broadcast(const Vec128<float> v) {
+HWY_API Vec128<float> Broadcast(const Vec128<float> v) {
   static_assert(0 <= kLane && kLane < 4, "Invalid lane");
   return Vec128<float>(vdupq_laneq_f32(v.raw, kLane));
 }
 template <int kLane, size_t N, HWY_IF_LE64(float, N)>
-HWY_INLINE Vec128<float, N> Broadcast(const Vec128<float, N> v) {
+HWY_API Vec128<float, N> Broadcast(const Vec128<float, N> v) {
   static_assert(0 <= kLane && kLane < N, "Invalid lane");
   return Vec128<float, N>(vdup_lane_f32(v.raw, kLane));
 }
 template <int kLane>
-HWY_INLINE Vec128<double> Broadcast(const Vec128<double> v) {
+HWY_API Vec128<double> Broadcast(const Vec128<double> v) {
   static_assert(0 <= kLane && kLane < 2, "Invalid lane");
   return Vec128<double>(vdupq_laneq_f64(v.raw, kLane));
 }
 template <int kLane>
-HWY_INLINE Vec128<double, 1> Broadcast(const Vec128<double, 1> v) {
+HWY_API Vec128<double, 1> Broadcast(const Vec128<double, 1> v) {
   static_assert(0 <= kLane && kLane < 1, "Invalid lane");
   return v;
 }
@@ -2997,27 +3203,27 @@ HWY_INLINE Vec128<double, 1> Broadcast(const Vec128<double, 1> v) {
 
 // Unsigned
 template <int kLane>
-HWY_INLINE Vec128<uint16_t> Broadcast(const Vec128<uint16_t> v) {
+HWY_API Vec128<uint16_t> Broadcast(const Vec128<uint16_t> v) {
   static_assert(0 <= kLane && kLane < 8, "Invalid lane");
   return Vec128<uint16_t>(vdupq_n_u16(vgetq_lane_u16(v.raw, kLane)));
 }
 template <int kLane, size_t N, HWY_IF_LE64(uint16_t, N)>
-HWY_INLINE Vec128<uint16_t, N> Broadcast(const Vec128<uint16_t, N> v) {
+HWY_API Vec128<uint16_t, N> Broadcast(const Vec128<uint16_t, N> v) {
   static_assert(0 <= kLane && kLane < N, "Invalid lane");
   return Vec128<uint16_t, N>(vdup_lane_u16(v.raw, kLane));
 }
 template <int kLane>
-HWY_INLINE Vec128<uint32_t> Broadcast(const Vec128<uint32_t> v) {
+HWY_API Vec128<uint32_t> Broadcast(const Vec128<uint32_t> v) {
   static_assert(0 <= kLane && kLane < 4, "Invalid lane");
   return Vec128<uint32_t>(vdupq_n_u32(vgetq_lane_u32(v.raw, kLane)));
 }
 template <int kLane, size_t N, HWY_IF_LE64(uint32_t, N)>
-HWY_INLINE Vec128<uint32_t, N> Broadcast(const Vec128<uint32_t, N> v) {
+HWY_API Vec128<uint32_t, N> Broadcast(const Vec128<uint32_t, N> v) {
   static_assert(0 <= kLane && kLane < N, "Invalid lane");
   return Vec128<uint32_t, N>(vdup_lane_u32(v.raw, kLane));
 }
 template <int kLane>
-HWY_INLINE Vec128<uint64_t> Broadcast(const Vec128<uint64_t> v) {
+HWY_API Vec128<uint64_t> Broadcast(const Vec128<uint64_t> v) {
   static_assert(0 <= kLane && kLane < 2, "Invalid lane");
   return Vec128<uint64_t>(vdupq_n_u64(vgetq_lane_u64(v.raw, kLane)));
 }
@@ -3025,27 +3231,27 @@ HWY_INLINE Vec128<uint64_t> Broadcast(const Vec128<uint64_t> v) {
 
 // Signed
 template <int kLane>
-HWY_INLINE Vec128<int16_t> Broadcast(const Vec128<int16_t> v) {
+HWY_API Vec128<int16_t> Broadcast(const Vec128<int16_t> v) {
   static_assert(0 <= kLane && kLane < 8, "Invalid lane");
   return Vec128<int16_t>(vdupq_n_s16(vgetq_lane_s16(v.raw, kLane)));
 }
 template <int kLane, size_t N, HWY_IF_LE64(int16_t, N)>
-HWY_INLINE Vec128<int16_t, N> Broadcast(const Vec128<int16_t, N> v) {
+HWY_API Vec128<int16_t, N> Broadcast(const Vec128<int16_t, N> v) {
   static_assert(0 <= kLane && kLane < N, "Invalid lane");
   return Vec128<int16_t, N>(vdup_lane_s16(v.raw, kLane));
 }
 template <int kLane>
-HWY_INLINE Vec128<int32_t> Broadcast(const Vec128<int32_t> v) {
+HWY_API Vec128<int32_t> Broadcast(const Vec128<int32_t> v) {
   static_assert(0 <= kLane && kLane < 4, "Invalid lane");
   return Vec128<int32_t>(vdupq_n_s32(vgetq_lane_s32(v.raw, kLane)));
 }
 template <int kLane, size_t N, HWY_IF_LE64(int32_t, N)>
-HWY_INLINE Vec128<int32_t, N> Broadcast(const Vec128<int32_t, N> v) {
+HWY_API Vec128<int32_t, N> Broadcast(const Vec128<int32_t, N> v) {
   static_assert(0 <= kLane && kLane < N, "Invalid lane");
   return Vec128<int32_t, N>(vdup_lane_s32(v.raw, kLane));
 }
 template <int kLane>
-HWY_INLINE Vec128<int64_t> Broadcast(const Vec128<int64_t> v) {
+HWY_API Vec128<int64_t> Broadcast(const Vec128<int64_t> v) {
   static_assert(0 <= kLane && kLane < 2, "Invalid lane");
   return Vec128<int64_t>(vdupq_n_s64(vgetq_lane_s64(v.raw, kLane)));
 }
@@ -3053,12 +3259,12 @@ HWY_INLINE Vec128<int64_t> Broadcast(const Vec128<int64_t> v) {
 
 // Float
 template <int kLane>
-HWY_INLINE Vec128<float> Broadcast(const Vec128<float> v) {
+HWY_API Vec128<float> Broadcast(const Vec128<float> v) {
   static_assert(0 <= kLane && kLane < 4, "Invalid lane");
   return Vec128<float>(vdupq_n_f32(vgetq_lane_f32(v.raw, kLane)));
 }
 template <int kLane, size_t N, HWY_IF_LE64(float, N)>
-HWY_INLINE Vec128<float, N> Broadcast(const Vec128<float, N> v) {
+HWY_API Vec128<float, N> Broadcast(const Vec128<float, N> v) {
   static_assert(0 <= kLane && kLane < N, "Invalid lane");
   return Vec128<float, N>(vdup_lane_f32(v.raw, kLane));
 }
@@ -3066,50 +3272,16 @@ HWY_INLINE Vec128<float, N> Broadcast(const Vec128<float, N> v) {
 #endif
 
 template <int kLane>
-HWY_INLINE Vec128<uint64_t, 1> Broadcast(const Vec128<uint64_t, 1> v) {
+HWY_API Vec128<uint64_t, 1> Broadcast(const Vec128<uint64_t, 1> v) {
   static_assert(0 <= kLane && kLane < 1, "Invalid lane");
   return v;
 }
 template <int kLane>
-HWY_INLINE Vec128<int64_t, 1> Broadcast(const Vec128<int64_t, 1> v) {
+HWY_API Vec128<int64_t, 1> Broadcast(const Vec128<int64_t, 1> v) {
   static_assert(0 <= kLane && kLane < 1, "Invalid lane");
   return v;
 }
 
-// ------------------------------ Shuffle bytes with variable indices
-
-// Returns vector of bytes[from[i]]. "from" is also interpreted as bytes, i.e.
-// lane indices in [0, 16).
-template <typename T>
-HWY_API Vec128<T> TableLookupBytes(const Vec128<T> bytes,
-                                   const Vec128<T> from) {
-  const Full128<T> d;
-  const Repartition<uint8_t, decltype(d)> d8;
-#if HWY_ARCH_ARM_A64
-  return BitCast(d, Vec128<uint8_t>(vqtbl1q_u8(BitCast(d8, bytes).raw,
-                                               BitCast(d8, from).raw)));
-#else
-  uint8x16_t table0 = BitCast(d8, bytes).raw;
-  uint8x8x2_t table;
-  table.val[0] = vget_low_u8(table0);
-  table.val[1] = vget_high_u8(table0);
-  uint8x16_t idx = BitCast(d8, from).raw;
-  uint8x8_t low = vtbl2_u8(table, vget_low_u8(idx));
-  uint8x8_t hi = vtbl2_u8(table, vget_high_u8(idx));
-  return BitCast(d, Vec128<uint8_t>(vcombine_u8(low, hi)));
-#endif
-}
-
-template <typename T, size_t N, typename TI, HWY_IF_LE64(T, N)>
-HWY_INLINE Vec128<T, N> TableLookupBytes(
-    const Vec128<T, N> bytes,
-    const Vec128<TI, N * sizeof(T) / sizeof(TI)> from) {
-  const Simd<T, N> d;
-  const Repartition<uint8_t, decltype(d)> d8;
-  return BitCast(d, decltype(Zero(d8))(vtbl1_u8(BitCast(d8, bytes).raw,
-                                                BitCast(d8, from).raw)));
-}
-
 // ------------------------------ TableLookupLanes
 
 // Returned by SetTableIndices for use by TableLookupLanes.
@@ -3119,7 +3291,7 @@ struct Indices128 {
 };
 
 template <typename T, size_t N, HWY_IF_LE128(T, N)>
-HWY_INLINE Indices128<T, N> SetTableIndices(Simd<T, N> d, const int32_t* idx) {
+HWY_API Indices128<T, N> SetTableIndices(Simd<T, N> d, const int32_t* idx) {
 #if !defined(NDEBUG) || defined(ADDRESS_SANITIZER)
   for (size_t i = 0; i < N; ++i) {
     HWY_DASSERT(0 <= idx[i] && idx[i] < static_cast<int32_t>(N));
@@ -3138,18 +3310,18 @@ HWY_INLINE Indices128<T, N> SetTableIndices(Simd<T, N> d, const int32_t* idx) {
 }
 
 template <size_t N>
-HWY_INLINE Vec128<uint32_t, N> TableLookupLanes(
+HWY_API Vec128<uint32_t, N> TableLookupLanes(
     const Vec128<uint32_t, N> v, const Indices128<uint32_t, N> idx) {
   return TableLookupBytes(v, Vec128<uint32_t, N>{idx.raw});
 }
 template <size_t N>
-HWY_INLINE Vec128<int32_t, N> TableLookupLanes(
-    const Vec128<int32_t, N> v, const Indices128<int32_t, N> idx) {
+HWY_API Vec128<int32_t, N> TableLookupLanes(const Vec128<int32_t, N> v,
+                                            const Indices128<int32_t, N> idx) {
   return TableLookupBytes(v, Vec128<int32_t, N>{idx.raw});
 }
 template <size_t N>
-HWY_INLINE Vec128<float, N> TableLookupLanes(const Vec128<float, N> v,
-                                             const Indices128<float, N> idx) {
+HWY_API Vec128<float, N> TableLookupLanes(const Vec128<float, N> v,
+                                          const Indices128<float, N> idx) {
   const Simd<int32_t, N> di;
   const auto idx_i = BitCast(di, Vec128<float, N>{idx.raw});
   return BitCast(Simd<float, N>(), TableLookupBytes(BitCast(di, v), idx_i));
@@ -3164,42 +3336,33 @@ HWY_INLINE Vec128<float, N> TableLookupLanes(const Vec128<float, N> v,
 
 // Swap 64-bit halves
 template <typename T>
-HWY_INLINE Vec128<T> Shuffle1032(const Vec128<T> v) {
-  return CombineShiftRightBytes<8>(v, v);
+HWY_API Vec128<T> Shuffle1032(const Vec128<T> v) {
+  return CombineShiftRightBytes<8>(Full128<T>(), v, v);
 }
 template <typename T>
-HWY_INLINE Vec128<T> Shuffle01(const Vec128<T> v) {
-  return CombineShiftRightBytes<8>(v, v);
+HWY_API Vec128<T> Shuffle01(const Vec128<T> v) {
+  return CombineShiftRightBytes<8>(Full128<T>(), v, v);
 }
 
 // Rotate right 32 bits
 template <typename T>
-HWY_INLINE Vec128<T> Shuffle0321(const Vec128<T> v) {
-  return CombineShiftRightBytes<4>(v, v);
+HWY_API Vec128<T> Shuffle0321(const Vec128<T> v) {
+  return CombineShiftRightBytes<4>(Full128<T>(), v, v);
 }
 
 // Rotate left 32 bits
 template <typename T>
-HWY_INLINE Vec128<T> Shuffle2103(const Vec128<T> v) {
-  return CombineShiftRightBytes<12>(v, v);
+HWY_API Vec128<T> Shuffle2103(const Vec128<T> v) {
+  return CombineShiftRightBytes<12>(Full128<T>(), v, v);
 }
 
 // Reverse
 template <typename T>
-HWY_INLINE Vec128<T> Shuffle0123(const Vec128<T> v) {
-  static_assert(sizeof(T) == 4,
-                "Shuffle0123 should only be applied to 32-bit types");
-  // TODO(janwas): more efficient implementation?,
-  // It is possible to use two instructions (vrev64q_u32 and vcombine_u32 of the
-  // high/low parts) instead of the extra memory and load.
-  static constexpr uint8_t bytes[16] = {12, 13, 14, 15, 8, 9, 10, 11,
-                                        4,  5,  6,  7,  0, 1, 2,  3};
-  const Full128<uint8_t> d8;
-  const Full128<T> d;
-  return TableLookupBytes(v, BitCast(d, Load(d8, bytes)));
+HWY_API Vec128<T> Shuffle0123(const Vec128<T> v) {
+  return Shuffle2301(Shuffle1032(v));
 }
 
-// ------------------------------ Interleave lanes
+// ------------------------------ InterleaveLower
 
 // Interleaves lanes from halves of the 128-bit blocks of "a" (which provides
 // the least-significant lane) and "b". To concatenate two half-width integers
@@ -3207,244 +3370,331 @@ HWY_INLINE Vec128<T> Shuffle0123(const Vec128<T> v) {
 HWY_NEON_DEF_FUNCTION_INT_8_16_32(InterleaveLower, vzip1, _, 2)
 HWY_NEON_DEF_FUNCTION_UINT_8_16_32(InterleaveLower, vzip1, _, 2)
 
-HWY_NEON_DEF_FUNCTION_INT_8_16_32(InterleaveUpper, vzip2, _, 2)
-HWY_NEON_DEF_FUNCTION_UINT_8_16_32(InterleaveUpper, vzip2, _, 2)
-
 #if HWY_ARCH_ARM_A64
-// For 64 bit types, we only have the "q" version of the function defined as
-// interleaving 64-wide registers with 64-wide types in them makes no sense.
-HWY_INLINE Vec128<uint64_t> InterleaveLower(const Vec128<uint64_t> a,
-                                            const Vec128<uint64_t> b) {
+// N=1 makes no sense (in that case, there would be no upper/lower).
+HWY_API Vec128<uint64_t> InterleaveLower(const Vec128<uint64_t> a,
+                                         const Vec128<uint64_t> b) {
   return Vec128<uint64_t>(vzip1q_u64(a.raw, b.raw));
 }
-HWY_INLINE Vec128<int64_t> InterleaveLower(const Vec128<int64_t> a,
-                                           const Vec128<int64_t> b) {
+HWY_API Vec128<int64_t> InterleaveLower(const Vec128<int64_t> a,
+                                        const Vec128<int64_t> b) {
   return Vec128<int64_t>(vzip1q_s64(a.raw, b.raw));
 }
-
-HWY_INLINE Vec128<uint64_t> InterleaveUpper(const Vec128<uint64_t> a,
-                                            const Vec128<uint64_t> b) {
-  return Vec128<uint64_t>(vzip2q_u64(a.raw, b.raw));
-}
-HWY_INLINE Vec128<int64_t> InterleaveUpper(const Vec128<int64_t> a,
-                                           const Vec128<int64_t> b) {
-  return Vec128<int64_t>(vzip2q_s64(a.raw, b.raw));
+HWY_API Vec128<double> InterleaveLower(const Vec128<double> a,
+                                       const Vec128<double> b) {
+  return Vec128<double>(vzip1q_f64(a.raw, b.raw));
 }
 #else
 // ARMv7 emulation.
-HWY_INLINE Vec128<uint64_t> InterleaveLower(const Vec128<uint64_t> a,
-                                            const Vec128<uint64_t> b) {
-  auto flip = CombineShiftRightBytes<8>(a, a);
-  return CombineShiftRightBytes<8>(b, flip);
-}
-HWY_INLINE Vec128<int64_t> InterleaveLower(const Vec128<int64_t> a,
-                                           const Vec128<int64_t> b) {
-  auto flip = CombineShiftRightBytes<8>(a, a);
-  return CombineShiftRightBytes<8>(b, flip);
+HWY_API Vec128<uint64_t> InterleaveLower(const Vec128<uint64_t> a,
+                                         const Vec128<uint64_t> b) {
+  return CombineShiftRightBytes<8>(Full128<uint64_t>(), b, Shuffle01(a));
 }
-
-HWY_INLINE Vec128<uint64_t> InterleaveUpper(const Vec128<uint64_t> a,
-                                            const Vec128<uint64_t> b) {
-  auto flip = CombineShiftRightBytes<8>(b, b);
-  return CombineShiftRightBytes<8>(flip, a);
-}
-HWY_INLINE Vec128<int64_t> InterleaveUpper(const Vec128<int64_t> a,
-                                           const Vec128<int64_t> b) {
-  auto flip = CombineShiftRightBytes<8>(b, b);
-  return CombineShiftRightBytes<8>(flip, a);
+HWY_API Vec128<int64_t> InterleaveLower(const Vec128<int64_t> a,
+                                        const Vec128<int64_t> b) {
+  return CombineShiftRightBytes<8>(Full128<int64_t>(), b, Shuffle01(a));
 }
 #endif
 
 // Floats
-HWY_INLINE Vec128<float> InterleaveLower(const Vec128<float> a,
-                                         const Vec128<float> b) {
+HWY_API Vec128<float> InterleaveLower(const Vec128<float> a,
+                                      const Vec128<float> b) {
   return Vec128<float>(vzip1q_f32(a.raw, b.raw));
 }
-#if HWY_ARCH_ARM_A64
-HWY_INLINE Vec128<double> InterleaveLower(const Vec128<double> a,
-                                          const Vec128<double> b) {
-  return Vec128<double>(vzip1q_f64(a.raw, b.raw));
+template <size_t N, HWY_IF_LE64(float, N)>
+HWY_API Vec128<float, N> InterleaveLower(const Vec128<float, N> a,
+                                         const Vec128<float, N> b) {
+  return Vec128<float, N>(vzip1_f32(a.raw, b.raw));
 }
-#endif
 
-HWY_INLINE Vec128<float> InterleaveUpper(const Vec128<float> a,
-                                         const Vec128<float> b) {
-  return Vec128<float>(vzip2q_f32(a.raw, b.raw));
+// < 64 bit parts
+template <typename T, size_t N, HWY_IF_LE32(T, N)>
+HWY_API Vec128<T, N> InterleaveLower(Vec128<T, N> a, Vec128<T, N> b) {
+  using V64 = Vec128<T, 8 / sizeof(T)>;
+  return Vec128<T, N>(InterleaveLower(V64(a.raw), V64(b.raw)).raw);
 }
+
+// Additional overload for the optional Simd<> tag.
+template <typename T, size_t N, class V = Vec128<T, N>>
+HWY_API V InterleaveLower(Simd<T, N> /* tag */, V a, V b) {
+  return InterleaveLower(a, b);
+}
+
+// ------------------------------ InterleaveUpper (UpperHalf)
+
+// All functions inside detail lack the required D parameter.
+namespace detail {
+HWY_NEON_DEF_FUNCTION_INT_8_16_32(InterleaveUpper, vzip2, _, 2)
+HWY_NEON_DEF_FUNCTION_UINT_8_16_32(InterleaveUpper, vzip2, _, 2)
+
 #if HWY_ARCH_ARM_A64
-HWY_INLINE Vec128<double> InterleaveUpper(const Vec128<double> a,
-                                          const Vec128<double> b) {
+// N=1 makes no sense (in that case, there would be no upper/lower).
+HWY_API Vec128<uint64_t> InterleaveUpper(const Vec128<uint64_t> a,
+                                         const Vec128<uint64_t> b) {
+  return Vec128<uint64_t>(vzip2q_u64(a.raw, b.raw));
+}
+HWY_API Vec128<int64_t> InterleaveUpper(Vec128<int64_t> a, Vec128<int64_t> b) {
+  return Vec128<int64_t>(vzip2q_s64(a.raw, b.raw));
+}
+HWY_API Vec128<double> InterleaveUpper(Vec128<double> a, Vec128<double> b) {
   return Vec128<double>(vzip2q_f64(a.raw, b.raw));
 }
+#else
+// ARMv7 emulation.
+HWY_API Vec128<uint64_t> InterleaveUpper(const Vec128<uint64_t> a,
+                                         const Vec128<uint64_t> b) {
+  return CombineShiftRightBytes<8>(Full128<uint64_t>(), Shuffle01(b), a);
+}
+HWY_API Vec128<int64_t> InterleaveUpper(Vec128<int64_t> a, Vec128<int64_t> b) {
+  return CombineShiftRightBytes<8>(Full128<int64_t>(), Shuffle01(b), a);
+}
 #endif
 
-// ------------------------------ Zip lanes
+HWY_API Vec128<float> InterleaveUpper(Vec128<float> a, Vec128<float> b) {
+  return Vec128<float>(vzip2q_f32(a.raw, b.raw));
+}
+HWY_API Vec128<float, 2> InterleaveUpper(const Vec128<float, 2> a,
+                                         const Vec128<float, 2> b) {
+  return Vec128<float, 2>(vzip2_f32(a.raw, b.raw));
+}
+
+}  // namespace detail
+
+// Full register
+template <typename T, size_t N, HWY_IF_GE64(T, N), class V = Vec128<T, N>>
+HWY_API V InterleaveUpper(Simd<T, N> /* tag */, V a, V b) {
+  return detail::InterleaveUpper(a, b);
+}
+
+// Partial
+template <typename T, size_t N, HWY_IF_LE32(T, N), class V = Vec128<T, N>>
+HWY_API V InterleaveUpper(Simd<T, N> d, V a, V b) {
+  const Half<decltype(d)> d2;
+  return InterleaveLower(d, V(UpperHalf(d2, a).raw), V(UpperHalf(d2, b).raw));
+}
+
+// ------------------------------ ZipLower/ZipUpper (InterleaveLower)
 
 // Same as Interleave*, except that the return lanes are double-width integers;
 // this is necessary because the single-lane scalar cannot return two values.
-
-// Full vectors
-HWY_INLINE Vec128<uint16_t> ZipLower(const Vec128<uint8_t> a,
-                                     const Vec128<uint8_t> b) {
-  return Vec128<uint16_t>(vreinterpretq_u16_u8(vzip1q_u8(a.raw, b.raw)));
+template <typename T, size_t N, class DW = RepartitionToWide<Simd<T, N>>>
+HWY_API VFromD<DW> ZipLower(Vec128<T, N> a, Vec128<T, N> b) {
+  return BitCast(DW(), InterleaveLower(a, b));
 }
-HWY_INLINE Vec128<uint32_t> ZipLower(const Vec128<uint16_t> a,
-                                     const Vec128<uint16_t> b) {
-  return Vec128<uint32_t>(vreinterpretq_u32_u16(vzip1q_u16(a.raw, b.raw)));
+template <typename T, size_t N, class D = Simd<T, N>,
+          class DW = RepartitionToWide<D>>
+HWY_API VFromD<DW> ZipLower(DW dw, Vec128<T, N> a, Vec128<T, N> b) {
+  return BitCast(dw, InterleaveLower(D(), a, b));
 }
-HWY_INLINE Vec128<uint64_t> ZipLower(const Vec128<uint32_t> a,
-                                     const Vec128<uint32_t> b) {
-  return Vec128<uint64_t>(vreinterpretq_u64_u32(vzip1q_u32(a.raw, b.raw)));
+
+template <typename T, size_t N, class D = Simd<T, N>,
+          class DW = RepartitionToWide<D>>
+HWY_API VFromD<DW> ZipUpper(DW dw, Vec128<T, N> a, Vec128<T, N> b) {
+  return BitCast(dw, InterleaveUpper(D(), a, b));
 }
 
-HWY_INLINE Vec128<int16_t> ZipLower(const Vec128<int8_t> a,
-                                    const Vec128<int8_t> b) {
-  return Vec128<int16_t>(vreinterpretq_s16_s8(vzip1q_s8(a.raw, b.raw)));
+// ================================================== COMBINE
+
+// ------------------------------ Combine (InterleaveLower)
+
+// Full result
+HWY_API Vec128<uint8_t> Combine(Full128<uint8_t> /* tag */,
+                                Vec128<uint8_t, 8> hi, Vec128<uint8_t, 8> lo) {
+  return Vec128<uint8_t>(vcombine_u8(lo.raw, hi.raw));
 }
-HWY_INLINE Vec128<int32_t> ZipLower(const Vec128<int16_t> a,
-                                    const Vec128<int16_t> b) {
-  return Vec128<int32_t>(vreinterpretq_s32_s16(vzip1q_s16(a.raw, b.raw)));
+HWY_API Vec128<uint16_t> Combine(Full128<uint16_t> /* tag */,
+                                 Vec128<uint16_t, 4> hi,
+                                 Vec128<uint16_t, 4> lo) {
+  return Vec128<uint16_t>(vcombine_u16(lo.raw, hi.raw));
 }
-HWY_INLINE Vec128<int64_t> ZipLower(const Vec128<int32_t> a,
-                                    const Vec128<int32_t> b) {
-  return Vec128<int64_t>(vreinterpretq_s64_s32(vzip1q_s32(a.raw, b.raw)));
+HWY_API Vec128<uint32_t> Combine(Full128<uint32_t> /* tag */,
+                                 Vec128<uint32_t, 2> hi,
+                                 Vec128<uint32_t, 2> lo) {
+  return Vec128<uint32_t>(vcombine_u32(lo.raw, hi.raw));
+}
+HWY_API Vec128<uint64_t> Combine(Full128<uint64_t> /* tag */,
+                                 Vec128<uint64_t, 1> hi,
+                                 Vec128<uint64_t, 1> lo) {
+  return Vec128<uint64_t>(vcombine_u64(lo.raw, hi.raw));
 }
 
-HWY_INLINE Vec128<uint16_t> ZipUpper(const Vec128<uint8_t> a,
-                                     const Vec128<uint8_t> b) {
-  return Vec128<uint16_t>(vreinterpretq_u16_u8(vzip2q_u8(a.raw, b.raw)));
+HWY_API Vec128<int8_t> Combine(Full128<int8_t> /* tag */, Vec128<int8_t, 8> hi,
+                               Vec128<int8_t, 8> lo) {
+  return Vec128<int8_t>(vcombine_s8(lo.raw, hi.raw));
 }
-HWY_INLINE Vec128<uint32_t> ZipUpper(const Vec128<uint16_t> a,
-                                     const Vec128<uint16_t> b) {
-  return Vec128<uint32_t>(vreinterpretq_u32_u16(vzip2q_u16(a.raw, b.raw)));
+HWY_API Vec128<int16_t> Combine(Full128<int16_t> /* tag */,
+                                Vec128<int16_t, 4> hi, Vec128<int16_t, 4> lo) {
+  return Vec128<int16_t>(vcombine_s16(lo.raw, hi.raw));
 }
-HWY_INLINE Vec128<uint64_t> ZipUpper(const Vec128<uint32_t> a,
-                                     const Vec128<uint32_t> b) {
-  return Vec128<uint64_t>(vreinterpretq_u64_u32(vzip2q_u32(a.raw, b.raw)));
+HWY_API Vec128<int32_t> Combine(Full128<int32_t> /* tag */,
+                                Vec128<int32_t, 2> hi, Vec128<int32_t, 2> lo) {
+  return Vec128<int32_t>(vcombine_s32(lo.raw, hi.raw));
 }
-
-HWY_INLINE Vec128<int16_t> ZipUpper(const Vec128<int8_t> a,
-                                    const Vec128<int8_t> b) {
-  return Vec128<int16_t>(vreinterpretq_s16_s8(vzip2q_s8(a.raw, b.raw)));
+HWY_API Vec128<int64_t> Combine(Full128<int64_t> /* tag */,
+                                Vec128<int64_t, 1> hi, Vec128<int64_t, 1> lo) {
+  return Vec128<int64_t>(vcombine_s64(lo.raw, hi.raw));
 }
-HWY_INLINE Vec128<int32_t> ZipUpper(const Vec128<int16_t> a,
-                                    const Vec128<int16_t> b) {
-  return Vec128<int32_t>(vreinterpretq_s32_s16(vzip2q_s16(a.raw, b.raw)));
+
+HWY_API Vec128<float> Combine(Full128<float> /* tag */, Vec128<float, 2> hi,
+                              Vec128<float, 2> lo) {
+  return Vec128<float>(vcombine_f32(lo.raw, hi.raw));
 }
-HWY_INLINE Vec128<int64_t> ZipUpper(const Vec128<int32_t> a,
-                                    const Vec128<int32_t> b) {
-  return Vec128<int64_t>(vreinterpretq_s64_s32(vzip2q_s32(a.raw, b.raw)));
+#if HWY_ARCH_ARM_A64
+HWY_API Vec128<double> Combine(Full128<double> /* tag */, Vec128<double, 1> hi,
+                               Vec128<double, 1> lo) {
+  return Vec128<double>(vcombine_f64(lo.raw, hi.raw));
 }
+#endif
 
-// Half vectors or less
-template <size_t N, HWY_IF_LE64(uint8_t, N)>
-HWY_INLINE Vec128<uint16_t, (N + 1) / 2> ZipLower(const Vec128<uint8_t, N> a,
-                                                  const Vec128<uint8_t, N> b) {
-  return Vec128<uint16_t, (N + 1) / 2>(
-      vreinterpret_u16_u8(vzip1_u8(a.raw, b.raw)));
-}
-template <size_t N, HWY_IF_LE64(uint16_t, N)>
-HWY_INLINE Vec128<uint32_t, (N + 1) / 2> ZipLower(const Vec128<uint16_t, N> a,
-                                                  const Vec128<uint16_t, N> b) {
-  return Vec128<uint32_t, (N + 1) / 2>(
-      vreinterpret_u32_u16(vzip1_u16(a.raw, b.raw)));
+// < 64bit input, <= 64 bit result
+template <typename T, size_t N, HWY_IF_LE64(T, N)>
+HWY_API Vec128<T, N> Combine(Simd<T, N> d, Vec128<T, N / 2> hi,
+                             Vec128<T, N / 2> lo) {
+  // First double N (only lower halves will be used).
+  const Vec128<T, N> hi2(hi.raw);
+  const Vec128<T, N> lo2(lo.raw);
+  // Repartition to two unsigned lanes (each the size of the valid input).
+  const Simd<UnsignedFromSize<N * sizeof(T) / 2>, 2> du;
+  return BitCast(d, InterleaveLower(BitCast(du, lo2), BitCast(du, hi2)));
 }
-template <size_t N, HWY_IF_LE64(uint32_t, N)>
-HWY_INLINE Vec128<uint64_t, (N + 1) / 2> ZipLower(const Vec128<uint32_t, N> a,
-                                                  const Vec128<uint32_t, N> b) {
-  return Vec128<uint64_t, (N + 1) / 2>(
-      vreinterpret_u64_u32(vzip1_u32(a.raw, b.raw)));
+
+// ------------------------------ ZeroExtendVector (Combine)
+
+template <typename T, size_t N>
+HWY_API Vec128<T, N> ZeroExtendVector(Simd<T, N> d, Vec128<T, N / 2> lo) {
+  return Combine(d, Zero(Half<decltype(d)>()), lo);
 }
 
-template <size_t N, HWY_IF_LE64(int8_t, N)>
-HWY_INLINE Vec128<int16_t, (N + 1) / 2> ZipLower(const Vec128<int8_t, N> a,
-                                                 const Vec128<int8_t, N> b) {
-  return Vec128<int16_t, (N + 1) / 2>(
-      vreinterpret_s16_s8(vzip1_s8(a.raw, b.raw)));
+// ------------------------------ ConcatLowerLower
+
+// 64 or 128-bit input: just interleave
+template <typename T, size_t N, HWY_IF_GE64(T, N)>
+HWY_API Vec128<T, N> ConcatLowerLower(const Simd<T, N> d, Vec128<T, N> hi,
+                                      Vec128<T, N> lo) {
+  // Treat half-width input as a single lane and interleave them.
+  const Repartition<UnsignedFromSize<N * sizeof(T) / 2>, decltype(d)> du;
+  return BitCast(d, InterleaveLower(BitCast(du, lo), BitCast(du, hi)));
 }
-template <size_t N, HWY_IF_LE64(int16_t, N)>
-HWY_INLINE Vec128<int32_t, (N + 1) / 2> ZipLower(const Vec128<int16_t, N> a,
-                                                 const Vec128<int16_t, N> b) {
-  return Vec128<int32_t, (N + 1) / 2>(
-      vreinterpret_s32_s16(vzip1_s16(a.raw, b.raw)));
+
+#if HWY_ARCH_ARM_A64
+namespace detail {
+
+HWY_INLINE Vec128<uint8_t, 2> ConcatEven(Vec128<uint8_t, 2> hi,
+                                         Vec128<uint8_t, 2> lo) {
+  return Vec128<uint8_t, 2>(vtrn1_u8(lo.raw, hi.raw));
 }
-template <size_t N, HWY_IF_LE64(int32_t, N)>
-HWY_INLINE Vec128<int64_t, (N + 1) / 2> ZipLower(const Vec128<int32_t, N> a,
-                                                 const Vec128<int32_t, N> b) {
-  return Vec128<int64_t, (N + 1) / 2>(
-      vreinterpret_s64_s32(vzip1_s32(a.raw, b.raw)));
+HWY_INLINE Vec128<uint16_t, 2> ConcatEven(Vec128<uint16_t, 2> hi,
+                                          Vec128<uint16_t, 2> lo) {
+  return Vec128<uint16_t, 2>(vtrn1_u16(lo.raw, hi.raw));
 }
 
-template <size_t N, HWY_IF_LE64(uint8_t, N)>
-HWY_INLINE Vec128<uint16_t, N / 2> ZipUpper(const Vec128<uint8_t, N> a,
-                                            const Vec128<uint8_t, N> b) {
-  return Vec128<uint16_t, N / 2>(vreinterpret_u16_u8(vzip2_u8(a.raw, b.raw)));
+}  // namespace detail
+
+// <= 32-bit input/output
+template <typename T, size_t N, HWY_IF_LE32(T, N)>
+HWY_API Vec128<T, N> ConcatLowerLower(const Simd<T, N> d, Vec128<T, N> hi,
+                                      Vec128<T, N> lo) {
+  // Treat half-width input as two lanes and take every second one.
+  const Repartition<UnsignedFromSize<N * sizeof(T) / 2>, decltype(d)> du;
+  return BitCast(d, detail::ConcatEven(BitCast(du, hi), BitCast(du, lo)));
 }
-template <size_t N, HWY_IF_LE64(uint16_t, N)>
-HWY_INLINE Vec128<uint32_t, N / 2> ZipUpper(const Vec128<uint16_t, N> a,
-                                            const Vec128<uint16_t, N> b) {
-  return Vec128<uint32_t, N / 2>(vreinterpret_u32_u16(vzip2_u16(a.raw, b.raw)));
+
+#else
+
+template <typename T, size_t N, HWY_IF_LE32(T, N)>
+HWY_API Vec128<T, N> ConcatLowerLower(const Simd<T, N> d, Vec128<T, N> hi,
+                                      Vec128<T, N> lo) {
+  const Half<decltype(d)> d2;
+  return Combine(LowerHalf(d2, hi), LowerHalf(d2, lo));
 }
-template <size_t N, HWY_IF_LE64(uint32_t, N)>
-HWY_INLINE Vec128<uint64_t, N / 2> ZipUpper(const Vec128<uint32_t, N> a,
-                                            const Vec128<uint32_t, N> b) {
-  return Vec128<uint64_t, N / 2>(vreinterpret_u64_u32(vzip2_u32(a.raw, b.raw)));
+#endif  // HWY_ARCH_ARM_A64
+
+// ------------------------------ ConcatUpperUpper
+
+// 64 or 128-bit input: just interleave
+template <typename T, size_t N, HWY_IF_GE64(T, N)>
+HWY_API Vec128<T, N> ConcatUpperUpper(const Simd<T, N> d, Vec128<T, N> hi,
+                                      Vec128<T, N> lo) {
+  // Treat half-width input as a single lane and interleave them.
+  const Repartition<UnsignedFromSize<N * sizeof(T) / 2>, decltype(d)> du;
+  return BitCast(d, InterleaveUpper(du, BitCast(du, lo), BitCast(du, hi)));
 }
 
-template <size_t N, HWY_IF_LE64(int8_t, N)>
-HWY_INLINE Vec128<int16_t, N / 2> ZipUpper(const Vec128<int8_t, N> a,
-                                           const Vec128<int8_t, N> b) {
-  return Vec128<int16_t, N / 2>(vreinterpret_s16_s8(vzip2_s8(a.raw, b.raw)));
+#if HWY_ARCH_ARM_A64
+namespace detail {
+
+HWY_INLINE Vec128<uint8_t, 2> ConcatOdd(Vec128<uint8_t, 2> hi,
+                                        Vec128<uint8_t, 2> lo) {
+  return Vec128<uint8_t, 2>(vtrn2_u8(lo.raw, hi.raw));
 }
-template <size_t N, HWY_IF_LE64(int16_t, N)>
-HWY_INLINE Vec128<int32_t, N / 2> ZipUpper(const Vec128<int16_t, N> a,
-                                           const Vec128<int16_t, N> b) {
-  return Vec128<int32_t, N / 2>(vreinterpret_s32_s16(vzip2_s16(a.raw, b.raw)));
+HWY_INLINE Vec128<uint16_t, 2> ConcatOdd(Vec128<uint16_t, 2> hi,
+                                         Vec128<uint16_t, 2> lo) {
+  return Vec128<uint16_t, 2>(vtrn2_u16(lo.raw, hi.raw));
 }
-template <size_t N, HWY_IF_LE64(int32_t, N)>
-HWY_INLINE Vec128<int64_t, N / 2> ZipUpper(const Vec128<int32_t, N> a,
-                                           const Vec128<int32_t, N> b) {
-  return Vec128<int64_t, N / 2>(vreinterpret_s64_s32(vzip2_s32(a.raw, b.raw)));
+
+}  // namespace detail
+
+// <= 32-bit input/output
+template <typename T, size_t N, HWY_IF_LE32(T, N)>
+HWY_API Vec128<T, N> ConcatUpperUpper(const Simd<T, N> d, Vec128<T, N> hi,
+                                      Vec128<T, N> lo) {
+  // Treat half-width input as two lanes and take every second one.
+  const Repartition<UnsignedFromSize<N * sizeof(T) / 2>, decltype(d)> du;
+  return BitCast(d, detail::ConcatOdd(BitCast(du, hi), BitCast(du, lo)));
 }
 
-// ------------------------------ Blocks
+#else
 
-// hiH,hiL loH,loL |-> hiL,loL (= lower halves)
-template <typename T>
-HWY_INLINE Vec128<T> ConcatLowerLower(const Vec128<T> hi, const Vec128<T> lo) {
-  const Full128<uint64_t> d64;
-  return BitCast(Full128<T>(),
-                 InterleaveLower(BitCast(d64, lo), BitCast(d64, hi)));
+template <typename T, size_t N, HWY_IF_LE32(T, N)>
+HWY_API Vec128<T, N> ConcatUpperUpper(const Simd<T, N> d, Vec128<T, N> hi,
+                                      Vec128<T, N> lo) {
+  const Half<decltype(d)> d2;
+  return Combine(UpperHalf(d2, hi), UpperHalf(d2, lo));
 }
 
-// hiH,hiL loH,loL |-> hiH,loH (= upper halves)
-template <typename T>
-HWY_INLINE Vec128<T> ConcatUpperUpper(const Vec128<T> hi, const Vec128<T> lo) {
-  const Full128<uint64_t> d64;
-  return BitCast(Full128<T>(),
-                 InterleaveUpper(BitCast(d64, lo), BitCast(d64, hi)));
+#endif  // HWY_ARCH_ARM_A64
+
+// ------------------------------ ConcatLowerUpper (ShiftLeftBytes)
+
+// 64 or 128-bit input: extract from concatenated
+template <typename T, size_t N, HWY_IF_GE64(T, N)>
+HWY_API Vec128<T, N> ConcatLowerUpper(const Simd<T, N> d, Vec128<T, N> hi,
+                                      Vec128<T, N> lo) {
+  return CombineShiftRightBytes<N * sizeof(T) / 2>(d, hi, lo);
 }
 
-// hiH,hiL loH,loL |-> hiL,loH (= inner halves)
-template <typename T>
-HWY_INLINE Vec128<T> ConcatLowerUpper(const Vec128<T> hi, const Vec128<T> lo) {
-  return CombineShiftRightBytes<8>(hi, lo);
+// <= 32-bit input/output
+template <typename T, size_t N, HWY_IF_LE32(T, N)>
+HWY_API Vec128<T, N> ConcatLowerUpper(const Simd<T, N> d, Vec128<T, N> hi,
+                                      Vec128<T, N> lo) {
+  constexpr size_t kSize = N * sizeof(T);
+  const Repartition<uint8_t, decltype(d)> d8;
+  const Simd<uint8_t, 8> d8x8;
+  const Simd<T, 8 / sizeof(T)> d64;
+  using V8x8 = VFromD<decltype(d8x8)>;
+  const V8x8 hi8x8(BitCast(d8, hi).raw);
+  // Move into most-significant bytes
+  const V8x8 lo8x8 = ShiftLeftBytes<8 - kSize>(V8x8(BitCast(d8, lo).raw));
+  const V8x8 r = CombineShiftRightBytes<8 - kSize / 2>(d8x8, hi8x8, lo8x8);
+  // Back to original lane type, then shrink N.
+  return Vec128<T, N>(BitCast(d64, r).raw);
 }
 
-// hiH,hiL loH,loL |-> hiH,loL (= outer halves)
-template <typename T>
-HWY_INLINE Vec128<T> ConcatUpperLower(const Vec128<T> hi, const Vec128<T> lo) {
-  // TODO(janwas): more efficient implementation?
-  alignas(16) const uint8_t kBytes[16] = {
-      0xFF, 0xFF, 0xFF, 0xFF, 0xFF, 0xFF, 0xFF, 0xFF, 0, 0, 0, 0, 0, 0, 0, 0};
-  const auto vec = BitCast(Full128<T>(), Load(Full128<uint8_t>(), kBytes));
-  return IfThenElse(MaskFromVec(vec), lo, hi);
+// ------------------------------ ConcatUpperLower
+
+// Works for all N.
+template <typename T, size_t N>
+HWY_API Vec128<T, N> ConcatUpperLower(Simd<T, N> d, Vec128<T, N> hi,
+                                      Vec128<T, N> lo) {
+  return IfThenElse(FirstN(d, Lanes(d) / 2), lo, hi);
 }
 
-// ------------------------------ Odd/even lanes
+// ------------------------------ OddEven (IfThenElse)
 
-template <typename T>
-HWY_INLINE Vec128<T> OddEven(const Vec128<T> a, const Vec128<T> b) {
+template <typename T, size_t N>
+HWY_API Vec128<T, N> OddEven(const Vec128<T, N> a, const Vec128<T, N> b) {
+  const Simd<T, N> d;
+  const Repartition<uint8_t, decltype(d)> d8;
   alignas(16) constexpr uint8_t kBytes[16] = {
       ((0 / sizeof(T)) & 1) ? 0 : 0xFF,  ((1 / sizeof(T)) & 1) ? 0 : 0xFF,
       ((2 / sizeof(T)) & 1) ? 0 : 0xFF,  ((3 / sizeof(T)) & 1) ? 0 : 0xFF,
@@ -3455,12 +3705,107 @@ HWY_INLINE Vec128<T> OddEven(const Vec128<T> a, const Vec128<T> b) {
       ((12 / sizeof(T)) & 1) ? 0 : 0xFF, ((13 / sizeof(T)) & 1) ? 0 : 0xFF,
       ((14 / sizeof(T)) & 1) ? 0 : 0xFF, ((15 / sizeof(T)) & 1) ? 0 : 0xFF,
   };
-  const auto vec = BitCast(Full128<T>(), Load(Full128<uint8_t>(), kBytes));
+  const auto vec = BitCast(d, Load(d8, kBytes));
   return IfThenElse(MaskFromVec(vec), b, a);
 }
 
+// ================================================== CRYPTO
+
+#if defined(__ARM_FEATURE_AES)
+
+// Per-target flag to prevent generic_ops-inl.h from defining AESRound.
+#ifdef HWY_NATIVE_AES
+#undef HWY_NATIVE_AES
+#else
+#define HWY_NATIVE_AES
+#endif
+
+HWY_API Vec128<uint8_t> AESRound(Vec128<uint8_t> state,
+                                 Vec128<uint8_t> round_key) {
+  // NOTE: it is important that AESE and AESMC be consecutive instructions so
+  // they can be fused. AESE includes AddRoundKey, which is a different ordering
+  // than the AES-NI semantics we adopted, so XOR by 0 and later with the actual
+  // round key (the compiler will hopefully optimize this for multiple rounds).
+  return Vec128<uint8_t>(vaesmcq_u8(vaeseq_u8(state.raw, vdupq_n_u8(0)))) ^
+         round_key;
+}
+
+HWY_API Vec128<uint64_t> CLMulLower(Vec128<uint64_t> a, Vec128<uint64_t> b) {
+  return Vec128<uint64_t>((uint64x2_t)vmull_p64(GetLane(a), GetLane(b)));
+}
+
+HWY_API Vec128<uint64_t> CLMulUpper(Vec128<uint64_t> a, Vec128<uint64_t> b) {
+  return Vec128<uint64_t>(
+      (uint64x2_t)vmull_high_p64((poly64x2_t)a.raw, (poly64x2_t)b.raw));
+}
+
+#endif  // __ARM_FEATURE_AES
+
 // ================================================== MISC
 
+// ------------------------------ TableLookupBytes (Combine, LowerHalf)
+
+// Both full
+template <typename T>
+HWY_API Vec128<T> TableLookupBytes(const Vec128<T> bytes,
+                                   const Vec128<T> from) {
+  const Full128<T> d;
+  const Repartition<uint8_t, decltype(d)> d8;
+#if HWY_ARCH_ARM_A64
+  return BitCast(d, Vec128<uint8_t>(vqtbl1q_u8(BitCast(d8, bytes).raw,
+                                               BitCast(d8, from).raw)));
+#else
+  uint8x16_t table0 = BitCast(d8, bytes).raw;
+  uint8x8x2_t table;
+  table.val[0] = vget_low_u8(table0);
+  table.val[1] = vget_high_u8(table0);
+  uint8x16_t idx = BitCast(d8, from).raw;
+  uint8x8_t low = vtbl2_u8(table, vget_low_u8(idx));
+  uint8x8_t hi = vtbl2_u8(table, vget_high_u8(idx));
+  return BitCast(d, Vec128<uint8_t>(vcombine_u8(low, hi)));
+#endif
+}
+
+// Partial index vector
+template <typename T, size_t N, HWY_IF_LE64(T, N)>
+HWY_API Vec128<T, N> TableLookupBytes(const Vec128<T> bytes,
+                                      const Vec128<T, N> from) {
+  const Full128<T> d_full;
+  const Vec128<T, 8 / sizeof(T)> from64(from.raw);
+  const auto idx_full = Combine(d_full, from64, from64);
+  const auto out_full = TableLookupBytes(bytes, idx_full);
+  return Vec128<T, N>(LowerHalf(Half<decltype(d_full)>(), out_full).raw);
+}
+
+// Partial table vector
+template <typename T, size_t N, HWY_IF_LE64(T, N)>
+HWY_API Vec128<T> TableLookupBytes(const Vec128<T, N> bytes,
+                                   const Vec128<T> from) {
+  const Full128<T> d_full;
+  return TableLookupBytes(Combine(d_full, bytes, bytes), from);
+}
+
+// Partial both
+template <typename T, size_t N, typename TI, size_t NI, HWY_IF_LE64(T, N),
+          HWY_IF_LE64(TI, NI)>
+HWY_API VFromD<Repartition<T, Simd<TI, NI>>> TableLookupBytes(
+    Vec128<T, N> bytes, Vec128<TI, NI> from) {
+  const Simd<T, N> d;
+  const Simd<TI, NI> d_idx;
+  const Repartition<uint8_t, decltype(d_idx)> d_idx8;
+  // uint8x8
+  const auto bytes8 = BitCast(Repartition<uint8_t, decltype(d)>(), bytes);
+  const auto from8 = BitCast(d_idx8, from);
+  const VFromD<decltype(d_idx8)> v8(vtbl1_u8(bytes8.raw, from8.raw));
+  return BitCast(d_idx, v8);
+}
+
+// For all vector widths; ARM anyway zeroes if >= 0x10.
+template <class V, class VI>
+HWY_API VI TableLookupBytesOr0(const V bytes, const VI from) {
+  return TableLookupBytes(bytes, from);
+}
+
 // ------------------------------ Scatter (Store)
 
 template <typename T, size_t N, typename Offset, HWY_IF_LE128(T, N)>
@@ -3536,33 +3881,33 @@ namespace detail {
 
 // N=1 for any T: no-op
 template <typename T>
-HWY_API Vec128<T, 1> SumOfLanes(const Vec128<T, 1> v) {
+HWY_INLINE Vec128<T, 1> SumOfLanes(const Vec128<T, 1> v) {
   return v;
 }
 template <typename T>
-HWY_API Vec128<T, 1> MinOfLanes(hwy::SizeTag<sizeof(T)> /* tag */,
-                                const Vec128<T, 1> v) {
+HWY_INLINE Vec128<T, 1> MinOfLanes(hwy::SizeTag<sizeof(T)> /* tag */,
+                                   const Vec128<T, 1> v) {
   return v;
 }
 template <typename T>
-HWY_API Vec128<T, 1> MaxOfLanes(hwy::SizeTag<sizeof(T)> /* tag */,
-                                const Vec128<T, 1> v) {
+HWY_INLINE Vec128<T, 1> MaxOfLanes(hwy::SizeTag<sizeof(T)> /* tag */,
+                                   const Vec128<T, 1> v) {
   return v;
 }
 
 // u32/i32/f32: N=2
 template <typename T, HWY_IF_LANE_SIZE(T, 4)>
-HWY_API Vec128<T, 2> SumOfLanes(const Vec128<T, 2> v10) {
+HWY_INLINE Vec128<T, 2> SumOfLanes(const Vec128<T, 2> v10) {
   return v10 + Shuffle2301(v10);
 }
 template <typename T>
-HWY_API Vec128<T, 2> MinOfLanes(hwy::SizeTag<4> /* tag */,
-                                const Vec128<T, 2> v10) {
+HWY_INLINE Vec128<T, 2> MinOfLanes(hwy::SizeTag<4> /* tag */,
+                                   const Vec128<T, 2> v10) {
   return Min(v10, Shuffle2301(v10));
 }
 template <typename T>
-HWY_API Vec128<T, 2> MaxOfLanes(hwy::SizeTag<4> /* tag */,
-                                const Vec128<T, 2> v10) {
+HWY_INLINE Vec128<T, 2> MaxOfLanes(hwy::SizeTag<4> /* tag */,
+                                   const Vec128<T, 2> v10) {
   return Max(v10, Shuffle2301(v10));
 }
 
@@ -3607,22 +3952,24 @@ HWY_INLINE Vec128<float> SumOfLanes(const Vec128<float> v) {
   return Vec128<float>(vaddq_f32(v1.val[0], v1.val[1]));
 }
 HWY_INLINE Vec128<uint64_t> SumOfLanes(const Vec128<uint64_t> v) {
-  return v + CombineShiftRightBytes<8>(v, v);
+  return v + Shuffle01(v);
 }
 HWY_INLINE Vec128<int64_t> SumOfLanes(const Vec128<int64_t> v) {
-  return v + CombineShiftRightBytes<8>(v, v);
+  return v + Shuffle01(v);
 }
 #endif
 
 template <typename T>
-HWY_API Vec128<T> MinOfLanes(hwy::SizeTag<4> /* tag */, const Vec128<T> v3210) {
+HWY_INLINE Vec128<T> MinOfLanes(hwy::SizeTag<4> /* tag */,
+                                const Vec128<T> v3210) {
   const Vec128<T> v1032 = Shuffle1032(v3210);
   const Vec128<T> v31_20_31_20 = Min(v3210, v1032);
   const Vec128<T> v20_31_20_31 = Shuffle0321(v31_20_31_20);
   return Min(v20_31_20_31, v31_20_31_20);
 }
 template <typename T>
-HWY_API Vec128<T> MaxOfLanes(hwy::SizeTag<4> /* tag */, const Vec128<T> v3210) {
+HWY_INLINE Vec128<T> MaxOfLanes(hwy::SizeTag<4> /* tag */,
+                                const Vec128<T> v3210) {
   const Vec128<T> v1032 = Shuffle1032(v3210);
   const Vec128<T> v31_20_31_20 = Max(v3210, v1032);
   const Vec128<T> v20_31_20_31 = Shuffle0321(v31_20_31_20);
@@ -3631,12 +3978,14 @@ HWY_API Vec128<T> MaxOfLanes(hwy::SizeTag<4> /* tag */, const Vec128<T> v3210) {
 
 // For u64/i64[/f64].
 template <typename T>
-HWY_API Vec128<T> MinOfLanes(hwy::SizeTag<8> /* tag */, const Vec128<T> v10) {
+HWY_INLINE Vec128<T> MinOfLanes(hwy::SizeTag<8> /* tag */,
+                                const Vec128<T> v10) {
   const Vec128<T> v01 = Shuffle01(v10);
   return Min(v10, v01);
 }
 template <typename T>
-HWY_API Vec128<T> MaxOfLanes(hwy::SizeTag<8> /* tag */, const Vec128<T> v10) {
+HWY_INLINE Vec128<T> MaxOfLanes(hwy::SizeTag<8> /* tag */,
+                                const Vec128<T> v10) {
   const Vec128<T> v01 = Shuffle01(v10);
   return Max(v10, v01);
 }
@@ -3644,15 +3993,15 @@ HWY_API Vec128<T> MaxOfLanes(hwy::SizeTag<8> /* tag */, const Vec128<T> v10) {
 }  // namespace detail
 
 template <typename T, size_t N>
-HWY_API Vec128<T, N> SumOfLanes(const Vec128<T, N> v) {
+HWY_API Vec128<T, N> SumOfLanes(Simd<T, N> /* tag */, const Vec128<T, N> v) {
   return detail::SumOfLanes(v);
 }
 template <typename T, size_t N>
-HWY_API Vec128<T, N> MinOfLanes(const Vec128<T, N> v) {
+HWY_API Vec128<T, N> MinOfLanes(Simd<T, N> /* tag */, const Vec128<T, N> v) {
   return detail::MinOfLanes(hwy::SizeTag<sizeof(T)>(), v);
 }
 template <typename T, size_t N>
-HWY_API Vec128<T, N> MaxOfLanes(const Vec128<T, N> v) {
+HWY_API Vec128<T, N> MaxOfLanes(Simd<T, N> /* tag */, const Vec128<T, N> v) {
   return detail::MaxOfLanes(hwy::SizeTag<sizeof(T)>(), v);
 }
 
@@ -3882,18 +4231,26 @@ HWY_INLINE size_t CountTrue(hwy::SizeTag<8> /*tag*/, const Mask128<T> mask) {
 
 // Full
 template <typename T>
-HWY_INLINE size_t CountTrue(const Mask128<T> mask) {
+HWY_API size_t CountTrue(Full128<T> /* tag */, const Mask128<T> mask) {
   return detail::CountTrue(hwy::SizeTag<sizeof(T)>(), mask);
 }
 
 // Partial
 template <typename T, size_t N, HWY_IF_LE64(T, N)>
-HWY_INLINE size_t CountTrue(const Mask128<T, N> mask) {
+HWY_API size_t CountTrue(Simd<T, N> /* tag */, const Mask128<T, N> mask) {
   return PopCount(detail::BitsFromMask(mask));
 }
 
 template <typename T, size_t N>
-HWY_INLINE size_t StoreMaskBits(const Mask128<T, N> mask, uint8_t* p) {
+HWY_API intptr_t FindFirstTrue(const Simd<T, N> /* tag */,
+                              const Mask128<T, N> mask) {
+  const uint64_t bits = detail::BitsFromMask(mask);
+  return bits ? Num0BitsBelowLS1Bit_Nonzero64(bits) : -1;
+}
+
+template <typename T, size_t N>
+HWY_API size_t StoreMaskBits(Simd<T, N> /* tag */, const Mask128<T, N> mask,
+                             uint8_t* p) {
   const uint64_t bits = detail::BitsFromMask(mask);
   const size_t kNumBytes = (N + 7) / 8;
   CopyBytes<kNumBytes>(&bits, p);
@@ -3902,13 +4259,13 @@ HWY_INLINE size_t StoreMaskBits(const Mask128<T, N> mask, uint8_t* p) {
 
 // Full
 template <typename T>
-HWY_INLINE bool AllFalse(const Mask128<T> m) {
+HWY_API bool AllFalse(const Full128<T> d, const Mask128<T> m) {
 #if HWY_ARCH_ARM_A64
   const Full128<uint32_t> d32;
-  const auto m32 = MaskFromVec(BitCast(d32, VecFromMask(Full128<T>(), m)));
+  const auto m32 = MaskFromVec(BitCast(d32, VecFromMask(d, m)));
   return (vmaxvq_u32(m32.raw) == 0);
 #else
-  const auto v64 = BitCast(Full128<uint64_t>(), VecFromMask(Full128<T>(), m));
+  const auto v64 = BitCast(Full128<uint64_t>(), VecFromMask(d, m));
   uint32x2_t a = vqmovn_u64(v64.raw);
   return vget_lane_u64(vreinterpret_u64_u32(a), 0) == 0;
 #endif
@@ -3916,13 +4273,12 @@ HWY_INLINE bool AllFalse(const Mask128<T> m) {
 
 // Partial
 template <typename T, size_t N, HWY_IF_LE64(T, N)>
-HWY_INLINE bool AllFalse(const Mask128<T, N> m) {
+HWY_API bool AllFalse(const Simd<T, N> /* tag */, const Mask128<T, N> m) {
   return detail::BitsFromMask(m) == 0;
 }
 
 template <typename T, size_t N>
-HWY_INLINE bool AllTrue(const Mask128<T, N> m) {
-  const Simd<T, N> d;
+HWY_API bool AllTrue(const Simd<T, N> d, const Mask128<T, N> m) {
   return AllFalse(VecFromMask(d, m) == Zero(d));
 }
 
@@ -4134,7 +4490,7 @@ HWY_INLINE Vec128<T, N> IdxFromBits(hwy::SizeTag<8> /*tag*/,
 // Helper function called by both Compress and CompressStore - avoids a
 // redundant BitsFromMask in the latter.
 template <typename T, size_t N>
-HWY_API Vec128<T, N> Compress(Vec128<T, N> v, const uint64_t mask_bits) {
+HWY_INLINE Vec128<T, N> Compress(Vec128<T, N> v, const uint64_t mask_bits) {
   const auto idx =
       detail::IdxFromBits<T, N>(hwy::SizeTag<sizeof(T)>(), mask_bits);
   using D = Simd<T, N>;
@@ -4232,6 +4588,102 @@ HWY_API void StoreInterleaved4(const Vec128<uint8_t, N> v0,
   CopyBytes<N * 4>(buf, unaligned);
 }
 
+// ================================================== DEPRECATED
+
+template <typename T, size_t N>
+HWY_API size_t StoreMaskBits(const Mask128<T, N> mask, uint8_t* p) {
+  return StoreMaskBits(Simd<T, N>(), mask, p);
+}
+
+template <typename T, size_t N>
+HWY_API bool AllTrue(const Mask128<T, N> mask) {
+  return AllTrue(Simd<T, N>(), mask);
+}
+
+template <typename T, size_t N>
+HWY_API bool AllFalse(const Mask128<T, N> mask) {
+  return AllFalse(Simd<T, N>(), mask);
+}
+
+template <typename T, size_t N>
+HWY_API size_t CountTrue(const Mask128<T, N> mask) {
+  return CountTrue(Simd<T, N>(), mask);
+}
+
+template <typename T, size_t N>
+HWY_API Vec128<T, N> SumOfLanes(const Vec128<T, N> v) {
+  return SumOfLanes(Simd<T, N>(), v);
+}
+template <typename T, size_t N>
+HWY_API Vec128<T, N> MinOfLanes(const Vec128<T, N> v) {
+  return MinOfLanes(Simd<T, N>(), v);
+}
+template <typename T, size_t N>
+HWY_API Vec128<T, N> MaxOfLanes(const Vec128<T, N> v) {
+  return MaxOfLanes(Simd<T, N>(), v);
+}
+
+template <typename T, size_t N>
+HWY_API Vec128<T, (N + 1) / 2> UpperHalf(Vec128<T, N> v) {
+  return UpperHalf(Half<Simd<T, N>>(), v);
+}
+
+template <int kBytes, typename T, size_t N>
+HWY_API Vec128<T, N> ShiftRightBytes(const Vec128<T, N> v) {
+  return ShiftRightBytes<kBytes>(Simd<T, N>(), v);
+}
+
+template <int kLanes, typename T, size_t N>
+HWY_API Vec128<T, N> ShiftRightLanes(const Vec128<T, N> v) {
+  return ShiftRightLanes<kLanes>(Simd<T, N>(), v);
+}
+
+template <size_t kBytes, typename T, size_t N>
+HWY_API Vec128<T, N> CombineShiftRightBytes(Vec128<T, N> hi, Vec128<T, N> lo) {
+  return CombineShiftRightBytes<kBytes>(Simd<T, N>(), hi, lo);
+}
+
+template <typename T, size_t N>
+HWY_API Vec128<T, N> InterleaveUpper(Vec128<T, N> a, Vec128<T, N> b) {
+  return InterleaveUpper(Simd<T, N>(), a, b);
+}
+
+template <typename T, size_t N, class D = Simd<T, N>>
+HWY_API VFromD<RepartitionToWide<D>> ZipUpper(Vec128<T, N> a, Vec128<T, N> b) {
+  return InterleaveUpper(RepartitionToWide<D>(), a, b);
+}
+
+template <typename T, size_t N2>
+HWY_API Vec128<T, N2 * 2> Combine(Vec128<T, N2> hi2, Vec128<T, N2> lo2) {
+  return Combine(Simd<T, N2 * 2>(), hi2, lo2);
+}
+
+template <typename T, size_t N2, HWY_IF_LE64(T, N2)>
+HWY_API Vec128<T, N2 * 2> ZeroExtendVector(Vec128<T, N2> lo) {
+  return ZeroExtendVector(Simd<T, N2 * 2>(), lo);
+}
+
+template <typename T, size_t N>
+HWY_API Vec128<T, N> ConcatLowerLower(Vec128<T, N> hi, Vec128<T, N> lo) {
+  return ConcatLowerLower(Simd<T, N>(), hi, lo);
+}
+
+template <typename T, size_t N>
+HWY_API Vec128<T, N> ConcatUpperUpper(Vec128<T, N> hi, Vec128<T, N> lo) {
+  return ConcatUpperUpper(Simd<T, N>(), hi, lo);
+}
+
+template <typename T, size_t N>
+HWY_API Vec128<T, N> ConcatLowerUpper(const Vec128<T, N> hi,
+                                      const Vec128<T, N> lo) {
+  return ConcatLowerUpper(Simd<T, N>(), hi, lo);
+}
+
+template <typename T, size_t N>
+HWY_API Vec128<T, N> ConcatUpperLower(Vec128<T, N> hi, Vec128<T, N> lo) {
+  return ConcatUpperLower(Simd<T, N>(), hi, lo);
+}
+
 // ================================================== Operator wrapper
 
 // These apply to all x86_*-inl.h because there are no restrictions on V.
@@ -4268,6 +4720,10 @@ HWY_API auto Eq(V a, V b) -> decltype(a == b) {
   return a == b;
 }
 template <class V>
+HWY_API auto Ne(V a, V b) -> decltype(a == b) {
+  return a != b;
+}
+template <class V>
 HWY_API auto Lt(V a, V b) -> decltype(a == b) {
   return a < b;
 }
diff --git a/third_party/highway/hwy/ops/arm_sve-inl.h b/third_party/highway/hwy/ops/arm_sve-inl.h
index 3d57d3586689d..81db96dbd6626 100644
--- a/third_party/highway/hwy/ops/arm_sve-inl.h
+++ b/third_party/highway/hwy/ops/arm_sve-inl.h
@@ -15,10 +15,15 @@
 // ARM SVE[2] vectors (length not known at compile time).
 // External include guard in highway.h - see comment there.
 
-#include <arm_sve.h>
 #include <stddef.h>
 #include <stdint.h>
 
+#if defined(HWY_EMULATE_SVE)
+#include "third_party/farm_sve/farm_sve.h"
+#else
+#include <arm_sve.h>
+#endif
+
 #include "hwy/base.h"
 #include "hwy/ops/shared-inl.h"
 
@@ -26,18 +31,22 @@ HWY_BEFORE_NAMESPACE();
 namespace hwy {
 namespace HWY_NAMESPACE {
 
+// SVE only supports fractions, not LMUL > 1.
+template <typename T, int kShift = 0>
+using Full = Simd<T, (kShift <= 0) ? (HWY_LANES(T) >> (-kShift)) : 0>;
+
 template <class V>
 struct DFromV_t {};  // specialized in macros
 template <class V>
-using DFromV = typename DFromV_t<V>::type;
+using DFromV = typename DFromV_t<RemoveConst<V>>::type;
 
 template <class V>
 using TFromV = TFromD<DFromV<V>>;
 
-#define HWY_IF_UNSIGNED_V(V) hwy::EnableIf<!IsSigned<TFromV<V>>()>* = nullptr
-#define HWY_IF_SIGNED_V(V) \
-  hwy::EnableIf<IsSigned<TFromV<V>>() && !IsFloat<TFromV<V>>()>* = nullptr
-#define HWY_IF_FLOAT_V(V) hwy::EnableIf<IsFloat<TFromV<V>>()>* = nullptr
+#define HWY_IF_UNSIGNED_V(V) HWY_IF_UNSIGNED(TFromV<V>)
+#define HWY_IF_SIGNED_V(V) HWY_IF_SIGNED(TFromV<V>)
+#define HWY_IF_FLOAT_V(V) HWY_IF_FLOAT(TFromV<V>)
+#define HWY_IF_LANE_SIZE_V(V, bytes) HWY_IF_LANE_SIZE(TFromV<V>, bytes)
 
 // ================================================== MACROS
 
@@ -53,10 +62,10 @@ namespace detail {  // for code folding
 #define HWY_SVE_FOREACH_U64(X_MACRO, NAME, OP) X_MACRO(uint, u, 64, NAME, OP)
 
 // Signed:
-#define HWY_SVE_FOREACH_I08(X_MACRO, NAME, OP) X_MACRO(int, i, 8, NAME, OP)
-#define HWY_SVE_FOREACH_I16(X_MACRO, NAME, OP) X_MACRO(int, i, 16, NAME, OP)
-#define HWY_SVE_FOREACH_I32(X_MACRO, NAME, OP) X_MACRO(int, i, 32, NAME, OP)
-#define HWY_SVE_FOREACH_I64(X_MACRO, NAME, OP) X_MACRO(int, i, 64, NAME, OP)
+#define HWY_SVE_FOREACH_I08(X_MACRO, NAME, OP) X_MACRO(int, s, 8, NAME, OP)
+#define HWY_SVE_FOREACH_I16(X_MACRO, NAME, OP) X_MACRO(int, s, 16, NAME, OP)
+#define HWY_SVE_FOREACH_I32(X_MACRO, NAME, OP) X_MACRO(int, s, 32, NAME, OP)
+#define HWY_SVE_FOREACH_I64(X_MACRO, NAME, OP) X_MACRO(int, s, 64, NAME, OP)
 
 // Float:
 #define HWY_SVE_FOREACH_F16(X_MACRO, NAME, OP) X_MACRO(float, f, 16, NAME, OP)
@@ -82,6 +91,10 @@ namespace detail {  // for code folding
   HWY_SVE_FOREACH_F64(X_MACRO, NAME, OP)
 
 // Commonly used type categories for a given element size:
+#define HWY_SVE_FOREACH_UI08(X_MACRO, NAME, OP) \
+  HWY_SVE_FOREACH_U08(X_MACRO, NAME, OP)        \
+  HWY_SVE_FOREACH_I08(X_MACRO, NAME, OP)
+
 #define HWY_SVE_FOREACH_UI16(X_MACRO, NAME, OP) \
   HWY_SVE_FOREACH_U16(X_MACRO, NAME, OP)        \
   HWY_SVE_FOREACH_I16(X_MACRO, NAME, OP)
@@ -94,11 +107,21 @@ namespace detail {  // for code folding
   HWY_SVE_FOREACH_U64(X_MACRO, NAME, OP)        \
   HWY_SVE_FOREACH_I64(X_MACRO, NAME, OP)
 
+#define HWY_SVE_FOREACH_UIF3264(X_MACRO, NAME, OP) \
+  HWY_SVE_FOREACH_UI32(X_MACRO, NAME, OP)          \
+  HWY_SVE_FOREACH_UI64(X_MACRO, NAME, OP)          \
+  HWY_SVE_FOREACH_F32(X_MACRO, NAME, OP)           \
+  HWY_SVE_FOREACH_F64(X_MACRO, NAME, OP)
+
 // Commonly used type categories:
 #define HWY_SVE_FOREACH_UI(X_MACRO, NAME, OP) \
   HWY_SVE_FOREACH_U(X_MACRO, NAME, OP)        \
   HWY_SVE_FOREACH_I(X_MACRO, NAME, OP)
 
+#define HWY_SVE_FOREACH_IF(X_MACRO, NAME, OP) \
+  HWY_SVE_FOREACH_I(X_MACRO, NAME, OP)        \
+  HWY_SVE_FOREACH_F(X_MACRO, NAME, OP)
+
 #define HWY_SVE_FOREACH(X_MACRO, NAME, OP) \
   HWY_SVE_FOREACH_U(X_MACRO, NAME, OP)     \
   HWY_SVE_FOREACH_I(X_MACRO, NAME, OP)     \
@@ -106,135 +129,213 @@ namespace detail {  // for code folding
 
 // Assemble types for use in x-macros
 #define HWY_SVE_T(BASE, BITS) BASE##BITS##_t
-#define HWY_SVE_D(CHAR, BITS) D##CHAR##BITS
+#define HWY_SVE_D(BASE, BITS, N) Simd<HWY_SVE_T(BASE, BITS), N>
 #define HWY_SVE_V(BASE, BITS) sv##BASE##BITS##_t
 
 }  // namespace detail
 
-// TODO(janwas): remove typedefs and only use HWY_SVE_V etc. directly
-
-#define HWY_SPECIALIZE(BASE, CHAR, BITS, NAME, OP)                   \
-  using HWY_SVE_D(CHAR, BITS) =                                      \
-      Simd<HWY_SVE_T(BASE, BITS), HWY_LANES(HWY_SVE_T(BASE, BITS))>; \
-  using V##CHAR##BITS = HWY_SVE_V(BASE, BITS);                       \
-  template <>                                                        \
-  struct DFromV_t<HWY_SVE_V(BASE, BITS)> {                           \
-    using Lane = HWY_SVE_T(BASE, BITS);                              \
-    using type = Simd<Lane, HWY_LANES(Lane)>;                        \
+#define HWY_SPECIALIZE(BASE, CHAR, BITS, NAME, OP)                        \
+  template <>                                                             \
+  struct DFromV_t<HWY_SVE_V(BASE, BITS)> {                                \
+    using type = HWY_SVE_D(BASE, BITS, HWY_LANES(HWY_SVE_T(BASE, BITS))); \
   };
-using Vf16 = svfloat16_t;
-using Df16 = Simd<float16_t, HWY_LANES(float16_t)>;
 
 HWY_SVE_FOREACH(HWY_SPECIALIZE, _, _)
 #undef HWY_SPECIALIZE
 
-// vector = f(d), e.g. Zero
-#define HWY_SVE_RETV_ARGD(BASE, CHAR, BITS, NAME, OP)           \
-  HWY_API HWY_SVE_V(BASE, BITS) NAME(HWY_SVE_D(CHAR, BITS) d) { \
-    (void)Lanes(d);                                             \
-    return v##OP##_##CHAR##BITS();                              \
+// vector = f(d), e.g. Undefined
+#define HWY_SVE_RETV_ARGD(BASE, CHAR, BITS, NAME, OP)              \
+  template <size_t N>                                              \
+  HWY_API HWY_SVE_V(BASE, BITS) NAME(HWY_SVE_D(BASE, BITS, N) d) { \
+    return sv##OP##_##CHAR##BITS();                                \
   }
 
+// Note: _x (don't-care value for inactive lanes) avoids additional MOVPRFX
+// instructions, and we anyway only use it when the predicate is ptrue.
+
 // vector = f(vector), e.g. Not
+#define HWY_SVE_RETV_ARGPV(BASE, CHAR, BITS, NAME, OP)          \
+  HWY_API HWY_SVE_V(BASE, BITS) NAME(HWY_SVE_V(BASE, BITS) v) { \
+    return sv##OP##_##CHAR##BITS##_x(HWY_SVE_PTRUE(BITS), v);   \
+  }
 #define HWY_SVE_RETV_ARGV(BASE, CHAR, BITS, NAME, OP)           \
   HWY_API HWY_SVE_V(BASE, BITS) NAME(HWY_SVE_V(BASE, BITS) v) { \
-    return v##OP##_v_##CHAR##BITS(v);                           \
+    return sv##OP##_##CHAR##BITS(v);                            \
   }
 
-// vector = f(vector, scalar), e.g. detail::Add
-#define HWY_SVE_RETV_ARGVS(BASE, CHAR, BITS, NAME, OP)         \
+// vector = f(vector, scalar), e.g. detail::AddK
+#define HWY_SVE_RETV_ARGPVN(BASE, CHAR, BITS, NAME, OP)          \
+  HWY_API HWY_SVE_V(BASE, BITS)                                  \
+      NAME(HWY_SVE_V(BASE, BITS) a, HWY_SVE_T(BASE, BITS) b) {   \
+    return sv##OP##_##CHAR##BITS##_x(HWY_SVE_PTRUE(BITS), a, b); \
+  }
+#define HWY_SVE_RETV_ARGVN(BASE, CHAR, BITS, NAME, OP)         \
   HWY_API HWY_SVE_V(BASE, BITS)                                \
       NAME(HWY_SVE_V(BASE, BITS) a, HWY_SVE_T(BASE, BITS) b) { \
-    return v##OP##_##CHAR##BITS(a, b);                         \
+    return sv##OP##_##CHAR##BITS(a, b);                        \
   }
 
 // vector = f(vector, vector), e.g. Add
+#define HWY_SVE_RETV_ARGPVV(BASE, CHAR, BITS, NAME, OP)          \
+  HWY_API HWY_SVE_V(BASE, BITS)                                  \
+      NAME(HWY_SVE_V(BASE, BITS) a, HWY_SVE_V(BASE, BITS) b) {   \
+    return sv##OP##_##CHAR##BITS##_x(HWY_SVE_PTRUE(BITS), a, b); \
+  }
 #define HWY_SVE_RETV_ARGVV(BASE, CHAR, BITS, NAME, OP)         \
   HWY_API HWY_SVE_V(BASE, BITS)                                \
       NAME(HWY_SVE_V(BASE, BITS) a, HWY_SVE_V(BASE, BITS) b) { \
-    return v##OP##_vv_##CHAR##BITS(a, b);                      \
+    return sv##OP##_##CHAR##BITS(a, b);                        \
   }
 
-// ================================================== INIT
-
 // ------------------------------ Lanes
 
-// WARNING: we want to query VLMAX/sizeof(T), but this actually changes VL!
-// vlenb is not exposed through intrinsics and vreadvl is not VLMAX.
-#define HWY_SVE_LANES(BASE, CHAR, BITS, NAME, OP) \
-  HWY_API size_t NAME(HWY_SVE_D(CHAR, BITS) /* d */) { return v##OP##BITS(); }
+namespace detail {
+
+// Returns actual lanes of a hardware vector, rounded down to a power of two.
+HWY_INLINE size_t HardwareLanes(hwy::SizeTag<1> /* tag */) {
+  return svcntb_pat(SV_POW2);
+}
+HWY_INLINE size_t HardwareLanes(hwy::SizeTag<2> /* tag */) {
+  return svcnth_pat(SV_POW2);
+}
+HWY_INLINE size_t HardwareLanes(hwy::SizeTag<8> /* tag */) {
+  return svcntd_pat(SV_POW2);
+}
+HWY_INLINE size_t HardwareLanes(hwy::SizeTag<4> /* tag */) {
+  return svcntw_pat(SV_POW2);
+}
 
-HWY_SVE_FOREACH(HWY_SVE_LANES, Lanes, setvlmax_e)
-#undef HWY_SVE_LANES
+}  // namespace detail
 
-// ------------------------------ Zero
+// Capped to <= 128-bit: SVE is at least that large, so no need to query actual.
+template <typename T, size_t N, HWY_IF_LE128(T, N)>
+HWY_API constexpr size_t Lanes(Simd<T, N> /* tag */) {
+  return N;
+}
 
-HWY_SVE_FOREACH(HWY_SVE_RETV_ARGD, Zero, zero)
+// Returns actual number of lanes after dividing by div={1,2,4,8}.
+// May return 0 if div > 16/sizeof(T): there is no "1/8th" of a u32x4, but it
+// would be valid for u32x8 (i.e. hardware vectors >= 256 bits).
+template <typename T, size_t N, HWY_IF_GT128(T, N)>
+HWY_API size_t Lanes(Simd<T, N> /* tag */) {
+  static_assert(N <= HWY_LANES(T), "N cannot exceed a full vector");
 
-template <class D>
-using VFromD = decltype(Zero(D()));
+  const size_t actual = detail::HardwareLanes(hwy::SizeTag<sizeof(T)>());
+  const size_t div = HWY_LANES(T) / N;
+  static_assert(div <= 8, "Invalid N - must be <=128 bit, or >=1/8th");
+  return actual / div;
+}
+
+// ================================================== MASK INIT
+
+// One mask bit per byte; only the one belonging to the lowest byte is valid.
+
+// ------------------------------ FirstN
+#define HWY_SVE_FIRSTN(BASE, CHAR, BITS, NAME, OP)                       \
+  template <size_t KN>                                                   \
+  HWY_API svbool_t NAME(HWY_SVE_D(BASE, BITS, KN) /* d */, uint32_t N) { \
+    return sv##OP##_b##BITS##_u32(uint32_t(0), N);                       \
+  }
+HWY_SVE_FOREACH(HWY_SVE_FIRSTN, FirstN, whilelt)
+#undef HWY_SVE_FIRSTN
+
+namespace detail {
+
+// All-true mask from a macro
+#define HWY_SVE_PTRUE(BITS) svptrue_pat_b##BITS(SV_POW2)
+
+#define HWY_SVE_WRAP_PTRUE(BASE, CHAR, BITS, NAME, OP) \
+  template <size_t N>                                  \
+  HWY_API svbool_t NAME(HWY_SVE_D(BASE, BITS, N) d) {  \
+    return HWY_SVE_PTRUE(BITS);                        \
+  }
+
+HWY_SVE_FOREACH(HWY_SVE_WRAP_PTRUE, PTrue, ptrue)  // return all-true
+#undef HWY_SVE_WRAP_PTRUE
+
+HWY_API svbool_t PFalse() { return svpfalse_b(); }
+
+// Returns all-true if d is HWY_FULL or FirstN(N) after capping N.
+//
+// This is used in functions that load/store memory; other functions (e.g.
+// arithmetic on partial vectors) can ignore d and use PTrue instead.
+template <typename T, size_t N>
+svbool_t Mask(Simd<T, N> d) {
+  return N == HWY_LANES(T) ? PTrue(d) : FirstN(d, Lanes(d));
+}
+
+}  // namespace detail
+
+// ================================================== INIT
 
 // ------------------------------ Set
 // vector = f(d, scalar), e.g. Set
-#define HWY_SVE_SET(BASE, CHAR, BITS, NAME, OP)                  \
-  HWY_API HWY_SVE_V(BASE, BITS)                                  \
-      NAME(HWY_SVE_D(CHAR, BITS) d, HWY_SVE_T(BASE, BITS) arg) { \
-    (void)Lanes(d);                                              \
-    return v##OP##_##CHAR##BITS(arg);                            \
+#define HWY_SVE_SET(BASE, CHAR, BITS, NAME, OP)                     \
+  template <size_t N>                                               \
+  HWY_API HWY_SVE_V(BASE, BITS)                                     \
+      NAME(HWY_SVE_D(BASE, BITS, N) d, HWY_SVE_T(BASE, BITS) arg) { \
+    return sv##OP##_##CHAR##BITS(arg);                              \
   }
 
-HWY_SVE_FOREACH_UI(HWY_SVE_SET, Set, mv_v_x)
-HWY_SVE_FOREACH_F(HWY_SVE_SET, Set, fmv_v_f)
+HWY_SVE_FOREACH(HWY_SVE_SET, Set, dup_n)
 #undef HWY_SVE_SET
 
+template <class D>
+using VFromD = decltype(Set(D(), 0));
+
+// ------------------------------ Zero
+
+template <class D>
+VFromD<D> Zero(D d) {
+  return Set(d, 0);
+}
+
 // ------------------------------ Undefined
 
-HWY_SVE_FOREACH(HWY_SVE_RETV_ARGD, Undefined, undefined)
+#if defined(HWY_EMULATE_SVE)
+template <class D>
+VFromD<D> Undefined(D d) {
+  return Zero(d);
+}
+#else
+HWY_SVE_FOREACH(HWY_SVE_RETV_ARGD, Undefined, undef)
+#endif
 
 // ------------------------------ BitCast
 
 namespace detail {
 
 // u8: no change
-#define HWY_SVE_CAST_NOP(BASE, CHAR, BITS, NAME, OP)                           \
-  HWY_API HWY_SVE_V(BASE, BITS) BitCastToByte(HWY_SVE_V(BASE, BITS) v) {       \
-    return v;                                                                  \
-  }                                                                            \
-  HWY_API HWY_SVE_V(BASE, BITS) BitCastFromByte(HWY_SVE_D(CHAR, BITS) /* d */, \
-                                                HWY_SVE_V(BASE, BITS) v) {     \
-    return v;                                                                  \
-  }
-
-// Other integers
-#define HWY_SVE_CAST_UI(BASE, CHAR, BITS, NAME, OP)                   \
-  HWY_API vuint8m##_t BitCastToByte(HWY_SVE_V(BASE, BITS) v) {        \
-    return v##OP##_v_##CHAR##BITS##_u8m(v);                           \
-  }                                                                   \
-  HWY_API HWY_SVE_V(BASE, BITS)                                       \
-      BitCastFromByte(HWY_SVE_D(CHAR, BITS) /* d */, vuint8m##_t v) { \
-    return v##OP##_v_u8m##_##CHAR##BITS(v);                           \
+#define HWY_SVE_CAST_NOP(BASE, CHAR, BITS, NAME, OP)                     \
+  HWY_API HWY_SVE_V(BASE, BITS) BitCastToByte(HWY_SVE_V(BASE, BITS) v) { \
+    return v;                                                            \
+  }                                                                      \
+  template <size_t N>                                                    \
+  HWY_API HWY_SVE_V(BASE, BITS) BitCastFromByte(                         \
+      HWY_SVE_D(BASE, BITS, N) /* d */, HWY_SVE_V(BASE, BITS) v) {       \
+    return v;                                                            \
   }
 
-// Float: first cast to/from unsigned
-#define HWY_SVE_CAST_F(BASE, CHAR, BITS, NAME, OP)                    \
-  HWY_API vuint8m##_t BitCastToByte(HWY_SVE_V(BASE, BITS) v) {        \
-    return v##OP##_v_u##BITS##_u8m(v##OP##_v_f##BITS##_u##BITS(v));   \
-  }                                                                   \
-  HWY_API HWY_SVE_V(BASE, BITS)                                       \
-      BitCastFromByte(HWY_SVE_D(CHAR, BITS) /* d */, vuint8m##_t v) { \
-    return v##OP##_v_u##BITS##_f##BITS(v##OP##_v_u8m##_u##BITS(v));   \
+// All other types
+#define HWY_SVE_CAST(BASE, CHAR, BITS, NAME, OP)                       \
+  HWY_INLINE svuint8_t BitCastToByte(HWY_SVE_V(BASE, BITS) v) {        \
+    return sv##OP##_u8_##CHAR##BITS(v);                                \
+  }                                                                    \
+  template <size_t N>                                                  \
+  HWY_INLINE HWY_SVE_V(BASE, BITS)                                     \
+      BitCastFromByte(HWY_SVE_D(BASE, BITS, N) /* d */, svuint8_t v) { \
+    return sv##OP##_##CHAR##BITS##_u8(v);                              \
   }
 
 HWY_SVE_FOREACH_U08(HWY_SVE_CAST_NOP, _, _)
-HWY_SVE_FOREACH_I08(HWY_SVE_CAST_UI, _, reinterpret)
-HWY_SVE_FOREACH_UI16(HWY_SVE_CAST_UI, _, reinterpret)
-HWY_SVE_FOREACH_UI32(HWY_SVE_CAST_UI, _, reinterpret)
-HWY_SVE_FOREACH_UI64(HWY_SVE_CAST_UI, _, reinterpret)
-HWY_SVE_FOREACH_F(HWY_SVE_CAST_F, _, reinterpret)
+HWY_SVE_FOREACH_I08(HWY_SVE_CAST, _, reinterpret)
+HWY_SVE_FOREACH_UI16(HWY_SVE_CAST, _, reinterpret)
+HWY_SVE_FOREACH_UI32(HWY_SVE_CAST, _, reinterpret)
+HWY_SVE_FOREACH_UI64(HWY_SVE_CAST, _, reinterpret)
+HWY_SVE_FOREACH_F(HWY_SVE_CAST, _, reinterpret)
 
 #undef HWY_SVE_CAST_NOP
-#undef HWY_SVE_CAST_UI
-#undef HWY_SVE_CAST_F
+#undef HWY_SVE_CAST
 
 }  // namespace detail
 
@@ -243,114 +344,120 @@ HWY_API VFromD<D> BitCast(D d, FromV v) {
   return detail::BitCastFromByte(d, detail::BitCastToByte(v));
 }
 
-namespace detail {
+// ================================================== LOGICAL
 
-template <class V, class DU = RebindToUnsigned<DFromV<V>>>
-HWY_API VFromD<DU> BitCastToUnsigned(V v) {
-  return BitCast(DU(), v);
-}
+// detail::*N() functions accept a scalar argument to avoid extra Set().
 
-}  // namespace detail
+// ------------------------------ Not
 
-// ------------------------------ Iota
+HWY_SVE_FOREACH_UI(HWY_SVE_RETV_ARGPV, Not, not )
+
+// ------------------------------ And
 
 namespace detail {
+HWY_SVE_FOREACH_UI(HWY_SVE_RETV_ARGPVN, AndN, and_n)
+}  // namespace detail
 
-HWY_SVE_FOREACH_U(HWY_SVE_RETV_ARGD, Iota0, id_v)
+HWY_SVE_FOREACH_UI(HWY_SVE_RETV_ARGPVV, And, and)
 
-template <class D, class DU = RebindToUnsigned<D>>
-HWY_API VFromD<DU> Iota0(const D /*d*/) {
-  Lanes(DU());
-  return BitCastToUnsigned(Iota0(DU()));
+template <class V, HWY_IF_FLOAT_V(V)>
+HWY_API V And(const V a, const V b) {
+  const DFromV<V> df;
+  const RebindToUnsigned<decltype(df)> du;
+  return BitCast(df, And(BitCast(du, a), BitCast(du, b)));
 }
 
-}  // namespace detail
-
-// ================================================== LOGICAL
-
-// ------------------------------ Not
+// ------------------------------ Or
 
-HWY_SVE_FOREACH_UI(HWY_SVE_RETV_ARGV, Not, not )
+HWY_SVE_FOREACH_UI(HWY_SVE_RETV_ARGPVV, Or, orr)
 
 template <class V, HWY_IF_FLOAT_V(V)>
-HWY_API V Not(const V v) {
-  using DF = DFromV<V>;
-  using DU = RebindToUnsigned<DF>;
-  return BitCast(DF(), Not(BitCast(DU(), v)));
+HWY_API V Or(const V a, const V b) {
+  const DFromV<V> df;
+  const RebindToUnsigned<decltype(df)> du;
+  return BitCast(df, Or(BitCast(du, a), BitCast(du, b)));
 }
 
-// ------------------------------ And
+// ------------------------------ Xor
 
-// Non-vector version (ideally immediate) for use with Iota0
 namespace detail {
-HWY_SVE_FOREACH_UI(HWY_SVE_RETV_ARGVS, And, and_vx)
+HWY_SVE_FOREACH_UI(HWY_SVE_RETV_ARGPVN, XorN, eor_n)
 }  // namespace detail
 
-HWY_SVE_FOREACH_UI(HWY_SVE_RETV_ARGVV, And, and)
+HWY_SVE_FOREACH_UI(HWY_SVE_RETV_ARGPVV, Xor, eor)
 
 template <class V, HWY_IF_FLOAT_V(V)>
-HWY_API V And(const V a, const V b) {
-  using DF = DFromV<V>;
-  using DU = RebindToUnsigned<DF>;
-  return BitCast(DF(), And(BitCast(DU(), a), BitCast(DU(), b)));
+HWY_API V Xor(const V a, const V b) {
+  const DFromV<V> df;
+  const RebindToUnsigned<decltype(df)> du;
+  return BitCast(df, Xor(BitCast(du, a), BitCast(du, b)));
 }
 
-// ------------------------------ Or
+// ------------------------------ AndNot
 
-// Scalar argument plus mask. Used by VecFromMask.
-#define HWY_SVE_OR_MASK(BASE, CHAR, BITS, NAME, OP)                 \
-  HWY_API HWY_SVE_V(BASE, BITS)                                     \
-      NAME(HWY_SVE_V(BASE, BITS) v, HWY_SVE_T(BASE, BITS) imm,      \
-           HWY_SVE_M(MLEN) mask, HWY_SVE_V(BASE, BITS) maskedoff) { \
-    return v##OP##_##CHAR##BITS##_m(mask, maskedoff, v, imm);       \
+namespace detail {
+#define HWY_SVE_RETV_ARGPVN_SWAP(BASE, CHAR, BITS, NAME, OP)     \
+  HWY_API HWY_SVE_V(BASE, BITS)                                  \
+      NAME(HWY_SVE_T(BASE, BITS) a, HWY_SVE_V(BASE, BITS) b) {   \
+    return sv##OP##_##CHAR##BITS##_x(HWY_SVE_PTRUE(BITS), b, a); \
   }
 
-namespace detail {
-HWY_SVE_FOREACH_UI(HWY_SVE_OR_MASK, Or, or_vx)
+HWY_SVE_FOREACH_UI(HWY_SVE_RETV_ARGPVN_SWAP, AndNotN, bic_n)
+#undef HWY_SVE_RETV_ARGPVN_SWAP
 }  // namespace detail
 
-#undef HWY_SVE_OR_MASK
-
-HWY_SVE_FOREACH_UI(HWY_SVE_RETV_ARGVV, Or, or)
+#define HWY_SVE_RETV_ARGPVV_SWAP(BASE, CHAR, BITS, NAME, OP)     \
+  HWY_API HWY_SVE_V(BASE, BITS)                                  \
+      NAME(HWY_SVE_V(BASE, BITS) a, HWY_SVE_V(BASE, BITS) b) {   \
+    return sv##OP##_##CHAR##BITS##_x(HWY_SVE_PTRUE(BITS), b, a); \
+  }
+HWY_SVE_FOREACH_UI(HWY_SVE_RETV_ARGPVV_SWAP, AndNot, bic)
+#undef HWY_SVE_RETV_ARGPVV_SWAP
 
 template <class V, HWY_IF_FLOAT_V(V)>
-HWY_API V Or(const V a, const V b) {
-  using DF = DFromV<V>;
-  using DU = RebindToUnsigned<DF>;
-  return BitCast(DF(), Or(BitCast(DU(), a), BitCast(DU(), b)));
+HWY_API V AndNot(const V a, const V b) {
+  const DFromV<V> df;
+  const RebindToUnsigned<decltype(df)> du;
+  return BitCast(df, AndNot(BitCast(du, a), BitCast(du, b)));
 }
 
-// ------------------------------ Xor
+// ------------------------------ PopulationCount
 
-// Non-vector version (ideally immediate) for use with Iota0
-namespace detail {
-HWY_SVE_FOREACH_UI(HWY_SVE_RETV_ARGVS, Xor, xor_vx)
-}  // namespace detail
+#ifdef HWY_NATIVE_POPCNT
+#undef HWY_NATIVE_POPCNT
+#else
+#define HWY_NATIVE_POPCNT
+#endif
 
-HWY_SVE_FOREACH_UI(HWY_SVE_RETV_ARGVV, Xor, xor)
+// Need to return original type instead of unsigned.
+#define HWY_SVE_POPCNT(BASE, CHAR, BITS, NAME, OP)                     \
+  HWY_API HWY_SVE_V(BASE, BITS) NAME(HWY_SVE_V(BASE, BITS) v) {        \
+    return BitCast(DFromV<decltype(v)>(),                              \
+                   sv##OP##_##CHAR##BITS##_x(HWY_SVE_PTRUE(BITS), v)); \
+  }
+HWY_SVE_FOREACH_UI(HWY_SVE_POPCNT, PopulationCount, cnt)
+#undef HWY_SVE_POPCNT
 
-template <class V, HWY_IF_FLOAT_V(V)>
-HWY_API V Xor(const V a, const V b) {
-  using DF = DFromV<V>;
-  using DU = RebindToUnsigned<DF>;
-  return BitCast(DF(), Xor(BitCast(DU(), a), BitCast(DU(), b)));
-}
+// ================================================== SIGN
 
-// ------------------------------ AndNot
+// ------------------------------ Neg
+HWY_SVE_FOREACH_IF(HWY_SVE_RETV_ARGPV, Neg, neg)
 
-template <class V>
-HWY_API V AndNot(const V not_a, const V b) {
-  return And(Not(not_a), b);
-}
+// ------------------------------ Abs
+HWY_SVE_FOREACH_IF(HWY_SVE_RETV_ARGPV, Abs, abs)
 
-// ------------------------------ CopySign
+// ------------------------------ CopySign[ToAbs]
 
-HWY_SVE_FOREACH_F(HWY_SVE_RETV_ARGVV, CopySign, fsgnj)
+template <class V>
+HWY_API V CopySign(const V magn, const V sign) {
+  const auto msb = SignBit(DFromV<V>());
+  return Or(AndNot(msb, magn), And(msb, sign));
+}
 
 template <class V>
 HWY_API V CopySignToAbs(const V abs, const V sign) {
-  // TODO(janwas): separate handling for abs < 0 or same?
-  return CopySign(abs, sign);
+  const auto msb = SignBit(DFromV<V>());
+  return Or(abs, And(msb, sign));
 }
 
 // ================================================== ARITHMETIC
@@ -358,1312 +465,1352 @@ HWY_API V CopySignToAbs(const V abs, const V sign) {
 // ------------------------------ Add
 
 namespace detail {
-HWY_SVE_FOREACH_UI(HWY_SVE_RETV_ARGVS, Add, add_vx)
-HWY_SVE_FOREACH_F(HWY_SVE_RETV_ARGVS, Add, fadd_vf)
+HWY_SVE_FOREACH(HWY_SVE_RETV_ARGPVN, AddN, add_n)
 }  // namespace detail
 
-HWY_SVE_FOREACH_UI(HWY_SVE_RETV_ARGVV, Add, add)
-HWY_SVE_FOREACH_F(HWY_SVE_RETV_ARGVV, Add, fadd)
+HWY_SVE_FOREACH(HWY_SVE_RETV_ARGPVV, Add, add)
 
 // ------------------------------ Sub
-HWY_SVE_FOREACH_UI(HWY_SVE_RETV_ARGVV, Sub, sub)
-HWY_SVE_FOREACH_F(HWY_SVE_RETV_ARGVV, Sub, fsub)
 
-// ------------------------------ SaturatedAdd
+namespace detail {
+// Can't use HWY_SVE_RETV_ARGPVN because caller wants to specify pg.
+#define HWY_SVE_RETV_ARGPVN_MASK(BASE, CHAR, BITS, NAME, OP)                \
+  HWY_API HWY_SVE_V(BASE, BITS)                                             \
+      NAME(svbool_t pg, HWY_SVE_V(BASE, BITS) a, HWY_SVE_T(BASE, BITS) b) { \
+    return sv##OP##_##CHAR##BITS##_z(pg, a, b);                             \
+  }
 
-HWY_SVE_FOREACH_U08(HWY_SVE_RETV_ARGVV, SaturatedAdd, saddu)
-HWY_SVE_FOREACH_U16(HWY_SVE_RETV_ARGVV, SaturatedAdd, saddu)
+HWY_SVE_FOREACH(HWY_SVE_RETV_ARGPVN_MASK, SubN, sub_n)
+#undef HWY_SVE_RETV_ARGPVN_MASK
+}  // namespace detail
 
-HWY_SVE_FOREACH_I08(HWY_SVE_RETV_ARGVV, SaturatedAdd, sadd)
-HWY_SVE_FOREACH_I16(HWY_SVE_RETV_ARGVV, SaturatedAdd, sadd)
+HWY_SVE_FOREACH(HWY_SVE_RETV_ARGPVV, Sub, sub)
 
-// ------------------------------ SaturatedSub
+// ------------------------------ SaturatedAdd
 
-HWY_SVE_FOREACH_U08(HWY_SVE_RETV_ARGVV, SaturatedSub, ssubu)
-HWY_SVE_FOREACH_U16(HWY_SVE_RETV_ARGVV, SaturatedSub, ssubu)
+HWY_SVE_FOREACH_UI08(HWY_SVE_RETV_ARGVV, SaturatedAdd, qadd)
+HWY_SVE_FOREACH_UI16(HWY_SVE_RETV_ARGVV, SaturatedAdd, qadd)
 
-HWY_SVE_FOREACH_I08(HWY_SVE_RETV_ARGVV, SaturatedSub, ssub)
-HWY_SVE_FOREACH_I16(HWY_SVE_RETV_ARGVV, SaturatedSub, ssub)
+// ------------------------------ SaturatedSub
 
-// ------------------------------ AverageRound
+HWY_SVE_FOREACH_UI08(HWY_SVE_RETV_ARGVV, SaturatedSub, qsub)
+HWY_SVE_FOREACH_UI16(HWY_SVE_RETV_ARGVV, SaturatedSub, qsub)
 
-// TODO(janwas): check vxrm rounding mode
-HWY_SVE_FOREACH_U08(HWY_SVE_RETV_ARGVV, AverageRound, aaddu)
-HWY_SVE_FOREACH_U16(HWY_SVE_RETV_ARGVV, AverageRound, aaddu)
+// ------------------------------ AbsDiff
+HWY_SVE_FOREACH_IF(HWY_SVE_RETV_ARGPVV, AbsDiff, abd)
 
 // ------------------------------ ShiftLeft[Same]
 
-// Intrinsics do not define .vi forms, so use .vx instead.
-#define HWY_SVE_SHIFT(BASE, CHAR, BITS, NAME, OP)                  \
-  template <int kBits>                                             \
-  HWY_API HWY_SVE_V(BASE, BITS) NAME(HWY_SVE_V(BASE, BITS) v) {    \
-    return v##OP##_vx_##CHAR##BITS(v, kBits);                      \
-  }                                                                \
-  HWY_API HWY_SVE_V(BASE, BITS)                                    \
-      NAME##Same(HWY_SVE_V(BASE, BITS) v, int bits) {              \
-    return v##OP##_vx_##CHAR##BITS(v, static_cast<uint8_t>(bits)); \
+#define HWY_SVE_SHIFT_N(BASE, CHAR, BITS, NAME, OP)                     \
+  template <int kBits>                                                  \
+  HWY_API HWY_SVE_V(BASE, BITS) NAME(HWY_SVE_V(BASE, BITS) v) {         \
+    return sv##OP##_##CHAR##BITS##_x(HWY_SVE_PTRUE(BITS), v, kBits);    \
+  }                                                                     \
+  HWY_API HWY_SVE_V(BASE, BITS)                                         \
+      NAME##Same(HWY_SVE_V(BASE, BITS) v, HWY_SVE_T(uint, BITS) bits) { \
+    return sv##OP##_##CHAR##BITS##_x(HWY_SVE_PTRUE(BITS), v, bits);     \
   }
 
-HWY_SVE_FOREACH_UI(HWY_SVE_SHIFT, ShiftLeft, sll)
+HWY_SVE_FOREACH_UI(HWY_SVE_SHIFT_N, ShiftLeft, lsl_n)
 
 // ------------------------------ ShiftRight[Same]
 
-HWY_SVE_FOREACH_U(HWY_SVE_SHIFT, ShiftRight, srl)
-HWY_SVE_FOREACH_I(HWY_SVE_SHIFT, ShiftRight, sra)
-
-#undef HWY_SVE_SHIFT
+HWY_SVE_FOREACH_U(HWY_SVE_SHIFT_N, ShiftRight, lsr_n)
+HWY_SVE_FOREACH_I(HWY_SVE_SHIFT_N, ShiftRight, asr_n)
 
-// ------------------------------ Shl
-#define HWY_SVE_SHIFT_VV(BASE, CHAR, BITS, NAME, OP)              \
-  HWY_API HWY_SVE_V(BASE, BITS)                                   \
-      NAME(HWY_SVE_V(BASE, BITS) v, HWY_SVE_V(BASE, BITS) bits) { \
-    return v##OP##_vv_##CHAR##BITS(v, bits);                      \
-  }
+#undef HWY_SVE_SHIFT_N
 
-HWY_SVE_FOREACH_U(HWY_SVE_SHIFT_VV, Shl, sll)
+// ------------------------------ Shl/r
 
-#define HWY_SVE_SHIFT_II(BASE, CHAR, BITS, NAME, OP)                    \
-  HWY_API HWY_SVE_V(BASE, BITS)                                         \
-      NAME(HWY_SVE_V(BASE, BITS) v, HWY_SVE_V(BASE, BITS) bits) {       \
-    return v##OP##_vv_##CHAR##BITS(v, detail::BitCastToUnsigned(bits)); \
+#define HWY_SVE_SHIFT(BASE, CHAR, BITS, NAME, OP)                          \
+  HWY_API HWY_SVE_V(BASE, BITS)                                            \
+      NAME(HWY_SVE_V(BASE, BITS) v, HWY_SVE_V(BASE, BITS) bits) {          \
+    using TU = HWY_SVE_T(uint, BITS);                                      \
+    return sv##OP##_##CHAR##BITS##_x(                                      \
+        HWY_SVE_PTRUE(BITS), v, BitCast(Simd<TU, HWY_LANES(TU)>(), bits)); \
   }
 
-HWY_SVE_FOREACH_I(HWY_SVE_SHIFT_II, Shl, sll)
-
-// ------------------------------ Shr
+HWY_SVE_FOREACH_UI(HWY_SVE_SHIFT, Shl, lsl)
 
-HWY_SVE_FOREACH_U(HWY_SVE_SHIFT_VV, Shr, srl)
-HWY_SVE_FOREACH_I(HWY_SVE_SHIFT_II, Shr, sra)
+HWY_SVE_FOREACH_U(HWY_SVE_SHIFT, Shr, lsr)
+HWY_SVE_FOREACH_I(HWY_SVE_SHIFT, Shr, asr)
 
-#undef HWY_SVE_SHIFT_II
-#undef HWY_SVE_SHIFT_VV
-
-// ------------------------------ Min
+#undef HWY_SVE_SHIFT
 
-HWY_SVE_FOREACH_U(HWY_SVE_RETV_ARGVV, Min, minu)
-HWY_SVE_FOREACH_I(HWY_SVE_RETV_ARGVV, Min, min)
-HWY_SVE_FOREACH_F(HWY_SVE_RETV_ARGVV, Min, fmin)
+// ------------------------------ Min/Max
 
-// ------------------------------ Max
+HWY_SVE_FOREACH_UI(HWY_SVE_RETV_ARGPVV, Min, min)
+HWY_SVE_FOREACH_UI(HWY_SVE_RETV_ARGPVV, Max, max)
+HWY_SVE_FOREACH_F(HWY_SVE_RETV_ARGPVV, Min, minnm)
+HWY_SVE_FOREACH_F(HWY_SVE_RETV_ARGPVV, Max, maxnm)
 
 namespace detail {
-
-HWY_SVE_FOREACH_U(HWY_SVE_RETV_ARGVS, Max, maxu_vx)
-HWY_SVE_FOREACH_I(HWY_SVE_RETV_ARGVS, Max, max_vx)
-HWY_SVE_FOREACH_F(HWY_SVE_RETV_ARGVS, Max, fmax_vf)
-
+HWY_SVE_FOREACH_UI(HWY_SVE_RETV_ARGPVN, MinN, min_n)
+HWY_SVE_FOREACH_UI(HWY_SVE_RETV_ARGPVN, MaxN, max_n)
 }  // namespace detail
 
-HWY_SVE_FOREACH_U(HWY_SVE_RETV_ARGVV, Max, maxu)
-HWY_SVE_FOREACH_I(HWY_SVE_RETV_ARGVV, Max, max)
-HWY_SVE_FOREACH_F(HWY_SVE_RETV_ARGVV, Max, fmax)
-
 // ------------------------------ Mul
-
-HWY_SVE_FOREACH_UI16(HWY_SVE_RETV_ARGVV, Mul, mul)
-HWY_SVE_FOREACH_UI32(HWY_SVE_RETV_ARGVV, Mul, mul)
-HWY_SVE_FOREACH_F(HWY_SVE_RETV_ARGVV, Mul, fmul)
+HWY_SVE_FOREACH_UI16(HWY_SVE_RETV_ARGPVV, Mul, mul)
+HWY_SVE_FOREACH_UIF3264(HWY_SVE_RETV_ARGPVV, Mul, mul)
 
 // ------------------------------ MulHigh
-
-HWY_SVE_FOREACH_U16(HWY_SVE_RETV_ARGVV, MulHigh, mulhu)
-HWY_SVE_FOREACH_I16(HWY_SVE_RETV_ARGVV, MulHigh, mulh)
+HWY_SVE_FOREACH_UI16(HWY_SVE_RETV_ARGPVV, MulHigh, mulh)
+namespace detail {
+HWY_SVE_FOREACH_UI32(HWY_SVE_RETV_ARGPVV, MulHigh, mulh)
+HWY_SVE_FOREACH_U64(HWY_SVE_RETV_ARGPVV, MulHigh, mulh)
+}  // namespace detail
 
 // ------------------------------ Div
-
-HWY_SVE_FOREACH_F(HWY_SVE_RETV_ARGVV, Div, fdiv)
+HWY_SVE_FOREACH_F(HWY_SVE_RETV_ARGPVV, Div, div)
 
 // ------------------------------ ApproximateReciprocal
-
-// TODO(janwas): not yet supported in intrinsics
-template <class V>
-HWY_API V ApproximateReciprocal(const V v) {
-  return Set(DFromV<V>(), 1) / v;
-}
-// HWY_SVE_FOREACH_F32(HWY_SVE_RETV_ARGV, ApproximateReciprocal, frece7)
+HWY_SVE_FOREACH_F32(HWY_SVE_RETV_ARGV, ApproximateReciprocal, recpe)
 
 // ------------------------------ Sqrt
-HWY_SVE_FOREACH_F(HWY_SVE_RETV_ARGV, Sqrt, fsqrt)
+HWY_SVE_FOREACH_F(HWY_SVE_RETV_ARGPV, Sqrt, sqrt)
 
 // ------------------------------ ApproximateReciprocalSqrt
-
-// TODO(janwas): not yet supported in intrinsics
-template <class V>
-HWY_API V ApproximateReciprocalSqrt(const V v) {
-  return ApproximateReciprocal(Sqrt(v));
-}
-// HWY_SVE_FOREACH_F32(HWY_SVE_RETV_ARGV, ApproximateReciprocalSqrt, frsqrte7)
+HWY_SVE_FOREACH_F32(HWY_SVE_RETV_ARGV, ApproximateReciprocalSqrt, rsqrte)
 
 // ------------------------------ MulAdd
-// Note: op is still named vv, not vvv.
-#define HWY_SVE_FMA(BASE, CHAR, BITS, NAME, OP)                \
-  HWY_API HWY_SVE_V(BASE, BITS)                                \
-      NAME(HWY_SVE_V(BASE, BITS) mul, HWY_SVE_V(BASE, BITS) x, \
-           HWY_SVE_V(BASE, BITS) add) {                        \
-    return v##OP##_vv_##CHAR##BITS(add, mul, x);               \
+#define HWY_SVE_FMA(BASE, CHAR, BITS, NAME, OP)                         \
+  HWY_API HWY_SVE_V(BASE, BITS)                                         \
+      NAME(HWY_SVE_V(BASE, BITS) mul, HWY_SVE_V(BASE, BITS) x,          \
+           HWY_SVE_V(BASE, BITS) add) {                                 \
+    return sv##OP##_##CHAR##BITS##_x(HWY_SVE_PTRUE(BITS), x, mul, add); \
   }
 
-HWY_SVE_FOREACH_F(HWY_SVE_FMA, MulAdd, fmacc)
+HWY_SVE_FOREACH_F(HWY_SVE_FMA, MulAdd, mad)
 
 // ------------------------------ NegMulAdd
-HWY_SVE_FOREACH_F(HWY_SVE_FMA, NegMulAdd, fnmsac)
+HWY_SVE_FOREACH_F(HWY_SVE_FMA, NegMulAdd, msb)
 
 // ------------------------------ MulSub
-HWY_SVE_FOREACH_F(HWY_SVE_FMA, MulSub, fmsac)
+HWY_SVE_FOREACH_F(HWY_SVE_FMA, MulSub, nmsb)
 
 // ------------------------------ NegMulSub
-HWY_SVE_FOREACH_F(HWY_SVE_FMA, NegMulSub, fnmacc)
+HWY_SVE_FOREACH_F(HWY_SVE_FMA, NegMulSub, nmad)
 
 #undef HWY_SVE_FMA
 
-// ================================================== COMPARE
-
-// Comparisons set a mask bit to 1 if the condition is true, else 0. The XX in
-// vboolXX_t is a power of two divisor for vector bits. SLEN 8 / LMUL 1 = 1/8th
-// of all bits; SLEN 8 / LMUL 4 = half of all bits.
-
-// mask = f(vector, vector)
-#define HWY_SVE_RETM_ARGVV(BASE, CHAR, BITS, NAME, OP)         \
-  HWY_API HWY_SVE_M(MLEN)                                      \
-      NAME(HWY_SVE_V(BASE, BITS) a, HWY_SVE_V(BASE, BITS) b) { \
-    (void)Lanes(DFromV<decltype(a)>());                        \
-    return v##OP##_vv_##CHAR##BITS##_b##MLEN(a, b);            \
-  }
-
-// ------------------------------ Eq
-HWY_SVE_FOREACH_UI(HWY_SVE_RETM_ARGVV, Eq, mseq)
-HWY_SVE_FOREACH_F(HWY_SVE_RETM_ARGVV, Eq, mfeq)
-
-// ------------------------------ Ne
-HWY_SVE_FOREACH_UI(HWY_SVE_RETM_ARGVV, Ne, msne)
-HWY_SVE_FOREACH_F(HWY_SVE_RETM_ARGVV, Ne, mfne)
+// ------------------------------ Round etc.
 
-// ------------------------------ Lt
-HWY_SVE_FOREACH_I(HWY_SVE_RETM_ARGVV, Lt, mslt)
-HWY_SVE_FOREACH_F(HWY_SVE_RETM_ARGVV, Lt, mflt)
+HWY_SVE_FOREACH_F(HWY_SVE_RETV_ARGPV, Round, rintn)
+HWY_SVE_FOREACH_F(HWY_SVE_RETV_ARGPV, Floor, rintm)
+HWY_SVE_FOREACH_F(HWY_SVE_RETV_ARGPV, Ceil, rintp)
+HWY_SVE_FOREACH_F(HWY_SVE_RETV_ARGPV, Trunc, rintz)
 
-// ------------------------------ Gt
+// ================================================== MASK
 
-template <class V>
-HWY_API auto Gt(const V a, const V b) -> decltype(Lt(a, b)) {
-  return Lt(b, a);
+// ------------------------------ RebindMask
+template <class D, typename MFrom>
+HWY_API svbool_t RebindMask(const D /*d*/, const MFrom mask) {
+  return mask;
 }
 
-// ------------------------------ Le
-HWY_SVE_FOREACH_F(HWY_SVE_RETM_ARGVV, Le, mfle)
-
-#undef HWY_SVE_RETM_ARGVV
-
-// ------------------------------ Ge
+// ------------------------------ Mask logical
 
-template <class V>
-HWY_API auto Ge(const V a, const V b) -> decltype(Le(a, b)) {
-  return Le(b, a);
+HWY_API svbool_t Not(svbool_t m) {
+  // We don't know the lane type, so assume 8-bit. For larger types, this will
+  // de-canonicalize the predicate, i.e. set bits to 1 even though they do not
+  // correspond to the lowest byte in the lane. Per ARM, such bits are ignored.
+  return svnot_b_z(HWY_SVE_PTRUE(8), m);
 }
-
-// ------------------------------ TestBit
-
-template <class V>
-HWY_API auto TestBit(const V a, const V bit) -> decltype(Eq(a, bit)) {
-  return Ne(And(a, bit), Zero(DFromV<V>()));
+HWY_API svbool_t And(svbool_t a, svbool_t b) {
+  return svand_b_z(b, b, a);  // same order as AndNot for consistency
+}
+HWY_API svbool_t AndNot(svbool_t a, svbool_t b) {
+  return svbic_b_z(b, b, a);  // reversed order like NEON
+}
+HWY_API svbool_t Or(svbool_t a, svbool_t b) {
+  return svsel_b(a, a, b);  // a ? true : b
+}
+HWY_API svbool_t Xor(svbool_t a, svbool_t b) {
+  return svsel_b(a, svnand_b_z(a, a, b), b);  // a ? !(a & b) : b.
 }
 
-// ------------------------------ Not
+// ------------------------------ CountTrue
 
-// mask = f(mask)
-#define HWY_SVE_RETM_ARGM(MLEN, NAME, OP)           \
-  HWY_API HWY_SVE_M(MLEN) NAME(HWY_SVE_M(MLEN) m) { \
-    return vm##OP##_m_b##MLEN(m);                   \
+#define HWY_SVE_COUNT_TRUE(BASE, CHAR, BITS, NAME, OP)          \
+  template <size_t N>                                           \
+  HWY_API size_t NAME(HWY_SVE_D(BASE, BITS, N) d, svbool_t m) { \
+    return sv##OP##_b##BITS(detail::Mask(d), m);                \
   }
 
-HWY_SVE_FOREACH_B(HWY_SVE_RETM_ARGM, Not, not )
-
-#undef HWY_SVE_RETM_ARGM
+HWY_SVE_FOREACH(HWY_SVE_COUNT_TRUE, CountTrue, cntp)
+#undef HWY_SVE_COUNT_TRUE
 
-// ------------------------------ And
+// For 16-bit Compress: full vector, not limited to SV_POW2.
+namespace detail {
 
-// mask = f(mask_a, mask_b) (note arg2,arg1 order!)
-#define HWY_SVE_RETM_ARGMM(MLEN, NAME, OP)                             \
-  HWY_API HWY_SVE_M(MLEN) NAME(HWY_SVE_M(MLEN) a, HWY_SVE_M(MLEN) b) { \
-    return vm##OP##_mm_b##MLEN(b, a);                                  \
+#define HWY_SVE_COUNT_TRUE_FULL(BASE, CHAR, BITS, NAME, OP)     \
+  template <size_t N>                                           \
+  HWY_API size_t NAME(HWY_SVE_D(BASE, BITS, N) d, svbool_t m) { \
+    return sv##OP##_b##BITS(svptrue_b##BITS(), m);              \
   }
 
-HWY_SVE_FOREACH_B(HWY_SVE_RETM_ARGMM, And, and)
+HWY_SVE_FOREACH(HWY_SVE_COUNT_TRUE_FULL, CountTrueFull, cntp)
+#undef HWY_SVE_COUNT_TRUE_FULL
 
-// ------------------------------ AndNot
-HWY_SVE_FOREACH_B(HWY_SVE_RETM_ARGMM, AndNot, andnot)
+}  // namespace detail
 
-// ------------------------------ Or
-HWY_SVE_FOREACH_B(HWY_SVE_RETM_ARGMM, Or, or)
+// ------------------------------ AllFalse
+template <typename T, size_t N>
+HWY_API bool AllFalse(Simd<T, N> d, svbool_t m) {
+  return !svptest_any(detail::Mask(d), m);
+}
 
-// ------------------------------ Xor
-HWY_SVE_FOREACH_B(HWY_SVE_RETM_ARGMM, Xor, xor)
+// ------------------------------ AllTrue
+template <typename T, size_t N>
+HWY_API bool AllTrue(Simd<T, N> d, svbool_t m) {
+  return CountTrue(d, m) == Lanes(d);
+}
 
-#undef HWY_SVE_RETM_ARGMM
+// ------------------------------ FindFirstTrue
+template <typename T, size_t N>
+HWY_API intptr_t FindFirstTrue(Simd<T, N> d, svbool_t m) {
+  return AllFalse(d, m) ? -1 : CountTrue(d, svbrkb_b_z(detail::Mask(d), m));
+}
 
 // ------------------------------ IfThenElse
-#define HWY_SVE_IF_THEN_ELSE(BASE, CHAR, BITS, NAME, OP) \
-  HWY_API HWY_SVE_V(BASE, BITS)                          \
-      NAME(HWY_SVE_M(MLEN) m, HWY_SVE_V(BASE, BITS) yes, \
-           HWY_SVE_V(BASE, BITS) no) {                   \
-    return v##OP##_vvm_##CHAR##BITS(m, no, yes);         \
+#define HWY_SVE_IF_THEN_ELSE(BASE, CHAR, BITS, NAME, OP)                      \
+  HWY_API HWY_SVE_V(BASE, BITS)                                               \
+      NAME(svbool_t m, HWY_SVE_V(BASE, BITS) yes, HWY_SVE_V(BASE, BITS) no) { \
+    return sv##OP##_##CHAR##BITS(m, yes, no);                                 \
   }
 
-HWY_SVE_FOREACH(HWY_SVE_IF_THEN_ELSE, IfThenElse, merge)
-
+HWY_SVE_FOREACH(HWY_SVE_IF_THEN_ELSE, IfThenElse, sel)
 #undef HWY_SVE_IF_THEN_ELSE
-// ------------------------------ IfThenElseZero
 
+// ------------------------------ IfThenElseZero
 template <class M, class V>
 HWY_API V IfThenElseZero(const M mask, const V yes) {
   return IfThenElse(mask, yes, Zero(DFromV<V>()));
 }
 
 // ------------------------------ IfThenZeroElse
-
 template <class M, class V>
 HWY_API V IfThenZeroElse(const M mask, const V no) {
   return IfThenElse(mask, Zero(DFromV<V>()), no);
 }
 
-// ------------------------------ MaskFromVec
+// ================================================== COMPARE
+
+// mask = f(vector, vector)
+#define HWY_SVE_COMPARE(BASE, CHAR, BITS, NAME, OP)                         \
+  HWY_API svbool_t NAME(HWY_SVE_V(BASE, BITS) a, HWY_SVE_V(BASE, BITS) b) { \
+    return sv##OP##_##CHAR##BITS(HWY_SVE_PTRUE(BITS), a, b);                \
+  }
+#define HWY_SVE_COMPARE_N(BASE, CHAR, BITS, NAME, OP)                       \
+  HWY_API svbool_t NAME(HWY_SVE_V(BASE, BITS) a, HWY_SVE_T(BASE, BITS) b) { \
+    return sv##OP##_##CHAR##BITS(HWY_SVE_PTRUE(BITS), a, b);                \
+  }
+
+// ------------------------------ Eq
+HWY_SVE_FOREACH(HWY_SVE_COMPARE, Eq, cmpeq)
+
+// ------------------------------ Ne
+HWY_SVE_FOREACH(HWY_SVE_COMPARE, Ne, cmpne)
+
+// ------------------------------ Lt
+HWY_SVE_FOREACH_IF(HWY_SVE_COMPARE, Lt, cmplt)
+namespace detail {
+HWY_SVE_FOREACH_IF(HWY_SVE_COMPARE_N, LtN, cmplt_n)
+}  // namespace detail
+
+// ------------------------------ Le
+HWY_SVE_FOREACH_F(HWY_SVE_COMPARE, Le, cmple)
+
+#undef HWY_SVE_COMPARE
+#undef HWY_SVE_COMPARE_N
+
+// ------------------------------ Gt/Ge (swapped order)
 
 template <class V>
-HWY_API auto MaskFromVec(const V v) -> decltype(Eq(v, v)) {
-  return Ne(v, Zero(DFromV<V>()));
+HWY_API svbool_t Gt(const V a, const V b) {
+  return Lt(b, a);
+}
+template <class V>
+HWY_API svbool_t Ge(const V a, const V b) {
+  return Le(b, a);
 }
 
-template <class D>
-using MFromD = decltype(MaskFromVec(Zero(D())));
+// ------------------------------ TestBit
+template <class V>
+HWY_API svbool_t TestBit(const V a, const V bit) {
+  return Ne(And(a, bit), Zero(DFromV<V>()));
+}
 
-template <class D, typename MFrom>
-HWY_API MFromD<D> RebindMask(const D /*d*/, const MFrom mask) {
-  // No need to check lane size/LMUL are the same: if not, casting MFrom to
-  // MFromD<D> would fail.
-  return mask;
+// ------------------------------ MaskFromVec (Ne)
+template <class V>
+HWY_API svbool_t MaskFromVec(const V v) {
+  return Ne(v, Zero(DFromV<V>()));
 }
 
 // ------------------------------ VecFromMask
 
 template <class D, HWY_IF_NOT_FLOAT_D(D)>
-HWY_API VFromD<D> VecFromMask(const D d, MFromD<D> mask) {
-  const auto v0 = Zero(d);
-  return detail::Or(v0, -1, mask, v0);
+HWY_API VFromD<D> VecFromMask(const D d, svbool_t mask) {
+  const auto v0 = Zero(RebindToSigned<decltype(d)>());
+  return BitCast(d, detail::SubN(mask, v0, 1));
 }
 
 template <class D, HWY_IF_FLOAT_D(D)>
-HWY_API VFromD<D> VecFromMask(const D d, MFromD<D> mask) {
+HWY_API VFromD<D> VecFromMask(const D d, svbool_t mask) {
   return BitCast(d, VecFromMask(RebindToUnsigned<D>(), mask));
 }
 
-// ------------------------------ ZeroIfNegative
-
-template <class V>
-HWY_API V ZeroIfNegative(const V v) {
-  const auto v0 = Zero(DFromV<V>());
-  // We already have a zero constant, so avoid IfThenZeroElse.
-  return IfThenElse(Lt(v, v0), v0, v);
-}
-
-// ------------------------------ BroadcastSignBit
-
-template <class V>
-HWY_API V BroadcastSignBit(const V v) {
-  return ShiftRight<sizeof(TFromV<V>) * 8 - 1>(v);
-}
+// ================================================== MEMORY
 
-// ------------------------------ AllFalse
+// ------------------------------ Load/Store/Stream
 
-#define HWY_SVE_ALL_FALSE(MLEN, NAME, OP)          \
-  HWY_API bool AllFalse(const HWY_SVE_M(MLEN) m) { \
-    return vfirst_m_b##MLEN(m) < 0;                \
+#define HWY_SVE_LOAD(BASE, CHAR, BITS, NAME, OP)           \
+  template <size_t N>                                      \
+  HWY_API HWY_SVE_V(BASE, BITS)                            \
+      NAME(HWY_SVE_D(BASE, BITS, N) d,                     \
+           const HWY_SVE_T(BASE, BITS) * HWY_RESTRICT p) { \
+    return sv##OP##_##CHAR##BITS(detail::Mask(d), p);      \
   }
-HWY_SVE_FOREACH_B(HWY_SVE_ALL_FALSE, _, _)
-#undef HWY_SVE_ALL_FALSE
-
-// ------------------------------ AllTrue
 
-#define HWY_SVE_ALL_TRUE(MLEN, NAME, OP)    \
-  HWY_API bool AllTrue(HWY_SVE_M(MLEN) m) { \
-    return AllFalse(vmnot_m_b##MLEN(m));    \
+#define HWY_SVE_LOAD_DUP128(BASE, CHAR, BITS, NAME, OP)    \
+  template <size_t N>                                      \
+  HWY_API HWY_SVE_V(BASE, BITS)                            \
+      NAME(HWY_SVE_D(BASE, BITS, N) d,                     \
+           const HWY_SVE_T(BASE, BITS) * HWY_RESTRICT p) { \
+    /* All-true predicate to load all 128 bits. */         \
+    return sv##OP##_##CHAR##BITS(HWY_SVE_PTRUE(8), p);     \
   }
-HWY_SVE_FOREACH_B(HWY_SVE_ALL_TRUE, _, _)
-#undef HWY_SVE_ALL_TRUE
-
-// ------------------------------ CountTrue
-
-#define HWY_SVE_COUNT_TRUE(MLEN, NAME, OP) \
-  HWY_API size_t CountTrue(HWY_SVE_M(MLEN) m) { return vpopc_m_b##MLEN(m); }
-HWY_SVE_FOREACH_B(HWY_SVE_COUNT_TRUE, _, _)
-#undef HWY_SVE_COUNT_TRUE
 
-// ================================================== MEMORY
+#define HWY_SVE_STORE(BASE, CHAR, BITS, NAME, OP)                        \
+  template <size_t N>                                                    \
+  HWY_API void NAME(HWY_SVE_V(BASE, BITS) v, HWY_SVE_D(BASE, BITS, N) d, \
+                    HWY_SVE_T(BASE, BITS) * HWY_RESTRICT p) {            \
+    sv##OP##_##CHAR##BITS(detail::Mask(d), p, v);                        \
+  }
 
-// ------------------------------ Load
+HWY_SVE_FOREACH(HWY_SVE_LOAD, Load, ld1)
+HWY_SVE_FOREACH(HWY_SVE_LOAD_DUP128, LoadDup128, ld1rq)
+HWY_SVE_FOREACH(HWY_SVE_STORE, Store, st1)
+HWY_SVE_FOREACH(HWY_SVE_STORE, Stream, stnt1)
 
-#define HWY_SVE_LOAD(BASE, CHAR, BITS, NAME, OP)                               \
-  HWY_API HWY_SVE_V(BASE, BITS) NAME(                                          \
-      HWY_SVE_D(CHAR, BITS) d, const HWY_SVE_T(BASE, BITS) * HWY_RESTRICT p) { \
-    (void)Lanes(d);                                                            \
-    return v##OP##BITS##_v_##CHAR##BITS(p);                                    \
-  }
-HWY_SVE_FOREACH(HWY_SVE_LOAD, Load, le)
 #undef HWY_SVE_LOAD
+#undef HWY_SVE_LOAD_DUP128
+#undef HWY_SVE_STORE
 
-// Partial load
-template <typename T, size_t N, HWY_IF_LE128(T, N)>
-HWY_API VFromD<Simd<T, N>> Load(Simd<T, N> d, const T* HWY_RESTRICT p) {
-  return Load(d, p);
-}
-
-// ------------------------------ LoadU
+// ------------------------------ Load/StoreU
 
-// SVE only requires lane alignment, not natural alignment of the entire vector.
+// SVE only requires lane alignment, not natural alignment of the entire
+// vector.
 template <class D>
 HWY_API VFromD<D> LoadU(D d, const TFromD<D>* HWY_RESTRICT p) {
   return Load(d, p);
 }
 
-// ------------------------------ Store
-
-#define HWY_SVE_RET_ARGVDP(BASE, CHAR, BITS, NAME, OP)                \
-  HWY_API void NAME(HWY_SVE_V(BASE, BITS) v, HWY_SVE_D(CHAR, BITS) d, \
-                    HWY_SVE_T(BASE, BITS) * HWY_RESTRICT p) {         \
-    (void)Lanes(d);                                                   \
-    return v##OP##BITS##_v_##CHAR##BITS(p, v);                        \
-  }
-HWY_SVE_FOREACH(HWY_SVE_RET_ARGVDP, Store, se)
-#undef HWY_SVE_RET_ARGVDP
-
-// ------------------------------ StoreU
-
-// SVE only requires lane alignment, not natural alignment of the entire vector.
 template <class V, class D>
 HWY_API void StoreU(const V v, D d, TFromD<D>* HWY_RESTRICT p) {
   Store(v, d, p);
 }
 
-// ------------------------------ Stream
-
-template <class V, class D, typename T>
-HWY_API void Stream(const V v, D d, T* HWY_RESTRICT aligned) {
-  Store(v, d, aligned);
-}
-
-// ------------------------------ ScatterOffset
+// ------------------------------ ScatterOffset/Index
 
-#define HWY_SVE_SCATTER(BASE, CHAR, BITS, NAME, OP)                         \
-  HWY_API void NAME(HWY_SVE_V(BASE, BITS) v, HWY_SVE_D(CHAR, BITS) /* d */, \
-                    HWY_SVE_T(BASE, BITS) * HWY_RESTRICT base,              \
-                    HWY_SVE_V(int, BITS) offset) {                          \
-    return v##OP##ei##BITS##_v_##CHAR##BITS(                                \
-        base, detail::BitCastToUnsigned(offset), v);                        \
+#define HWY_SVE_SCATTER_OFFSET(BASE, CHAR, BITS, NAME, OP)                   \
+  template <size_t N>                                                        \
+  HWY_API void NAME(HWY_SVE_V(BASE, BITS) v, HWY_SVE_D(BASE, BITS, N) d,     \
+                    HWY_SVE_T(BASE, BITS) * HWY_RESTRICT base,               \
+                    HWY_SVE_V(int, BITS) offset) {                           \
+    sv##OP##_s##BITS##offset_##CHAR##BITS(detail::Mask(d), base, offset, v); \
   }
-HWY_SVE_FOREACH(HWY_SVE_SCATTER, ScatterOffset, sx)
-#undef HWY_SVE_SCATTER
 
-// ------------------------------ ScatterIndex
-
-template <class D, HWY_IF_LANE_SIZE_D(D, 4)>
-HWY_API void ScatterIndex(VFromD<D> v, D d, TFromD<D>* HWY_RESTRICT base,
-                          const VFromD<RebindToSigned<D>> index) {
-  return ScatterOffset(v, d, base, ShiftLeft<2>(index));
-}
+#define HWY_SVE_SCATTER_INDEX(BASE, CHAR, BITS, NAME, OP)                  \
+  template <size_t N>                                                      \
+  HWY_API void NAME(HWY_SVE_V(BASE, BITS) v, HWY_SVE_D(BASE, BITS, N) d,   \
+                    HWY_SVE_T(BASE, BITS) * HWY_RESTRICT base,             \
+                    HWY_SVE_V(int, BITS) index) {                          \
+    sv##OP##_s##BITS##index_##CHAR##BITS(detail::Mask(d), base, index, v); \
+  }
 
-template <class D, HWY_IF_LANE_SIZE_D(D, 8)>
-HWY_API void ScatterIndex(VFromD<D> v, D d, TFromD<D>* HWY_RESTRICT base,
-                          const VFromD<RebindToSigned<D>> index) {
-  return ScatterOffset(v, d, base, ShiftLeft<3>(index));
-}
+HWY_SVE_FOREACH_UIF3264(HWY_SVE_SCATTER_OFFSET, ScatterOffset, st1_scatter)
+HWY_SVE_FOREACH_UIF3264(HWY_SVE_SCATTER_INDEX, ScatterIndex, st1_scatter)
+#undef HWY_SVE_SCATTER_OFFSET
+#undef HWY_SVE_SCATTER_INDEX
 
-// ------------------------------ GatherOffset
+// ------------------------------ GatherOffset/Index
 
-#define HWY_SVE_GATHER(BASE, CHAR, BITS, NAME, OP)          \
-  HWY_API HWY_SVE_V(BASE, BITS)                             \
-      NAME(HWY_SVE_D(CHAR, BITS) /* d */,                   \
-           const HWY_SVE_T(BASE, BITS) * HWY_RESTRICT base, \
-           HWY_SVE_V(int, BITS) offset) {                   \
-    return v##OP##ei##BITS##_v_##CHAR##BITS(                \
-        base, detail::BitCastToUnsigned(offset));           \
+#define HWY_SVE_GATHER_OFFSET(BASE, CHAR, BITS, NAME, OP)               \
+  template <size_t N>                                                   \
+  HWY_API HWY_SVE_V(BASE, BITS)                                         \
+      NAME(HWY_SVE_D(BASE, BITS, N) d,                                  \
+           const HWY_SVE_T(BASE, BITS) * HWY_RESTRICT base,             \
+           HWY_SVE_V(int, BITS) offset) {                               \
+    return sv##OP##_s##BITS##offset_##CHAR##BITS(detail::Mask(d), base, \
+                                                 offset);               \
+  }
+#define HWY_SVE_GATHER_INDEX(BASE, CHAR, BITS, NAME, OP)                       \
+  template <size_t N>                                                          \
+  HWY_API HWY_SVE_V(BASE, BITS)                                                \
+      NAME(HWY_SVE_D(BASE, BITS, N) d,                                         \
+           const HWY_SVE_T(BASE, BITS) * HWY_RESTRICT base,                    \
+           HWY_SVE_V(int, BITS) index) {                                       \
+    return sv##OP##_s##BITS##index_##CHAR##BITS(detail::Mask(d), base, index); \
   }
-HWY_SVE_FOREACH(HWY_SVE_GATHER, GatherOffset, lx)
-#undef HWY_SVE_GATHER
-
-// ------------------------------ GatherIndex
-
-template <class D, HWY_IF_LANE_SIZE_D(D, 4)>
-HWY_API VFromD<D> GatherIndex(D d, const TFromD<D>* HWY_RESTRICT base,
-                              const VFromD<RebindToSigned<D>> index) {
-  return GatherOffset(d, base, ShiftLeft<2>(index));
-}
 
-template <class D, HWY_IF_LANE_SIZE_D(D, 8)>
-HWY_API VFromD<D> GatherIndex(D d, const TFromD<D>* HWY_RESTRICT base,
-                              const VFromD<RebindToSigned<D>> index) {
-  return GatherOffset(d, base, ShiftLeft<3>(index));
-}
+HWY_SVE_FOREACH_UIF3264(HWY_SVE_GATHER_OFFSET, GatherOffset, ld1_gather)
+HWY_SVE_FOREACH_UIF3264(HWY_SVE_GATHER_INDEX, GatherIndex, ld1_gather)
+#undef HWY_SVE_GATHER_OFFSET
+#undef HWY_SVE_GATHER_INDEX
 
 // ------------------------------ StoreInterleaved3
 
-#define HWY_SVE_STORE3(BASE, CHAR, BITS, NAME, OP)                          \
-  HWY_API void NAME(HWY_SVE_V(BASE, BITS) a, HWY_SVE_V(BASE, BITS) b,       \
-                    HWY_SVE_V(BASE, BITS) c, HWY_SVE_D(CHAR, BITS) /* d */, \
-                    HWY_SVE_T(BASE, BITS) * HWY_RESTRICT unaligned) {       \
-    const v##BASE##BITS##x3_t triple = vcreate_##CHAR##BITS##x3(a, b, c);   \
-    return v##OP##e8_v_##CHAR##BITS##x3(unaligned, triple);                 \
+#define HWY_SVE_STORE3(BASE, CHAR, BITS, NAME, OP)                            \
+  template <size_t N>                                                         \
+  HWY_API void NAME(HWY_SVE_V(BASE, BITS) v0, HWY_SVE_V(BASE, BITS) v1,       \
+                    HWY_SVE_V(BASE, BITS) v2, HWY_SVE_D(BASE, BITS, N) d,     \
+                    HWY_SVE_T(BASE, BITS) * HWY_RESTRICT unaligned) {         \
+    const sv##BASE##BITS##x3_t triple = svcreate3##_##CHAR##BITS(v0, v1, v2); \
+    sv##OP##_##CHAR##BITS(detail::Mask(d), unaligned, triple);                \
   }
-// Segments are limited to 8 registers, so we can only go up to LMUL=2.
-HWY_SVE_STORE3(uint, u, 8, 1, 8, StoreInterleaved3, sseg3)
-HWY_SVE_STORE3(uint, u, 8, 2, 4, StoreInterleaved3, sseg3)
+HWY_SVE_FOREACH_U08(HWY_SVE_STORE3, StoreInterleaved3, st3)
 
 #undef HWY_SVE_STORE3
 
 // ------------------------------ StoreInterleaved4
 
-#define HWY_SVE_STORE4(BASE, CHAR, BITS, NAME, OP)                             \
-  HWY_API void NAME(HWY_SVE_V(BASE, BITS) v0, HWY_SVE_V(BASE, BITS) v1,        \
-                    HWY_SVE_V(BASE, BITS) v2, HWY_SVE_V(BASE, BITS) v3,        \
-                    HWY_SVE_D(CHAR, BITS) /* d */,                             \
-                    HWY_SVE_T(BASE, BITS) * HWY_RESTRICT aligned) {            \
-    const v##BASE##BITS##x4_t quad = vcreate_##CHAR##BITS##x4(v0, v1, v2, v3); \
-    return v##OP##e8_v_##CHAR##BITS##x4(aligned, quad);                        \
+#define HWY_SVE_STORE4(BASE, CHAR, BITS, NAME, OP)                      \
+  template <size_t N>                                                   \
+  HWY_API void NAME(HWY_SVE_V(BASE, BITS) v0, HWY_SVE_V(BASE, BITS) v1, \
+                    HWY_SVE_V(BASE, BITS) v2, HWY_SVE_V(BASE, BITS) v3, \
+                    HWY_SVE_D(BASE, BITS, N) d,                         \
+                    HWY_SVE_T(BASE, BITS) * HWY_RESTRICT unaligned) {   \
+    const sv##BASE##BITS##x4_t quad =                                   \
+        svcreate4##_##CHAR##BITS(v0, v1, v2, v3);                       \
+    sv##OP##_##CHAR##BITS(detail::Mask(d), unaligned, quad);            \
   }
-// Segments are limited to 8 registers, so we can only go up to LMUL=2.
-HWY_SVE_STORE4(uint, u, 8, 1, 8, StoreInterleaved4, sseg4)
-HWY_SVE_STORE4(uint, u, 8, 2, 4, StoreInterleaved4, sseg4)
+HWY_SVE_FOREACH_U08(HWY_SVE_STORE4, StoreInterleaved4, st4)
 
 #undef HWY_SVE_STORE4
 
 // ================================================== CONVERT
 
-// ------------------------------ PromoteTo U
-
-HWY_API Vu16m2 PromoteTo(Du16m2 /* d */, Vu8m1 v) { return vzext_vf2_u16m2(v); }
-HWY_API Vu16m4 PromoteTo(Du16m4 /* d */, Vu8m2 v) { return vzext_vf2_u16m4(v); }
-HWY_API Vu16m8 PromoteTo(Du16m8 /* d */, Vu8m4 v) { return vzext_vf2_u16m8(v); }
+// ------------------------------ PromoteTo
+
+// Same sign
+#define HWY_SVE_PROMOTE_TO(BASE, CHAR, BITS, NAME, OP)        \
+  template <size_t N>                                         \
+  HWY_API HWY_SVE_V(BASE, BITS)                               \
+      NAME(HWY_SVE_D(BASE, BITS, N) /* tag */,                \
+           VFromD<Simd<MakeNarrow<HWY_SVE_T(BASE, BITS)>,     \
+                       HWY_LANES(HWY_SVE_T(BASE, BITS)) * 2>> \
+               v) {                                           \
+    return sv##OP##_##CHAR##BITS(v);                          \
+  }
 
-HWY_API Vu32m4 PromoteTo(Du32m4 /* d */, Vu8m1 v) { return vzext_vf4_u32m4(v); }
-HWY_API Vu32m8 PromoteTo(Du32m8 /* d */, Vu8m2 v) { return vzext_vf4_u32m8(v); }
+HWY_SVE_FOREACH_UI16(HWY_SVE_PROMOTE_TO, PromoteTo, unpklo)
+HWY_SVE_FOREACH_UI32(HWY_SVE_PROMOTE_TO, PromoteTo, unpklo)
+HWY_SVE_FOREACH_UI64(HWY_SVE_PROMOTE_TO, PromoteTo, unpklo)
 
-HWY_API Vu32m2 PromoteTo(Du32m2 /* d */, const Vu16m1 v) {
-  return vzext_vf2_u32m2(v);
+// 2x
+template <size_t N>
+HWY_API svuint32_t PromoteTo(Simd<uint32_t, N> dto, svuint8_t vfrom) {
+  const RepartitionToWide<DFromV<decltype(vfrom)>> d2;
+  return PromoteTo(dto, PromoteTo(d2, vfrom));
 }
-HWY_API Vu32m4 PromoteTo(Du32m4 /* d */, const Vu16m2 v) {
-  return vzext_vf2_u32m4(v);
+template <size_t N>
+HWY_API svint32_t PromoteTo(Simd<int32_t, N> dto, svint8_t vfrom) {
+  const RepartitionToWide<DFromV<decltype(vfrom)>> d2;
+  return PromoteTo(dto, PromoteTo(d2, vfrom));
 }
-HWY_API Vu32m8 PromoteTo(Du32m8 /* d */, const Vu16m4 v) {
-  return vzext_vf2_u32m8(v);
+template <size_t N>
+HWY_API svuint32_t U32FromU8(svuint8_t v) {
+  return PromoteTo(Simd<uint32_t, N>(), v);
 }
 
-HWY_API Vu64m2 PromoteTo(Du64m2 /* d */, const Vu32m1 v) {
-  return vzext_vf2_u64m2(v);
+// Sign change
+template <size_t N>
+HWY_API svint16_t PromoteTo(Simd<int16_t, N> dto, svuint8_t vfrom) {
+  const RebindToUnsigned<decltype(dto)> du;
+  return BitCast(dto, PromoteTo(du, vfrom));
 }
-HWY_API Vu64m4 PromoteTo(Du64m4 /* d */, const Vu32m2 v) {
-  return vzext_vf2_u64m4(v);
+template <size_t N>
+HWY_API svint32_t PromoteTo(Simd<int32_t, N> dto, svuint16_t vfrom) {
+  const RebindToUnsigned<decltype(dto)> du;
+  return BitCast(dto, PromoteTo(du, vfrom));
 }
-HWY_API Vu64m8 PromoteTo(Du64m8 /* d */, const Vu32m4 v) {
-  return vzext_vf2_u64m8(v);
+template <size_t N>
+HWY_API svint32_t PromoteTo(Simd<int32_t, N> dto, svuint8_t vfrom) {
+  const Repartition<uint16_t, DFromV<decltype(vfrom)>> du16;
+  const Repartition<int16_t, decltype(du16)> di16;
+  return PromoteTo(dto, BitCast(di16, PromoteTo(du16, vfrom)));
 }
 
+// ------------------------------ PromoteTo F
+
 template <size_t N>
-HWY_API VFromD<Simd<int16_t, N>> PromoteTo(Simd<int16_t, N> d,
-                                           VFromD<Simd<uint8_t, N>> v) {
-  return BitCast(d, PromoteTo(Simd<uint16_t, N>(), v));
+HWY_API svfloat32_t PromoteTo(Simd<float32_t, N> /* d */, const svfloat16_t v) {
+  return svcvt_f32_f16_x(detail::PTrue(Simd<float16_t, N>()), v);
 }
 
 template <size_t N>
-HWY_API VFromD<Simd<int32_t, N>> PromoteTo(Simd<int32_t, N> d,
-                                           VFromD<Simd<uint8_t, N>> v) {
-  return BitCast(d, PromoteTo(Simd<uint32_t, N>(), v));
+HWY_API svfloat64_t PromoteTo(Simd<float64_t, N> /* d */, const svfloat32_t v) {
+  return svcvt_f64_f32_x(detail::PTrue(Simd<float32_t, N>()), v);
 }
 
 template <size_t N>
-HWY_API VFromD<Simd<int32_t, N>> PromoteTo(Simd<int32_t, N> d,
-                                           VFromD<Simd<uint16_t, N>> v) {
-  return BitCast(d, PromoteTo(Simd<uint32_t, N>(), v));
+HWY_API svfloat64_t PromoteTo(Simd<float64_t, N> /* d */, const svint32_t v) {
+  return svcvt_f64_s32_x(detail::PTrue(Simd<int32_t, N>()), v);
 }
 
-// ------------------------------ PromoteTo I
-
-HWY_API Vi16m2 PromoteTo(Di16m2 /* d */, Vi8m1 v) { return vsext_vf2_i16m2(v); }
-HWY_API Vi16m4 PromoteTo(Di16m4 /* d */, Vi8m2 v) { return vsext_vf2_i16m4(v); }
-HWY_API Vi16m8 PromoteTo(Di16m8 /* d */, Vi8m4 v) { return vsext_vf2_i16m8(v); }
-
-HWY_API Vi32m4 PromoteTo(Di32m4 /* d */, Vi8m1 v) { return vsext_vf4_i32m4(v); }
-HWY_API Vi32m8 PromoteTo(Di32m8 /* d */, Vi8m2 v) { return vsext_vf4_i32m8(v); }
+// For 16-bit Compress
+namespace detail {
+HWY_SVE_FOREACH_UI32(HWY_SVE_PROMOTE_TO, PromoteUpperTo, unpkhi)
+#undef HWY_SVE_PROMOTE_TO
 
-HWY_API Vi32m2 PromoteTo(Di32m2 /* d */, const Vi16m1 v) {
-  return vsext_vf2_i32m2(v);
-}
-HWY_API Vi32m4 PromoteTo(Di32m4 /* d */, const Vi16m2 v) {
-  return vsext_vf2_i32m4(v);
-}
-HWY_API Vi32m8 PromoteTo(Di32m8 /* d */, const Vi16m4 v) {
-  return vsext_vf2_i32m8(v);
+template <size_t N>
+HWY_API svfloat32_t PromoteUpperTo(Simd<float, N> df, const svfloat16_t v) {
+  const RebindToUnsigned<decltype(df)> du;
+  const RepartitionToNarrow<decltype(du)> dn;
+  return BitCast(df, PromoteUpperTo(du, BitCast(dn, v)));
 }
 
-HWY_API Vi64m2 PromoteTo(Di64m2 /* d */, const Vi32m1 v) {
-  return vsext_vf2_i64m2(v);
-}
-HWY_API Vi64m4 PromoteTo(Di64m4 /* d */, const Vi32m2 v) {
-  return vsext_vf2_i64m4(v);
-}
-HWY_API Vi64m8 PromoteTo(Di64m8 /* d */, const Vi32m4 v) {
-  return vsext_vf2_i64m8(v);
-}
+}  // namespace detail
 
-// ------------------------------ PromoteTo F
+// ------------------------------ DemoteTo U
 
-HWY_API Vf32m2 PromoteTo(Df32m2 /* d */, const Vf16m1 v) {
-  return vfwcvt_f_f_v_f32m2(v);
-}
-HWY_API Vf32m4 PromoteTo(Df32m4 /* d */, const Vf16m2 v) {
-  return vfwcvt_f_f_v_f32m4(v);
-}
-HWY_API Vf32m8 PromoteTo(Df32m8 /* d */, const Vf16m4 v) {
-  return vfwcvt_f_f_v_f32m8(v);
-}
+namespace detail {
 
-HWY_API Vf64m2 PromoteTo(Df64m2 /* d */, const Vf32m1 v) {
-  return vfwcvt_f_f_v_f64m2(v);
-}
-HWY_API Vf64m4 PromoteTo(Df64m4 /* d */, const Vf32m2 v) {
-  return vfwcvt_f_f_v_f64m4(v);
-}
-HWY_API Vf64m8 PromoteTo(Df64m8 /* d */, const Vf32m4 v) {
-  return vfwcvt_f_f_v_f64m8(v);
+// Saturates unsigned vectors to half/quarter-width TN.
+template <typename TN, class VU>
+VU SaturateU(VU v) {
+  return detail::MinN(v, static_cast<TFromV<VU>>(LimitsMax<TN>()));
 }
 
-HWY_API Vf64m2 PromoteTo(Df64m2 /* d */, const Vi32m1 v) {
-  return vfwcvt_f_x_v_f64m2(v);
-}
-HWY_API Vf64m4 PromoteTo(Df64m4 /* d */, const Vi32m2 v) {
-  return vfwcvt_f_x_v_f64m4(v);
-}
-HWY_API Vf64m8 PromoteTo(Df64m8 /* d */, const Vi32m4 v) {
-  return vfwcvt_f_x_v_f64m8(v);
+// Saturates unsigned vectors to half/quarter-width TN.
+template <typename TN, class VI>
+VI SaturateI(VI v) {
+  const DFromV<VI> di;
+  return detail::MinN(detail::MaxN(v, LimitsMin<TN>()), LimitsMax<TN>());
 }
 
-// ------------------------------ DemoteTo U
-
-// First clamp negative numbers to zero to match x86 packus.
-HWY_API Vu16m1 DemoteTo(Du16m1 /* d */, const Vi32m2 v) {
-  return vnclipu_wx_u16m1(detail::BitCastToUnsigned(detail::Max(v, 0)), 0);
-}
-HWY_API Vu16m2 DemoteTo(Du16m2 /* d */, const Vi32m4 v) {
-  return vnclipu_wx_u16m2(detail::BitCastToUnsigned(detail::Max(v, 0)), 0);
-}
-HWY_API Vu16m4 DemoteTo(Du16m4 /* d */, const Vi32m8 v) {
-  return vnclipu_wx_u16m4(detail::BitCastToUnsigned(detail::Max(v, 0)), 0);
-}
+}  // namespace detail
 
-HWY_API Vu8m1 DemoteTo(Du8m1 /* d */, const Vi32m4 v) {
-  return vnclipu_wx_u8m1(DemoteTo(Du16m2(), v), 0);
-}
-HWY_API Vu8m2 DemoteTo(Du8m2 /* d */, const Vi32m8 v) {
-  return vnclipu_wx_u8m2(DemoteTo(Du16m4(), v), 0);
+template <size_t N>
+HWY_API svuint8_t DemoteTo(Simd<uint8_t, N> dn, const svint16_t v) {
+  const DFromV<decltype(v)> di;
+  const RebindToUnsigned<decltype(di)> du;
+  using TN = TFromD<decltype(dn)>;
+  // First clamp negative numbers to zero and cast to unsigned.
+  const svuint16_t clamped = BitCast(du, Max(Zero(di), v));
+  // Saturate to unsigned-max and halve the width.
+  const svuint8_t vn = BitCast(dn, detail::SaturateU<TN>(clamped));
+  return svuzp1_u8(vn, vn);
 }
 
-HWY_API Vu8m1 DemoteTo(Du8m1 /* d */, const Vi16m2 v) {
-  return vnclipu_wx_u8m1(detail::BitCastToUnsigned(detail::Max(v, 0)), 0);
-}
-HWY_API Vu8m2 DemoteTo(Du8m2 /* d */, const Vi16m4 v) {
-  return vnclipu_wx_u8m2(detail::BitCastToUnsigned(detail::Max(v, 0)), 0);
-}
-HWY_API Vu8m4 DemoteTo(Du8m4 /* d */, const Vi16m8 v) {
-  return vnclipu_wx_u8m4(detail::BitCastToUnsigned(detail::Max(v, 0)), 0);
+template <size_t N>
+HWY_API svuint16_t DemoteTo(Simd<uint16_t, N> dn, const svint32_t v) {
+  const DFromV<decltype(v)> di;
+  const RebindToUnsigned<decltype(di)> du;
+  using TN = TFromD<decltype(dn)>;
+  // First clamp negative numbers to zero and cast to unsigned.
+  const svuint32_t clamped = BitCast(du, Max(Zero(di), v));
+  // Saturate to unsigned-max and halve the width.
+  const svuint16_t vn = BitCast(dn, detail::SaturateU<TN>(clamped));
+  return svuzp1_u16(vn, vn);
 }
 
-HWY_API Vu8m1 U8FromU32(const Vu32m4 v) {
-  return vnclipu_wx_u8m1(vnclipu_wx_u16m2(v, 0), 0);
-}
-HWY_API Vu8m2 U8FromU32(const Vu32m8 v) {
-  return vnclipu_wx_u8m2(vnclipu_wx_u16m4(v, 0), 0);
+template <size_t N>
+HWY_API svuint8_t DemoteTo(Simd<uint8_t, N> dn, const svint32_t v) {
+  const DFromV<decltype(v)> di;
+  const RebindToUnsigned<decltype(di)> du;
+  const RepartitionToNarrow<decltype(du)> d2;
+  using TN = TFromD<decltype(dn)>;
+  // First clamp negative numbers to zero and cast to unsigned.
+  const svuint32_t clamped = BitCast(du, Max(Zero(di), v));
+  // Saturate to unsigned-max and quarter the width.
+  const svuint16_t cast16 = BitCast(d2, detail::SaturateU<TN>(clamped));
+  const svuint8_t x2 = BitCast(dn, svuzp1_u16(cast16, cast16));
+  return svuzp1_u8(x2, x2);
+}
+
+HWY_API svuint8_t U8FromU32(const svuint32_t v) {
+  const DFromV<svuint32_t> du32;
+  const RepartitionToNarrow<decltype(du32)> du16;
+  const RepartitionToNarrow<decltype(du16)> du8;
+
+  const svuint16_t cast16 = BitCast(du16, v);
+  const svuint16_t x2 = svuzp1_u16(cast16, cast16);
+  const svuint8_t cast8 = BitCast(du8, x2);
+  return svuzp1_u8(cast8, cast8);
 }
 
 // ------------------------------ DemoteTo I
 
-HWY_API Vi8m1 DemoteTo(Di8m1 /* d */, const Vi16m2 v) {
-  return vnclip_wx_i8m1(v, 0);
-}
-HWY_API Vi8m2 DemoteTo(Di8m2 /* d */, const Vi16m4 v) {
-  return vnclip_wx_i8m2(v, 0);
-}
-HWY_API Vi8m4 DemoteTo(Di8m4 /* d */, const Vi16m8 v) {
-  return vnclip_wx_i8m4(v, 0);
+template <size_t N>
+HWY_API svint8_t DemoteTo(Simd<int8_t, N> dn, const svint16_t v) {
+  const DFromV<decltype(v)> di;
+  using TN = TFromD<decltype(dn)>;
+#if HWY_TARGET == HWY_SVE2
+  const svint8_t vn = BitCast(dn, svqxtnb_s16(v));
+#else
+  const svint8_t vn = BitCast(dn, detail::SaturateI<TN>(v));
+#endif
+  return svuzp1_s8(vn, vn);
 }
 
-HWY_API Vi16m1 DemoteTo(Di16m1 /* d */, const Vi32m2 v) {
-  return vnclip_wx_i16m1(v, 0);
-}
-HWY_API Vi16m2 DemoteTo(Di16m2 /* d */, const Vi32m4 v) {
-  return vnclip_wx_i16m2(v, 0);
-}
-HWY_API Vi16m4 DemoteTo(Di16m4 /* d */, const Vi32m8 v) {
-  return vnclip_wx_i16m4(v, 0);
+template <size_t N>
+HWY_API svint16_t DemoteTo(Simd<int16_t, N> dn, const svint32_t v) {
+  const DFromV<decltype(v)> di;
+  using TN = TFromD<decltype(dn)>;
+#if HWY_TARGET == HWY_SVE2
+  const svint16_t vn = BitCast(dn, svqxtnb_s32(v));
+#else
+  const svint16_t vn = BitCast(dn, detail::SaturateI<TN>(v));
+#endif
+  return svuzp1_s16(vn, vn);
 }
 
-HWY_API Vi8m1 DemoteTo(Di8m1 d, const Vi32m4 v) {
-  return DemoteTo(d, DemoteTo(Di16m2(), v));
-}
-HWY_API Vi8m2 DemoteTo(Di8m2 d, const Vi32m8 v) {
-  return DemoteTo(d, DemoteTo(Di16m4(), v));
+template <size_t N>
+HWY_API svint8_t DemoteTo(Simd<int8_t, N> dn, const svint32_t v) {
+  const DFromV<decltype(v)> di;
+  using TN = TFromD<decltype(dn)>;
+  const RepartitionToWide<decltype(dn)> d2;
+#if HWY_TARGET == HWY_SVE2
+  const svint16_t cast16 = BitCast(d2, svqxtnb_s16(svqxtnb_s32(v)));
+#else
+  const svint16_t cast16 = BitCast(d2, detail::SaturateI<TN>(v));
+#endif
+  const svint8_t v2 = BitCast(dn, svuzp1_s16(cast16, cast16));
+  return BitCast(dn, svuzp1_s8(v2, v2));
 }
 
 // ------------------------------ DemoteTo F
 
-HWY_API Vf16m1 DemoteTo(Df16m1 /* d */, const Vf32m2 v) {
-  return vfncvt_rod_f_f_w_f16m1(v);
-}
-HWY_API Vf16m2 DemoteTo(Df16m2 /* d */, const Vf32m4 v) {
-  return vfncvt_rod_f_f_w_f16m2(v);
-}
-HWY_API Vf16m4 DemoteTo(Df16m4 /* d */, const Vf32m8 v) {
-  return vfncvt_rod_f_f_w_f16m4(v);
+template <size_t N>
+HWY_API svfloat16_t DemoteTo(Simd<float16_t, N> d, const svfloat32_t v) {
+  return svcvt_f16_f32_x(detail::PTrue(d), v);
 }
 
-HWY_API Vf32m1 DemoteTo(Df32m1 /* d */, const Vf64m2 v) {
-  return vfncvt_rod_f_f_w_f32m1(v);
-}
-HWY_API Vf32m2 DemoteTo(Df32m2 /* d */, const Vf64m4 v) {
-  return vfncvt_rod_f_f_w_f32m2(v);
-}
-HWY_API Vf32m4 DemoteTo(Df32m4 /* d */, const Vf64m8 v) {
-  return vfncvt_rod_f_f_w_f32m4(v);
+template <size_t N>
+HWY_API svfloat32_t DemoteTo(Simd<float32_t, N> d, const svfloat64_t v) {
+  return svcvt_f32_f64_x(detail::PTrue(d), v);
 }
 
-HWY_API Vi32m1 DemoteTo(Di32m1 /* d */, const Vf64m2 v) {
-  return vfncvt_rtz_x_f_w_i32m1(v);
-}
-HWY_API Vi32m2 DemoteTo(Di32m2 /* d */, const Vf64m4 v) {
-  return vfncvt_rtz_x_f_w_i32m2(v);
-}
-HWY_API Vi32m4 DemoteTo(Di32m4 /* d */, const Vf64m8 v) {
-  return vfncvt_rtz_x_f_w_i32m4(v);
+template <size_t N>
+HWY_API svint32_t DemoteTo(Simd<int32_t, N> d, const svfloat64_t v) {
+  return svcvt_s32_f64_x(detail::PTrue(d), v);
 }
 
 // ------------------------------ ConvertTo F
 
-#define HWY_SVE_CONVERT(BASE, CHAR, BITS, NAME, OP)                      \
-  HWY_API HWY_SVE_V(BASE, BITS)                                          \
-      ConvertTo(HWY_SVE_D(CHAR, BITS) /* d */, HWY_SVE_V(int, BITS) v) { \
-    return vfcvt_f_x_v_f##BITS(v);                                       \
-  }                                                                      \
-  /* Truncates (rounds toward zero). */                                  \
-  HWY_API HWY_SVE_V(int, BITS)                                           \
-      ConvertTo(HWY_SVE_D(i, BITS) /* d */, HWY_SVE_V(BASE, BITS) v) {   \
-    return vfcvt_rtz_x_f_v_i##BITS(v);                                   \
-  }                                                                      \
-  /* Uses default rounding mode. */                                      \
-  HWY_API HWY_SVE_V(int, BITS) NearestInt(HWY_SVE_V(BASE, BITS) v) {     \
-    return vfcvt_x_f_v_i##BITS(v);                                       \
+#define HWY_SVE_CONVERT(BASE, CHAR, BITS, NAME, OP)                     \
+  template <size_t N>                                                   \
+  HWY_API HWY_SVE_V(BASE, BITS)                                         \
+      NAME(HWY_SVE_D(BASE, BITS, N) /* d */, HWY_SVE_V(int, BITS) v) {  \
+    return sv##OP##_##CHAR##BITS##_s##BITS##_x(HWY_SVE_PTRUE(BITS), v); \
+  }                                                                     \
+  /* Truncates (rounds toward zero). */                                 \
+  template <size_t N>                                                   \
+  HWY_API HWY_SVE_V(int, BITS)                                          \
+      NAME(HWY_SVE_D(int, BITS, N) /* d */, HWY_SVE_V(BASE, BITS) v) {  \
+    return sv##OP##_s##BITS##_##CHAR##BITS##_x(HWY_SVE_PTRUE(BITS), v); \
   }
 
-// API only requires f32 but we provide f64 for internal use (otherwise, it
-// seems difficult to implement Iota without a _mf2 vector half).
-HWY_SVE_FOREACH_F(HWY_SVE_CONVERT, _, _)
+// API only requires f32 but we provide f64 for use by Iota.
+HWY_SVE_FOREACH_F(HWY_SVE_CONVERT, ConvertTo, cvt)
 #undef HWY_SVE_CONVERT
 
-// ================================================== SWIZZLE
+// ------------------------------ NearestInt (Round, ConvertTo)
 
-// ------------------------------ Compress
+template <class VF, class DI = RebindToSigned<DFromV<VF>>>
+HWY_API VFromD<DI> NearestInt(VF v) {
+  // No single instruction, round then truncate.
+  return ConvertTo(DI(), Round(v));
+}
 
-#define HWY_SVE_COMPRESS(BASE, CHAR, BITS, NAME, OP)        \
-  HWY_API HWY_SVE_V(BASE, BITS)                             \
-      NAME(HWY_SVE_V(BASE, BITS) v, HWY_SVE_M(MLEN) mask) { \
-    return v##OP##_vm_##CHAR##BITS(mask, v, v);             \
-  }
+// ------------------------------ Iota (Add, ConvertTo)
 
-HWY_SVE_FOREACH_UI16(HWY_SVE_COMPRESS, Compress, compress)
-HWY_SVE_FOREACH_UI32(HWY_SVE_COMPRESS, Compress, compress)
-HWY_SVE_FOREACH_UI64(HWY_SVE_COMPRESS, Compress, compress)
-HWY_SVE_FOREACH_F(HWY_SVE_COMPRESS, Compress, compress)
-#undef HWY_SVE_COMPRESS
+#define HWY_SVE_IOTA(BASE, CHAR, BITS, NAME, OP)                      \
+  template <size_t N>                                                 \
+  HWY_API HWY_SVE_V(BASE, BITS)                                       \
+      NAME(HWY_SVE_D(BASE, BITS, N) d, HWY_SVE_T(BASE, BITS) first) { \
+    return sv##OP##_##CHAR##BITS(first, 1);                           \
+  }
 
-// ------------------------------ CompressStore
+HWY_SVE_FOREACH_UI(HWY_SVE_IOTA, Iota, index)
+#undef HWY_SVE_IOTA
 
-template <class V, class M, class D>
-HWY_API size_t CompressStore(const V v, const M mask, const D d,
-                             TFromD<D>* HWY_RESTRICT aligned) {
-  Store(Compress(v, mask), d, aligned);
-  return CountTrue(mask);
+template <class D, HWY_IF_FLOAT_D(D)>
+HWY_API VFromD<D> Iota(const D d, TFromD<D> first) {
+  const RebindToSigned<D> di;
+  return detail::AddN(ConvertTo(d, Iota(di, 0)), first);
 }
 
-// ------------------------------ TableLookupLanes
+// ================================================== COMBINE
 
-template <class D, class DU = RebindToUnsigned<D>>
-HWY_API VFromD<DU> SetTableIndices(D d, const TFromD<DU>* idx) {
-#if !defined(NDEBUG) || defined(ADDRESS_SANITIZER)
-  const size_t N = Lanes(d);
-  for (size_t i = 0; i < N; ++i) {
-    HWY_DASSERT(0 <= idx[i] && idx[i] < static_cast<TFromD<DU>>(N));
-  }
-#endif
-  return Load(DU(), idx);
+namespace detail {
+
+template <typename T, size_t N>
+svbool_t MaskLowerHalf(Simd<T, N> d) {
+  return FirstN(d, Lanes(d) / 2);
+}
+template <typename T, size_t N>
+svbool_t MaskUpperHalf(Simd<T, N> d) {
+  // For Splice to work as intended, make sure bits above Lanes(d) are zero.
+  return AndNot(MaskLowerHalf(d), detail::Mask(d));
 }
 
-// <32bit are not part of Highway API, but used in Broadcast. This limits VLMAX
-// to 2048! We could instead use vrgatherei16.
-#define HWY_SVE_TABLE(BASE, CHAR, BITS, NAME, OP)                \
+// Right-shift vector pair by constexpr; can be used to slide down (=N) or up
+// (=Lanes()-N).
+#define HWY_SVE_EXT(BASE, CHAR, BITS, NAME, OP)                  \
+  template <size_t kIndex>                                       \
   HWY_API HWY_SVE_V(BASE, BITS)                                  \
-      NAME(HWY_SVE_V(BASE, BITS) v, HWY_SVE_V(uint, BITS) idx) { \
-    return v##OP##_vv_##CHAR##BITS(v, idx);                      \
+      NAME(HWY_SVE_V(BASE, BITS) hi, HWY_SVE_V(BASE, BITS) lo) { \
+    return sv##OP##_##CHAR##BITS(lo, hi, kIndex);                \
   }
+HWY_SVE_FOREACH(HWY_SVE_EXT, Ext, ext)
+#undef HWY_SVE_EXT
+
+// Used to slide up / shift whole register left; mask indicates which range
+// to take from lo, and the rest is filled from hi starting at its lowest.
+#define HWY_SVE_SPLICE(BASE, CHAR, BITS, NAME, OP)                         \
+  HWY_API HWY_SVE_V(BASE, BITS) NAME(                                      \
+      HWY_SVE_V(BASE, BITS) hi, HWY_SVE_V(BASE, BITS) lo, svbool_t mask) { \
+    return sv##OP##_##CHAR##BITS(mask, lo, hi);                            \
+  }
+HWY_SVE_FOREACH(HWY_SVE_SPLICE, Splice, splice)
+#undef HWY_SVE_SPLICE
 
-HWY_SVE_FOREACH(HWY_SVE_TABLE, TableLookupLanes, rgather)
-#undef HWY_SVE_TABLE
-
-// ------------------------------ Shuffle01
+}  // namespace detail
 
-template <class V>
-HWY_API V Shuffle01(const V v) {
-  using D = DFromV<V>;
-  static_assert(sizeof(TFromD<D>) == 8, "Defined for 64-bit types");
-  const auto idx = detail::Xor(detail::Iota0(D()), 1);
-  return TableLookupLanes(v, idx);
+// ------------------------------ ConcatUpperLower
+template <class D, class V>
+HWY_API V ConcatUpperLower(const D d, const V hi, const V lo) {
+  return IfThenElse(detail::MaskLowerHalf(d), lo, hi);
 }
 
-// ------------------------------ Shuffle2301
-
-template <class V>
-HWY_API V Shuffle2301(const V v) {
-  using D = DFromV<V>;
-  static_assert(sizeof(TFromD<D>) == 4, "Defined for 32-bit types");
-  const auto idx = detail::Xor(detail::Iota0(D()), 1);
-  return TableLookupLanes(v, idx);
+// ------------------------------ ConcatLowerLower
+template <class D, class V>
+HWY_API V ConcatLowerLower(const D d, const V hi, const V lo) {
+  return detail::Splice(hi, lo, detail::MaskLowerHalf(d));
 }
 
-// ------------------------------ Shuffle1032
-
-template <class V>
-HWY_API V Shuffle1032(const V v) {
-  using D = DFromV<V>;
-  static_assert(sizeof(TFromD<D>) == 4, "Defined for 32-bit types");
-  const auto idx = detail::Xor(detail::Iota0(D()), 2);
-  return TableLookupLanes(v, idx);
+// ------------------------------ ConcatLowerUpper
+template <class D, class V>
+HWY_API V ConcatLowerUpper(const D d, const V hi, const V lo) {
+  return detail::Splice(hi, lo, detail::MaskUpperHalf(d));
 }
 
-// ------------------------------ Shuffle0123
-
-template <class V>
-HWY_API V Shuffle0123(const V v) {
-  using D = DFromV<V>;
-  static_assert(sizeof(TFromD<D>) == 4, "Defined for 32-bit types");
-  const auto idx = detail::Xor(detail::Iota0(D()), 3);
-  return TableLookupLanes(v, idx);
+// ------------------------------ ConcatUpperUpper
+template <class D, class V>
+HWY_API V ConcatUpperUpper(const D d, const V hi, const V lo) {
+  const svbool_t mask_upper = detail::MaskUpperHalf(d);
+  const V lo_upper = detail::Splice(lo, lo, mask_upper);
+  return IfThenElse(mask_upper, hi, lo_upper);
 }
 
-// ------------------------------ Shuffle2103
-
-template <class V>
-HWY_API V Shuffle2103(const V v) {
-  using D = DFromV<V>;
-  static_assert(sizeof(TFromD<D>) == 4, "Defined for 32-bit types");
-  // This shuffle is a rotation. We can compute subtraction modulo 4 (number of
-  // lanes per 128-bit block) via bitwise ops.
-  const auto i = detail::Xor(detail::Iota0(D()), 1);
-  const auto lsb = detail::And(i, 1);
-  const auto borrow = Add(lsb, lsb);
-  const auto idx = Xor(i, borrow);
-  return TableLookupLanes(v, idx);
+// ------------------------------ Combine
+template <class D, class V2>
+HWY_API VFromD<D> Combine(const D d, const V2 hi, const V2 lo) {
+  return ConcatLowerLower(d, hi, lo);
 }
 
-// ------------------------------ Shuffle0321
+// ------------------------------ ZeroExtendVector
 
-template <class V>
-HWY_API V Shuffle0321(const V v) {
-  using D = DFromV<V>;
-  static_assert(sizeof(TFromD<D>) == 4, "Defined for 32-bit types");
-  // This shuffle is a rotation. We can compute subtraction modulo 4 (number of
-  // lanes per 128-bit block) via bitwise ops.
-  const auto i = detail::Xor(detail::Iota0(D()), 3);
-  const auto lsb = detail::And(i, 1);
-  const auto borrow = Add(lsb, lsb);
-  const auto idx = Xor(i, borrow);
-  return TableLookupLanes(v, idx);
+template <class D, class V>
+HWY_API V ZeroExtendVector(const D d, const V lo) {
+  return Combine(d, Zero(Half<D>()), lo);
 }
 
-// ------------------------------ TableLookupBytes
-
-namespace detail {
+// ------------------------------ Lower/UpperHalf
 
-// For x86-compatible behaviour mandated by Highway API: TableLookupBytes
-// offsets are implicitly relative to the start of their 128-bit block.
-template <class D>
-constexpr size_t LanesPerBlock(D) {
-  return 16 / sizeof(TFromD<D>);
+template <class D2, class V>
+HWY_API V LowerHalf(D2 /* tag */, const V v) {
+  return v;
 }
 
-template <class D, class V>
-HWY_API V OffsetsOf128BitBlocks(const D d, const V iota0) {
-  using T = MakeUnsigned<TFromD<D>>;
-  return detail::And(iota0, static_cast<T>(~(LanesPerBlock(d) - 1)));
-}
-
-}  // namespace detail
-
 template <class V>
-HWY_API V TableLookupBytes(const V v, const V idx) {
-  using D = DFromV<V>;
-  const Repartition<uint8_t, D> d8;
-  const auto offsets128 = detail::OffsetsOf128BitBlocks(d8, detail::Iota0(d8));
-  const auto idx8 = Add(BitCast(d8, idx), offsets128);
-  return BitCast(D(), TableLookupLanes(BitCast(d8, v), idx8));
+HWY_API V LowerHalf(const V v) {
+  return v;
 }
 
-// ------------------------------ Broadcast
-
-template <int kLane, class V>
-HWY_API V Broadcast(const V v) {
-  const DFromV<V> d;
-  constexpr size_t kLanesPerBlock = detail::LanesPerBlock(d);
-  static_assert(0 <= kLane && kLane < kLanesPerBlock, "Invalid lane");
-  auto idx = detail::OffsetsOf128BitBlocks(d, detail::Iota0(d));
-  if (kLane != 0) {
-    idx = detail::Add(idx, kLane);
-  }
-  return TableLookupLanes(v, idx);
+template <class D2, class V>
+HWY_API V UpperHalf(const D2 d2, const V v) {
+  return detail::Splice(v, v, detail::MaskUpperHalf(Twice<D2>()));
 }
 
+// ================================================== SWIZZLE
+
 // ------------------------------ GetLane
 
 #define HWY_SVE_GET_LANE(BASE, CHAR, BITS, NAME, OP)            \
   HWY_API HWY_SVE_T(BASE, BITS) NAME(HWY_SVE_V(BASE, BITS) v) { \
-    return v##OP##_s_##CHAR##BITS##_##CHAR##BITS(v);            \
+    return sv##OP##_##CHAR##BITS(detail::PFalse(), v);          \
   }
 
-HWY_SVE_FOREACH_UI(HWY_SVE_GET_LANE, GetLane, mv_x)
-HWY_SVE_FOREACH_F(HWY_SVE_GET_LANE, GetLane, fmv_f)
+HWY_SVE_FOREACH(HWY_SVE_GET_LANE, GetLane, lasta)
 #undef HWY_SVE_GET_LANE
 
-// ------------------------------ ShiftLeftLanes
-
-// vector = f(vector, vector, size_t)
-#define HWY_SVE_SLIDE(BASE, CHAR, BITS, NAME, OP)                           \
-  HWY_API HWY_SVE_V(BASE, BITS) NAME(                                       \
-      HWY_SVE_V(BASE, BITS) dst, HWY_SVE_V(BASE, BITS) src, size_t lanes) { \
-    return v##OP##_vx_##CHAR##BITS(dst, src, lanes);                        \
-  }
+// ------------------------------ OddEven
 
 namespace detail {
-HWY_SVE_FOREACH(HWY_SVE_SLIDE, SlideUp, slideup)
+HWY_SVE_FOREACH(HWY_SVE_RETV_ARGVN, Insert, insr_n)
+HWY_SVE_FOREACH(HWY_SVE_RETV_ARGVV, InterleaveEven, trn1)
+HWY_SVE_FOREACH(HWY_SVE_RETV_ARGVV, InterleaveOdd, trn2)
 }  // namespace detail
 
-template <size_t kLanes, class V>
-HWY_API V ShiftLeftLanes(const V v) {
-  using D = DFromV<V>;
-  const RebindToSigned<D> di;
-  const auto shifted = detail::SlideUp(v, v, kLanes);
-  // Match x86 semantics by zeroing lower lanes in 128-bit blocks
-  constexpr size_t kLanesPerBlock = detail::LanesPerBlock(di);
-  const auto idx_mod = detail::And(detail::Iota0(di), kLanesPerBlock - 1);
-  const auto clear = Lt(BitCast(di, idx_mod), Set(di, kLanes));
-  return IfThenZeroElse(clear, shifted);
+template <class V>
+HWY_API V OddEven(const V odd, const V even) {
+  const auto even_in_odd = detail::Insert(even, 0);
+  return detail::InterleaveOdd(even_in_odd, odd);
 }
 
-// ------------------------------ ShiftLeftBytes
+// ------------------------------ TableLookupLanes
 
-template <int kBytes, class V>
-HWY_API V ShiftLeftBytes(const V v) {
-  using D = DFromV<V>;
-  const Repartition<uint8_t, D> d8;
-  Lanes(d8);
-  return BitCast(D(), ShiftLeftLanes<kBytes>(BitCast(d8, v)));
+template <class D, class DI = RebindToSigned<D>>
+HWY_API VFromD<DI> SetTableIndices(D d, const TFromD<DI>* idx) {
+#if !defined(NDEBUG) || defined(ADDRESS_SANITIZER)
+  const size_t N = Lanes(d);
+  for (size_t i = 0; i < N; ++i) {
+    HWY_DASSERT(0 <= idx[i] && idx[i] < static_cast<TFromD<DI>>(N));
+  }
+#endif
+  return Load(DI(), idx);
 }
 
-// ------------------------------ ShiftRightLanes
+// <32bit are not part of Highway API, but used in Broadcast.
+#define HWY_SVE_TABLE(BASE, CHAR, BITS, NAME, OP)                             \
+  HWY_API HWY_SVE_V(BASE, BITS)                                               \
+      NAME(HWY_SVE_V(BASE, BITS) v, HWY_SVE_V(int, BITS) idx) {               \
+    const auto idx_u = BitCast(RebindToUnsigned<DFromV<decltype(v)>>(), idx); \
+    return sv##OP##_##CHAR##BITS(v, idx_u);                                   \
+  }
+
+HWY_SVE_FOREACH(HWY_SVE_TABLE, TableLookupLanes, tbl)
+#undef HWY_SVE_TABLE
+
+// ------------------------------ Compress (PromoteTo)
 
 namespace detail {
-HWY_SVE_FOREACH(HWY_SVE_SLIDE, SlideDown, slidedown)
+
+#define HWY_SVE_CONCAT_EVERY_SECOND(BASE, CHAR, BITS, NAME, OP)  \
+  HWY_API HWY_SVE_V(BASE, BITS)                                  \
+      NAME(HWY_SVE_V(BASE, BITS) hi, HWY_SVE_V(BASE, BITS) lo) { \
+    return sv##OP##_##CHAR##BITS(lo, hi);                        \
+  }
+HWY_SVE_FOREACH(HWY_SVE_CONCAT_EVERY_SECOND, ConcatEven, uzp1)
+HWY_SVE_FOREACH(HWY_SVE_CONCAT_EVERY_SECOND, ConcatOdd, uzp2)
+#undef HWY_SVE_CONCAT_EVERY_SECOND
+
 }  // namespace detail
 
-#undef HWY_SVE_SLIDE
+#define HWY_SVE_COMPRESS(BASE, CHAR, BITS, NAME, OP)                           \
+  HWY_API HWY_SVE_V(BASE, BITS) NAME(HWY_SVE_V(BASE, BITS) v, svbool_t mask) { \
+    return sv##OP##_##CHAR##BITS(mask, v);                                     \
+  }
 
-template <size_t kLanes, class V>
-HWY_API V ShiftRightLanes(const V v) {
-  using D = DFromV<V>;
-  const RebindToSigned<D> di;
-  const auto shifted = detail::SlideDown(v, v, kLanes);
-  // Match x86 semantics by zeroing upper lanes in 128-bit blocks
-  constexpr size_t kLanesPerBlock = detail::LanesPerBlock(di);
-  const auto idx_mod = detail::And(detail::Iota0(di), kLanesPerBlock - 1);
-  const auto keep = Lt(BitCast(di, idx_mod), Set(di, kLanesPerBlock - kLanes));
-  return IfThenElseZero(keep, shifted);
+HWY_SVE_FOREACH_UIF3264(HWY_SVE_COMPRESS, Compress, compact)
+#undef HWY_SVE_COMPRESS
+
+template <class V, HWY_IF_LANE_SIZE_V(V, 2)>
+HWY_API V Compress(V v, svbool_t mask16) {
+  static_assert(!IsSame<V, svfloat16_t>(), "Must use overload");
+  const DFromV<V> d16;
+
+  // Promote vector and mask to 32-bit
+  const RepartitionToWide<decltype(d16)> dw;
+  const auto v32L = PromoteTo(dw, v);
+  const auto v32H = detail::PromoteUpperTo(dw, v);
+  const svbool_t mask32L = svunpklo_b(mask16);
+  const svbool_t mask32H = svunpkhi_b(mask16);
+
+  const auto compressedL = Compress(v32L, mask32L);
+  const auto compressedH = Compress(v32H, mask32H);
+
+  // Demote to 16-bit (already in range) - separately so we can splice
+  const V evenL = BitCast(d16, compressedL);
+  const V evenH = BitCast(d16, compressedH);
+  const V v16L = detail::ConcatEven(evenL, evenL);
+  const V v16H = detail::ConcatEven(evenH, evenH);
+
+  // We need to combine two vectors of non-constexpr length, so the only option
+  // is Splice, which requires us to synthesize a mask. NOTE: this function uses
+  // full vectors (SV_ALL instead of SV_POW2), hence we need unmasked svcnt.
+  const size_t countL = detail::CountTrueFull(dw, mask32L);
+  const auto compressed_maskL = FirstN(d16, countL);
+  return detail::Splice(v16H, v16L, compressed_maskL);
+}
+
+// Must treat float16_t as integers so we can ConcatEven.
+HWY_API svfloat16_t Compress(svfloat16_t v, svbool_t mask16) {
+  const DFromV<decltype(v)> df;
+  const RebindToSigned<decltype(df)> di;
+  return BitCast(df, Compress(BitCast(di, v), mask16));
 }
 
-// ------------------------------ ShiftRightBytes
+// ------------------------------ CompressStore
 
-template <int kBytes, class V>
-HWY_API V ShiftRightBytes(const V v) {
-  using D = DFromV<V>;
-  const Repartition<uint8_t, D> d8;
-  Lanes(d8);
-  return BitCast(D(), ShiftRightLanes<kBytes>(BitCast(d8, v)));
+template <class V, class M, class D>
+HWY_API size_t CompressStore(const V v, const M mask, const D d,
+                             TFromD<D>* HWY_RESTRICT aligned) {
+  Store(Compress(v, mask), d, aligned);
+  return CountTrue(d, mask);
 }
 
-// ------------------------------ OddEven
+// ================================================== BLOCKWISE
 
-template <class V>
-HWY_API V OddEven(const V a, const V b) {
-  const RebindToUnsigned<DFromV<V>> du;  // Iota0 is unsigned only
-  const auto is_even = Eq(detail::And(detail::Iota0(du), 1), Zero(du));
-  return IfThenElse(is_even, b, a);
-}
+// ------------------------------ CombineShiftRightBytes
 
-// ------------------------------ ConcatUpperLower
+namespace detail {
 
-template <class V>
-HWY_API V ConcatUpperLower(const V hi, const V lo) {
-  const RebindToSigned<DFromV<V>> di;
-  const auto idx_half = Set(di, Lanes(di) / 2);
-  const auto is_lower_half = Lt(BitCast(di, detail::Iota0(di)), idx_half);
-  return IfThenElse(is_lower_half, lo, hi);
+// For x86-compatible behaviour mandated by Highway API: TableLookupBytes
+// offsets are implicitly relative to the start of their 128-bit block.
+template <typename T, size_t N>
+constexpr size_t LanesPerBlock(Simd<T, N> /* tag */) {
+  // We might have a capped vector smaller than a block, so honor that.
+  return HWY_MIN(16 / sizeof(T), N);
 }
 
-// ------------------------------ ConcatLowerLower
+template <class D, class V>
+HWY_INLINE V OffsetsOf128BitBlocks(const D d, const V iota0) {
+  using T = MakeUnsigned<TFromD<D>>;
+  return detail::AndNotN(static_cast<T>(LanesPerBlock(d) - 1), iota0);
+}
 
-template <class V>
-HWY_API V ConcatLowerLower(const V hi, const V lo) {
-  // Move lower half into upper
-  const auto hi_up = detail::SlideUp(hi, hi, Lanes(DFromV<V>()) / 2);
-  return ConcatUpperLower(hi_up, lo);
+template <size_t kLanes, class D>
+svbool_t FirstNPerBlock(D d) {
+  const RebindToSigned<D> di;
+  constexpr size_t kLanesPerBlock = detail::LanesPerBlock(di);
+  const auto idx_mod = detail::AndN(Iota(di, 0), kLanesPerBlock - 1);
+  return detail::LtN(BitCast(di, idx_mod), kLanes);
 }
 
-// ------------------------------ ConcatUpperUpper
+}  // namespace detail
 
-template <class V>
-HWY_API V ConcatUpperUpper(const V hi, const V lo) {
-  // Move upper half into lower
-  const auto lo_down = detail::SlideDown(lo, lo, Lanes(DFromV<V>()) / 2);
-  return ConcatUpperLower(hi, lo_down);
+template <size_t kBytes, class D, class V = VFromD<D>>
+HWY_API V CombineShiftRightBytes(const D d, const V hi, const V lo) {
+  const Repartition<uint8_t, decltype(d)> d8;
+  const auto hi8 = BitCast(d8, hi);
+  const auto lo8 = BitCast(d8, lo);
+  const auto hi_up = detail::Splice(hi8, hi8, FirstN(d8, 16 - kBytes));
+  const auto lo_down = detail::Ext<kBytes>(lo8, lo8);
+  const svbool_t is_lo = detail::FirstNPerBlock<16 - kBytes>(d8);
+  return BitCast(d, IfThenElse(is_lo, lo_down, hi_up));
 }
 
-// ------------------------------ ConcatLowerUpper
+// ------------------------------ Shuffle2301
 
-template <class V>
-HWY_API V ConcatLowerUpper(const V hi, const V lo) {
-  // Move half of both inputs to the other half
-  const auto hi_up = detail::SlideUp(hi, hi, Lanes(DFromV<V>()) / 2);
-  const auto lo_down = detail::SlideDown(lo, lo, Lanes(DFromV<V>()) / 2);
-  return ConcatUpperLower(hi_up, lo_down);
-}
+#define HWY_SVE_SHUFFLE_2301(BASE, CHAR, BITS, NAME, OP)                      \
+  HWY_API HWY_SVE_V(BASE, BITS) NAME(HWY_SVE_V(BASE, BITS) v) {               \
+    const DFromV<decltype(v)> d;                                              \
+    const svuint64_t vu64 = BitCast(Repartition<uint64_t, decltype(d)>(), v); \
+    return BitCast(d, sv##OP##_u64_x(HWY_SVE_PTRUE(64), vu64));               \
+  }
 
-// ------------------------------ InterleaveLower
+HWY_SVE_FOREACH_UI32(HWY_SVE_SHUFFLE_2301, Shuffle2301, revw)
+#undef HWY_SVE_SHUFFLE_2301
+
+template <class V, HWY_IF_FLOAT_V(V)>
+HWY_API V Shuffle2301(const V v) {
+  const DFromV<V> df;
+  const RebindToUnsigned<decltype(df)> du;
+  return BitCast(df, Shuffle2301(BitCast(du, v)));
+}
 
+// ------------------------------ Shuffle2103
 template <class V>
-HWY_API V InterleaveLower(const V a, const V b) {
+HWY_API V Shuffle2103(const V v) {
   const DFromV<V> d;
-  const RebindToUnsigned<decltype(d)> du;
-  constexpr size_t kLanesPerBlock = detail::LanesPerBlock(d);
-  const auto i = detail::Iota0(d);
-  const auto idx_mod = ShiftRight<1>(detail::And(i, kLanesPerBlock - 1));
-  const auto idx = Add(idx_mod, detail::OffsetsOf128BitBlocks(d, i));
-  const auto is_even = Eq(detail::And(i, 1), Zero(du));
-  return IfThenElse(is_even, TableLookupLanes(a, idx),
-                    TableLookupLanes(b, idx));
+  const Repartition<uint8_t, decltype(d)> d8;
+  static_assert(sizeof(TFromD<decltype(d)>) == 4, "Defined for 32-bit types");
+  const svuint8_t v8 = BitCast(d8, v);
+  return BitCast(d, CombineShiftRightBytes<12>(d8, v8, v8));
 }
 
-// ------------------------------ InterleaveUpper
-
+// ------------------------------ Shuffle0321
 template <class V>
-HWY_API V InterleaveUpper(const V a, const V b) {
+HWY_API V Shuffle0321(const V v) {
   const DFromV<V> d;
-  const RebindToUnsigned<decltype(d)> du;
-  constexpr size_t kLanesPerBlock = detail::LanesPerBlock(d);
-  const auto i = detail::Iota0(d);
-  const auto idx_mod = ShiftRight<1>(detail::And(i, kLanesPerBlock - 1));
-  const auto idx_lower = Add(idx_mod, detail::OffsetsOf128BitBlocks(d, i));
-  const auto idx = detail::Add(idx_lower, kLanesPerBlock / 2);
-  const auto is_even = Eq(detail::And(i, 1), Zero(du));
-  return IfThenElse(is_even, TableLookupLanes(a, idx),
-                    TableLookupLanes(b, idx));
+  const Repartition<uint8_t, decltype(d)> d8;
+  static_assert(sizeof(TFromD<decltype(d)>) == 4, "Defined for 32-bit types");
+  const svuint8_t v8 = BitCast(d8, v);
+  return BitCast(d, CombineShiftRightBytes<4>(d8, v8, v8));
 }
 
-// ------------------------------ ZipLower
-
+// ------------------------------ Shuffle1032
 template <class V>
-HWY_API VFromD<RepartitionToWide<DFromV<V>>> ZipLower(const V a, const V b) {
-  RepartitionToWide<DFromV<V>> dw;
-  return BitCast(dw, InterleaveLower(a, b));
+HWY_API V Shuffle1032(const V v) {
+  const DFromV<V> d;
+  const Repartition<uint8_t, decltype(d)> d8;
+  static_assert(sizeof(TFromD<decltype(d)>) == 4, "Defined for 32-bit types");
+  const svuint8_t v8 = BitCast(d8, v);
+  return BitCast(d, CombineShiftRightBytes<8>(d8, v8, v8));
 }
 
-// ------------------------------ ZipUpper
-
+// ------------------------------ Shuffle01
 template <class V>
-HWY_API VFromD<RepartitionToWide<DFromV<V>>> ZipUpper(const V a, const V b) {
-  RepartitionToWide<DFromV<V>> dw;
-  return BitCast(dw, InterleaveUpper(a, b));
+HWY_API V Shuffle01(const V v) {
+  const DFromV<V> d;
+  const Repartition<uint8_t, decltype(d)> d8;
+  static_assert(sizeof(TFromD<decltype(d)>) == 8, "Defined for 64-bit types");
+  const svuint8_t v8 = BitCast(d8, v);
+  return BitCast(d, CombineShiftRightBytes<8>(d8, v8, v8));
 }
 
-// ------------------------------ Combine
-
-// TODO(janwas): implement after LMUL ext/trunc
-#if 0
-
+// ------------------------------ Shuffle0123
 template <class V>
-HWY_API V Combine(const V a, const V b) {
-  using D = DFromV<V>;
-  // double LMUL of inputs, then SlideUp with Lanes().
+HWY_API V Shuffle0123(const V v) {
+  return Shuffle2301(Shuffle1032(v));
 }
 
+// ------------------------------ TableLookupBytes
+
+template <class V, class VI>
+HWY_API VI TableLookupBytes(const V v, const VI idx) {
+  const DFromV<VI> d;
+  const Repartition<uint8_t, decltype(d)> du8;
+  const Repartition<int8_t, decltype(d)> di8;
+  const auto offsets128 = detail::OffsetsOf128BitBlocks(du8, Iota(du8, 0));
+  const auto idx8 = BitCast(di8, Add(BitCast(du8, idx), offsets128));
+  return BitCast(d, TableLookupLanes(BitCast(du8, v), idx8));
+}
+
+template <class V, class VI>
+HWY_API VI TableLookupBytesOr0(const V v, const VI idx) {
+  const DFromV<VI> d;
+  // Mask size must match vector type, so cast everything to this type.
+  const Repartition<int8_t, decltype(d)> di8;
+
+  auto idx8 = BitCast(di8, idx);
+  const auto msb = Lt(idx8, Zero(di8));
+// Prevent overflow in table lookups (unnecessary if native)
+#if defined(HWY_EMULATE_SVE)
+  idx8 = IfThenZeroElse(msb, idx8);
 #endif
 
-// ================================================== REDUCE
+  const auto lookup = TableLookupBytes(BitCast(di8, v), idx8);
+  return BitCast(d, IfThenZeroElse(msb, lookup));
+}
 
-// vector = f(vector, zero_m1)
-#define HWY_SVE_REDUCE(BASE, CHAR, BITS, NAME, OP)                       \
-  HWY_API HWY_SVE_V(BASE, BITS)                                          \
-      NAME(HWY_SVE_V(BASE, BITS) v, HWY_SVE_V(BASE, BITS, 1) v0) {       \
-    vsetvlmax_e##BITS();                                                 \
-    return Set(                                                          \
-        HWY_SVE_D(CHAR, BITS)(),                                         \
-        GetLane(v##OP##_vs_##CHAR##BITS##_##CHAR##BITS##m1(v0, v, v0))); \
+// ------------------------------ Broadcast
+
+template <int kLane, class V>
+HWY_API V Broadcast(const V v) {
+  const DFromV<V> d;
+  const RebindToSigned<decltype(d)> di;
+  constexpr size_t kLanesPerBlock = detail::LanesPerBlock(di);
+  static_assert(0 <= kLane && kLane < kLanesPerBlock, "Invalid lane");
+  auto idx = detail::OffsetsOf128BitBlocks(di, Iota(di, 0));
+  if (kLane != 0) {
+    idx = detail::AddN(idx, kLane);
   }
+  return TableLookupLanes(v, idx);
+}
 
-// ------------------------------ SumOfLanes
+// ------------------------------ ShiftLeftLanes
 
-namespace detail {
+template <size_t kLanes, class D, class V = VFromD<D>>
+HWY_API V ShiftLeftLanes(D d, const V v) {
+  const RebindToSigned<decltype(d)> di;
+  const auto zero = Zero(d);
+  const auto shifted = detail::Splice(v, zero, FirstN(d, kLanes));
+  // Match x86 semantics by zeroing lower lanes in 128-bit blocks
+  return IfThenElse(detail::FirstNPerBlock<kLanes>(d), zero, shifted);
+}
 
-HWY_SVE_FOREACH_UI(HWY_SVE_REDUCE, RedSum, redsum)
-HWY_SVE_FOREACH_F(HWY_SVE_REDUCE, RedSum, fredsum)
+template <size_t kLanes, class V>
+HWY_API V ShiftLeftLanes(const V v) {
+  return ShiftLeftLanes<kLanes>(DFromV<V>(), v);
+}
 
-}  // namespace detail
+// ------------------------------ ShiftRightLanes
+template <size_t kLanes, typename T, size_t N, class V = VFromD<Simd<T, N>>>
+HWY_API V ShiftRightLanes(Simd<T, N> d, V v) {
+  const RebindToSigned<decltype(d)> di;
+  // For partial vectors, clear upper lanes so we shift in zeros.
+  if (N != HWY_LANES(T)) {
+    v = IfThenElseZero(detail::Mask(d), v);
+  }
 
-template <class V>
-HWY_API V SumOfLanes(const V v) {
-  using T = TFromV<V>;
-  const auto v0 = Zero(Simd<T, HWY_LANES(T)>());  // always m1
-  return detail::RedSum(v, v0);
+  const auto shifted = detail::Ext<kLanes>(v, v);
+  // Match x86 semantics by zeroing upper lanes in 128-bit blocks
+  constexpr size_t kLanesPerBlock = detail::LanesPerBlock(d);
+  const svbool_t mask = detail::FirstNPerBlock<kLanesPerBlock - kLanes>(d);
+  return IfThenElseZero(mask, shifted);
 }
 
-// ------------------------------ MinOfLanes
-namespace detail {
+// ------------------------------ ShiftLeftBytes
 
-HWY_SVE_FOREACH_U(HWY_SVE_REDUCE, RedMin, redminu)
-HWY_SVE_FOREACH_I(HWY_SVE_REDUCE, RedMin, redmin)
-HWY_SVE_FOREACH_F(HWY_SVE_REDUCE, RedMin, fredmin)
+template <int kBytes, class D, class V = VFromD<D>>
+HWY_API V ShiftLeftBytes(const D d, const V v) {
+  const Repartition<uint8_t, decltype(d)> d8;
+  return BitCast(d, ShiftLeftLanes<kBytes>(BitCast(d8, v)));
+}
 
-}  // namespace detail
+template <int kBytes, class V>
+HWY_API V ShiftLeftBytes(const V v) {
+  return ShiftLeftBytes<kBytes>(DFromV<V>(), v);
+}
 
-template <class V>
-HWY_API V MinOfLanes(const V v) {
-  using T = TFromV<V>;
-  const Simd<T, HWY_LANES(T)> d1;  // always m1
-  const auto neutral = Set(d1, HighestValue<T>());
-  return detail::RedMin(v, neutral);
+// ------------------------------ ShiftRightBytes
+template <int kBytes, class D, class V = VFromD<D>>
+HWY_API V ShiftRightBytes(const D d, const V v) {
+  const Repartition<uint8_t, decltype(d)> d8;
+  return BitCast(d, ShiftRightLanes<kBytes>(d8, BitCast(d8, v)));
 }
 
-// ------------------------------ MaxOfLanes
+// ------------------------------ InterleaveLower
+
 namespace detail {
+HWY_SVE_FOREACH(HWY_SVE_RETV_ARGVV, ZipLower, zip1)
+}  // namespace detail
 
-HWY_SVE_FOREACH_U(HWY_SVE_REDUCE, RedMax, redmaxu)
-HWY_SVE_FOREACH_I(HWY_SVE_REDUCE, RedMax, redmax)
-HWY_SVE_FOREACH_F(HWY_SVE_REDUCE, RedMax, fredmax)
+template <class D, class V>
+HWY_API V InterleaveLower(D d, const V a, const V b) {
+  static_assert(IsSame<TFromD<D>, TFromV<V>>(), "D/V mismatch");
+  // Move lower halves of blocks to lower half of vector.
+  const Repartition<uint64_t, decltype(d)> d64;
+  const auto a64 = BitCast(d64, a);
+  const auto b64 = BitCast(d64, b);
+  const auto a_blocks = detail::ConcatEven(a64, a64);
+  const auto b_blocks = detail::ConcatEven(b64, b64);
 
-}  // namespace detail
+  return detail::ZipLower(BitCast(d, a_blocks), BitCast(d, b_blocks));
+}
 
 template <class V>
-HWY_API V MaxOfLanes(const V v) {
-  using T = TFromV<V>;
-  const Simd<T, HWY_LANES(T)> d1;  // always m1
-  const auto neutral = Set(d1, LowestValue<T>());
-  return detail::RedMax(v, neutral);
+HWY_API V InterleaveLower(const V a, const V b) {
+  return InterleaveLower(DFromV<V>(), a, b);
 }
 
-#undef HWY_SVE_REDUCE
-
-// ================================================== Ops with dependencies
-
-// ------------------------------ LoadDup128
-
-template <class D>
-HWY_API VFromD<D> LoadDup128(D d, const TFromD<D>* const HWY_RESTRICT p) {
-  // TODO(janwas): set VL
-  const auto loaded = Load(d, p);
-  constexpr size_t kLanesPerBlock = detail::LanesPerBlock(d);
-  // Broadcast the first block
-  const auto idx = detail::And(detail::Iota0(d), kLanesPerBlock - 1);
-  return TableLookupLanes(loaded, idx);
-}
+// ------------------------------ InterleaveUpper
 
-// ------------------------------ StoreMaskBits
-#define HWY_SVE_STORE_MASK_BITS(MLEN, NAME, OP)                 \
-  HWY_API size_t StoreMaskBits(HWY_SVE_M(MLEN) m, uint8_t* p) { \
-    /* LMUL=1 is always enough */                               \
-    Simd<uint8_t, HWY_LANES(uint8_t)> d8;                       \
-    const size_t num_bytes = (Lanes(d8) + MLEN - 1) / MLEN;     \
-    /* TODO(janwas): how to convert vbool* to vuint?*/          \
-    /*Store(m, d8, p);*/                                        \
-    (void)m;                                                    \
-    (void)p;                                                    \
-    return num_bytes;                                           \
+// Full vector: guaranteed to have at least one block
+template <typename T, class V = VFromD<Full<T>>>
+HWY_API V InterleaveUpper(Simd<T, HWY_LANES(T)> d, const V a, const V b) {
+  // Move upper halves of blocks to lower half of vector.
+  const Repartition<uint64_t, decltype(d)> d64;
+  const auto a64 = BitCast(d64, a);
+  const auto b64 = BitCast(d64, b);
+  const auto a_blocks = detail::ConcatOdd(a64, a64);
+  const auto b_blocks = detail::ConcatOdd(b64, b64);
+  return detail::ZipLower(BitCast(d, a_blocks), BitCast(d, b_blocks));
+}
+
+// Capped: less than one block
+template <typename T, size_t N, HWY_IF_LE64(T, N), class V = VFromD<Simd<T, N>>>
+HWY_API V InterleaveUpper(Simd<T, N> d, const V a, const V b) {
+  static_assert(IsSame<T, TFromV<V>>(), "D/V mismatch");
+  const Half<decltype(d)> d2;
+  return InterleaveLower(d, UpperHalf(d2, a), UpperHalf(d2, b));
+}
+
+// Partial: need runtime check
+template <typename T, size_t N,
+          hwy::EnableIf<(N < HWY_LANES(T) && N * sizeof(T) >= 16)>* = nullptr,
+          class V = VFromD<Simd<T, N>>>
+HWY_API V InterleaveUpper(Simd<T, N> d, const V a, const V b) {
+  static_assert(IsSame<T, TFromV<V>>(), "D/V mismatch");
+  // Less than one block: treat as capped
+  if (Lanes(d) * sizeof(T) < 16) {
+    const Half<decltype(d)> d2;
+    return InterleaveLower(d, UpperHalf(d2, a), UpperHalf(d2, b));
   }
-HWY_SVE_FOREACH_B(HWY_SVE_STORE_MASK_BITS, _, _)
-#undef HWY_SVE_STORE_MASK_BITS
+  return InterleaveUpper(Full<T>(), a, b);
+}
 
-// ------------------------------ FirstN (Iota0, Lt, RebindMask, SlideUp)
+// ------------------------------ ZipLower
 
-// Disallow for 8-bit because Iota is likely to overflow.
-template <class D, HWY_IF_NOT_LANE_SIZE_D(D, 1)>
-HWY_API MFromD<D> FirstN(const D d, const size_t n) {
-  const RebindToSigned<D> di;
-  return RebindMask(d, Lt(BitCast(di, detail::Iota0(d)), Set(di, n)));
+template <class V, class DW = RepartitionToWide<DFromV<V>>>
+HWY_API VFromD<DW> ZipLower(DW dw, V a, V b) {
+  const RepartitionToNarrow<DW> dn;
+  static_assert(IsSame<TFromD<decltype(dn)>, TFromV<V>>(), "D/V mismatch");
+  return BitCast(dw, InterleaveLower(dn, a, b));
 }
-
-template <class D, HWY_IF_LANE_SIZE_D(D, 1)>
-HWY_API MFromD<D> FirstN(const D d, const size_t n) {
-  const auto zero = Zero(d);
-  const auto one = Set(d, 1);
-  return Eq(detail::SlideUp(one, zero, n), one);
+template <class V, class D = DFromV<V>, class DW = RepartitionToWide<D>>
+HWY_API VFromD<DW> ZipLower(const V a, const V b) {
+  return BitCast(DW(), InterleaveLower(D(), a, b));
 }
 
-// ------------------------------ Neg
-
-template <class V, HWY_IF_SIGNED_V(V)>
-HWY_API V Neg(const V v) {
-  return Sub(Zero(DFromV<V>()), v);
+// ------------------------------ ZipUpper
+template <class V, class DW = RepartitionToWide<DFromV<V>>>
+HWY_API VFromD<DW> ZipUpper(DW dw, V a, V b) {
+  const RepartitionToNarrow<DW> dn;
+  static_assert(IsSame<TFromD<decltype(dn)>, TFromV<V>>(), "D/V mismatch");
+  return BitCast(dw, InterleaveUpper(dn, a, b));
 }
 
-// vector = f(vector), but argument is repeated
-#define HWY_SVE_RETV_ARGV2(BASE, CHAR, BITS, NAME, OP)          \
-  HWY_API HWY_SVE_V(BASE, BITS) NAME(HWY_SVE_V(BASE, BITS) v) { \
-    return v##OP##_vv_##CHAR##BITS(v, v);                       \
+// ================================================== REDUCE
+
+#define HWY_SVE_REDUCE(BASE, CHAR, BITS, NAME, OP)                \
+  template <size_t N>                                             \
+  HWY_API HWY_SVE_V(BASE, BITS)                                   \
+      NAME(HWY_SVE_D(BASE, BITS, N) d, HWY_SVE_V(BASE, BITS) v) { \
+    return Set(d, sv##OP##_##CHAR##BITS(detail::Mask(d), v));     \
   }
 
-HWY_SVE_FOREACH_F(HWY_SVE_RETV_ARGV2, Neg, fsgnjn)
+HWY_SVE_FOREACH(HWY_SVE_REDUCE, SumOfLanes, addv)
+HWY_SVE_FOREACH_UI(HWY_SVE_REDUCE, MinOfLanes, minv)
+HWY_SVE_FOREACH_UI(HWY_SVE_REDUCE, MaxOfLanes, maxv)
+// NaN if all are
+HWY_SVE_FOREACH_F(HWY_SVE_REDUCE, MinOfLanes, minnmv)
+HWY_SVE_FOREACH_F(HWY_SVE_REDUCE, MaxOfLanes, maxnmv)
 
-// ------------------------------ Abs
+#undef HWY_SVE_REDUCE
 
-template <class V, HWY_IF_SIGNED_V(V)>
-HWY_API V Abs(const V v) {
-  return Max(v, Neg(v));
-}
+// ================================================== Ops with dependencies
 
-HWY_SVE_FOREACH_F(HWY_SVE_RETV_ARGV2, Abs, fsgnjx)
+// ------------------------------ ZeroIfNegative (Lt, IfThenElse)
+template <class V>
+HWY_API V ZeroIfNegative(const V v) {
+  const auto v0 = Zero(DFromV<V>());
+  // We already have a zero constant, so avoid IfThenZeroElse.
+  return IfThenElse(Lt(v, v0), v0, v);
+}
 
-#undef HWY_SVE_RETV_ARGV2
+// ------------------------------ BroadcastSignBit (ShiftRight)
+template <class V>
+HWY_API V BroadcastSignBit(const V v) {
+  return ShiftRight<sizeof(TFromV<V>) * 8 - 1>(v);
+}
 
-// ------------------------------ AbsDiff
+// ------------------------------ AverageRound (ShiftRight)
 
+#if HWY_TARGET == HWY_SVE2
+HWY_SVE_FOREACH_U08(HWY_SVE_RETV_ARGPVV, AverageRound, rhadd)
+HWY_SVE_FOREACH_U16(HWY_SVE_RETV_ARGPVV, AverageRound, rhadd)
+#else
 template <class V>
-HWY_API V AbsDiff(const V a, const V b) {
-  return Abs(Sub(a, b));
+V AverageRound(const V a, const V b) {
+  return ShiftRight<1>(Add(Add(a, b), Set(DFromV<V>(), 1)));
 }
+#endif  // HWY_TARGET == HWY_SVE2
 
-// ------------------------------ Round
-
-// IEEE-754 roundToIntegralTiesToEven returns floating-point, but we do not have
-// a dedicated instruction for that. Rounding to integer and converting back to
-// float is correct except when the input magnitude is large, in which case the
-// input was already an integer (because mantissa >> exponent is zero).
+// ------------------------------ StoreMaskBits
 
 namespace detail {
-enum RoundingModes { kNear, kTrunc, kDown, kUp };
 
-template <class V>
-HWY_API auto UseInt(const V v) -> decltype(MaskFromVec(v)) {
-  return Lt(Abs(v), Set(DFromV<V>(), MantissaEnd<TFromV<V>>()));
+// Returns mask ? 1 : 0 in BYTE lanes.
+template <typename T, size_t N, HWY_IF_LANE_SIZE(T, 1)>
+HWY_API svuint8_t BoolFromMask(Simd<T, N> d, svbool_t m) {
+  return svdup_n_u8_z(m, 1);
+}
+template <typename T, size_t N, HWY_IF_LANE_SIZE(T, 2)>
+HWY_API svuint8_t BoolFromMask(Simd<T, N> d, svbool_t m) {
+  const Repartition<uint8_t, decltype(d)> d8;
+  const svuint8_t b16 = BitCast(d8, svdup_n_u16_z(m, 1));
+  return detail::ConcatEven(b16, b16);
+}
+template <typename T, size_t N, HWY_IF_LANE_SIZE(T, 4)>
+HWY_API svuint8_t BoolFromMask(Simd<T, N> d, svbool_t m) {
+  return U8FromU32(svdup_n_u32_z(m, 1));
+}
+template <typename T, size_t N, HWY_IF_LANE_SIZE(T, 8)>
+HWY_API svuint8_t BoolFromMask(Simd<T, N> d, svbool_t m) {
+  const Repartition<uint32_t, decltype(d)> d32;
+  const svuint32_t b64 = BitCast(d32, svdup_n_u64_z(m, 1));
+  return U8FromU32(detail::ConcatEven(b64, b64));
 }
 
 }  // namespace detail
 
-template <class V>
-HWY_API V Round(const V v) {
-  const DFromV<V> df;
-
-  const auto integer = NearestInt(v);  // round using current mode
-  const auto int_f = ConvertTo(df, integer);
+template <typename T, size_t N>
+HWY_API size_t StoreMaskBits(Simd<T, N> d, svbool_t m, uint8_t* p) {
+  const Repartition<uint8_t, decltype(d)> d8;
+  const Repartition<uint16_t, decltype(d)> d16;
+  const Repartition<uint32_t, decltype(d)> d32;
+  const Repartition<uint64_t, decltype(d)> d64;
+  auto x = detail::BoolFromMask(d, m);
+  // Compact bytes to bits. Could use SVE2 BDEP, but it's optional.
+  x = Or(x, BitCast(d8, ShiftRight<7>(BitCast(d16, x))));
+  x = Or(x, BitCast(d8, ShiftRight<14>(BitCast(d32, x))));
+  x = Or(x, BitCast(d8, ShiftRight<28>(BitCast(d64, x))));
+
+  const size_t num_bits = Lanes(d);
+  const size_t num_bytes = (num_bits + 8 - 1) / 8;  // Round up, see below
+
+  // Truncate to 8 bits and store.
+  svst1b_u64(FirstN(d64, num_bytes), p, BitCast(d64, x));
+
+  // Non-full byte, need to clear the undefined upper bits. Can happen for
+  // capped/partial vectors or large T and small hardware vectors.
+  if (num_bits < 8) {
+    const int mask = (1 << num_bits) - 1;
+    p[num_bytes - 1] = static_cast<uint8_t>(p[num_bytes - 1] & mask);
+  }
+  // Else: we wrote full bytes because num_bits is a power of two >= 8.
 
-  return IfThenElse(detail::UseInt(v), CopySign(int_f, v), v);
+  return num_bytes;
 }
 
-// ------------------------------ Trunc
-
-template <class V>
-HWY_API V Trunc(const V v) {
-  const DFromV<V> df;
-  const RebindToSigned<decltype(df)> di;
+// ------------------------------ MulEven (InterleaveEven)
 
-  const auto integer = ConvertTo(di, v);  // round toward 0
-  const auto int_f = ConvertTo(df, integer);
+#if HWY_TARGET == HWY_SVE2
+namespace detail {
+HWY_SVE_FOREACH_UI32(HWY_SVE_RETV_ARGPVV, MulEven, mullb)
+}  // namespace detail
+#endif
 
-  return IfThenElse(detail::UseInt(v), CopySign(int_f, v), v);
+template <class V, class DW = RepartitionToWide<DFromV<V>>>
+HWY_API VFromD<DW> MulEven(const V a, const V b) {
+#if HWY_TARGET == HWY_SVE2
+  return BitCast(DW(), detail::MulEven(a, b));
+#else
+  const auto lo = Mul(a, b);
+  const auto hi = detail::MulHigh(a, b);
+  return BitCast(DW(), detail::InterleaveEven(lo, hi));
+#endif
 }
 
-// ------------------------------ Ceil
+HWY_API svuint64_t MulEven(const svuint64_t a, const svuint64_t b) {
+  const auto lo = Mul(a, b);
+  const auto hi = detail::MulHigh(a, b);
+  return detail::InterleaveEven(lo, hi);
+}
 
-template <class V>
-HWY_API V Ceil(const V v) {
-  asm volatile("fsrm %0" ::"r"(detail::kUp));
-  const auto ret = Round(v);
-  asm volatile("fsrm %0" ::"r"(detail::kNear));
-  return ret;
+HWY_API svuint64_t MulOdd(const svuint64_t a, const svuint64_t b) {
+  const auto lo = Mul(a, b);
+  const auto hi = detail::MulHigh(a, b);
+  return detail::InterleaveOdd(lo, hi);
 }
 
-// ------------------------------ Floor
+// ------------------------------ AESRound / CLMul
 
-template <class V>
-HWY_API V Floor(const V v) {
-  asm volatile("fsrm %0" ::"r"(detail::kDown));
-  const auto ret = Round(v);
-  asm volatile("fsrm %0" ::"r"(detail::kNear));
-  return ret;
-}
+#if defined(__ARM_FEATURE_SVE2_AES)
 
-// ------------------------------ Iota
+// Per-target flag to prevent generic_ops-inl.h from defining AESRound.
+#ifdef HWY_NATIVE_AES
+#undef HWY_NATIVE_AES
+#else
+#define HWY_NATIVE_AES
+#endif
 
-template <class D, HWY_IF_UNSIGNED_D(D)>
-HWY_API VFromD<D> Iota(const D d, TFromD<D> first) {
-  return Add(detail::Iota0(d), Set(d, first));
+HWY_API svuint8_t AESRound(svuint8_t state, svuint8_t round_key) {
+  // NOTE: it is important that AESE and AESMC be consecutive instructions so
+  // they can be fused. AESE includes AddRoundKey, which is a different ordering
+  // than the AES-NI semantics we adopted, so XOR by 0 and later with the actual
+  // round key (the compiler will hopefully optimize this for multiple rounds).
+  const svuint8_t zero = Zero(HWY_FULL(uint8_t)());
+  return Xor(vaesmcq_u8(vaeseq_u8(state, zero), round_key));
 }
 
-template <class D, HWY_IF_SIGNED_D(D)>
-HWY_API VFromD<D> Iota(const D d, TFromD<D> first) {
-  const RebindToUnsigned<D> du;
-  return Add(BitCast(d, detail::Iota0(du)), Set(d, first));
+HWY_API svuint64_t CLMulLower(const svuint64_t a, const svuint64_t b) {
+  return svpmullb_pair(a, b);
 }
 
-template <class D, HWY_IF_FLOAT_D(D)>
-HWY_API VFromD<D> Iota(const D d, TFromD<D> first) {
-  const RebindToUnsigned<D> du;
-  const RebindToSigned<D> di;
-  return detail::Add(ConvertTo(d, BitCast(di, detail::Iota0(du))), first);
+HWY_API svuint64_t CLMulUpper(const svuint64_t a, const svuint64_t b) {
+  return svpmullt_pair(a, b);
 }
 
-// ------------------------------ MulEven
-
-// Using vwmul does not work for m8, so use mulh instead. Highway only provides
-// MulHigh for 16-bit, so use a private wrapper.
-namespace detail {
-
-HWY_SVE_FOREACH_U32(HWY_SVE_RETV_ARGVV, MulHigh, mulhu)
-HWY_SVE_FOREACH_I32(HWY_SVE_RETV_ARGVV, MulHigh, mulh)
-
-}  // namespace detail
-
-template <class V>
-HWY_API VFromD<RepartitionToWide<DFromV<V>>> MulEven(const V a, const V b) {
-  const DFromV<V> d;
-  Lanes(d);
-  const auto lo = Mul(a, b);
-  const auto hi = detail::MulHigh(a, b);
-  const RepartitionToWide<DFromV<V>> dw;
-  return BitCast(dw, OddEven(detail::SlideUp(hi, hi, 1), lo));
-}
+#endif  // __ARM_FEATURE_SVE2_AES
 
 // ================================================== END MACROS
 namespace detail {  // for code folding
 #undef HWY_IF_FLOAT_V
+#undef HWY_IF_LANE_SIZE_V
 #undef HWY_IF_SIGNED_V
 #undef HWY_IF_UNSIGNED_V
-
+#undef HWY_SVE_D
 #undef HWY_SVE_FOREACH
-#undef HWY_SVE_FOREACH_08
-#undef HWY_SVE_FOREACH_16
-#undef HWY_SVE_FOREACH_32
-#undef HWY_SVE_FOREACH_64
-#undef HWY_SVE_FOREACH_B
 #undef HWY_SVE_FOREACH_F
+#undef HWY_SVE_FOREACH_F16
 #undef HWY_SVE_FOREACH_F32
 #undef HWY_SVE_FOREACH_F64
 #undef HWY_SVE_FOREACH_I
@@ -1671,25 +1818,28 @@ namespace detail {  // for code folding
 #undef HWY_SVE_FOREACH_I16
 #undef HWY_SVE_FOREACH_I32
 #undef HWY_SVE_FOREACH_I64
+#undef HWY_SVE_FOREACH_IF
 #undef HWY_SVE_FOREACH_U
 #undef HWY_SVE_FOREACH_U08
 #undef HWY_SVE_FOREACH_U16
 #undef HWY_SVE_FOREACH_U32
 #undef HWY_SVE_FOREACH_U64
 #undef HWY_SVE_FOREACH_UI
+#undef HWY_SVE_FOREACH_UI08
 #undef HWY_SVE_FOREACH_UI16
 #undef HWY_SVE_FOREACH_UI32
 #undef HWY_SVE_FOREACH_UI64
-
+#undef HWY_SVE_FOREACH_UIF3264
+#undef HWY_SVE_PTRUE
 #undef HWY_SVE_RETV_ARGD
+#undef HWY_SVE_RETV_ARGPV
+#undef HWY_SVE_RETV_ARGPVN
+#undef HWY_SVE_RETV_ARGPVV
 #undef HWY_SVE_RETV_ARGV
-#undef HWY_SVE_RETV_ARGVS
+#undef HWY_SVE_RETV_ARGVN
 #undef HWY_SVE_RETV_ARGVV
-
 #undef HWY_SVE_T
-#undef HWY_SVE_D
 #undef HWY_SVE_V
-#undef HWY_SVE_M
 
 }  // namespace detail
 // NOLINTNEXTLINE(google-readability-namespace-comments)
diff --git a/third_party/highway/hwy/ops/generic_ops-inl.h b/third_party/highway/hwy/ops/generic_ops-inl.h
new file mode 100644
index 0000000000000..35cec12f75f03
--- /dev/null
+++ b/third_party/highway/hwy/ops/generic_ops-inl.h
@@ -0,0 +1,324 @@
+// Copyright 2021 Google LLC
+//
+// Licensed under the Apache License, Version 2.0 (the "License");
+// you may not use this file except in compliance with the License.
+// You may obtain a copy of the License at
+//
+//      http://www.apache.org/licenses/LICENSE-2.0
+//
+// Unless required by applicable law or agreed to in writing, software
+// distributed under the License is distributed on an "AS IS" BASIS,
+// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+// See the License for the specific language governing permissions and
+// limitations under the License.
+
+// Target-independent types/functions defined after target-specific ops.
+
+// Relies on the external include guard in highway.h.
+HWY_BEFORE_NAMESPACE();
+namespace hwy {
+namespace HWY_NAMESPACE {
+
+// The lane type of a vector type, e.g. float for Vec<Simd<float, 4>>.
+template <class V>
+using LaneType = decltype(GetLane(V()));
+
+// Vector type, e.g. Vec128<float> for Simd<float, 4>. Useful as the return type
+// of functions that do not take a vector argument, or as an argument type if
+// the function only has a template argument for D, or for explicit type names
+// instead of auto. This may be a built-in type.
+template <class D>
+using Vec = decltype(Zero(D()));
+
+// Mask type. Useful as the return type of functions that do not take a mask
+// argument, or as an argument type if the function only has a template argument
+// for D, or for explicit type names instead of auto.
+template <class D>
+using Mask = decltype(MaskFromVec(Zero(D())));
+
+// Returns the closest value to v within [lo, hi].
+template <class V>
+HWY_API V Clamp(const V v, const V lo, const V hi) {
+  return Min(Max(lo, v), hi);
+}
+
+// CombineShiftRightBytes (and -Lanes) are not available for the scalar target,
+// and RVV has its own implementation of -Lanes.
+#if HWY_TARGET != HWY_SCALAR && HWY_TARGET != HWY_RVV
+
+template <size_t kLanes, class D, class V = VFromD<D>>
+HWY_API V CombineShiftRightLanes(D d, const V hi, const V lo) {
+  constexpr size_t kBytes = kLanes * sizeof(LaneType<V>);
+  static_assert(kBytes < 16, "Shift count is per-block");
+  return CombineShiftRightBytes<kBytes>(d, hi, lo);
+}
+
+// DEPRECATED
+template <size_t kLanes, class V>
+HWY_API V CombineShiftRightLanes(const V hi, const V lo) {
+  return CombineShiftRightLanes<kLanes>(DFromV<V>(), hi, lo);
+}
+
+#endif
+
+// Returns lanes with the most significant bit set and all other bits zero.
+template <class D>
+HWY_API Vec<D> SignBit(D d) {
+  using Unsigned = MakeUnsigned<TFromD<D>>;
+  const Unsigned bit = Unsigned(1) << (sizeof(Unsigned) * 8 - 1);
+  return BitCast(d, Set(Rebind<Unsigned, D>(), bit));
+}
+
+// Returns quiet NaN.
+template <class D>
+HWY_API Vec<D> NaN(D d) {
+  const RebindToSigned<D> di;
+  // LimitsMax sets all exponent and mantissa bits to 1. The exponent plus
+  // mantissa MSB (to indicate quiet) would be sufficient.
+  return BitCast(d, Set(di, LimitsMax<TFromD<decltype(di)>>()));
+}
+
+// ------------------------------ AESRound
+
+// Cannot implement on scalar: need at least 16 bytes for TableLookupBytes.
+#if HWY_TARGET != HWY_SCALAR
+
+// Define for white-box testing, even if native instructions are available.
+namespace detail {
+
+// Constant-time: computes inverse in GF(2^4) based on "Accelerating AES with
+// Vector Permute Instructions" and the accompanying assembly language
+// implementation: https://crypto.stanford.edu/vpaes/vpaes.tgz. See also Botan:
+// https://botan.randombit.net/doxygen/aes__vperm_8cpp_source.html .
+//
+// A brute-force 256 byte table lookup can also be made constant-time, and
+// possibly competitive on NEON, but this is more performance-portable
+// especially for x86 and large vectors.
+template <class V>  // u8
+HWY_INLINE V SubBytes(V state) {
+  const DFromV<V> du;
+  const auto mask = Set(du, 0xF);
+
+  // Change polynomial basis to GF(2^4)
+  {
+    alignas(16) static constexpr uint8_t basisL[16] = {
+        0x00, 0x70, 0x2A, 0x5A, 0x98, 0xE8, 0xB2, 0xC2,
+        0x08, 0x78, 0x22, 0x52, 0x90, 0xE0, 0xBA, 0xCA};
+    alignas(16) static constexpr uint8_t basisU[16] = {
+        0x00, 0x4D, 0x7C, 0x31, 0x7D, 0x30, 0x01, 0x4C,
+        0x81, 0xCC, 0xFD, 0xB0, 0xFC, 0xB1, 0x80, 0xCD};
+    const auto sL = And(state, mask);
+    const auto sU = ShiftRight<4>(state);  // byte shift => upper bits are zero
+    const auto gf4L = TableLookupBytes(LoadDup128(du, basisL), sL);
+    const auto gf4U = TableLookupBytes(LoadDup128(du, basisU), sU);
+    state = Xor(gf4L, gf4U);
+  }
+
+  // Inversion in GF(2^4). Elements 0 represent "infinity" (division by 0) and
+  // cause TableLookupBytesOr0 to return 0.
+  alignas(16) static constexpr uint8_t kZetaInv[16] = {
+      0x80, 7, 11, 15, 6, 10, 4, 1, 9, 8, 5, 2, 12, 14, 13, 3};
+  alignas(16) static constexpr uint8_t kInv[16] = {
+      0x80, 1, 8, 13, 15, 6, 5, 14, 2, 12, 11, 10, 9, 3, 7, 4};
+  const auto tbl = LoadDup128(du, kInv);
+  const auto sL = And(state, mask);      // L=low nibble, U=upper
+  const auto sU = ShiftRight<4>(state);  // byte shift => upper bits are zero
+  const auto sX = Xor(sU, sL);
+  const auto invL = TableLookupBytes(LoadDup128(du, kZetaInv), sL);
+  const auto invU = TableLookupBytes(tbl, sU);
+  const auto invX = TableLookupBytes(tbl, sX);
+  const auto outL = Xor(sX, TableLookupBytesOr0(tbl, Xor(invL, invU)));
+  const auto outU = Xor(sU, TableLookupBytesOr0(tbl, Xor(invL, invX)));
+
+  // Linear skew (cannot bake 0x63 bias into the table because out* indices
+  // may have the infinity flag set).
+  alignas(16) static constexpr uint8_t kAffineL[16] = {
+      0x00, 0xC7, 0xBD, 0x6F, 0x17, 0x6D, 0xD2, 0xD0,
+      0x78, 0xA8, 0x02, 0xC5, 0x7A, 0xBF, 0xAA, 0x15};
+  alignas(16) static constexpr uint8_t kAffineU[16] = {
+      0x00, 0x6A, 0xBB, 0x5F, 0xA5, 0x74, 0xE4, 0xCF,
+      0xFA, 0x35, 0x2B, 0x41, 0xD1, 0x90, 0x1E, 0x8E};
+  const auto affL = TableLookupBytesOr0(LoadDup128(du, kAffineL), outL);
+  const auto affU = TableLookupBytesOr0(LoadDup128(du, kAffineU), outU);
+  return Xor(Xor(affL, affU), Set(du, 0x63));
+}
+
+}  // namespace detail
+
+#endif  // HWY_TARGET != HWY_SCALAR
+
+// "Include guard": skip if native AES instructions are available.
+#if (defined(HWY_NATIVE_AES) == defined(HWY_TARGET_TOGGLE))
+#ifdef HWY_NATIVE_AES
+#undef HWY_NATIVE_AES
+#else
+#define HWY_NATIVE_AES
+#endif
+
+// (Must come after HWY_TARGET_TOGGLE, else we don't reset it for scalar)
+#if HWY_TARGET != HWY_SCALAR
+
+namespace detail {
+
+template <class V>  // u8
+HWY_API V ShiftRows(const V state) {
+  const DFromV<V> du;
+  alignas(16) static constexpr uint8_t kShiftRow[16] = {
+      0,  5,  10, 15,  // transposed: state is column major
+      4,  9,  14, 3,   //
+      8,  13, 2,  7,   //
+      12, 1,  6,  11};
+  const auto shift_row = LoadDup128(du, kShiftRow);
+  return TableLookupBytes(state, shift_row);
+}
+
+template <class V>  // u8
+HWY_API V MixColumns(const V state) {
+  const DFromV<V> du;
+  // For each column, the rows are the sum of GF(2^8) matrix multiplication by:
+  // 2 3 1 1  // Let s := state*1, d := state*2, t := state*3.
+  // 1 2 3 1  // d are on diagonal, no permutation needed.
+  // 1 1 2 3  // t1230 indicates column indices of threes for the 4 rows.
+  // 3 1 1 2  // We also need to compute s2301 and s3012 (=1230 o 2301).
+  alignas(16) static constexpr uint8_t k2301[16] = {
+      2, 3, 0, 1, 6, 7, 4, 5, 10, 11, 8, 9, 14, 15, 12, 13};
+  alignas(16) static constexpr uint8_t k1230[16] = {
+      1, 2, 3, 0, 5, 6, 7, 4, 9, 10, 11, 8, 13, 14, 15, 12};
+  const RebindToSigned<decltype(du)> di;  // can only do signed comparisons
+  const auto msb = Lt(BitCast(di, state), Zero(di));
+  const auto overflow = BitCast(du, IfThenElseZero(msb, Set(di, 0x1B)));
+  const auto d = Xor(Add(state, state), overflow);  // = state*2 in GF(2^8).
+  const auto s2301 = TableLookupBytes(state, LoadDup128(du, k2301));
+  const auto d_s2301 = Xor(d, s2301);
+  const auto t_s2301 = Xor(state, d_s2301);  // t(s*3) = XOR-sum {s, d(s*2)}
+  const auto t1230_s3012 = TableLookupBytes(t_s2301, LoadDup128(du, k1230));
+  return Xor(d_s2301, t1230_s3012);  // XOR-sum of 4 terms
+}
+
+}  // namespace detail
+
+template <class V>  // u8
+HWY_API V AESRound(V state, const V round_key) {
+  // Intel docs swap the first two steps, but it does not matter because
+  // ShiftRows is a permutation and SubBytes is independent of lane index.
+  state = detail::SubBytes(state);
+  state = detail::ShiftRows(state);
+  state = detail::MixColumns(state);
+  state = Xor(state, round_key);  // AddRoundKey
+  return state;
+}
+
+// Constant-time implementation inspired by
+// https://www.bearssl.org/constanttime.html, but about half the cost because we
+// use 64x64 multiplies and 128-bit XORs.
+template <class V>
+HWY_API V CLMulLower(V a, V b) {
+  const DFromV<V> d;
+  static_assert(IsSame<TFromD<decltype(d)>, uint64_t>(), "V must be u64");
+  const auto k1 = Set(d, 0x1111111111111111ULL);
+  const auto k2 = Set(d, 0x2222222222222222ULL);
+  const auto k4 = Set(d, 0x4444444444444444ULL);
+  const auto k8 = Set(d, 0x8888888888888888ULL);
+  const auto a0 = And(a, k1);
+  const auto a1 = And(a, k2);
+  const auto a2 = And(a, k4);
+  const auto a3 = And(a, k8);
+  const auto b0 = And(b, k1);
+  const auto b1 = And(b, k2);
+  const auto b2 = And(b, k4);
+  const auto b3 = And(b, k8);
+
+  auto m0 = Xor(MulEven(a0, b0), MulEven(a1, b3));
+  auto m1 = Xor(MulEven(a0, b1), MulEven(a1, b0));
+  auto m2 = Xor(MulEven(a0, b2), MulEven(a1, b1));
+  auto m3 = Xor(MulEven(a0, b3), MulEven(a1, b2));
+  m0 = Xor(m0, Xor(MulEven(a2, b2), MulEven(a3, b1)));
+  m1 = Xor(m1, Xor(MulEven(a2, b3), MulEven(a3, b2)));
+  m2 = Xor(m2, Xor(MulEven(a2, b0), MulEven(a3, b3)));
+  m3 = Xor(m3, Xor(MulEven(a2, b1), MulEven(a3, b0)));
+  return Or(Or(And(m0, k1), And(m1, k2)), Or(And(m2, k4), And(m3, k8)));
+}
+
+template <class V>
+HWY_API V CLMulUpper(V a, V b) {
+  const DFromV<V> d;
+  static_assert(IsSame<TFromD<decltype(d)>, uint64_t>(), "V must be u64");
+  const auto k1 = Set(d, 0x1111111111111111ULL);
+  const auto k2 = Set(d, 0x2222222222222222ULL);
+  const auto k4 = Set(d, 0x4444444444444444ULL);
+  const auto k8 = Set(d, 0x8888888888888888ULL);
+  const auto a0 = And(a, k1);
+  const auto a1 = And(a, k2);
+  const auto a2 = And(a, k4);
+  const auto a3 = And(a, k8);
+  const auto b0 = And(b, k1);
+  const auto b1 = And(b, k2);
+  const auto b2 = And(b, k4);
+  const auto b3 = And(b, k8);
+
+  auto m0 = Xor(MulOdd(a0, b0), MulOdd(a1, b3));
+  auto m1 = Xor(MulOdd(a0, b1), MulOdd(a1, b0));
+  auto m2 = Xor(MulOdd(a0, b2), MulOdd(a1, b1));
+  auto m3 = Xor(MulOdd(a0, b3), MulOdd(a1, b2));
+  m0 = Xor(m0, Xor(MulOdd(a2, b2), MulOdd(a3, b1)));
+  m1 = Xor(m1, Xor(MulOdd(a2, b3), MulOdd(a3, b2)));
+  m2 = Xor(m2, Xor(MulOdd(a2, b0), MulOdd(a3, b3)));
+  m3 = Xor(m3, Xor(MulOdd(a2, b1), MulOdd(a3, b0)));
+  return Or(Or(And(m0, k1), And(m1, k2)), Or(And(m2, k4), And(m3, k8)));
+}
+
+#endif  // HWY_NATIVE_AES
+#endif  // HWY_TARGET != HWY_SCALAR
+
+// "Include guard": skip if native POPCNT-related instructions are available.
+#if (defined(HWY_NATIVE_POPCNT) == defined(HWY_TARGET_TOGGLE))
+#ifdef HWY_NATIVE_POPCNT
+#undef HWY_NATIVE_POPCNT
+#else
+#define HWY_NATIVE_POPCNT
+#endif
+
+template <typename V, HWY_IF_LANES_ARE(uint8_t, V)>
+HWY_API V PopulationCount(V v) {
+  constexpr DFromV<V> d;
+  HWY_ALIGN constexpr uint8_t kLookup[16] = {
+      0, 1, 1, 2, 1, 2, 2, 3, 1, 2, 2, 3, 2, 3, 3, 4,
+  };
+  auto lo = And(v, Set(d, 0xF));
+  auto hi = ShiftRight<4>(v);
+  auto lookup = LoadDup128(Simd<uint8_t, HWY_MAX(16, MaxLanes(d))>(), kLookup);
+  return Add(TableLookupBytes(lookup, hi), TableLookupBytes(lookup, lo));
+}
+
+template <typename V, HWY_IF_LANES_ARE(uint16_t, V)>
+HWY_API V PopulationCount(V v) {
+  const DFromV<V> d;
+  Repartition<uint8_t, decltype(d)> d8;
+  auto vals = BitCast(d, PopulationCount(BitCast(d8, v)));
+  return Add(ShiftRight<8>(vals), And(vals, Set(d, 0xFF)));
+}
+
+template <typename V, HWY_IF_LANES_ARE(uint32_t, V)>
+HWY_API V PopulationCount(V v) {
+  const DFromV<V> d;
+  Repartition<uint16_t, decltype(d)> d16;
+  auto vals = BitCast(d, PopulationCount(BitCast(d16, v)));
+  return Add(ShiftRight<16>(vals), And(vals, Set(d, 0xFF)));
+}
+
+#if HWY_CAP_INTEGER64
+template <typename V, HWY_IF_LANES_ARE(uint64_t, V)>
+HWY_API V PopulationCount(V v) {
+  const DFromV<V> d;
+  Repartition<uint32_t, decltype(d)> d32;
+  auto vals = BitCast(d, PopulationCount(BitCast(d32, v)));
+  return Add(ShiftRight<32>(vals), And(vals, Set(d, 0xFF)));
+}
+#endif
+
+#endif  // HWY_NATIVE_POPCNT
+
+// NOLINTNEXTLINE(google-readability-namespace-comments)
+}  // namespace HWY_NAMESPACE
+}  // namespace hwy
+HWY_AFTER_NAMESPACE();
diff --git a/third_party/highway/hwy/ops/rvv-inl.h b/third_party/highway/hwy/ops/rvv-inl.h
index 6da8720995974..236069603a333 100644
--- a/third_party/highway/hwy/ops/rvv-inl.h
+++ b/third_party/highway/hwy/ops/rvv-inl.h
@@ -29,15 +29,16 @@ namespace HWY_NAMESPACE {
 template <class V>
 struct DFromV_t {};  // specialized in macros
 template <class V>
-using DFromV = typename DFromV_t<V>::type;
+using DFromV = typename DFromV_t<RemoveConst<V>>::type;
 
 template <class V>
 using TFromV = TFromD<DFromV<V>>;
 
-#define HWY_IF_UNSIGNED_V(V) hwy::EnableIf<!IsSigned<TFromV<V>>()>* = nullptr
-#define HWY_IF_SIGNED_V(V) \
-  hwy::EnableIf<IsSigned<TFromV<V>>() && !IsFloat<TFromV<V>>()>* = nullptr
-#define HWY_IF_FLOAT_V(V) hwy::EnableIf<IsFloat<TFromV<V>>()>* = nullptr
+template <typename T, size_t N>
+HWY_INLINE constexpr size_t MLenFromD(Simd<T, N> /* tag */) {
+  // Returns divisor = type bits / LMUL
+  return sizeof(T) * 8 / (N / HWY_LANES(T));
+}
 
 // kShift = log2 of multiplier: 0 for m1, 1 for m2, -2 for mf4
 template <typename T, int kShift = 0>
@@ -202,7 +203,7 @@ HWY_RVV_FOREACH(HWY_SPECIALIZE, _, _)
     return v##OP##_v_##CHAR##SEW##LMUL(v);                                \
   }
 
-// vector = f(vector, scalar), e.g. detail::Add
+// vector = f(vector, scalar), e.g. detail::AddS
 #define HWY_RVV_RETV_ARGVS(BASE, CHAR, SEW, LMUL, SHIFT, MLEN, NAME, OP) \
   HWY_API HWY_RVV_V(BASE, SEW, LMUL)                                     \
       NAME(HWY_RVV_V(BASE, SEW, LMUL) a, HWY_RVV_T(BASE, SEW) b) {       \
@@ -340,7 +341,7 @@ HWY_API VFromD<Simd<T, N>> BitCast(Simd<T, N> /*tag*/, FromV v) {
 namespace detail {
 
 template <class V, class DU = RebindToUnsigned<DFromV<V>>>
-HWY_API VFromD<DU> BitCastToUnsigned(V v) {
+HWY_INLINE VFromD<DU> BitCastToUnsigned(V v) {
   return BitCast(DU(), v);
 }
 
@@ -353,14 +354,14 @@ namespace detail {
 HWY_RVV_FOREACH_U(HWY_RVV_RETV_ARGD, Iota0, id_v)
 
 template <class D, class DU = RebindToUnsigned<D>>
-HWY_API VFromD<DU> Iota0(const D /*d*/) {
+HWY_INLINE VFromD<DU> Iota0(const D /*d*/) {
   Lanes(DU());
   return BitCastToUnsigned(Iota0(DU()));
 }
 
 // Partial
 template <typename T, size_t N, HWY_IF_LE128(T, N)>
-HWY_API VFromD<Simd<T, N>> Iota0(Simd<T, N> /*tag*/) {
+HWY_INLINE VFromD<Simd<T, N>> Iota0(Simd<T, N> /*tag*/) {
   return Iota0(Full<T>());
 }
 
@@ -383,7 +384,7 @@ HWY_API V Not(const V v) {
 
 // Non-vector version (ideally immediate) for use with Iota0
 namespace detail {
-HWY_RVV_FOREACH_UI(HWY_RVV_RETV_ARGVS, And, and_vx)
+HWY_RVV_FOREACH_UI(HWY_RVV_RETV_ARGVS, AndS, and_vx)
 }  // namespace detail
 
 HWY_RVV_FOREACH_UI(HWY_RVV_RETV_ARGVV, And, and)
@@ -397,18 +398,6 @@ HWY_API V And(const V a, const V b) {
 
 // ------------------------------ Or
 
-// Scalar argument plus mask. Used by VecFromMask.
-#define HWY_RVV_OR_MASK(BASE, CHAR, SEW, LMUL, SHIFT, MLEN, NAME, OP)    \
-  HWY_API HWY_RVV_V(BASE, SEW, LMUL)                                     \
-      NAME(HWY_RVV_V(BASE, SEW, LMUL) v, HWY_RVV_T(BASE, SEW) imm,       \
-           HWY_RVV_M(MLEN) mask, HWY_RVV_V(BASE, SEW, LMUL) maskedoff) { \
-    return v##OP##_##CHAR##SEW##LMUL##_m(mask, maskedoff, v, imm);       \
-  }
-
-namespace detail {
-HWY_RVV_FOREACH_UI(HWY_RVV_OR_MASK, Or, or_vx)
-}  // namespace detail
-
 #undef HWY_RVV_OR_MASK
 
 HWY_RVV_FOREACH_UI(HWY_RVV_RETV_ARGVV, Or, or)
@@ -424,7 +413,7 @@ HWY_API V Or(const V a, const V b) {
 
 // Non-vector version (ideally immediate) for use with Iota0
 namespace detail {
-HWY_RVV_FOREACH_UI(HWY_RVV_RETV_ARGVS, Xor, xor_vx)
+HWY_RVV_FOREACH_UI(HWY_RVV_RETV_ARGVS, XorS, xor_vx)
 }  // namespace detail
 
 HWY_RVV_FOREACH_UI(HWY_RVV_RETV_ARGVV, Xor, xor)
@@ -458,8 +447,8 @@ HWY_API V CopySignToAbs(const V abs, const V sign) {
 // ------------------------------ Add
 
 namespace detail {
-HWY_RVV_FOREACH_UI(HWY_RVV_RETV_ARGVS, Add, add_vx)
-HWY_RVV_FOREACH_F(HWY_RVV_RETV_ARGVS, Add, fadd_vf)
+HWY_RVV_FOREACH_UI(HWY_RVV_RETV_ARGVS, AddS, add_vx)
+HWY_RVV_FOREACH_F(HWY_RVV_RETV_ARGVS, AddS, fadd_vf)
 }  // namespace detail
 
 HWY_RVV_FOREACH_UI(HWY_RVV_RETV_ARGVV, Add, add)
@@ -560,17 +549,30 @@ HWY_RVV_FOREACH_F(HWY_RVV_RETV_ARGVV, Max, fmax)
 
 // ------------------------------ Mul
 
+// Only for internal use (Highway only promises Mul for 16/32-bit inputs).
+// Used by MulLower.
+namespace detail {
+HWY_RVV_FOREACH_U64(HWY_RVV_RETV_ARGVV, Mul, mul)
+}  // namespace detail
+
 HWY_RVV_FOREACH_UI16(HWY_RVV_RETV_ARGVV, Mul, mul)
 HWY_RVV_FOREACH_UI32(HWY_RVV_RETV_ARGVV, Mul, mul)
 HWY_RVV_FOREACH_F(HWY_RVV_RETV_ARGVV, Mul, fmul)
 
 // ------------------------------ MulHigh
 
+// Only for internal use (Highway only promises MulHigh for 16-bit inputs).
+// Used by MulEven; vwmul does not work for m8.
+namespace detail {
+HWY_RVV_FOREACH_I32(HWY_RVV_RETV_ARGVV, MulHigh, mulh)
+HWY_RVV_FOREACH_U32(HWY_RVV_RETV_ARGVV, MulHigh, mulhu)
+HWY_RVV_FOREACH_U64(HWY_RVV_RETV_ARGVV, MulHigh, mulhu)
+}  // namespace detail
+
 HWY_RVV_FOREACH_U16(HWY_RVV_RETV_ARGVV, MulHigh, mulhu)
 HWY_RVV_FOREACH_I16(HWY_RVV_RETV_ARGVV, MulHigh, mulh)
 
 // ------------------------------ Div
-
 HWY_RVV_FOREACH_F(HWY_RVV_RETV_ARGVV, Div, fdiv)
 
 // ------------------------------ ApproximateReciprocal
@@ -630,6 +632,14 @@ HWY_RVV_FOREACH_F(HWY_RVV_FMA, NegMulSub, fnmacc)
     return v##OP##_vv_##CHAR##SEW##LMUL##_b##MLEN(a, b);                 \
   }
 
+// mask = f(vector, scalar)
+#define HWY_RVV_RETM_ARGVS(BASE, CHAR, SEW, LMUL, SHIFT, MLEN, NAME, OP) \
+  HWY_API HWY_RVV_M(MLEN)                                                \
+      NAME(HWY_RVV_V(BASE, SEW, LMUL) a, HWY_RVV_T(BASE, SEW) b) {       \
+    (void)Lanes(DFromV<decltype(a)>());                                  \
+    return v##OP##_vx_##CHAR##SEW##LMUL##_b##MLEN(a, b);                 \
+  }
+
 // ------------------------------ Eq
 HWY_RVV_FOREACH_UI(HWY_RVV_RETM_ARGVV, Eq, mseq)
 HWY_RVV_FOREACH_F(HWY_RVV_RETM_ARGVV, Eq, mfeq)
@@ -642,27 +652,29 @@ HWY_RVV_FOREACH_F(HWY_RVV_RETM_ARGVV, Ne, mfne)
 HWY_RVV_FOREACH_I(HWY_RVV_RETM_ARGVV, Lt, mslt)
 HWY_RVV_FOREACH_F(HWY_RVV_RETM_ARGVV, Lt, mflt)
 
-// ------------------------------ Gt
-
-template <class V>
-HWY_API auto Gt(const V a, const V b) -> decltype(Lt(a, b)) {
-  return Lt(b, a);
-}
+namespace detail {
+HWY_RVV_FOREACH_I(HWY_RVV_RETM_ARGVS, LtS, mslt)
+}  // namespace detail
 
 // ------------------------------ Le
 HWY_RVV_FOREACH_F(HWY_RVV_RETM_ARGVV, Le, mfle)
 
 #undef HWY_RVV_RETM_ARGVV
+#undef HWY_RVV_RETM_ARGVS
 
-// ------------------------------ Ge
+// ------------------------------ Gt/Ge
 
 template <class V>
 HWY_API auto Ge(const V a, const V b) -> decltype(Le(a, b)) {
   return Le(b, a);
 }
 
-// ------------------------------ TestBit
+template <class V>
+HWY_API auto Gt(const V a, const V b) -> decltype(Lt(a, b)) {
+  return Lt(b, a);
+}
 
+// ------------------------------ TestBit
 template <class V>
 HWY_API auto TestBit(const V a, const V bit) -> decltype(Eq(a, bit)) {
   return Ne(And(a, bit), Zero(DFromV<V>()));
@@ -671,9 +683,9 @@ HWY_API auto TestBit(const V a, const V bit) -> decltype(Eq(a, bit)) {
 // ------------------------------ Not
 
 // mask = f(mask)
-#define HWY_RVV_RETM_ARGM(MLEN, NAME, OP)           \
-  HWY_API HWY_RVV_M(MLEN) NAME(HWY_RVV_M(MLEN) m) { \
-    return vm##OP##_m_b##MLEN(m);                   \
+#define HWY_RVV_RETM_ARGM(MLEN, NAME, OP)                 \
+  HWY_API HWY_RVV_M(MLEN) NAME(HWY_RVV_M(MLEN) m) {       \
+    return vm##OP##_m_b##MLEN(m);                         \
   }
 
 HWY_RVV_FOREACH_B(HWY_RVV_RETM_ARGM, Not, not )
@@ -712,15 +724,14 @@ HWY_RVV_FOREACH_B(HWY_RVV_RETM_ARGMM, Xor, xor)
 HWY_RVV_FOREACH(HWY_RVV_IF_THEN_ELSE, IfThenElse, merge)
 
 #undef HWY_RVV_IF_THEN_ELSE
-// ------------------------------ IfThenElseZero
 
+// ------------------------------ IfThenElseZero
 template <class M, class V>
 HWY_API V IfThenElseZero(const M mask, const V yes) {
   return IfThenElse(mask, yes, Zero(DFromV<V>()));
 }
 
 // ------------------------------ IfThenZeroElse
-
 template <class M, class V>
 HWY_API V IfThenZeroElse(const M mask, const V no) {
   return IfThenElse(mask, Zero(DFromV<V>()), no);
@@ -745,10 +756,20 @@ HWY_API MFromD<D> RebindMask(const D /*d*/, const MFrom mask) {
 
 // ------------------------------ VecFromMask
 
+namespace detail {
+#define HWY_RVV_VEC_FROM_MASK(BASE, CHAR, SEW, LMUL, SHIFT, MLEN, NAME, OP) \
+  HWY_API HWY_RVV_V(BASE, SEW, LMUL)                                        \
+      NAME(HWY_RVV_V(BASE, SEW, LMUL) v0, HWY_RVV_M(MLEN) m) {              \
+    return v##OP##_##CHAR##SEW##LMUL##_m(m, v0, v0, 1);                     \
+  }
+
+HWY_RVV_FOREACH_UI(HWY_RVV_VEC_FROM_MASK, SubS, sub_vx)
+#undef HWY_RVV_VEC_FROM_MASK
+}  // namespace detail
+
 template <class D, HWY_IF_NOT_FLOAT_D(D)>
 HWY_API VFromD<D> VecFromMask(const D d, MFromD<D> mask) {
-  const auto v0 = Zero(d);
-  return detail::Or(v0, -1, mask, v0);
+  return detail::SubS(Zero(d), mask);
 }
 
 template <class D, HWY_IF_FLOAT_D(D)>
@@ -757,7 +778,6 @@ HWY_API VFromD<D> VecFromMask(const D d, MFromD<D> mask) {
 }
 
 // ------------------------------ ZeroIfNegative
-
 template <class V>
 HWY_API V ZeroIfNegative(const V v) {
   const auto v0 = Zero(DFromV<V>());
@@ -766,7 +786,6 @@ HWY_API V ZeroIfNegative(const V v) {
 }
 
 // ------------------------------ BroadcastSignBit
-
 template <class V>
 HWY_API V BroadcastSignBit(const V v) {
   return ShiftRight<sizeof(TFromV<V>) * 8 - 1>(v);
@@ -774,29 +793,61 @@ HWY_API V BroadcastSignBit(const V v) {
 
 // ------------------------------ AllFalse
 
-#define HWY_RVV_ALL_FALSE(MLEN, NAME, OP)          \
-  HWY_API bool AllFalse(const HWY_RVV_M(MLEN) m) { \
-    return vfirst_m_b##MLEN(m) < 0;                \
+#define HWY_RVV_ALL_FALSE(MLEN, NAME, OP)                     \
+  template <class D>                                          \
+  HWY_API bool AllFalse(const D d, const HWY_RVV_M(MLEN) m) { \
+    static_assert(MLenFromD(d) == MLEN, "Type mismatch");     \
+    return vfirst_m_b##MLEN(m) < 0;                           \
+  }                                                           \
+  /* DEPRECATED */                                            \
+  HWY_API bool AllFalse(const HWY_RVV_M(MLEN) m) {            \
+    return vfirst_m_b##MLEN(m) < 0;                           \
   }
 HWY_RVV_FOREACH_B(HWY_RVV_ALL_FALSE, _, _)
 #undef HWY_RVV_ALL_FALSE
 
 // ------------------------------ AllTrue
 
-#define HWY_RVV_ALL_TRUE(MLEN, NAME, OP)    \
-  HWY_API bool AllTrue(HWY_RVV_M(MLEN) m) { \
-    return AllFalse(vmnot_m_b##MLEN(m));    \
+#define HWY_RVV_ALL_TRUE(MLEN, NAME, OP)                  \
+  template <class D>                                      \
+  HWY_API bool AllTrue(D d, HWY_RVV_M(MLEN) m) {          \
+    static_assert(MLenFromD(d) == MLEN, "Type mismatch"); \
+    return AllFalse(vmnot_m_b##MLEN(m));                  \
+  }                                                       \
+  /* DEPRECATED */                                        \
+  HWY_API bool AllTrue(HWY_RVV_M(MLEN) m) {               \
+    return AllFalse(vmnot_m_b##MLEN(m));                  \
   }
+
 HWY_RVV_FOREACH_B(HWY_RVV_ALL_TRUE, _, _)
 #undef HWY_RVV_ALL_TRUE
 
 // ------------------------------ CountTrue
 
-#define HWY_RVV_COUNT_TRUE(MLEN, NAME, OP) \
+#define HWY_RVV_COUNT_TRUE(MLEN, NAME, OP)                \
+  template <class D>                                      \
+  HWY_API size_t CountTrue(D d, HWY_RVV_M(MLEN) m) {      \
+    static_assert(MLenFromD(d) == MLEN, "Type mismatch"); \
+    return vpopc_m_b##MLEN(m);                            \
+  }                                                       \
+  /* DEPRECATED */                                        \
   HWY_API size_t CountTrue(HWY_RVV_M(MLEN) m) { return vpopc_m_b##MLEN(m); }
+
 HWY_RVV_FOREACH_B(HWY_RVV_COUNT_TRUE, _, _)
 #undef HWY_RVV_COUNT_TRUE
 
+// ------------------------------ FindFirstTrue
+
+#define HWY_RVV_FIND_FIRST_TRUE(MLEN, NAME, OP)           \
+  template <class D>                                      \
+  HWY_API intptr_t FindFirstTrue(D d, HWY_RVV_M(MLEN) m) { \
+    static_assert(MLenFromD(d) == MLEN, "Type mismatch"); \
+    return vfirst_m_b##MLEN(m);                           \
+  }
+
+HWY_RVV_FOREACH_B(HWY_RVV_FIND_FIRST_TRUE, _, _)
+#undef HWY_RVV_FIND_FIRST_TRUE
+
 // ================================================== MEMORY
 
 // ------------------------------ Load
@@ -852,7 +903,6 @@ HWY_API void StoreU(const V v, D d, TFromD<D>* HWY_RESTRICT p) {
 }
 
 // ------------------------------ Stream
-
 template <class V, class D, typename T>
 HWY_API void Stream(const V v, D d, T* HWY_RESTRICT aligned) {
   Store(v, d, aligned);
@@ -1167,29 +1217,110 @@ HWY_API VFromD<Simd<T, N>> ConvertTo(Simd<T, N> /*tag*/, FromV v) {
   return ConvertTo(Full<T>(), v);
 }
 
-// ================================================== SWIZZLE
+// ================================================== COMBINE
 
-// ------------------------------ Compress
+namespace detail {
 
-#define HWY_RVV_COMPRESS(BASE, CHAR, SEW, LMUL, SHIFT, MLEN, NAME, OP) \
-  HWY_API HWY_RVV_V(BASE, SEW, LMUL)                                   \
-      NAME(HWY_RVV_V(BASE, SEW, LMUL) v, HWY_RVV_M(MLEN) mask) {       \
-    return v##OP##_vm_##CHAR##SEW##LMUL(mask, v, v);                   \
+// For x86-compatible behaviour mandated by Highway API: TableLookupBytes
+// offsets are implicitly relative to the start of their 128-bit block.
+template <typename T, size_t N>
+constexpr size_t LanesPerBlock(Simd<T, N> /* tag */) {
+  // Also cap to the limit imposed by D (for fixed-size <= 128-bit vectors).
+  return HWY_MIN(16 / sizeof(T), N);
+}
+
+template <class D, class V>
+HWY_INLINE V OffsetsOf128BitBlocks(const D d, const V iota0) {
+  using T = MakeUnsigned<TFromD<D>>;
+  return AndS(iota0, static_cast<T>(~(LanesPerBlock(d) - 1)));
+}
+
+template <size_t kLanes, class D>
+HWY_INLINE MFromD<D> FirstNPerBlock(D /* tag */) {
+  const RebindToUnsigned<D> du;
+  const RebindToSigned<D> di;
+  constexpr size_t kLanesPerBlock = LanesPerBlock(du);
+  const auto idx_mod = AndS(Iota0(du), kLanesPerBlock - 1);
+  return LtS(BitCast(di, idx_mod), static_cast<TFromD<decltype(di)>>(kLanes));
+}
+
+// vector = f(vector, vector, size_t)
+#define HWY_RVV_SLIDE(BASE, CHAR, SEW, LMUL, SHIFT, MLEN, NAME, OP)        \
+  HWY_API HWY_RVV_V(BASE, SEW, LMUL)                                       \
+      NAME(HWY_RVV_V(BASE, SEW, LMUL) dst, HWY_RVV_V(BASE, SEW, LMUL) src, \
+           size_t lanes) {                                                 \
+    return v##OP##_vx_##CHAR##SEW##LMUL(dst, src, lanes);                  \
   }
 
-HWY_RVV_FOREACH_UI16(HWY_RVV_COMPRESS, Compress, compress)
-HWY_RVV_FOREACH_UI32(HWY_RVV_COMPRESS, Compress, compress)
-HWY_RVV_FOREACH_UI64(HWY_RVV_COMPRESS, Compress, compress)
-HWY_RVV_FOREACH_F(HWY_RVV_COMPRESS, Compress, compress)
-#undef HWY_RVV_COMPRESS
+HWY_RVV_FOREACH(HWY_RVV_SLIDE, SlideUp, slideup)
+HWY_RVV_FOREACH(HWY_RVV_SLIDE, SlideDown, slidedown)
 
-// ------------------------------ CompressStore
+#undef HWY_RVV_SLIDE
 
-template <class V, class M, class D>
-HWY_API size_t CompressStore(const V v, const M mask, const D d,
-                             TFromD<D>* HWY_RESTRICT aligned) {
-  Store(Compress(v, mask), d, aligned);
-  return CountTrue(mask);
+}  // namespace detail
+
+// ------------------------------ ConcatUpperLower
+template <class V>
+HWY_API V ConcatUpperLower(const V hi, const V lo) {
+  const RebindToSigned<DFromV<V>> di;
+  return IfThenElse(FirstN(di, Lanes(di) / 2), lo, hi);
+}
+
+// ------------------------------ ConcatLowerLower
+template <class V>
+HWY_API V ConcatLowerLower(const V hi, const V lo) {
+  return detail::SlideUp(lo, hi, Lanes(DFromV<V>()) / 2);
+}
+
+// ------------------------------ ConcatUpperUpper
+template <class V>
+HWY_API V ConcatUpperUpper(const V hi, const V lo) {
+  // Move upper half into lower
+  const auto lo_down = detail::SlideDown(lo, lo, Lanes(DFromV<V>()) / 2);
+  return ConcatUpperLower(hi, lo_down);
+}
+
+// ------------------------------ ConcatLowerUpper
+template <class V>
+HWY_API V ConcatLowerUpper(const V hi, const V lo) {
+  // Move half of both inputs to the other half
+  const auto hi_up = detail::SlideUp(hi, hi, Lanes(DFromV<V>()) / 2);
+  const auto lo_down = detail::SlideDown(lo, lo, Lanes(DFromV<V>()) / 2);
+  return ConcatUpperLower(hi_up, lo_down);
+}
+
+// ------------------------------ Combine
+
+// TODO(janwas): implement after LMUL ext/trunc
+#if 0
+
+template <class V>
+HWY_API V Combine(const V a, const V b) {
+  using D = DFromV<V>;
+  // double LMUL of inputs, then SlideUp with Lanes().
+}
+
+#endif
+
+// ================================================== SWIZZLE
+
+// ------------------------------ GetLane
+
+#define HWY_RVV_GET_LANE(BASE, CHAR, SEW, LMUL, SHIFT, MLEN, NAME, OP) \
+  HWY_API HWY_RVV_T(BASE, SEW) NAME(HWY_RVV_V(BASE, SEW, LMUL) v) {    \
+    return v##OP##_s_##CHAR##SEW##LMUL##_##CHAR##SEW(v);               \
+  }
+
+HWY_RVV_FOREACH_UI(HWY_RVV_GET_LANE, GetLane, mv_x)
+HWY_RVV_FOREACH_F(HWY_RVV_GET_LANE, GetLane, fmv_f)
+#undef HWY_RVV_GET_LANE
+
+// ------------------------------ OddEven
+template <class V>
+HWY_API V OddEven(const V a, const V b) {
+  const RebindToUnsigned<DFromV<V>> du;  // Iota0 is unsigned only
+  const auto is_even = Eq(detail::AndS(detail::Iota0(du), 1), Zero(du));
+  return IfThenElse(is_even, b, a);
 }
 
 // ------------------------------ TableLookupLanes
@@ -1216,105 +1347,134 @@ HWY_API VFromD<DU> SetTableIndices(D d, const TFromD<DU>* idx) {
 HWY_RVV_FOREACH(HWY_RVV_TABLE, TableLookupLanes, rgather)
 #undef HWY_RVV_TABLE
 
-// ------------------------------ Shuffle01
+// ------------------------------ Compress
 
-template <class V>
-HWY_API V Shuffle01(const V v) {
-  using D = DFromV<V>;
-  static_assert(sizeof(TFromD<D>) == 8, "Defined for 64-bit types");
-  const auto idx = detail::Xor(detail::Iota0(D()), 1);
-  return TableLookupLanes(v, idx);
-}
+#define HWY_RVV_COMPRESS(BASE, CHAR, SEW, LMUL, SHIFT, MLEN, NAME, OP) \
+  HWY_API HWY_RVV_V(BASE, SEW, LMUL)                                   \
+      NAME(HWY_RVV_V(BASE, SEW, LMUL) v, HWY_RVV_M(MLEN) mask) {       \
+    return v##OP##_vm_##CHAR##SEW##LMUL(mask, v, v);                   \
+  }
 
-// ------------------------------ Shuffle2301
+HWY_RVV_FOREACH_UI16(HWY_RVV_COMPRESS, Compress, compress)
+HWY_RVV_FOREACH_UI32(HWY_RVV_COMPRESS, Compress, compress)
+HWY_RVV_FOREACH_UI64(HWY_RVV_COMPRESS, Compress, compress)
+HWY_RVV_FOREACH_F(HWY_RVV_COMPRESS, Compress, compress)
+#undef HWY_RVV_COMPRESS
 
-template <class V>
-HWY_API V Shuffle2301(const V v) {
-  using D = DFromV<V>;
-  static_assert(sizeof(TFromD<D>) == 4, "Defined for 32-bit types");
-  const auto idx = detail::Xor(detail::Iota0(D()), 1);
-  return TableLookupLanes(v, idx);
+// ------------------------------ CompressStore
+template <class V, class M, class D>
+HWY_API size_t CompressStore(const V v, const M mask, const D d,
+                             TFromD<D>* HWY_RESTRICT aligned) {
+  Store(Compress(v, mask), d, aligned);
+  return CountTrue(d, mask);
 }
 
-// ------------------------------ Shuffle1032
+// ================================================== BLOCKWISE
 
-template <class V>
-HWY_API V Shuffle1032(const V v) {
-  using D = DFromV<V>;
-  static_assert(sizeof(TFromD<D>) == 4, "Defined for 32-bit types");
-  const auto idx = detail::Xor(detail::Iota0(D()), 2);
-  return TableLookupLanes(v, idx);
+// ------------------------------ CombineShiftRightBytes
+template <size_t kBytes, class D, class V = VFromD<D>>
+HWY_API V CombineShiftRightBytes(const D d, const V hi, V lo) {
+  const Repartition<uint8_t, decltype(d)> d8;
+  Lanes(d8);
+  const auto hi8 = BitCast(d8, hi);
+  const auto lo8 = BitCast(d8, lo);
+  const auto hi_up = detail::SlideUp(hi8, hi8, 16 - kBytes);
+  const auto lo_down = detail::SlideDown(lo8, lo8, kBytes);
+  const auto is_lo = detail::FirstNPerBlock<16 - kBytes>(d8);
+  const auto combined = BitCast(d, IfThenElse(is_lo, lo_down, hi_up));
+  Lanes(d);
+  return combined;
 }
 
-// ------------------------------ Shuffle0123
+// ------------------------------ CombineShiftRightLanes
+template <size_t kLanes, class D, class V = VFromD<D>>
+HWY_API V CombineShiftRightLanes(const D d, const V hi, V lo) {
+  constexpr size_t kLanesUp = 16 / sizeof(TFromV<V>) - kLanes;
+  const auto hi_up = detail::SlideUp(hi, hi, kLanesUp);
+  const auto lo_down = detail::SlideDown(lo, lo, kLanes);
+  const auto is_lo = detail::FirstNPerBlock<kLanesUp>(d);
+  return IfThenElse(is_lo, lo_down, hi_up);
+}
 
+// ------------------------------ Shuffle2301 (ShiftLeft)
 template <class V>
-HWY_API V Shuffle0123(const V v) {
-  using D = DFromV<V>;
-  static_assert(sizeof(TFromD<D>) == 4, "Defined for 32-bit types");
-  const auto idx = detail::Xor(detail::Iota0(D()), 3);
-  return TableLookupLanes(v, idx);
+HWY_API V Shuffle2301(const V v) {
+  const DFromV<V> d;
+  static_assert(sizeof(TFromD<decltype(d)>) == 4, "Defined for 32-bit types");
+  const Repartition<uint64_t, decltype(d)> du64;
+  const auto v64 = BitCast(du64, v);
+  Lanes(du64);
+  const auto rotated = BitCast(d, Or(ShiftRight<32>(v64), ShiftLeft<32>(v64)));
+  Lanes(d);
+  return rotated;
 }
 
 // ------------------------------ Shuffle2103
-
 template <class V>
 HWY_API V Shuffle2103(const V v) {
-  using D = DFromV<V>;
-  static_assert(sizeof(TFromD<D>) == 4, "Defined for 32-bit types");
-  // This shuffle is a rotation. We can compute subtraction modulo 4 (number of
-  // lanes per 128-bit block) via bitwise ops.
-  const auto i = detail::Xor(detail::Iota0(D()), 1);
-  const auto lsb = detail::And(i, 1);
-  const auto borrow = Add(lsb, lsb);
-  const auto idx = Xor(i, borrow);
-  return TableLookupLanes(v, idx);
+  const DFromV<V> d;
+  static_assert(sizeof(TFromD<decltype(d)>) == 4, "Defined for 32-bit types");
+  return CombineShiftRightLanes<3>(d, v, v);
 }
 
 // ------------------------------ Shuffle0321
-
 template <class V>
 HWY_API V Shuffle0321(const V v) {
-  using D = DFromV<V>;
-  static_assert(sizeof(TFromD<D>) == 4, "Defined for 32-bit types");
-  // This shuffle is a rotation. We can compute subtraction modulo 4 (number of
-  // lanes per 128-bit block) via bitwise ops.
-  const auto i = detail::Xor(detail::Iota0(D()), 3);
-  const auto lsb = detail::And(i, 1);
-  const auto borrow = Add(lsb, lsb);
-  const auto idx = Xor(i, borrow);
-  return TableLookupLanes(v, idx);
+  const DFromV<V> d;
+  static_assert(sizeof(TFromD<decltype(d)>) == 4, "Defined for 32-bit types");
+  return CombineShiftRightLanes<1>(d, v, v);
 }
 
-// ------------------------------ TableLookupBytes
-
-namespace detail {
-
-// For x86-compatible behaviour mandated by Highway API: TableLookupBytes
-// offsets are implicitly relative to the start of their 128-bit block.
-template <class D>
-constexpr size_t LanesPerBlock(D) {
-  return 16 / sizeof(TFromD<D>);
+// ------------------------------ Shuffle1032
+template <class V>
+HWY_API V Shuffle1032(const V v) {
+  const DFromV<V> d;
+  static_assert(sizeof(TFromD<decltype(d)>) == 4, "Defined for 32-bit types");
+  return CombineShiftRightLanes<2>(d, v, v);
 }
 
-template <class D, class V>
-HWY_API V OffsetsOf128BitBlocks(const D d, const V iota0) {
-  using T = MakeUnsigned<TFromD<D>>;
-  return detail::And(iota0, static_cast<T>(~(LanesPerBlock(d) - 1)));
+// ------------------------------ Shuffle01
+template <class V>
+HWY_API V Shuffle01(const V v) {
+  const DFromV<V> d;
+  static_assert(sizeof(TFromD<decltype(d)>) == 8, "Defined for 64-bit types");
+  return CombineShiftRightLanes<1>(d, v, v);
 }
-}  // namespace detail
 
+// ------------------------------ Shuffle0123
 template <class V>
-HWY_API V TableLookupBytes(const V v, const V idx) {
-  using D = DFromV<V>;
-  const Repartition<uint8_t, D> d8;
+HWY_API V Shuffle0123(const V v) {
+  return Shuffle2301(Shuffle1032(v));
+}
+
+// ------------------------------ TableLookupBytes
+
+template <class V, class VI>
+HWY_API VI TableLookupBytes(const V v, const VI idx) {
+  const DFromV<VI> d;
+  const Repartition<uint8_t, decltype(d)> d8;
+  Lanes(d8);
   const auto offsets128 = detail::OffsetsOf128BitBlocks(d8, detail::Iota0(d8));
   const auto idx8 = Add(BitCast(d8, idx), offsets128);
-  return BitCast(D(), TableLookupLanes(BitCast(d8, v), idx8));
+  const auto out = BitCast(d, TableLookupLanes(BitCast(d8, v), idx8));
+  Lanes(d);
+  return out;
+}
+
+template <class V, class VI>
+HWY_API V TableLookupBytesOr0(const VI v, const V idx) {
+  const DFromV<VI> d;
+  // Mask size must match vector type, so cast everything to this type.
+  const Repartition<int8_t, decltype(d)> di8;
+  Lanes(di8);
+  const auto lookup = TableLookupBytes(BitCast(di8, v), BitCast(di8, idx));
+  const auto msb = Lt(BitCast(di8, idx), Zero(di8));
+  const auto out = BitCast(d, IfThenZeroElse(msb, lookup));
+  Lanes(d);
+  return out;
 }
 
 // ------------------------------ Broadcast
-
 template <int kLane, class V>
 HWY_API V Broadcast(const V v) {
   const DFromV<V> d;
@@ -1322,195 +1482,134 @@ HWY_API V Broadcast(const V v) {
   static_assert(0 <= kLane && kLane < kLanesPerBlock, "Invalid lane");
   auto idx = detail::OffsetsOf128BitBlocks(d, detail::Iota0(d));
   if (kLane != 0) {
-    idx = detail::Add(idx, kLane);
+    idx = detail::AddS(idx, kLane);
   }
   return TableLookupLanes(v, idx);
 }
 
-// ------------------------------ GetLane
-
-#define HWY_RVV_GET_LANE(BASE, CHAR, SEW, LMUL, SHIFT, MLEN, NAME, OP) \
-  HWY_API HWY_RVV_T(BASE, SEW) NAME(HWY_RVV_V(BASE, SEW, LMUL) v) {    \
-    return v##OP##_s_##CHAR##SEW##LMUL##_##CHAR##SEW(v);               \
-  }
-
-HWY_RVV_FOREACH_UI(HWY_RVV_GET_LANE, GetLane, mv_x)
-HWY_RVV_FOREACH_F(HWY_RVV_GET_LANE, GetLane, fmv_f)
-#undef HWY_RVV_GET_LANE
-
 // ------------------------------ ShiftLeftLanes
 
-// vector = f(vector, vector, size_t)
-#define HWY_RVV_SLIDE(BASE, CHAR, SEW, LMUL, SHIFT, MLEN, NAME, OP)        \
-  HWY_API HWY_RVV_V(BASE, SEW, LMUL)                                       \
-      NAME(HWY_RVV_V(BASE, SEW, LMUL) dst, HWY_RVV_V(BASE, SEW, LMUL) src, \
-           size_t lanes) {                                                 \
-    return v##OP##_vx_##CHAR##SEW##LMUL(dst, src, lanes);                  \
-  }
-
-namespace detail {
-HWY_RVV_FOREACH(HWY_RVV_SLIDE, SlideUp, slideup)
-}  // namespace detail
-
-template <size_t kLanes, class V>
-HWY_API V ShiftLeftLanes(const V v) {
-  using D = DFromV<V>;
-  const RebindToSigned<D> di;
+template <size_t kLanes, class D, class V = VFromD<D>>
+HWY_API V ShiftLeftLanes(const D d, const V v) {
+  const RebindToSigned<decltype(d)> di;
   const auto shifted = detail::SlideUp(v, v, kLanes);
   // Match x86 semantics by zeroing lower lanes in 128-bit blocks
   constexpr size_t kLanesPerBlock = detail::LanesPerBlock(di);
-  const auto idx_mod = detail::And(detail::Iota0(di), kLanesPerBlock - 1);
+  const auto idx_mod = detail::AndS(detail::Iota0(di), kLanesPerBlock - 1);
   const auto clear = Lt(BitCast(di, idx_mod), Set(di, kLanes));
   return IfThenZeroElse(clear, shifted);
 }
 
+template <size_t kLanes, class V>
+HWY_API V ShiftLeftLanes(const V v) {
+  return ShiftLeftLanes<kLanes>(DFromV<V>(), v);
+}
+
 // ------------------------------ ShiftLeftBytes
 
 template <int kBytes, class V>
-HWY_API V ShiftLeftBytes(const V v) {
-  using D = DFromV<V>;
-  const Repartition<uint8_t, D> d8;
+HWY_API V ShiftLeftBytes(DFromV<V> d, const V v) {
+  const Repartition<uint8_t, decltype(d)> d8;
   Lanes(d8);
-  return BitCast(D(), ShiftLeftLanes<kBytes>(BitCast(d8, v)));
+  const auto shifted = BitCast(d, ShiftLeftLanes<kBytes>(BitCast(d8, v)));
+  Lanes(d);
+  return shifted;
 }
 
-// ------------------------------ ShiftRightLanes
-
-namespace detail {
-HWY_RVV_FOREACH(HWY_RVV_SLIDE, SlideDown, slidedown)
-}  // namespace detail
+template <int kBytes, class V>
+HWY_API V ShiftLeftBytes(const V v) {
+  return ShiftLeftBytes<kBytes>(DFromV<V>(), v);
+}
 
-#undef HWY_RVV_SLIDE
+// ------------------------------ ShiftRightLanes
+template <size_t kLanes, typename T, size_t N, class V = VFromD<Simd<T, N>>>
+HWY_API V ShiftRightLanes(const Simd<T, N> d, V v) {
+  const RebindToSigned<decltype(d)> di;
+  // For partial vectors, clear upper lanes so we shift in zeros.
+  if (N <= 16 / sizeof(T)) {
+    v = IfThenElseZero(FirstN(d, N), v);
+  }
 
-template <size_t kLanes, class V>
-HWY_API V ShiftRightLanes(const V v) {
-  using D = DFromV<V>;
-  const RebindToSigned<D> di;
   const auto shifted = detail::SlideDown(v, v, kLanes);
   // Match x86 semantics by zeroing upper lanes in 128-bit blocks
   constexpr size_t kLanesPerBlock = detail::LanesPerBlock(di);
-  const auto idx_mod = detail::And(detail::Iota0(di), kLanesPerBlock - 1);
+  const auto idx_mod = detail::AndS(detail::Iota0(di), kLanesPerBlock - 1);
   const auto keep = Lt(BitCast(di, idx_mod), Set(di, kLanesPerBlock - kLanes));
   return IfThenElseZero(keep, shifted);
 }
 
 // ------------------------------ ShiftRightBytes
-
-template <int kBytes, class V>
-HWY_API V ShiftRightBytes(const V v) {
-  using D = DFromV<V>;
-  const Repartition<uint8_t, D> d8;
+template <int kBytes, class D, class V = VFromD<D>>
+HWY_API V ShiftRightBytes(const D d, const V v) {
+  const Repartition<uint8_t, decltype(d)> d8;
   Lanes(d8);
-  return BitCast(D(), ShiftRightLanes<kBytes>(BitCast(d8, v)));
-}
-
-// ------------------------------ OddEven
-
-template <class V>
-HWY_API V OddEven(const V a, const V b) {
-  const RebindToUnsigned<DFromV<V>> du;  // Iota0 is unsigned only
-  const auto is_even = Eq(detail::And(detail::Iota0(du), 1), Zero(du));
-  return IfThenElse(is_even, b, a);
-}
-
-// ------------------------------ ConcatUpperLower
-
-template <class V>
-HWY_API V ConcatUpperLower(const V hi, const V lo) {
-  const RebindToSigned<DFromV<V>> di;
-  const auto idx_half = Set(di, Lanes(di) / 2);
-  const auto is_lower_half = Lt(BitCast(di, detail::Iota0(di)), idx_half);
-  return IfThenElse(is_lower_half, lo, hi);
-}
-
-// ------------------------------ ConcatLowerLower
-
-template <class V>
-HWY_API V ConcatLowerLower(const V hi, const V lo) {
-  // Move lower half into upper
-  const auto hi_up = detail::SlideUp(hi, hi, Lanes(DFromV<V>()) / 2);
-  return ConcatUpperLower(hi_up, lo);
-}
-
-// ------------------------------ ConcatUpperUpper
-
-template <class V>
-HWY_API V ConcatUpperUpper(const V hi, const V lo) {
-  // Move upper half into lower
-  const auto lo_down = detail::SlideDown(lo, lo, Lanes(DFromV<V>()) / 2);
-  return ConcatUpperLower(hi, lo_down);
-}
-
-// ------------------------------ ConcatLowerUpper
-
-template <class V>
-HWY_API V ConcatLowerUpper(const V hi, const V lo) {
-  // Move half of both inputs to the other half
-  const auto hi_up = detail::SlideUp(hi, hi, Lanes(DFromV<V>()) / 2);
-  const auto lo_down = detail::SlideDown(lo, lo, Lanes(DFromV<V>()) / 2);
-  return ConcatUpperLower(hi_up, lo_down);
+  const auto shifted = BitCast(d, ShiftRightLanes<kBytes>(d8, BitCast(d8, v)));
+  Lanes(d);
+  return shifted;
 }
 
 // ------------------------------ InterleaveLower
 
-template <class V>
-HWY_API V InterleaveLower(const V a, const V b) {
-  const DFromV<V> d;
+// TODO(janwas): PromoteTo(LowerHalf), slide1up, add
+template <class D, class V>
+HWY_API V InterleaveLower(D d, const V a, const V b) {
+  static_assert(IsSame<TFromD<D>, TFromV<V>>(), "D/V mismatch");
   const RebindToUnsigned<decltype(d)> du;
-  constexpr size_t kLanesPerBlock = detail::LanesPerBlock(d);
-  const auto i = detail::Iota0(d);
-  const auto idx_mod = ShiftRight<1>(detail::And(i, kLanesPerBlock - 1));
+  constexpr size_t kLanesPerBlock = detail::LanesPerBlock(du);
+  const auto i = detail::Iota0(du);
+  const auto idx_mod = ShiftRight<1>(detail::AndS(i, kLanesPerBlock - 1));
   const auto idx = Add(idx_mod, detail::OffsetsOf128BitBlocks(d, i));
-  const auto is_even = Eq(detail::And(i, 1), Zero(du));
+  const auto is_even = Eq(detail::AndS(i, 1), Zero(du));
   return IfThenElse(is_even, TableLookupLanes(a, idx),
                     TableLookupLanes(b, idx));
 }
 
+template <class V>
+HWY_API V InterleaveLower(const V a, const V b) {
+  return InterleaveLower(DFromV<V>(), a, b);
+}
+
 // ------------------------------ InterleaveUpper
 
-template <class V>
-HWY_API V InterleaveUpper(const V a, const V b) {
-  const DFromV<V> d;
+template <class D, class V>
+HWY_API V InterleaveUpper(const D d, const V a, const V b) {
+  static_assert(IsSame<TFromD<D>, TFromV<V>>(), "D/V mismatch");
   const RebindToUnsigned<decltype(d)> du;
-  constexpr size_t kLanesPerBlock = detail::LanesPerBlock(d);
-  const auto i = detail::Iota0(d);
-  const auto idx_mod = ShiftRight<1>(detail::And(i, kLanesPerBlock - 1));
+  constexpr size_t kLanesPerBlock = detail::LanesPerBlock(du);
+  const auto i = detail::Iota0(du);
+  const auto idx_mod = ShiftRight<1>(detail::AndS(i, kLanesPerBlock - 1));
   const auto idx_lower = Add(idx_mod, detail::OffsetsOf128BitBlocks(d, i));
-  const auto idx = detail::Add(idx_lower, kLanesPerBlock / 2);
-  const auto is_even = Eq(detail::And(i, 1), Zero(du));
+  const auto idx = detail::AddS(idx_lower, kLanesPerBlock / 2);
+  const auto is_even = Eq(detail::AndS(i, 1), Zero(du));
   return IfThenElse(is_even, TableLookupLanes(a, idx),
                     TableLookupLanes(b, idx));
 }
 
 // ------------------------------ ZipLower
 
-template <class V>
-HWY_API VFromD<RepartitionToWide<DFromV<V>>> ZipLower(const V a, const V b) {
-  RepartitionToWide<DFromV<V>> dw;
-  return BitCast(dw, InterleaveLower(a, b));
+template <class V, class DW = RepartitionToWide<DFromV<V>>>
+HWY_API VFromD<DW> ZipLower(DW dw, V a, V b) {
+  const RepartitionToNarrow<DW> dn;
+  static_assert(IsSame<TFromD<decltype(dn)>, TFromV<V>>(), "D/V mismatch");
+  const auto zipped = BitCast(dw, InterleaveLower(dn, a, b));
+  Lanes(dw);
+  return zipped;
 }
-
-// ------------------------------ ZipUpper
-
-template <class V>
-HWY_API VFromD<RepartitionToWide<DFromV<V>>> ZipUpper(const V a, const V b) {
-  RepartitionToWide<DFromV<V>> dw;
-  return BitCast(dw, InterleaveUpper(a, b));
+template <class V, class DW = RepartitionToWide<DFromV<V>>>
+HWY_API VFromD<DW> ZipLower(const V a, const V b) {
+  return ZipLower(DW(), a, b);
 }
 
-// ------------------------------ Combine
-
-// TODO(janwas): implement after LMUL ext/trunc
-#if 0
-
-template <class V>
-HWY_API V Combine(const V a, const V b) {
-  using D = DFromV<V>;
-  // double LMUL of inputs, then SlideUp with Lanes().
+// ------------------------------ ZipUpper
+template <class DW, class V>
+HWY_API VFromD<DW> ZipUpper(DW dw, V a, V b) {
+  const RepartitionToNarrow<DW> dn;
+  static_assert(IsSame<TFromD<decltype(dn)>, TFromV<V>>(), "D/V mismatch");
+  const auto zipped = BitCast(dw, InterleaveUpper(dn, a, b));
+  Lanes(dw);
+  return zipped;
 }
 
-#endif
-
 // ================================================== REDUCE
 
 // vector = f(vector, zero_m1)
@@ -1530,10 +1629,9 @@ HWY_RVV_FOREACH_UI(HWY_RVV_REDUCE, RedSum, redsum)
 HWY_RVV_FOREACH_F(HWY_RVV_REDUCE, RedSum, fredsum)
 }  // namespace detail
 
-template <class V>
-HWY_API V SumOfLanes(const V v) {
-  using T = TFromV<V>;
-  const auto v0 = Zero(Full<T>());  // always m1
+template <class D>
+HWY_API VFromD<D> SumOfLanes(D /* d */, const VFromD<D> v) {
+  const auto v0 = Zero(Full<TFromD<D>>());  // always m1
   return detail::RedSum(v, v0);
 }
 
@@ -1544,9 +1642,9 @@ HWY_RVV_FOREACH_I(HWY_RVV_REDUCE, RedMin, redmin)
 HWY_RVV_FOREACH_F(HWY_RVV_REDUCE, RedMin, fredmin)
 }  // namespace detail
 
-template <class V>
-HWY_API V MinOfLanes(const V v) {
-  using T = TFromV<V>;
+template <class D>
+HWY_API VFromD<D> MinOfLanes(D /* d */, const VFromD<D> v) {
+  using T = TFromD<D>;
   const Full<T> d1;  // always m1
   const auto neutral = Set(d1, HighestValue<T>());
   return detail::RedMin(v, neutral);
@@ -1559,9 +1657,9 @@ HWY_RVV_FOREACH_I(HWY_RVV_REDUCE, RedMax, redmax)
 HWY_RVV_FOREACH_F(HWY_RVV_REDUCE, RedMax, fredmax)
 }  // namespace detail
 
-template <class V>
-HWY_API V MaxOfLanes(const V v) {
-  using T = TFromV<V>;
+template <class D>
+HWY_API VFromD<D> MaxOfLanes(D /* d */, const VFromD<D> v) {
+  using T = TFromD<D>;
   const Full<T> d1;  // always m1
   const auto neutral = Set(d1, LowestValue<T>());
   return detail::RedMax(v, neutral);
@@ -1579,21 +1677,26 @@ HWY_API VFromD<D> LoadDup128(D d, const TFromD<D>* const HWY_RESTRICT p) {
   const auto loaded = Load(d, p);
   constexpr size_t kLanesPerBlock = detail::LanesPerBlock(d);
   // Broadcast the first block
-  const auto idx = detail::And(detail::Iota0(d), kLanesPerBlock - 1);
+  const auto idx = detail::AndS(detail::Iota0(d), kLanesPerBlock - 1);
   return TableLookupLanes(loaded, idx);
 }
 
 // ------------------------------ StoreMaskBits
-#define HWY_RVV_STORE_MASK_BITS(MLEN, NAME, OP)                 \
-  HWY_API size_t StoreMaskBits(HWY_RVV_M(MLEN) m, uint8_t* p) { \
-    /* LMUL=1 is always enough */                               \
-    Full<uint8_t> d8;                                           \
-    const size_t num_bytes = (Lanes(d8) + MLEN - 1) / MLEN;     \
-    /* TODO(janwas): how to convert vbool* to vuint?*/          \
-    /*Store(m, d8, p);*/                                        \
-    (void)m;                                                    \
-    (void)p;                                                    \
-    return num_bytes;                                           \
+#define HWY_RVV_STORE_MASK_BITS(MLEN, NAME, OP)                              \
+  /* DEPRECATED */                                                           \
+  HWY_API size_t StoreMaskBits(HWY_RVV_M(MLEN) m, uint8_t* p) {              \
+    /* LMUL=1 is always enough */                                            \
+    Full<uint8_t> d8;                                                        \
+    const size_t num_bytes = (Lanes(d8) + MLEN - 1) / MLEN;                  \
+    /* TODO(janwas): how to convert vbool* to vuint?*/                       \
+    /*Store(m, d8, p);*/                                                     \
+    (void)m;                                                                 \
+    (void)p;                                                                 \
+    return num_bytes;                                                        \
+  }                                                                          \
+  template <class D>                                                         \
+  HWY_API size_t StoreMaskBits(D /* tag */, HWY_RVV_M(MLEN) m, uint8_t* p) { \
+    return StoreMaskBits(m, p);                                              \
   }
 HWY_RVV_FOREACH_B(HWY_RVV_STORE_MASK_BITS, _, _)
 #undef HWY_RVV_STORE_MASK_BITS
@@ -1614,7 +1717,7 @@ HWY_API MFromD<D> FirstN(const D d, const size_t n) {
   return Eq(detail::SlideUp(one, zero, n), one);
 }
 
-// ------------------------------ Neg
+// ------------------------------ Neg (Sub)
 
 template <class V, HWY_IF_SIGNED_V(V)>
 HWY_API V Neg(const V v) {
@@ -1629,7 +1732,7 @@ HWY_API V Neg(const V v) {
 
 HWY_RVV_FOREACH_F(HWY_RVV_RETV_ARGV2, Neg, fsgnjn)
 
-// ------------------------------ Abs
+// ------------------------------ Abs (Max, Neg)
 
 template <class V, HWY_IF_SIGNED_V(V)>
 HWY_API V Abs(const V v) {
@@ -1640,14 +1743,13 @@ HWY_RVV_FOREACH_F(HWY_RVV_RETV_ARGV2, Abs, fsgnjx)
 
 #undef HWY_RVV_RETV_ARGV2
 
-// ------------------------------ AbsDiff
-
+// ------------------------------ AbsDiff (Abs, Sub)
 template <class V>
 HWY_API V AbsDiff(const V a, const V b) {
   return Abs(Sub(a, b));
 }
 
-// ------------------------------ Round
+// ------------------------------ Round  (NearestInt, ConvertTo, CopySign)
 
 // IEEE-754 roundToIntegralTiesToEven returns floating-point, but we do not have
 // a dedicated instruction for that. Rounding to integer and converting back to
@@ -1658,9 +1760,10 @@ namespace detail {
 enum RoundingModes { kNear, kTrunc, kDown, kUp };
 
 template <class V>
-HWY_API auto UseInt(const V v) -> decltype(MaskFromVec(v)) {
+HWY_INLINE auto UseInt(const V v) -> decltype(MaskFromVec(v)) {
   return Lt(Abs(v), Set(DFromV<V>(), MantissaEnd<TFromV<V>>()));
 }
+
 }  // namespace detail
 
 template <class V>
@@ -1673,8 +1776,7 @@ HWY_API V Round(const V v) {
   return IfThenElse(detail::UseInt(v), CopySign(int_f, v), v);
 }
 
-// ------------------------------ Trunc
-
+// ------------------------------ Trunc (ConvertTo)
 template <class V>
 HWY_API V Trunc(const V v) {
   const DFromV<V> df;
@@ -1687,7 +1789,6 @@ HWY_API V Trunc(const V v) {
 }
 
 // ------------------------------ Ceil
-
 template <class V>
 HWY_API V Ceil(const V v) {
   asm volatile("fsrm %0" ::"r"(detail::kUp));
@@ -1697,7 +1798,6 @@ HWY_API V Ceil(const V v) {
 }
 
 // ------------------------------ Floor
-
 template <class V>
 HWY_API V Floor(const V v) {
   asm volatile("fsrm %0" ::"r"(detail::kDown));
@@ -1706,7 +1806,7 @@ HWY_API V Floor(const V v) {
   return ret;
 }
 
-// ------------------------------ Iota
+// ------------------------------ Iota (ConvertTo)
 
 template <class D, HWY_IF_UNSIGNED_D(D)>
 HWY_API VFromD<D> Iota(const D d, TFromD<D> first) {
@@ -1723,26 +1823,50 @@ template <class D, HWY_IF_FLOAT_D(D)>
 HWY_API VFromD<D> Iota(const D d, TFromD<D> first) {
   const RebindToUnsigned<D> du;
   const RebindToSigned<D> di;
-  return detail::Add(ConvertTo(d, BitCast(di, detail::Iota0(du))), first);
+  return detail::AddS(ConvertTo(d, BitCast(di, detail::Iota0(du))), first);
 }
 
-// ------------------------------ MulEven
+// ------------------------------ MulEven/Odd (Mul, OddEven)
 
-// Using vwmul does not work for m8, so use mulh instead. Highway only provides
-// MulHigh for 16-bit, so use a private wrapper.
 namespace detail {
-HWY_RVV_FOREACH_U32(HWY_RVV_RETV_ARGVV, MulHigh, mulhu)
-HWY_RVV_FOREACH_I32(HWY_RVV_RETV_ARGVV, MulHigh, mulh)
+// Special instruction for 1 lane is presumably faster?
+#define HWY_RVV_SLIDE1(BASE, CHAR, SEW, LMUL, SHIFT, MLEN, NAME, OP)      \
+  HWY_API HWY_RVV_V(BASE, SEW, LMUL) NAME(HWY_RVV_V(BASE, SEW, LMUL) v) { \
+    return v##OP##_vx_##CHAR##SEW##LMUL(v, 0);                            \
+  }
+
+HWY_RVV_FOREACH_UI32(HWY_RVV_SLIDE1, Slide1Up, slide1up)
+HWY_RVV_FOREACH_U64(HWY_RVV_SLIDE1, Slide1Up, slide1up)
+HWY_RVV_FOREACH_UI32(HWY_RVV_SLIDE1, Slide1Down, slide1down)
+HWY_RVV_FOREACH_U64(HWY_RVV_SLIDE1, Slide1Down, slide1down)
+#undef HWY_RVV_SLIDE1
 }  // namespace detail
 
-template <class V>
+template <class V, HWY_IF_LANE_SIZE_V(V, 4)>
 HWY_API VFromD<RepartitionToWide<DFromV<V>>> MulEven(const V a, const V b) {
   const DFromV<V> d;
   Lanes(d);
   const auto lo = Mul(a, b);
   const auto hi = detail::MulHigh(a, b);
   const RepartitionToWide<DFromV<V>> dw;
-  return BitCast(dw, OddEven(detail::SlideUp(hi, hi, 1), lo));
+  const auto wide = BitCast(dw, OddEven(detail::Slide1Up(hi), lo));
+  Lanes(dw);
+  return wide;
+}
+
+// There is no 64x64 vwmul.
+template <class V, HWY_IF_LANE_SIZE_V(V, 8)>
+HWY_INLINE V MulEven(const V a, const V b) {
+  const auto lo = detail::Mul(a, b);
+  const auto hi = detail::MulHigh(a, b);
+  return OddEven(detail::Slide1Up(hi), lo);
+}
+
+template <class V, HWY_IF_LANE_SIZE_V(V, 8)>
+HWY_INLINE V MulOdd(const V a, const V b) {
+  const auto lo = detail::Mul(a, b);
+  const auto hi = detail::MulHigh(a, b);
+  return OddEven(hi, detail::Slide1Down(lo));
 }
 
 // ================================================== END MACROS
diff --git a/third_party/highway/hwy/ops/scalar-inl.h b/third_party/highway/hwy/ops/scalar-inl.h
index a32d88692e6f4..1fa37c19b1613 100644
--- a/third_party/highway/hwy/ops/scalar-inl.h
+++ b/third_party/highway/hwy/ops/scalar-inl.h
@@ -18,8 +18,6 @@
 #include <stddef.h>
 #include <stdint.h>
 
-#include <algorithm>  // std::min
-
 #include "hwy/base.h"
 #include "hwy/ops/shared-inl.h"
 
@@ -79,10 +77,25 @@ class Mask1 {
   Raw bits;
 };
 
+namespace detail {
+
+// Deduce Sisd<T> from Vec1<T>
+struct Deduce1 {
+  template <typename T>
+  Sisd<T> operator()(Vec1<T>) const {
+    return Sisd<T>();
+  }
+};
+
+}  // namespace detail
+
+template <class V>
+using DFromV = decltype(detail::Deduce1()(V()));
+
 // ------------------------------ BitCast
 
 template <typename T, typename FromT>
-HWY_INLINE Vec1<T> BitCast(Sisd<T> /* tag */, Vec1<FromT> v) {
+HWY_API Vec1<T> BitCast(Sisd<T> /* tag */, Vec1<FromT> v) {
   static_assert(sizeof(T) <= sizeof(FromT), "Promoting is undefined");
   T to;
   CopyBytes<sizeof(FromT)>(&v.raw, &to);
@@ -92,22 +105,22 @@ HWY_INLINE Vec1<T> BitCast(Sisd<T> /* tag */, Vec1<FromT> v) {
 // ------------------------------ Set
 
 template <typename T>
-HWY_INLINE Vec1<T> Zero(Sisd<T> /* tag */) {
+HWY_API Vec1<T> Zero(Sisd<T> /* tag */) {
   return Vec1<T>(T(0));
 }
 
 template <typename T, typename T2>
-HWY_INLINE Vec1<T> Set(Sisd<T> /* tag */, const T2 t) {
+HWY_API Vec1<T> Set(Sisd<T> /* tag */, const T2 t) {
   return Vec1<T>(static_cast<T>(t));
 }
 
 template <typename T>
-HWY_INLINE Vec1<T> Undefined(Sisd<T> d) {
+HWY_API Vec1<T> Undefined(Sisd<T> d) {
   return Zero(d);
 }
 
 template <typename T, typename T2>
-Vec1<T> Iota(const Sisd<T> /* tag */, const T2 first) {
+HWY_API Vec1<T> Iota(const Sisd<T> /* tag */, const T2 first) {
   return Vec1<T>(static_cast<T>(first));
 }
 
@@ -116,7 +129,7 @@ Vec1<T> Iota(const Sisd<T> /* tag */, const T2 first) {
 // ------------------------------ Not
 
 template <typename T>
-HWY_INLINE Vec1<T> Not(const Vec1<T> v) {
+HWY_API Vec1<T> Not(const Vec1<T> v) {
   using TU = MakeUnsigned<T>;
   const Sisd<TU> du;
   return BitCast(Sisd<T>(), Vec1<TU>(~BitCast(du, v).raw));
@@ -125,20 +138,20 @@ HWY_INLINE Vec1<T> Not(const Vec1<T> v) {
 // ------------------------------ And
 
 template <typename T>
-HWY_INLINE Vec1<T> And(const Vec1<T> a, const Vec1<T> b) {
+HWY_API Vec1<T> And(const Vec1<T> a, const Vec1<T> b) {
   using TU = MakeUnsigned<T>;
   const Sisd<TU> du;
   return BitCast(Sisd<T>(), Vec1<TU>(BitCast(du, a).raw & BitCast(du, b).raw));
 }
 template <typename T>
-HWY_INLINE Vec1<T> operator&(const Vec1<T> a, const Vec1<T> b) {
+HWY_API Vec1<T> operator&(const Vec1<T> a, const Vec1<T> b) {
   return And(a, b);
 }
 
 // ------------------------------ AndNot
 
 template <typename T>
-HWY_INLINE Vec1<T> AndNot(const Vec1<T> a, const Vec1<T> b) {
+HWY_API Vec1<T> AndNot(const Vec1<T> a, const Vec1<T> b) {
   using TU = MakeUnsigned<T>;
   const Sisd<TU> du;
   return BitCast(Sisd<T>(), Vec1<TU>(~BitCast(du, a).raw & BitCast(du, b).raw));
@@ -147,26 +160,26 @@ HWY_INLINE Vec1<T> AndNot(const Vec1<T> a, const Vec1<T> b) {
 // ------------------------------ Or
 
 template <typename T>
-HWY_INLINE Vec1<T> Or(const Vec1<T> a, const Vec1<T> b) {
+HWY_API Vec1<T> Or(const Vec1<T> a, const Vec1<T> b) {
   using TU = MakeUnsigned<T>;
   const Sisd<TU> du;
   return BitCast(Sisd<T>(), Vec1<TU>(BitCast(du, a).raw | BitCast(du, b).raw));
 }
 template <typename T>
-HWY_INLINE Vec1<T> operator|(const Vec1<T> a, const Vec1<T> b) {
+HWY_API Vec1<T> operator|(const Vec1<T> a, const Vec1<T> b) {
   return Or(a, b);
 }
 
 // ------------------------------ Xor
 
 template <typename T>
-HWY_INLINE Vec1<T> Xor(const Vec1<T> a, const Vec1<T> b) {
+HWY_API Vec1<T> Xor(const Vec1<T> a, const Vec1<T> b) {
   using TU = MakeUnsigned<T>;
   const Sisd<TU> du;
   return BitCast(Sisd<T>(), Vec1<TU>(BitCast(du, a).raw ^ BitCast(du, b).raw));
 }
 template <typename T>
-HWY_INLINE Vec1<T> operator^(const Vec1<T> a, const Vec1<T> b) {
+HWY_API Vec1<T> operator^(const Vec1<T> a, const Vec1<T> b) {
   return Xor(a, b);
 }
 
@@ -193,6 +206,19 @@ HWY_API Vec1<T> BroadcastSignBit(const Vec1<T> v) {
   return v.raw < 0 ? Vec1<T>(T(-1)) : Vec1<T>(0);
 }
 
+// ------------------------------ PopulationCount
+
+#ifdef HWY_NATIVE_POPCNT
+#undef HWY_NATIVE_POPCNT
+#else
+#define HWY_NATIVE_POPCNT
+#endif
+
+template <typename T>
+HWY_API Vec1<T> PopulationCount(Vec1<T> v) {
+  return Vec1<T>(static_cast<T>(PopCount(v.raw)));
+}
+
 // ------------------------------ Mask
 
 template <typename TFrom, typename TTo>
@@ -203,7 +229,7 @@ HWY_API Mask1<TTo> RebindMask(Sisd<TTo> /*tag*/, Mask1<TFrom> m) {
 
 // v must be 0 or FF..FF.
 template <typename T>
-HWY_INLINE Mask1<T> MaskFromVec(const Vec1<T> v) {
+HWY_API Mask1<T> MaskFromVec(const Vec1<T> v) {
   Mask1<T> mask;
   CopyBytes<sizeof(mask.bits)>(&v.raw, &mask.bits);
   return mask;
@@ -224,29 +250,29 @@ Vec1<T> VecFromMask(Sisd<T> /* tag */, const Mask1<T> mask) {
 }
 
 template <typename T>
-HWY_INLINE Mask1<T> FirstN(Sisd<T> /*tag*/, size_t n) {
+HWY_API Mask1<T> FirstN(Sisd<T> /*tag*/, size_t n) {
   return Mask1<T>::FromBool(n != 0);
 }
 
 // Returns mask ? yes : no.
 template <typename T>
-HWY_INLINE Vec1<T> IfThenElse(const Mask1<T> mask, const Vec1<T> yes,
-                              const Vec1<T> no) {
+HWY_API Vec1<T> IfThenElse(const Mask1<T> mask, const Vec1<T> yes,
+                           const Vec1<T> no) {
   return mask.bits ? yes : no;
 }
 
 template <typename T>
-HWY_INLINE Vec1<T> IfThenElseZero(const Mask1<T> mask, const Vec1<T> yes) {
+HWY_API Vec1<T> IfThenElseZero(const Mask1<T> mask, const Vec1<T> yes) {
   return mask.bits ? yes : Vec1<T>(0);
 }
 
 template <typename T>
-HWY_INLINE Vec1<T> IfThenZeroElse(const Mask1<T> mask, const Vec1<T> no) {
+HWY_API Vec1<T> IfThenZeroElse(const Mask1<T> mask, const Vec1<T> no) {
   return mask.bits ? Vec1<T>(0) : no;
 }
 
 template <typename T>
-HWY_INLINE Vec1<T> ZeroIfNegative(const Vec1<T> v) {
+HWY_API Vec1<T> ZeroIfNegative(const Vec1<T> v) {
   return v.raw < 0 ? Vec1<T>(0) : v;
 }
 
@@ -254,8 +280,7 @@ HWY_INLINE Vec1<T> ZeroIfNegative(const Vec1<T> v) {
 
 template <typename T>
 HWY_API Mask1<T> Not(const Mask1<T> m) {
-  const Sisd<T> d;
-  return MaskFromVec(Not(VecFromMask(d, m)));
+  return MaskFromVec(Not(VecFromMask(Sisd<T>(), m)));
 }
 
 template <typename T>
@@ -287,13 +312,13 @@ HWY_API Mask1<T> Xor(const Mask1<T> a, Mask1<T> b) {
 // ------------------------------ ShiftLeft (BroadcastSignBit)
 
 template <int kBits, typename T>
-HWY_INLINE Vec1<T> ShiftLeft(const Vec1<T> v) {
+HWY_API Vec1<T> ShiftLeft(const Vec1<T> v) {
   static_assert(0 <= kBits && kBits < sizeof(T) * 8, "Invalid shift");
   return Vec1<T>(static_cast<hwy::MakeUnsigned<T>>(v.raw) << kBits);
 }
 
 template <int kBits, typename T>
-HWY_INLINE Vec1<T> ShiftRight(const Vec1<T> v) {
+HWY_API Vec1<T> ShiftRight(const Vec1<T> v) {
   static_assert(0 <= kBits && kBits < sizeof(T) * 8, "Invalid shift");
 #if __cplusplus >= 202002L
   // Signed right shift is now guaranteed to be arithmetic (rounding toward
@@ -318,12 +343,12 @@ HWY_INLINE Vec1<T> ShiftRight(const Vec1<T> v) {
 // ------------------------------ ShiftLeftSame (BroadcastSignBit)
 
 template <typename T>
-HWY_INLINE Vec1<T> ShiftLeftSame(const Vec1<T> v, int bits) {
+HWY_API Vec1<T> ShiftLeftSame(const Vec1<T> v, int bits) {
   return Vec1<T>(static_cast<hwy::MakeUnsigned<T>>(v.raw) << bits);
 }
 
 template <typename T>
-HWY_INLINE Vec1<T> ShiftRightSame(const Vec1<T> v, int bits) {
+HWY_API Vec1<T> ShiftRightSame(const Vec1<T> v, int bits) {
 #if __cplusplus >= 202002L
   // Signed right shift is now guaranteed to be arithmetic (rounding toward
   // negative infinity, i.e. shifting in the sign bit).
@@ -348,40 +373,40 @@ HWY_INLINE Vec1<T> ShiftRightSame(const Vec1<T> v, int bits) {
 
 // Single-lane => same as ShiftLeftSame except for the argument type.
 template <typename T>
-HWY_INLINE Vec1<T> operator<<(const Vec1<T> v, const Vec1<T> bits) {
+HWY_API Vec1<T> operator<<(const Vec1<T> v, const Vec1<T> bits) {
   return ShiftLeftSame(v, static_cast<int>(bits.raw));
 }
 
 template <typename T>
-HWY_INLINE Vec1<T> operator>>(const Vec1<T> v, const Vec1<T> bits) {
+HWY_API Vec1<T> operator>>(const Vec1<T> v, const Vec1<T> bits) {
   return ShiftRightSame(v, static_cast<int>(bits.raw));
 }
 
 // ================================================== ARITHMETIC
 
 template <typename T>
-HWY_INLINE Vec1<T> operator+(Vec1<T> a, Vec1<T> b) {
+HWY_API Vec1<T> operator+(Vec1<T> a, Vec1<T> b) {
   const uint64_t a64 = static_cast<uint64_t>(a.raw);
   const uint64_t b64 = static_cast<uint64_t>(b.raw);
   return Vec1<T>(static_cast<T>((a64 + b64) & static_cast<uint64_t>(~T(0))));
 }
-HWY_INLINE Vec1<float> operator+(const Vec1<float> a, const Vec1<float> b) {
+HWY_API Vec1<float> operator+(const Vec1<float> a, const Vec1<float> b) {
   return Vec1<float>(a.raw + b.raw);
 }
-HWY_INLINE Vec1<double> operator+(const Vec1<double> a, const Vec1<double> b) {
+HWY_API Vec1<double> operator+(const Vec1<double> a, const Vec1<double> b) {
   return Vec1<double>(a.raw + b.raw);
 }
 
 template <typename T>
-HWY_INLINE Vec1<T> operator-(Vec1<T> a, Vec1<T> b) {
+HWY_API Vec1<T> operator-(Vec1<T> a, Vec1<T> b) {
   const uint64_t a64 = static_cast<uint64_t>(a.raw);
   const uint64_t b64 = static_cast<uint64_t>(b.raw);
   return Vec1<T>(static_cast<T>((a64 - b64) & static_cast<uint64_t>(~T(0))));
 }
-HWY_INLINE Vec1<float> operator-(const Vec1<float> a, const Vec1<float> b) {
+HWY_API Vec1<float> operator-(const Vec1<float> a, const Vec1<float> b) {
   return Vec1<float>(a.raw - b.raw);
 }
-HWY_INLINE Vec1<double> operator-(const Vec1<double> a, const Vec1<double> b) {
+HWY_API Vec1<double> operator-(const Vec1<double> a, const Vec1<double> b) {
   return Vec1<double>(a.raw - b.raw);
 }
 
@@ -390,25 +415,24 @@ HWY_INLINE Vec1<double> operator-(const Vec1<double> a, const Vec1<double> b) {
 // Returns a + b clamped to the destination range.
 
 // Unsigned
-HWY_INLINE Vec1<uint8_t> SaturatedAdd(const Vec1<uint8_t> a,
-                                      const Vec1<uint8_t> b) {
+HWY_API Vec1<uint8_t> SaturatedAdd(const Vec1<uint8_t> a,
+                                   const Vec1<uint8_t> b) {
   return Vec1<uint8_t>(
       static_cast<uint8_t>(HWY_MIN(HWY_MAX(0, a.raw + b.raw), 255)));
 }
-HWY_INLINE Vec1<uint16_t> SaturatedAdd(const Vec1<uint16_t> a,
-                                       const Vec1<uint16_t> b) {
+HWY_API Vec1<uint16_t> SaturatedAdd(const Vec1<uint16_t> a,
+                                    const Vec1<uint16_t> b) {
   return Vec1<uint16_t>(
       static_cast<uint16_t>(HWY_MIN(HWY_MAX(0, a.raw + b.raw), 65535)));
 }
 
 // Signed
-HWY_INLINE Vec1<int8_t> SaturatedAdd(const Vec1<int8_t> a,
-                                     const Vec1<int8_t> b) {
+HWY_API Vec1<int8_t> SaturatedAdd(const Vec1<int8_t> a, const Vec1<int8_t> b) {
   return Vec1<int8_t>(
       static_cast<int8_t>(HWY_MIN(HWY_MAX(-128, a.raw + b.raw), 127)));
 }
-HWY_INLINE Vec1<int16_t> SaturatedAdd(const Vec1<int16_t> a,
-                                      const Vec1<int16_t> b) {
+HWY_API Vec1<int16_t> SaturatedAdd(const Vec1<int16_t> a,
+                                   const Vec1<int16_t> b) {
   return Vec1<int16_t>(
       static_cast<int16_t>(HWY_MIN(HWY_MAX(-32768, a.raw + b.raw), 32767)));
 }
@@ -418,25 +442,24 @@ HWY_INLINE Vec1<int16_t> SaturatedAdd(const Vec1<int16_t> a,
 // Returns a - b clamped to the destination range.
 
 // Unsigned
-HWY_INLINE Vec1<uint8_t> SaturatedSub(const Vec1<uint8_t> a,
-                                      const Vec1<uint8_t> b) {
+HWY_API Vec1<uint8_t> SaturatedSub(const Vec1<uint8_t> a,
+                                   const Vec1<uint8_t> b) {
   return Vec1<uint8_t>(
       static_cast<uint8_t>(HWY_MIN(HWY_MAX(0, a.raw - b.raw), 255)));
 }
-HWY_INLINE Vec1<uint16_t> SaturatedSub(const Vec1<uint16_t> a,
-                                       const Vec1<uint16_t> b) {
+HWY_API Vec1<uint16_t> SaturatedSub(const Vec1<uint16_t> a,
+                                    const Vec1<uint16_t> b) {
   return Vec1<uint16_t>(
       static_cast<uint16_t>(HWY_MIN(HWY_MAX(0, a.raw - b.raw), 65535)));
 }
 
 // Signed
-HWY_INLINE Vec1<int8_t> SaturatedSub(const Vec1<int8_t> a,
-                                     const Vec1<int8_t> b) {
+HWY_API Vec1<int8_t> SaturatedSub(const Vec1<int8_t> a, const Vec1<int8_t> b) {
   return Vec1<int8_t>(
       static_cast<int8_t>(HWY_MIN(HWY_MAX(-128, a.raw - b.raw), 127)));
 }
-HWY_INLINE Vec1<int16_t> SaturatedSub(const Vec1<int16_t> a,
-                                      const Vec1<int16_t> b) {
+HWY_API Vec1<int16_t> SaturatedSub(const Vec1<int16_t> a,
+                                   const Vec1<int16_t> b) {
   return Vec1<int16_t>(
       static_cast<int16_t>(HWY_MIN(HWY_MAX(-32768, a.raw - b.raw), 32767)));
 }
@@ -445,50 +468,50 @@ HWY_INLINE Vec1<int16_t> SaturatedSub(const Vec1<int16_t> a,
 
 // Returns (a + b + 1) / 2
 
-HWY_INLINE Vec1<uint8_t> AverageRound(const Vec1<uint8_t> a,
-                                      const Vec1<uint8_t> b) {
+HWY_API Vec1<uint8_t> AverageRound(const Vec1<uint8_t> a,
+                                   const Vec1<uint8_t> b) {
   return Vec1<uint8_t>(static_cast<uint8_t>((a.raw + b.raw + 1) / 2));
 }
-HWY_INLINE Vec1<uint16_t> AverageRound(const Vec1<uint16_t> a,
-                                       const Vec1<uint16_t> b) {
+HWY_API Vec1<uint16_t> AverageRound(const Vec1<uint16_t> a,
+                                    const Vec1<uint16_t> b) {
   return Vec1<uint16_t>(static_cast<uint16_t>((a.raw + b.raw + 1) / 2));
 }
 
 // ------------------------------ Absolute value
 
 template <typename T>
-HWY_INLINE Vec1<T> Abs(const Vec1<T> a) {
+HWY_API Vec1<T> Abs(const Vec1<T> a) {
   const T i = a.raw;
   return (i >= 0 || i == hwy::LimitsMin<T>()) ? a : Vec1<T>(-i);
 }
-HWY_INLINE Vec1<float> Abs(const Vec1<float> a) {
+HWY_API Vec1<float> Abs(const Vec1<float> a) {
   return Vec1<float>(std::abs(a.raw));
 }
-HWY_INLINE Vec1<double> Abs(const Vec1<double> a) {
+HWY_API Vec1<double> Abs(const Vec1<double> a) {
   return Vec1<double>(std::abs(a.raw));
 }
 
 // ------------------------------ min/max
 
 template <typename T, HWY_IF_NOT_FLOAT(T)>
-HWY_INLINE Vec1<T> Min(const Vec1<T> a, const Vec1<T> b) {
+HWY_API Vec1<T> Min(const Vec1<T> a, const Vec1<T> b) {
   return Vec1<T>(HWY_MIN(a.raw, b.raw));
 }
 
 template <typename T, HWY_IF_FLOAT(T)>
-HWY_INLINE Vec1<T> Min(const Vec1<T> a, const Vec1<T> b) {
+HWY_API Vec1<T> Min(const Vec1<T> a, const Vec1<T> b) {
   if (std::isnan(a.raw)) return b;
   if (std::isnan(b.raw)) return a;
   return Vec1<T>(HWY_MIN(a.raw, b.raw));
 }
 
 template <typename T, HWY_IF_NOT_FLOAT(T)>
-HWY_INLINE Vec1<T> Max(const Vec1<T> a, const Vec1<T> b) {
+HWY_API Vec1<T> Max(const Vec1<T> a, const Vec1<T> b) {
   return Vec1<T>(HWY_MAX(a.raw, b.raw));
 }
 
 template <typename T, HWY_IF_FLOAT(T)>
-HWY_INLINE Vec1<T> Max(const Vec1<T> a, const Vec1<T> b) {
+HWY_API Vec1<T> Max(const Vec1<T> a, const Vec1<T> b) {
   if (std::isnan(a.raw)) return b;
   if (std::isnan(b.raw)) return a;
   return Vec1<T>(HWY_MAX(a.raw, b.raw));
@@ -497,19 +520,19 @@ HWY_INLINE Vec1<T> Max(const Vec1<T> a, const Vec1<T> b) {
 // ------------------------------ Floating-point negate
 
 template <typename T, HWY_IF_FLOAT(T)>
-HWY_INLINE Vec1<T> Neg(const Vec1<T> v) {
+HWY_API Vec1<T> Neg(const Vec1<T> v) {
   return Xor(v, SignBit(Sisd<T>()));
 }
 
 template <typename T, HWY_IF_NOT_FLOAT(T)>
-HWY_INLINE Vec1<T> Neg(const Vec1<T> v) {
+HWY_API Vec1<T> Neg(const Vec1<T> v) {
   return Zero(Sisd<T>()) - v;
 }
 
 // ------------------------------ mul/div
 
 template <typename T>
-HWY_INLINE Vec1<T> operator*(const Vec1<T> a, const Vec1<T> b) {
+HWY_API Vec1<T> operator*(const Vec1<T> a, const Vec1<T> b) {
   if (hwy::IsFloat<T>()) {
     return Vec1<T>(static_cast<T>(double(a.raw) * b.raw));
   } else if (hwy::IsSigned<T>()) {
@@ -520,16 +543,15 @@ HWY_INLINE Vec1<T> operator*(const Vec1<T> a, const Vec1<T> b) {
 }
 
 template <typename T>
-HWY_INLINE Vec1<T> operator/(const Vec1<T> a, const Vec1<T> b) {
+HWY_API Vec1<T> operator/(const Vec1<T> a, const Vec1<T> b) {
   return Vec1<T>(a.raw / b.raw);
 }
 
 // Returns the upper 16 bits of a * b in each lane.
-HWY_INLINE Vec1<int16_t> MulHigh(const Vec1<int16_t> a, const Vec1<int16_t> b) {
+HWY_API Vec1<int16_t> MulHigh(const Vec1<int16_t> a, const Vec1<int16_t> b) {
   return Vec1<int16_t>(static_cast<int16_t>((a.raw * b.raw) >> 16));
 }
-HWY_INLINE Vec1<uint16_t> MulHigh(const Vec1<uint16_t> a,
-                                  const Vec1<uint16_t> b) {
+HWY_API Vec1<uint16_t> MulHigh(const Vec1<uint16_t> a, const Vec1<uint16_t> b) {
   // Cast to uint32_t first to prevent overflow. Otherwise the result of
   // uint16_t * uint16_t is in "int" which may overflow. In practice the result
   // is the same but this way it is also defined.
@@ -538,18 +560,17 @@ HWY_INLINE Vec1<uint16_t> MulHigh(const Vec1<uint16_t> a,
 }
 
 // Multiplies even lanes (0, 2 ..) and returns the double-wide result.
-HWY_INLINE Vec1<int64_t> MulEven(const Vec1<int32_t> a, const Vec1<int32_t> b) {
+HWY_API Vec1<int64_t> MulEven(const Vec1<int32_t> a, const Vec1<int32_t> b) {
   const int64_t a64 = a.raw;
   return Vec1<int64_t>(a64 * b.raw);
 }
-HWY_INLINE Vec1<uint64_t> MulEven(const Vec1<uint32_t> a,
-                                  const Vec1<uint32_t> b) {
+HWY_API Vec1<uint64_t> MulEven(const Vec1<uint32_t> a, const Vec1<uint32_t> b) {
   const uint64_t a64 = a.raw;
   return Vec1<uint64_t>(a64 * b.raw);
 }
 
 // Approximate reciprocal
-HWY_INLINE Vec1<float> ApproximateReciprocal(const Vec1<float> v) {
+HWY_API Vec1<float> ApproximateReciprocal(const Vec1<float> v) {
   // Zero inputs are allowed, but callers are responsible for replacing the
   // return value with something else (typically using IfThenElse). This check
   // avoids a ubsan error. The return value is arbitrary.
@@ -558,40 +579,38 @@ HWY_INLINE Vec1<float> ApproximateReciprocal(const Vec1<float> v) {
 }
 
 // Absolute value of difference.
-HWY_INLINE Vec1<float> AbsDiff(const Vec1<float> a, const Vec1<float> b) {
+HWY_API Vec1<float> AbsDiff(const Vec1<float> a, const Vec1<float> b) {
   return Abs(a - b);
 }
 
 // ------------------------------ Floating-point multiply-add variants
 
 template <typename T>
-HWY_INLINE Vec1<T> MulAdd(const Vec1<T> mul, const Vec1<T> x,
-                          const Vec1<T> add) {
+HWY_API Vec1<T> MulAdd(const Vec1<T> mul, const Vec1<T> x, const Vec1<T> add) {
   return mul * x + add;
 }
 
 template <typename T>
-HWY_INLINE Vec1<T> NegMulAdd(const Vec1<T> mul, const Vec1<T> x,
-                             const Vec1<T> add) {
+HWY_API Vec1<T> NegMulAdd(const Vec1<T> mul, const Vec1<T> x,
+                          const Vec1<T> add) {
   return add - mul * x;
 }
 
 template <typename T>
-HWY_INLINE Vec1<T> MulSub(const Vec1<T> mul, const Vec1<T> x,
-                          const Vec1<T> sub) {
+HWY_API Vec1<T> MulSub(const Vec1<T> mul, const Vec1<T> x, const Vec1<T> sub) {
   return mul * x - sub;
 }
 
 template <typename T>
-HWY_INLINE Vec1<T> NegMulSub(const Vec1<T> mul, const Vec1<T> x,
-                             const Vec1<T> sub) {
+HWY_API Vec1<T> NegMulSub(const Vec1<T> mul, const Vec1<T> x,
+                          const Vec1<T> sub) {
   return Neg(mul) * x - sub;
 }
 
 // ------------------------------ Floating-point square root
 
 // Approximate reciprocal square root
-HWY_INLINE Vec1<float> ApproximateReciprocalSqrt(const Vec1<float> v) {
+HWY_API Vec1<float> ApproximateReciprocalSqrt(const Vec1<float> v) {
   float f = v.raw;
   const float half = f * 0.5f;
   uint32_t bits;
@@ -604,17 +623,17 @@ HWY_INLINE Vec1<float> ApproximateReciprocalSqrt(const Vec1<float> v) {
 }
 
 // Square root
-HWY_INLINE Vec1<float> Sqrt(const Vec1<float> v) {
+HWY_API Vec1<float> Sqrt(const Vec1<float> v) {
   return Vec1<float>(std::sqrt(v.raw));
 }
-HWY_INLINE Vec1<double> Sqrt(const Vec1<double> v) {
+HWY_API Vec1<double> Sqrt(const Vec1<double> v) {
   return Vec1<double>(std::sqrt(v.raw));
 }
 
 // ------------------------------ Floating-point rounding
 
 template <typename T>
-HWY_INLINE Vec1<T> Round(const Vec1<T> v) {
+HWY_API Vec1<T> Round(const Vec1<T> v) {
   using TI = MakeSigned<T>;
   if (!(Abs(v).raw < MantissaEnd<T>())) {  // Huge or NaN
     return v;
@@ -630,7 +649,7 @@ HWY_INLINE Vec1<T> Round(const Vec1<T> v) {
 }
 
 // Round-to-nearest even.
-HWY_INLINE Vec1<int32_t> NearestInt(const Vec1<float> v) {
+HWY_API Vec1<int32_t> NearestInt(const Vec1<float> v) {
   using T = float;
   using TI = int32_t;
 
@@ -655,7 +674,7 @@ HWY_INLINE Vec1<int32_t> NearestInt(const Vec1<float> v) {
 }
 
 template <typename T>
-HWY_INLINE Vec1<T> Trunc(const Vec1<T> v) {
+HWY_API Vec1<T> Trunc(const Vec1<T> v) {
   using TI = MakeSigned<T>;
   if (!(Abs(v).raw <= MantissaEnd<T>())) {  // Huge or NaN
     return v;
@@ -730,49 +749,54 @@ V Floor(const V v) {
 }
 
 // Toward +infinity, aka ceiling
-HWY_INLINE Vec1<float> Ceil(const Vec1<float> v) {
+HWY_API Vec1<float> Ceil(const Vec1<float> v) {
   return Ceiling<float, uint32_t, 23, 8>(v);
 }
-HWY_INLINE Vec1<double> Ceil(const Vec1<double> v) {
+HWY_API Vec1<double> Ceil(const Vec1<double> v) {
   return Ceiling<double, uint64_t, 52, 11>(v);
 }
 
 // Toward -infinity, aka floor
-HWY_INLINE Vec1<float> Floor(const Vec1<float> v) {
+HWY_API Vec1<float> Floor(const Vec1<float> v) {
   return Floor<float, uint32_t, 23, 8>(v);
 }
-HWY_INLINE Vec1<double> Floor(const Vec1<double> v) {
+HWY_API Vec1<double> Floor(const Vec1<double> v) {
   return Floor<double, uint64_t, 52, 11>(v);
 }
 
 // ================================================== COMPARE
 
 template <typename T>
-HWY_INLINE Mask1<T> operator==(const Vec1<T> a, const Vec1<T> b) {
+HWY_API Mask1<T> operator==(const Vec1<T> a, const Vec1<T> b) {
   return Mask1<T>::FromBool(a.raw == b.raw);
 }
 
 template <typename T>
-HWY_INLINE Mask1<T> TestBit(const Vec1<T> v, const Vec1<T> bit) {
+HWY_API Mask1<T> operator!=(const Vec1<T> a, const Vec1<T> b) {
+  return Mask1<T>::FromBool(a.raw != b.raw);
+}
+
+template <typename T>
+HWY_API Mask1<T> TestBit(const Vec1<T> v, const Vec1<T> bit) {
   static_assert(!hwy::IsFloat<T>(), "Only integer vectors supported");
   return (v & bit) == bit;
 }
 
 template <typename T>
-HWY_INLINE Mask1<T> operator<(const Vec1<T> a, const Vec1<T> b) {
+HWY_API Mask1<T> operator<(const Vec1<T> a, const Vec1<T> b) {
   return Mask1<T>::FromBool(a.raw < b.raw);
 }
 template <typename T>
-HWY_INLINE Mask1<T> operator>(const Vec1<T> a, const Vec1<T> b) {
+HWY_API Mask1<T> operator>(const Vec1<T> a, const Vec1<T> b) {
   return Mask1<T>::FromBool(a.raw > b.raw);
 }
 
 template <typename T>
-HWY_INLINE Mask1<T> operator<=(const Vec1<T> a, const Vec1<T> b) {
+HWY_API Mask1<T> operator<=(const Vec1<T> a, const Vec1<T> b) {
   return Mask1<T>::FromBool(a.raw <= b.raw);
 }
 template <typename T>
-HWY_INLINE Mask1<T> operator>=(const Vec1<T> a, const Vec1<T> b) {
+HWY_API Mask1<T> operator>=(const Vec1<T> a, const Vec1<T> b) {
   return Mask1<T>::FromBool(a.raw >= b.raw);
 }
 
@@ -781,33 +805,33 @@ HWY_INLINE Mask1<T> operator>=(const Vec1<T> a, const Vec1<T> b) {
 // ------------------------------ Load
 
 template <typename T>
-HWY_INLINE Vec1<T> Load(Sisd<T> /* tag */, const T* HWY_RESTRICT aligned) {
+HWY_API Vec1<T> Load(Sisd<T> /* tag */, const T* HWY_RESTRICT aligned) {
   T t;
   CopyBytes<sizeof(T)>(aligned, &t);
   return Vec1<T>(t);
 }
 
 template <typename T>
-HWY_INLINE Vec1<T> LoadU(Sisd<T> d, const T* HWY_RESTRICT p) {
+HWY_API Vec1<T> LoadU(Sisd<T> d, const T* HWY_RESTRICT p) {
   return Load(d, p);
 }
 
 // In some use cases, "load single lane" is sufficient; otherwise avoid this.
 template <typename T>
-HWY_INLINE Vec1<T> LoadDup128(Sisd<T> d, const T* HWY_RESTRICT aligned) {
+HWY_API Vec1<T> LoadDup128(Sisd<T> d, const T* HWY_RESTRICT aligned) {
   return Load(d, aligned);
 }
 
 // ------------------------------ Store
 
 template <typename T>
-HWY_INLINE void Store(const Vec1<T> v, Sisd<T> /* tag */,
-                      T* HWY_RESTRICT aligned) {
+HWY_API void Store(const Vec1<T> v, Sisd<T> /* tag */,
+                   T* HWY_RESTRICT aligned) {
   CopyBytes<sizeof(T)>(&v.raw, aligned);
 }
 
 template <typename T>
-HWY_INLINE void StoreU(const Vec1<T> v, Sisd<T> d, T* HWY_RESTRICT p) {
+HWY_API void StoreU(const Vec1<T> v, Sisd<T> d, T* HWY_RESTRICT p) {
   return Store(v, d, p);
 }
 
@@ -834,23 +858,23 @@ HWY_API void StoreInterleaved4(const Vec1<uint8_t> v0, const Vec1<uint8_t> v1,
 // ------------------------------ Stream
 
 template <typename T>
-HWY_INLINE void Stream(const Vec1<T> v, Sisd<T> d, T* HWY_RESTRICT aligned) {
+HWY_API void Stream(const Vec1<T> v, Sisd<T> d, T* HWY_RESTRICT aligned) {
   return Store(v, d, aligned);
 }
 
 // ------------------------------ Scatter
 
 template <typename T, typename Offset>
-HWY_INLINE void ScatterOffset(Vec1<T> v, Sisd<T> d, T* base,
-                              const Vec1<Offset> offset) {
+HWY_API void ScatterOffset(Vec1<T> v, Sisd<T> d, T* base,
+                           const Vec1<Offset> offset) {
   static_assert(sizeof(T) == sizeof(Offset), "Must match for portability");
   uint8_t* const base8 = reinterpret_cast<uint8_t*>(base) + offset.raw;
   return Store(v, d, reinterpret_cast<T*>(base8));
 }
 
 template <typename T, typename Index>
-HWY_INLINE void ScatterIndex(Vec1<T> v, Sisd<T> d, T* HWY_RESTRICT base,
-                             const Vec1<Index> index) {
+HWY_API void ScatterIndex(Vec1<T> v, Sisd<T> d, T* HWY_RESTRICT base,
+                          const Vec1<Index> index) {
   static_assert(sizeof(T) == sizeof(Index), "Must match for portability");
   return Store(v, d, base + index.raw);
 }
@@ -858,16 +882,16 @@ HWY_INLINE void ScatterIndex(Vec1<T> v, Sisd<T> d, T* HWY_RESTRICT base,
 // ------------------------------ Gather
 
 template <typename T, typename Offset>
-HWY_INLINE Vec1<T> GatherOffset(Sisd<T> d, const T* base,
-                                const Vec1<Offset> offset) {
+HWY_API Vec1<T> GatherOffset(Sisd<T> d, const T* base,
+                             const Vec1<Offset> offset) {
   static_assert(sizeof(T) == sizeof(Offset), "Must match for portability");
   const uintptr_t addr = reinterpret_cast<uintptr_t>(base) + offset.raw;
   return Load(d, reinterpret_cast<const T*>(addr));
 }
 
 template <typename T, typename Index>
-HWY_INLINE Vec1<T> GatherIndex(Sisd<T> d, const T* HWY_RESTRICT base,
-                               const Vec1<Index> index) {
+HWY_API Vec1<T> GatherIndex(Sisd<T> d, const T* HWY_RESTRICT base,
+                            const Vec1<Index> index) {
   static_assert(sizeof(T) == sizeof(Index), "Must match for portability");
   return Load(d, base + index.raw);
 }
@@ -878,14 +902,14 @@ HWY_INLINE Vec1<T> GatherIndex(Sisd<T> d, const T* HWY_RESTRICT base,
 // (rounding toward zero).
 
 template <typename FromT, typename ToT>
-HWY_INLINE Vec1<ToT> PromoteTo(Sisd<ToT> /* tag */, Vec1<FromT> from) {
+HWY_API Vec1<ToT> PromoteTo(Sisd<ToT> /* tag */, Vec1<FromT> from) {
   static_assert(sizeof(ToT) > sizeof(FromT), "Not promoting");
   // For bits Y > X, floatX->floatY and intX->intY are always representable.
   return Vec1<ToT>(static_cast<ToT>(from.raw));
 }
 
 template <typename FromT, typename ToT, HWY_IF_FLOAT(FromT)>
-HWY_INLINE Vec1<ToT> DemoteTo(Sisd<ToT> /* tag */, Vec1<FromT> from) {
+HWY_API Vec1<ToT> DemoteTo(Sisd<ToT> /* tag */, Vec1<FromT> from) {
   static_assert(sizeof(ToT) < sizeof(FromT), "Not demoting");
 
   // Prevent ubsan errors when converting float to narrower integer/float
@@ -898,17 +922,15 @@ HWY_INLINE Vec1<ToT> DemoteTo(Sisd<ToT> /* tag */, Vec1<FromT> from) {
 }
 
 template <typename FromT, typename ToT, HWY_IF_NOT_FLOAT(FromT)>
-HWY_INLINE Vec1<ToT> DemoteTo(Sisd<ToT> /* tag */, Vec1<FromT> from) {
+HWY_API Vec1<ToT> DemoteTo(Sisd<ToT> /* tag */, Vec1<FromT> from) {
   static_assert(sizeof(ToT) < sizeof(FromT), "Not demoting");
 
   // Int to int: choose closest value in ToT to `from` (avoids UB)
-  from.raw = std::min<FromT>(std::max<FromT>(LimitsMin<ToT>(), from.raw),
-                             LimitsMax<ToT>());
+  from.raw = HWY_MIN(HWY_MAX(LimitsMin<ToT>(), from.raw), LimitsMax<ToT>());
   return Vec1<ToT>(static_cast<ToT>(from.raw));
 }
 
-static HWY_INLINE Vec1<float> PromoteTo(Sisd<float> /* tag */,
-                                        const Vec1<float16_t> v) {
+HWY_API Vec1<float> PromoteTo(Sisd<float> /* tag */, const Vec1<float16_t> v) {
 #if HWY_NATIVE_FLOAT16
   uint16_t bits16;
   CopyBytes<2>(&v.raw, &bits16);
@@ -935,8 +957,8 @@ static HWY_INLINE Vec1<float> PromoteTo(Sisd<float> /* tag */,
   return Vec1<float>(out);
 }
 
-static HWY_INLINE Vec1<float16_t> DemoteTo(Sisd<float16_t> /* tag */,
-                                           const Vec1<float> v) {
+HWY_API Vec1<float16_t> DemoteTo(Sisd<float16_t> /* tag */,
+                                 const Vec1<float> v) {
   uint32_t bits32;
   CopyBytes<4>(&v.raw, &bits32);
   const uint32_t sign = bits32 >> 31;
@@ -985,7 +1007,7 @@ static HWY_INLINE Vec1<float16_t> DemoteTo(Sisd<float16_t> /* tag */,
 }
 
 template <typename FromT, typename ToT, HWY_IF_FLOAT(FromT)>
-HWY_INLINE Vec1<ToT> ConvertTo(Sisd<ToT> /* tag */, Vec1<FromT> from) {
+HWY_API Vec1<ToT> ConvertTo(Sisd<ToT> /* tag */, Vec1<FromT> from) {
   static_assert(sizeof(ToT) == sizeof(FromT), "Should have same size");
   // float## -> int##: return closest representable value. We cannot exactly
   // represent LimitsMax<ToT> in FromT, so use double.
@@ -999,35 +1021,65 @@ HWY_INLINE Vec1<ToT> ConvertTo(Sisd<ToT> /* tag */, Vec1<FromT> from) {
 }
 
 template <typename FromT, typename ToT, HWY_IF_NOT_FLOAT(FromT)>
-HWY_INLINE Vec1<ToT> ConvertTo(Sisd<ToT> /* tag */, Vec1<FromT> from) {
+HWY_API Vec1<ToT> ConvertTo(Sisd<ToT> /* tag */, Vec1<FromT> from) {
   static_assert(sizeof(ToT) == sizeof(FromT), "Should have same size");
   // int## -> float##: no check needed
   return Vec1<ToT>(static_cast<ToT>(from.raw));
 }
 
-HWY_INLINE Vec1<uint8_t> U8FromU32(const Vec1<uint32_t> v) {
+HWY_API Vec1<uint8_t> U8FromU32(const Vec1<uint32_t> v) {
   return DemoteTo(Sisd<uint8_t>(), v);
 }
 
-// ================================================== SWIZZLE
+// ================================================== COMBINE
+// UpperHalf, ZeroExtendVector, Combine, Concat* are unsupported.
 
-// Unsupported: Shift*Bytes, CombineShiftRightBytes, Interleave*, Shuffle*,
-// UpperHalf - these require more than one lane and/or actual 128-bit vectors.
+template <typename T>
+HWY_API Vec1<T> LowerHalf(Vec1<T> v) {
+  return v;
+}
+
+template <typename T>
+HWY_API Vec1<T> LowerHalf(Sisd<T> /* tag */, Vec1<T> v) {
+  return v;
+}
+
+// ================================================== SWIZZLE
+// OddEven is unsupported.
 
 template <typename T>
-HWY_INLINE T GetLane(const Vec1<T> v) {
+HWY_API T GetLane(const Vec1<T> v) {
   return v.raw;
 }
 
+// ------------------------------ TableLookupLanes
+
+// Returned by SetTableIndices for use by TableLookupLanes.
+template <typename T>
+struct Indices1 {
+  int raw;
+};
+
+template <typename T>
+HWY_API Indices1<T> SetTableIndices(Sisd<T>, const int32_t* idx) {
+#if !defined(NDEBUG) || defined(ADDRESS_SANITIZER)
+  HWY_DASSERT(idx[0] == 0);
+#endif
+  return Indices1<T>{idx[0]};
+}
+
 template <typename T>
-HWY_INLINE Vec1<T> LowerHalf(Vec1<T> v) {
+HWY_API Vec1<T> TableLookupLanes(const Vec1<T> v, const Indices1<T> /* idx */) {
   return v;
 }
 
+// ================================================== BLOCKWISE
+// Shift*Bytes, CombineShiftRightBytes, Interleave*, Shuffle* are unsupported.
+
 // ------------------------------ Broadcast/splat any lane
 
 template <int kLane, typename T>
-HWY_INLINE Vec1<T> Broadcast(const Vec1<T> v) {
+HWY_API Vec1<T> Broadcast(const Vec1<T> v) {
   static_assert(kLane == 0, "Scalar only has one lane");
   return v;
 }
@@ -1051,75 +1103,77 @@ HWY_API Vec1<T> TableLookupBytes(const Vec1<T> in, const Vec1<T> from) {
   return Vec1<T>{out};
 }
 
-// ------------------------------ TableLookupLanes
-
-// Returned by SetTableIndices for use by TableLookupLanes.
 template <typename T>
-struct Indices1 {
-  int raw;
-};
-
-template <typename T>
-HWY_API Indices1<T> SetTableIndices(Sisd<T>, const int32_t* idx) {
-#if !defined(NDEBUG) || defined(ADDRESS_SANITIZER)
-  HWY_DASSERT(idx[0] == 0);
-#endif
-  return Indices1<T>{idx[0]};
-}
-
-template <typename T>
-HWY_API Vec1<T> TableLookupLanes(const Vec1<T> v, const Indices1<T> /* idx */) {
-  return v;
+HWY_API Vec1<T> TableLookupBytesOr0(const Vec1<T> in, const Vec1<T> from) {
+  uint8_t in_bytes[sizeof(T)];
+  uint8_t from_bytes[sizeof(T)];
+  uint8_t out_bytes[sizeof(T)];
+  CopyBytes<sizeof(T)>(&in, &in_bytes);
+  CopyBytes<sizeof(T)>(&from, &from_bytes);
+  for (size_t i = 0; i < sizeof(T); ++i) {
+    out_bytes[i] = from_bytes[i] & 0x80 ? 0 : in_bytes[from_bytes[i]];
+  }
+  T out;
+  CopyBytes<sizeof(T)>(&out_bytes, &out);
+  return Vec1<T>{out};
 }
 
-// ------------------------------ Zip/unpack
+// ------------------------------ ZipLower
 
-HWY_INLINE Vec1<uint16_t> ZipLower(const Vec1<uint8_t> a,
-                                   const Vec1<uint8_t> b) {
+HWY_API Vec1<uint16_t> ZipLower(const Vec1<uint8_t> a, const Vec1<uint8_t> b) {
   return Vec1<uint16_t>(static_cast<uint16_t>((uint32_t(b.raw) << 8) + a.raw));
 }
-HWY_INLINE Vec1<uint32_t> ZipLower(const Vec1<uint16_t> a,
-                                   const Vec1<uint16_t> b) {
+HWY_API Vec1<uint32_t> ZipLower(const Vec1<uint16_t> a,
+                                const Vec1<uint16_t> b) {
   return Vec1<uint32_t>((uint32_t(b.raw) << 16) + a.raw);
 }
-HWY_INLINE Vec1<uint64_t> ZipLower(const Vec1<uint32_t> a,
-                                   const Vec1<uint32_t> b) {
+HWY_API Vec1<uint64_t> ZipLower(const Vec1<uint32_t> a,
+                                const Vec1<uint32_t> b) {
   return Vec1<uint64_t>((uint64_t(b.raw) << 32) + a.raw);
 }
-HWY_INLINE Vec1<int16_t> ZipLower(const Vec1<int8_t> a, const Vec1<int8_t> b) {
+HWY_API Vec1<int16_t> ZipLower(const Vec1<int8_t> a, const Vec1<int8_t> b) {
   return Vec1<int16_t>(static_cast<int16_t>((int32_t(b.raw) << 8) + a.raw));
 }
-HWY_INLINE Vec1<int32_t> ZipLower(const Vec1<int16_t> a,
-                                  const Vec1<int16_t> b) {
+HWY_API Vec1<int32_t> ZipLower(const Vec1<int16_t> a, const Vec1<int16_t> b) {
   return Vec1<int32_t>((int32_t(b.raw) << 16) + a.raw);
 }
-HWY_INLINE Vec1<int64_t> ZipLower(const Vec1<int32_t> a,
-                                  const Vec1<int32_t> b) {
+HWY_API Vec1<int64_t> ZipLower(const Vec1<int32_t> a, const Vec1<int32_t> b) {
   return Vec1<int64_t>((int64_t(b.raw) << 32) + a.raw);
 }
 
-// ------------------------------ Mask
+template <typename T, typename TW = MakeWide<T>, class VW = Vec1<TW>>
+HWY_API VW ZipLower(Sisd<TW> /* tag */, Vec1<T> a, Vec1<T> b) {
+  return VW((TW(b.raw) << (sizeof(T) * 8)) + a.raw);
+}
+
+// ================================================== MASK
 
 template <typename T>
-HWY_INLINE bool AllFalse(const Mask1<T> mask) {
+HWY_API bool AllFalse(Sisd<T> /* tag */, const Mask1<T> mask) {
   return mask.bits == 0;
 }
 
 template <typename T>
-HWY_INLINE bool AllTrue(const Mask1<T> mask) {
+HWY_API bool AllTrue(Sisd<T> /* tag */, const Mask1<T> mask) {
   return mask.bits != 0;
 }
 
 template <typename T>
-HWY_INLINE size_t StoreMaskBits(const Mask1<T> mask, uint8_t* p) {
-  *p = AllTrue(mask);
+HWY_API size_t StoreMaskBits(Sisd<T> d, const Mask1<T> mask, uint8_t* p) {
+  *p = AllTrue(d, mask);
   return 1;
 }
+
 template <typename T>
-HWY_INLINE size_t CountTrue(const Mask1<T> mask) {
+HWY_API size_t CountTrue(Sisd<T> /* tag */, const Mask1<T> mask) {
   return mask.bits == 0 ? 0 : 1;
 }
 
+template <typename T>
+HWY_API intptr_t FindFirstTrue(Sisd<T> /* tag */, const Mask1<T> mask) {
+  return mask.bits == 0 ? -1 : 0;
+}
+
 template <typename T>
 HWY_API Vec1<T> Compress(Vec1<T> v, const Mask1<T> /* mask */) {
   // Upper lanes are undefined, so result is the same independent of mask.
@@ -1132,25 +1186,60 @@ template <typename T>
 HWY_API size_t CompressStore(Vec1<T> v, const Mask1<T> mask, Sisd<T> d,
                              T* HWY_RESTRICT aligned) {
   Store(Compress(v, mask), d, aligned);
-  return CountTrue(mask);
+  return CountTrue(d, mask);
 }
 
-// ------------------------------ Reductions
+// ================================================== REDUCTIONS
 
 // Sum of all lanes, i.e. the only one.
 template <typename T>
-HWY_INLINE Vec1<T> SumOfLanes(const Vec1<T> v0) {
-  return v0;
+HWY_API Vec1<T> SumOfLanes(Sisd<T> /* tag */, const Vec1<T> v) {
+  return v;
 }
 template <typename T>
-HWY_INLINE Vec1<T> MinOfLanes(const Vec1<T> v) {
+HWY_API Vec1<T> MinOfLanes(Sisd<T> /* tag */, const Vec1<T> v) {
   return v;
 }
 template <typename T>
-HWY_INLINE Vec1<T> MaxOfLanes(const Vec1<T> v) {
+HWY_API Vec1<T> MaxOfLanes(Sisd<T> /* tag */, const Vec1<T> v) {
   return v;
 }
 
+// ================================================== DEPRECATED
+
+template <typename T>
+HWY_API size_t StoreMaskBits(const Mask1<T> mask, uint8_t* p) {
+  return StoreMaskBits(Sisd<T>(), mask, p);
+}
+
+template <typename T>
+HWY_API bool AllTrue(const Mask1<T> mask) {
+  return AllTrue(Sisd<T>(), mask);
+}
+
+template <typename T>
+HWY_API bool AllFalse(const Mask1<T> mask) {
+  return AllFalse(Sisd<T>(), mask);
+}
+
+template <typename T>
+HWY_API size_t CountTrue(const Mask1<T> mask) {
+  return CountTrue(Sisd<T>(), mask);
+}
+
+template <typename T>
+HWY_API Vec1<T> SumOfLanes(const Vec1<T> v) {
+  return SumOfLanes(Sisd<T>(), v);
+}
+template <typename T>
+HWY_API Vec1<T> MinOfLanes(const Vec1<T> v) {
+  return MinOfLanes(Sisd<T>(), v);
+}
+template <typename T>
+HWY_API Vec1<T> MaxOfLanes(const Vec1<T> v) {
+  return MaxOfLanes(Sisd<T>(), v);
+}
+
 // ================================================== Operator wrapper
 
 template <class V>
@@ -1185,6 +1274,10 @@ HWY_API auto Eq(V a, V b) -> decltype(a == b) {
   return a == b;
 }
 template <class V>
+HWY_API auto Ne(V a, V b) -> decltype(a == b) {
+  return a != b;
+}
+template <class V>
 HWY_API auto Lt(V a, V b) -> decltype(a == b) {
   return a < b;
 }
diff --git a/third_party/highway/hwy/ops/set_macros-inl.h b/third_party/highway/hwy/ops/set_macros-inl.h
index 8188d56e3bca5..a4faf7935317c 100644
--- a/third_party/highway/hwy/ops/set_macros-inl.h
+++ b/third_party/highway/hwy/ops/set_macros-inl.h
@@ -25,35 +25,81 @@
 
 #endif  // HWY_SET_MACROS_PER_TARGET
 
-#include "hwy/targets.h"
+#include "hwy/detect_targets.h"
 
 #undef HWY_NAMESPACE
 #undef HWY_ALIGN
 #undef HWY_LANES
 
 #undef HWY_CAP_INTEGER64
+#undef HWY_CAP_FLOAT16
 #undef HWY_CAP_FLOAT64
 #undef HWY_CAP_GE256
 #undef HWY_CAP_GE512
 
 #undef HWY_TARGET_STR
 
+#if defined(HWY_DISABLE_PCLMUL_AES)
+#define HWY_TARGET_STR_PCLMUL_AES ""
+#else
+#define HWY_TARGET_STR_PCLMUL_AES ",pclmul,aes"
+#endif
+
+#if defined(HWY_DISABLE_BMI2_FMA)
+#define HWY_TARGET_STR_BMI2_FMA ""
+#else
+#define HWY_TARGET_STR_BMI2_FMA ",bmi,bmi2,fma"
+#endif
+
+#if defined(HWY_DISABLE_F16C)
+#define HWY_TARGET_STR_F16C ""
+#else
+#define HWY_TARGET_STR_F16C ",f16c"
+#endif
+
+#define HWY_TARGET_STR_SSSE3 "sse2,ssse3"
+
+#define HWY_TARGET_STR_SSE4 \
+  HWY_TARGET_STR_SSSE3 ",sse4.1,sse4.2" HWY_TARGET_STR_PCLMUL_AES
+// Include previous targets, which are the half-vectors of the next target.
+#define HWY_TARGET_STR_AVX2 \
+  HWY_TARGET_STR_SSE4 ",avx,avx2" HWY_TARGET_STR_BMI2_FMA HWY_TARGET_STR_F16C
+#define HWY_TARGET_STR_AVX3 \
+  HWY_TARGET_STR_AVX2 ",avx512f,avx512vl,avx512dq,avx512bw"
+
 // Before include guard so we redefine HWY_TARGET_STR on each include,
 // governed by the current HWY_TARGET.
 //-----------------------------------------------------------------------------
+// SSSE3
+#if HWY_TARGET == HWY_SSSE3
+
+#define HWY_NAMESPACE N_SSSE3
+#define HWY_ALIGN alignas(16)
+#define HWY_LANES(T) (16 / sizeof(T))
+
+#define HWY_CAP_INTEGER64 1
+#define HWY_CAP_FLOAT16 1
+#define HWY_CAP_FLOAT64 1
+#define HWY_CAP_AES 0
+#define HWY_CAP_GE256 0
+#define HWY_CAP_GE512 0
+
+#define HWY_TARGET_STR HWY_TARGET_STR_SSSE3
+//-----------------------------------------------------------------------------
 // SSE4
-#if HWY_TARGET == HWY_SSE4
+#elif HWY_TARGET == HWY_SSE4
 
 #define HWY_NAMESPACE N_SSE4
 #define HWY_ALIGN alignas(16)
 #define HWY_LANES(T) (16 / sizeof(T))
 
 #define HWY_CAP_INTEGER64 1
+#define HWY_CAP_FLOAT16 1
 #define HWY_CAP_FLOAT64 1
 #define HWY_CAP_GE256 0
 #define HWY_CAP_GE512 0
 
-#define HWY_TARGET_STR "sse2,ssse3,sse4.1"
+#define HWY_TARGET_STR HWY_TARGET_STR_SSE4
 
 //-----------------------------------------------------------------------------
 // AVX2
@@ -64,35 +110,36 @@
 #define HWY_LANES(T) (32 / sizeof(T))
 
 #define HWY_CAP_INTEGER64 1
+#define HWY_CAP_FLOAT16 1
 #define HWY_CAP_FLOAT64 1
 #define HWY_CAP_GE256 1
 #define HWY_CAP_GE512 0
 
-#if defined(HWY_DISABLE_BMI2_FMA)
-#define HWY_TARGET_STR "avx,avx2,f16c"
-#else
-#define HWY_TARGET_STR "avx,avx2,bmi,bmi2,fma,f16c"
-#endif
+#define HWY_TARGET_STR HWY_TARGET_STR_AVX2
 
 //-----------------------------------------------------------------------------
-// AVX3
-#elif HWY_TARGET == HWY_AVX3
+// AVX3[_DL]
+#elif HWY_TARGET == HWY_AVX3 || HWY_TARGET == HWY_AVX3_DL
 
 #define HWY_ALIGN alignas(64)
 #define HWY_LANES(T) (64 / sizeof(T))
 
 #define HWY_CAP_INTEGER64 1
+#define HWY_CAP_FLOAT16 1
 #define HWY_CAP_FLOAT64 1
 #define HWY_CAP_GE256 1
 #define HWY_CAP_GE512 1
 
+#if HWY_TARGET == HWY_AVX3
 #define HWY_NAMESPACE N_AVX3
-
-// Must include AVX2 because an AVX3 test may call AVX2 functions (e.g. when
-// converting to half-vectors). HWY_DISABLE_BMI2_FMA is not relevant because if
-// we have AVX3, we should also have BMI2/FMA.
+#define HWY_TARGET_STR HWY_TARGET_STR_AVX3
+#elif HWY_TARGET == HWY_AVX3_DL
+#define HWY_NAMESPACE N_AVX3_DL
 #define HWY_TARGET_STR \
-  "avx,avx2,bmi,bmi2,fma,f16c,avx512f,avx512vl,avx512dq,avx512bw"
+  HWY_TARGET_STR_AVX3 ",vpclmulqdq,vaes,avxvnni,avx512bitalg,avx512vpopcntdq"
+#else
+#error "Logic error"
+#endif  // HWY_TARGET == HWY_AVX3_DL
 
 //-----------------------------------------------------------------------------
 // PPC8
@@ -102,6 +149,7 @@
 #define HWY_LANES(T) (16 / sizeof(T))
 
 #define HWY_CAP_INTEGER64 1
+#define HWY_CAP_FLOAT16 0
 #define HWY_CAP_FLOAT64 1
 #define HWY_CAP_GE256 0
 #define HWY_CAP_GE512 0
@@ -118,6 +166,7 @@
 #define HWY_LANES(T) (16 / sizeof(T))
 
 #define HWY_CAP_INTEGER64 1
+#define HWY_CAP_FLOAT16 1
 #define HWY_CAP_GE256 0
 #define HWY_CAP_GE512 0
 
@@ -135,12 +184,24 @@
 // SVE[2]
 #elif HWY_TARGET == HWY_SVE2 || HWY_TARGET == HWY_SVE
 
+#if defined(HWY_EMULATE_SVE) && !defined(__F16C__)
+#error "Disable HWY_CAP_FLOAT16 or ensure farm_sve actually converts to f16"
+#endif
+
 // SVE only requires lane alignment, not natural alignment of the entire vector.
 #define HWY_ALIGN alignas(8)
-// Upper bound, not the actual lane count!
-#define HWY_LANES(T) (256 / sizeof(T))
+
+// <= 16 bytes: exact size (from HWY_CAPPED). 2048 bytes denotes a full vector.
+// In between: fraction of the full length, a power of two; HWY_LANES(T)/4
+// denotes 1/4 the actual length (a power of two because we use SV_POW2).
+//
+// The upper bound for SVE is actually 256 bytes, but we need to be able to
+// differentiate 1/8th of a vector, subsequently demoted to 1/4 the lane width,
+// from an exact size <= 16 bytes.
+#define HWY_LANES(T) (2048 / sizeof(T))
 
 #define HWY_CAP_INTEGER64 1
+#define HWY_CAP_FLOAT16 1
 #define HWY_CAP_FLOAT64 1
 #define HWY_CAP_GE256 0
 #define HWY_CAP_GE512 0
@@ -151,7 +212,7 @@
 #define HWY_NAMESPACE N_SVE
 #endif
 
-// HWY_TARGET_STR remains undefined - TODO(janwas): attribute for SVE?
+// HWY_TARGET_STR remains undefined
 
 //-----------------------------------------------------------------------------
 // WASM
@@ -161,6 +222,7 @@
 #define HWY_LANES(T) (16 / sizeof(T))
 
 #define HWY_CAP_INTEGER64 0
+#define HWY_CAP_FLOAT16 1
 #define HWY_CAP_FLOAT64 0
 #define HWY_CAP_GE256 0
 #define HWY_CAP_GE512 0
@@ -178,15 +240,21 @@
 #define HWY_ALIGN
 
 // Arbitrary constant, not the actual lane count! Large enough that we can
-// mul/div by 8 for LMUL. Value matches kMaxVectorSize, see base.h.
+// mul/div by 8 for LMUL.
+// TODO(janwas): update to actual upper bound 64K, plus headroom for 1/8.
 #define HWY_LANES(T) (4096 / sizeof(T))
 
-
 #define HWY_CAP_INTEGER64 1
 #define HWY_CAP_FLOAT64 1
 #define HWY_CAP_GE256 0
 #define HWY_CAP_GE512 0
 
+#if defined(__riscv_zfh)
+#define HWY_CAP_FLOAT16 1
+#else
+#define HWY_CAP_FLOAT16 0
+#endif
+
 #define HWY_NAMESPACE N_RVV
 
 // HWY_TARGET_STR remains undefined so HWY_ATTR is a no-op.
@@ -201,6 +269,7 @@
 #define HWY_LANES(T) 1
 
 #define HWY_CAP_INTEGER64 1
+#define HWY_CAP_FLOAT16 1
 #define HWY_CAP_FLOAT64 1
 #define HWY_CAP_GE256 0
 #define HWY_CAP_GE512 0
diff --git a/third_party/highway/hwy/ops/shared-inl.h b/third_party/highway/hwy/ops/shared-inl.h
index 11a7b616f806e..95ee01c199ab9 100644
--- a/third_party/highway/hwy/ops/shared-inl.h
+++ b/third_party/highway/hwy/ops/shared-inl.h
@@ -98,6 +98,17 @@ using Twice = typename D::Twice;
 #define HWY_IF_LANE_SIZE_D(D, bytes) HWY_IF_LANE_SIZE(TFromD<D>, bytes)
 #define HWY_IF_NOT_LANE_SIZE_D(D, bytes) HWY_IF_NOT_LANE_SIZE(TFromD<D>, bytes)
 
+// Same, but with a vector argument.
+#define HWY_IF_UNSIGNED_V(V) HWY_IF_UNSIGNED(TFromV<V>)
+#define HWY_IF_SIGNED_V(V) HWY_IF_SIGNED(TFromV<V>)
+#define HWY_IF_FLOAT_V(V) HWY_IF_FLOAT(TFromV<V>)
+#define HWY_IF_LANE_SIZE_V(V, bytes) HWY_IF_LANE_SIZE(TFromV<V>, bytes)
+
+// For implementing functions for a specific type.
+// IsSame<...>() in template arguments is broken on MSVC2015.
+#define HWY_IF_LANES_ARE(T, V) \
+  EnableIf<IsSameT<T, TFromD<DFromV<V>>>::value>* = nullptr
+
 // Compile-time-constant, (typically but not guaranteed) an upper bound on the
 // number of lanes.
 // Prefer instead using Lanes() and dynamic allocation, or Rebind, or
diff --git a/third_party/highway/hwy/ops/wasm_128-inl.h b/third_party/highway/hwy/ops/wasm_128-inl.h
index e235d10b45122..a5c43bb65ce53 100644
--- a/third_party/highway/hwy/ops/wasm_128-inl.h
+++ b/third_party/highway/hwy/ops/wasm_128-inl.h
@@ -22,10 +22,37 @@
 #include "hwy/base.h"
 #include "hwy/ops/shared-inl.h"
 
+#ifdef HWY_WASM_OLD_NAMES
+#define wasm_i8x16_shuffle wasm_v8x16_shuffle
+#define wasm_i16x8_shuffle wasm_v16x8_shuffle
+#define wasm_i32x4_shuffle wasm_v32x4_shuffle
+#define wasm_i64x2_shuffle wasm_v64x2_shuffle
+#define wasm_u16x8_extend_low_u8x16 wasm_i16x8_widen_low_u8x16
+#define wasm_u32x4_extend_low_u16x8 wasm_i32x4_widen_low_u16x8
+#define wasm_i32x4_extend_low_i16x8 wasm_i32x4_widen_low_i16x8
+#define wasm_i16x8_extend_low_i8x16 wasm_i16x8_widen_low_i8x16
+#define wasm_u32x4_extend_high_u16x8 wasm_i32x4_widen_high_u16x8
+#define wasm_i32x4_extend_high_i16x8 wasm_i32x4_widen_high_i16x8
+#define wasm_i32x4_trunc_sat_f32x4 wasm_i32x4_trunc_saturate_f32x4
+#define wasm_u8x16_add_sat wasm_u8x16_add_saturate
+#define wasm_u8x16_sub_sat wasm_u8x16_sub_saturate
+#define wasm_u16x8_add_sat wasm_u16x8_add_saturate
+#define wasm_u16x8_sub_sat wasm_u16x8_sub_saturate
+#define wasm_i8x16_add_sat wasm_i8x16_add_saturate
+#define wasm_i8x16_sub_sat wasm_i8x16_sub_saturate
+#define wasm_i16x8_add_sat wasm_i16x8_add_saturate
+#define wasm_i16x8_sub_sat wasm_i16x8_sub_saturate
+#endif
+
 HWY_BEFORE_NAMESPACE();
 namespace hwy {
 namespace HWY_NAMESPACE {
 
+template <typename T>
+using Full128 = Simd<T, 16 / sizeof(T)>;
+
+namespace detail {
+
 template <typename T>
 struct Raw128 {
   using type = __v128_u;
@@ -35,12 +62,11 @@ struct Raw128<float> {
   using type = __f32x4;
 };
 
-template <typename T>
-using Full128 = Simd<T, 16 / sizeof(T)>;
+} // namespace detail
 
 template <typename T, size_t N = 16 / sizeof(T)>
 class Vec128 {
-  using Raw = typename Raw128<T>::type;
+  using Raw = typename detail::Raw128<T>::type;
 
  public:
   // Compound assignment. Only usable if there is a corresponding non-member
@@ -70,29 +96,41 @@ class Vec128 {
   Raw raw;
 };
 
-// Integer: FF..FF or 0. Float: MSB, all other bits undefined - see README.
+// FF..FF or 0.
 template <typename T, size_t N = 16 / sizeof(T)>
-class Mask128 {
-  using Raw = typename Raw128<T>::type;
+struct Mask128 {
+  typename detail::Raw128<T>::type raw;
+};
 
- public:
-  Raw raw;
+namespace detail {
+
+// Deduce Simd<T, N> from Vec128<T, N>
+struct DeduceD {
+  template <typename T, size_t N>
+  Simd<T, N> operator()(Vec128<T, N>) const {
+    return Simd<T, N>();
+  }
 };
 
+}  // namespace detail
+
+template <class V>
+using DFromV = decltype(detail::DeduceD()(V()));
+
 // ------------------------------ BitCast
 
 namespace detail {
 
-HWY_API __v128_u BitCastToInteger(__v128_u v) { return v; }
-HWY_API __v128_u BitCastToInteger(__f32x4 v) {
+HWY_INLINE __v128_u BitCastToInteger(__v128_u v) { return v; }
+HWY_INLINE __v128_u BitCastToInteger(__f32x4 v) {
   return static_cast<__v128_u>(v);
 }
-HWY_API __v128_u BitCastToInteger(__f64x2 v) {
+HWY_INLINE __v128_u BitCastToInteger(__f64x2 v) {
   return static_cast<__v128_u>(v);
 }
 
 template <typename T, size_t N>
-HWY_API Vec128<uint8_t, N * sizeof(T)> BitCastToByte(Vec128<T, N> v) {
+HWY_INLINE Vec128<uint8_t, N * sizeof(T)> BitCastToByte(Vec128<T, N> v) {
   return Vec128<uint8_t, N * sizeof(T)>{BitCastToInteger(v.raw)};
 }
 
@@ -107,8 +145,8 @@ struct BitCastFromInteger128<float> {
 };
 
 template <typename T, size_t N>
-HWY_API Vec128<T, N> BitCastFromByte(Simd<T, N> /* tag */,
-                                     Vec128<uint8_t, N * sizeof(T)> v) {
+HWY_INLINE Vec128<T, N> BitCastFromByte(Simd<T, N> /* tag */,
+                                        Vec128<uint8_t, N * sizeof(T)> v) {
   return Vec128<T, N>{BitCastFromInteger128<T>()(v.raw)};
 }
 
@@ -120,7 +158,7 @@ HWY_API Vec128<T, N> BitCast(Simd<T, N> d,
   return detail::BitCastFromByte(d, detail::BitCastToByte(v));
 }
 
-// ------------------------------ Set
+// ------------------------------ Zero
 
 // Returns an all-zero vector/part.
 template <typename T, size_t N, HWY_IF_LE128(T, N)>
@@ -132,6 +170,11 @@ HWY_API Vec128<float, N> Zero(Simd<float, N> /* tag */) {
   return Vec128<float, N>{wasm_f32x4_splat(0.0f)};
 }
 
+template <class D>
+using VFromD = decltype(Zero(D()));
+
+// ------------------------------ Set
+
 // Returns a vector/part with all lanes set to "t".
 template <size_t N, HWY_IF_LE128(uint8_t, N)>
 HWY_API Vec128<uint8_t, N> Set(Simd<uint8_t, N> /* tag */, const uint8_t t) {
@@ -145,6 +188,10 @@ template <size_t N, HWY_IF_LE128(uint32_t, N)>
 HWY_API Vec128<uint32_t, N> Set(Simd<uint32_t, N> /* tag */, const uint32_t t) {
   return Vec128<uint32_t, N>{wasm_i32x4_splat(t)};
 }
+template <size_t N, HWY_IF_LE128(uint64_t, N)>
+HWY_API Vec128<uint64_t, N> Set(Simd<uint64_t, N> /* tag */, const uint64_t t) {
+  return Vec128<uint64_t, N>{wasm_i64x2_splat(t)};
+}
 
 template <size_t N, HWY_IF_LE128(int8_t, N)>
 HWY_API Vec128<int8_t, N> Set(Simd<int8_t, N> /* tag */, const int8_t t) {
@@ -158,6 +205,10 @@ template <size_t N, HWY_IF_LE128(int32_t, N)>
 HWY_API Vec128<int32_t, N> Set(Simd<int32_t, N> /* tag */, const int32_t t) {
   return Vec128<int32_t, N>{wasm_i32x4_splat(t)};
 }
+template <size_t N, HWY_IF_LE128(int64_t, N)>
+HWY_API Vec128<int64_t, N> Set(Simd<int64_t, N> /* tag */, const int64_t t) {
+  return Vec128<int64_t, N>{wasm_i64x2_splat(t)};
+}
 
 template <size_t N, HWY_IF_LE128(float, N)>
 HWY_API Vec128<float, N> Set(Simd<float, N> /* tag */, const float t) {
@@ -281,24 +332,24 @@ HWY_API Vec128<float, N> operator-(const Vec128<float, N> a,
 template <size_t N>
 HWY_API Vec128<uint8_t, N> SaturatedAdd(const Vec128<uint8_t, N> a,
                                         const Vec128<uint8_t, N> b) {
-  return Vec128<uint8_t, N>{wasm_u8x16_add_saturate(a.raw, b.raw)};
+  return Vec128<uint8_t, N>{wasm_u8x16_add_sat(a.raw, b.raw)};
 }
 template <size_t N>
 HWY_API Vec128<uint16_t, N> SaturatedAdd(const Vec128<uint16_t, N> a,
                                          const Vec128<uint16_t, N> b) {
-  return Vec128<uint16_t, N>{wasm_u16x8_add_saturate(a.raw, b.raw)};
+  return Vec128<uint16_t, N>{wasm_u16x8_add_sat(a.raw, b.raw)};
 }
 
 // Signed
 template <size_t N>
 HWY_API Vec128<int8_t, N> SaturatedAdd(const Vec128<int8_t, N> a,
                                        const Vec128<int8_t, N> b) {
-  return Vec128<int8_t, N>{wasm_i8x16_add_saturate(a.raw, b.raw)};
+  return Vec128<int8_t, N>{wasm_i8x16_add_sat(a.raw, b.raw)};
 }
 template <size_t N>
 HWY_API Vec128<int16_t, N> SaturatedAdd(const Vec128<int16_t, N> a,
                                         const Vec128<int16_t, N> b) {
-  return Vec128<int16_t, N>{wasm_i16x8_add_saturate(a.raw, b.raw)};
+  return Vec128<int16_t, N>{wasm_i16x8_add_sat(a.raw, b.raw)};
 }
 
 // ------------------------------ Saturating subtraction
@@ -309,24 +360,24 @@ HWY_API Vec128<int16_t, N> SaturatedAdd(const Vec128<int16_t, N> a,
 template <size_t N>
 HWY_API Vec128<uint8_t, N> SaturatedSub(const Vec128<uint8_t, N> a,
                                         const Vec128<uint8_t, N> b) {
-  return Vec128<uint8_t, N>{wasm_u8x16_sub_saturate(a.raw, b.raw)};
+  return Vec128<uint8_t, N>{wasm_u8x16_sub_sat(a.raw, b.raw)};
 }
 template <size_t N>
 HWY_API Vec128<uint16_t, N> SaturatedSub(const Vec128<uint16_t, N> a,
                                          const Vec128<uint16_t, N> b) {
-  return Vec128<uint16_t, N>{wasm_u16x8_sub_saturate(a.raw, b.raw)};
+  return Vec128<uint16_t, N>{wasm_u16x8_sub_sat(a.raw, b.raw)};
 }
 
 // Signed
 template <size_t N>
 HWY_API Vec128<int8_t, N> SaturatedSub(const Vec128<int8_t, N> a,
                                        const Vec128<int8_t, N> b) {
-  return Vec128<int8_t, N>{wasm_i8x16_sub_saturate(a.raw, b.raw)};
+  return Vec128<int8_t, N>{wasm_i8x16_sub_sat(a.raw, b.raw)};
 }
 template <size_t N>
 HWY_API Vec128<int16_t, N> SaturatedSub(const Vec128<int16_t, N> a,
                                         const Vec128<int16_t, N> b) {
-  return Vec128<int16_t, N>{wasm_i16x8_sub_saturate(a.raw, b.raw)};
+  return Vec128<int16_t, N>{wasm_i16x8_sub_sat(a.raw, b.raw)};
 }
 
 // ------------------------------ Average
@@ -362,9 +413,7 @@ HWY_API Vec128<int32_t, N> Abs(const Vec128<int32_t, N> v) {
 }
 template <size_t N>
 HWY_API Vec128<int64_t, N> Abs(const Vec128<int64_t, N> v) {
-  // TODO(janwas): use wasm_i64x2_abs when available
-  const Vec128<int64_t, N> mask = wasm_i64x2_shr(v.raw, 63);
-  return ((v ^ mask) - mask);
+  return Vec128<int32_t, N>{wasm_i62x2_abs(v.raw)};
 }
 
 template <size_t N>
@@ -537,12 +586,10 @@ HWY_API Vec128<uint64_t, N> Min(const Vec128<uint64_t, N> a,
                                 const Vec128<uint64_t, N> b) {
   alignas(16) float min[4];
   min[0] =
-      std::min(wasm_u64x2_extract_lane(a, 0), wasm_u64x2_extract_lane(b, 0));
+      HWY_MIN(wasm_u64x2_extract_lane(a, 0), wasm_u64x2_extract_lane(b, 0));
   min[1] =
-      std::min(wasm_u64x2_extract_lane(a, 1), wasm_u64x2_extract_lane(b, 1));
+      HWY_MIN(wasm_u64x2_extract_lane(a, 1), wasm_u64x2_extract_lane(b, 1));
   return Vec128<uint64_t, N>{wasm_v128_load(min)};
-  // TODO(janwas): new op?
-  // return Vec128<uint64_t, N>{wasm_u64x2_min(a.raw, b.raw)};
 }
 
 // Signed
@@ -566,12 +613,10 @@ HWY_API Vec128<int64_t, N> Min(const Vec128<int64_t, N> a,
                                const Vec128<int64_t, N> b) {
   alignas(16) float min[4];
   min[0] =
-      std::min(wasm_i64x2_extract_lane(a, 0), wasm_i64x2_extract_lane(b, 0));
+      HWY_MIN(wasm_i64x2_extract_lane(a, 0), wasm_i64x2_extract_lane(b, 0));
   min[1] =
-      std::min(wasm_i64x2_extract_lane(a, 1), wasm_i64x2_extract_lane(b, 1));
+      HWY_MIN(wasm_i64x2_extract_lane(a, 1), wasm_i64x2_extract_lane(b, 1));
   return Vec128<int64_t, N>{wasm_v128_load(min)};
-  // TODO(janwas): new op? (also do not yet have wasm_u64x2_make)
-  // return Vec128<int64_t, N>{wasm_i64x2_min(a.raw, b.raw)};
 }
 
 // Float
@@ -604,12 +649,10 @@ HWY_API Vec128<uint64_t, N> Max(const Vec128<uint64_t, N> a,
                                 const Vec128<uint64_t, N> b) {
   alignas(16) float max[4];
   max[0] =
-      std::max(wasm_u64x2_extract_lane(a, 0), wasm_u64x2_extract_lane(b, 0));
+      HWY_MAX(wasm_u64x2_extract_lane(a, 0), wasm_u64x2_extract_lane(b, 0));
   max[1] =
-      std::max(wasm_u64x2_extract_lane(a, 1), wasm_u64x2_extract_lane(b, 1));
+      HWY_MAX(wasm_u64x2_extract_lane(a, 1), wasm_u64x2_extract_lane(b, 1));
   return Vec128<int64_t, N>{wasm_v128_load(max)};
-  // TODO(janwas): new op? (also do not yet have wasm_u64x2_make)
-  // return Vec128<uint64_t, N>{wasm_u64x2_max(a.raw, b.raw)};
 }
 
 // Signed
@@ -633,12 +676,10 @@ HWY_API Vec128<int64_t, N> Max(const Vec128<int64_t, N> a,
                                const Vec128<int64_t, N> b) {
   alignas(16) float max[4];
   max[0] =
-      std::max(wasm_i64x2_extract_lane(a, 0), wasm_i64x2_extract_lane(b, 0));
+      HWY_MAX(wasm_i64x2_extract_lane(a, 0), wasm_i64x2_extract_lane(b, 0));
   max[1] =
-      std::max(wasm_i64x2_extract_lane(a, 1), wasm_i64x2_extract_lane(b, 1));
+      HWY_MAX(wasm_i64x2_extract_lane(a, 1), wasm_i64x2_extract_lane(b, 1));
   return Vec128<int64_t, N>{wasm_v128_load(max)};
-  // TODO(janwas): new op? (also do not yet have wasm_u64x2_make)
-  // return Vec128<int64_t, N>{wasm_i64x2_max(a.raw, b.raw)};
 }
 
 // Float
@@ -679,29 +720,29 @@ template <size_t N>
 HWY_API Vec128<uint16_t, N> MulHigh(const Vec128<uint16_t, N> a,
                                     const Vec128<uint16_t, N> b) {
   // TODO(eustas): replace, when implemented in WASM.
-  const auto al = wasm_i32x4_widen_low_u16x8(a.raw);
-  const auto ah = wasm_i32x4_widen_high_u16x8(a.raw);
-  const auto bl = wasm_i32x4_widen_low_u16x8(b.raw);
-  const auto bh = wasm_i32x4_widen_high_u16x8(b.raw);
+  const auto al = wasm_u32x4_extend_low_u16x8(a.raw);
+  const auto ah = wasm_u32x4_extend_high_u16x8(a.raw);
+  const auto bl = wasm_u32x4_extend_low_u16x8(b.raw);
+  const auto bh = wasm_u32x4_extend_high_u16x8(b.raw);
   const auto l = wasm_i32x4_mul(al, bl);
   const auto h = wasm_i32x4_mul(ah, bh);
   // TODO(eustas): shift-right + narrow?
   return Vec128<uint16_t, N>{
-      wasm_v16x8_shuffle(l, h, 1, 3, 5, 7, 9, 11, 13, 15)};
+      wasm_i16x8_shuffle(l, h, 1, 3, 5, 7, 9, 11, 13, 15)};
 }
 template <size_t N>
 HWY_API Vec128<int16_t, N> MulHigh(const Vec128<int16_t, N> a,
                                    const Vec128<int16_t, N> b) {
   // TODO(eustas): replace, when implemented in WASM.
-  const auto al = wasm_i32x4_widen_low_i16x8(a.raw);
-  const auto ah = wasm_i32x4_widen_high_i16x8(a.raw);
-  const auto bl = wasm_i32x4_widen_low_i16x8(b.raw);
-  const auto bh = wasm_i32x4_widen_high_i16x8(b.raw);
+  const auto al = wasm_i32x4_extend_low_i16x8(a.raw);
+  const auto ah = wasm_i32x4_extend_high_i16x8(a.raw);
+  const auto bl = wasm_i32x4_extend_low_i16x8(b.raw);
+  const auto bh = wasm_i32x4_extend_high_i16x8(b.raw);
   const auto l = wasm_i32x4_mul(al, bl);
   const auto h = wasm_i32x4_mul(ah, bh);
   // TODO(eustas): shift-right + narrow?
   return Vec128<int16_t, N>{
-      wasm_v16x8_shuffle(l, h, 1, 3, 5, 7, 9, 11, 13, 15)};
+      wasm_i16x8_shuffle(l, h, 1, 3, 5, 7, 9, 11, 13, 15)};
 }
 
 // Multiplies even lanes (0, 2 ..) and returns the double-width result.
@@ -764,7 +805,6 @@ HWY_API Vec128<float, N> operator/(const Vec128<float, N> a,
 // Approximate reciprocal
 template <size_t N>
 HWY_API Vec128<float, N> ApproximateReciprocal(const Vec128<float, N> v) {
-  // TODO(eustas): replace, when implemented in WASM.
   const Vec128<float, N> one = Vec128<float, N>{wasm_f32x4_splat(1.0f)};
   return one / v;
 }
@@ -837,76 +877,25 @@ HWY_API Vec128<float, N> ApproximateReciprocalSqrt(const Vec128<float, N> v) {
 // Toward nearest integer, ties to even
 template <size_t N>
 HWY_API Vec128<float, N> Round(const Vec128<float, N> v) {
-  // IEEE-754 roundToIntegralTiesToEven returns floating-point, but we do not
-  // yet have an instruction for that (f32x4.nearest is not implemented). We
-  // rely on rounding after addition with a large value such that no mantissa
-  // bits remain (assuming the current mode is nearest-even). We may need a
-  // compiler flag for precise floating-point to prevent "optimizing" this out.
-  const Simd<float, N> df;
-  const auto max = Set(df, MantissaEnd<float>());
-  const auto large = CopySignToAbs(max, v);
-  const auto added = large + v;
-  const auto rounded = added - large;
-
-  // Keep original if NaN or the magnitude is large (already an int).
-  return IfThenElse(Abs(v) < max, rounded, v);
+  return Vec128<float, N>{wasm_f32x4_nearest(v.raw)};
 }
 
-namespace detail {
-
-// Truncating to integer and converting back to float is correct except when the
-// input magnitude is large, in which case the input was already an integer
-// (because mantissa >> exponent is zero).
-template <size_t N>
-HWY_API Mask128<float, N> UseInt(const Vec128<float, N> v) {
-  return Abs(v) < Set(Simd<float, N>(), MantissaEnd<float>());
-}
-
-}  // namespace detail
-
 // Toward zero, aka truncate
 template <size_t N>
 HWY_API Vec128<float, N> Trunc(const Vec128<float, N> v) {
-  // TODO(eustas): is it f32x4.trunc? (not implemented yet)
-  const Simd<float, N> df;
-  const RebindToSigned<decltype(df)> di;
-
-  const auto integer = ConvertTo(di, v);  // round toward 0
-  const auto int_f = ConvertTo(df, integer);
-
-  return IfThenElse(detail::UseInt(v), CopySign(int_f, v), v);
+  return Vec128<float, N>{wasm_f32x4_trunc(v.raw)};
 }
 
 // Toward +infinity, aka ceiling
 template <size_t N>
-HWY_INLINE Vec128<float, N> Ceil(const Vec128<float, N> v) {
-  // TODO(eustas): is it f32x4.ceil? (not implemented yet)
-  const Simd<float, N> df;
-  const RebindToSigned<decltype(df)> di;
-
-  const auto integer = ConvertTo(di, v);  // round toward 0
-  const auto int_f = ConvertTo(df, integer);
-
-  // Truncating a positive non-integer ends up smaller; if so, add 1.
-  const auto neg1 = ConvertTo(df, VecFromMask(di, RebindMask(di, int_f < v)));
-
-  return IfThenElse(detail::UseInt(v), int_f - neg1, v);
+HWY_API Vec128<float, N> Ceil(const Vec128<float, N> v) {
+  return Vec128<float, N>{wasm_f32x4_ceil(v.raw)};
 }
 
 // Toward -infinity, aka floor
 template <size_t N>
-HWY_INLINE Vec128<float, N> Floor(const Vec128<float, N> v) {
-  // TODO(eustas): is it f32x4.floor? (not implemented yet)
-  const Simd<float, N> df;
-  const RebindToSigned<decltype(df)> di;
-
-  const auto integer = ConvertTo(di, v);  // round toward 0
-  const auto int_f = ConvertTo(df, integer);
-
-  // Truncating a negative non-integer ends up larger; if so, subtract 1.
-  const auto neg1 = ConvertTo(df, VecFromMask(di, RebindMask(di, int_f > v)));
-
-  return IfThenElse(detail::UseInt(v), int_f + neg1, v);
+HWY_API Vec128<float, N> Floor(const Vec128<float, N> v) {
+  return Vec128<float, N>{wasm_f32x4_floor(v.raw)};
 }
 
 // ================================================== COMPARE
@@ -919,6 +908,12 @@ HWY_API Mask128<TTo, N> RebindMask(Simd<TTo, N> /*tag*/, Mask128<TFrom, N> m) {
   return Mask128<TTo, N>{m.raw};
 }
 
+template <typename T, size_t N>
+HWY_API Mask128<T, N> TestBit(Vec128<T, N> v, Vec128<T, N> bit) {
+  static_assert(!hwy::IsFloat<T>(), "Only integer vectors supported");
+  return (v & bit) == bit;
+}
+
 // ------------------------------ Equality
 
 // Unsigned
@@ -962,10 +957,47 @@ HWY_API Mask128<float, N> operator==(const Vec128<float, N> a,
   return Mask128<float, N>{wasm_f32x4_eq(a.raw, b.raw)};
 }
 
-template <typename T, size_t N>
-HWY_API Mask128<T, N> TestBit(Vec128<T, N> v, Vec128<T, N> bit) {
-  static_assert(!hwy::IsFloat<T>(), "Only integer vectors supported");
-  return (v & bit) == bit;
+// ------------------------------ Inequality
+
+// Unsigned
+template <size_t N>
+HWY_API Mask128<uint8_t, N> operator!=(const Vec128<uint8_t, N> a,
+                                       const Vec128<uint8_t, N> b) {
+  return Mask128<uint8_t, N>{wasm_i8x16_ne(a.raw, b.raw)};
+}
+template <size_t N>
+HWY_API Mask128<uint16_t, N> operator!=(const Vec128<uint16_t, N> a,
+                                        const Vec128<uint16_t, N> b) {
+  return Mask128<uint16_t, N>{wasm_i16x8_ne(a.raw, b.raw)};
+}
+template <size_t N>
+HWY_API Mask128<uint32_t, N> operator!=(const Vec128<uint32_t, N> a,
+                                        const Vec128<uint32_t, N> b) {
+  return Mask128<uint32_t, N>{wasm_i32x4_ne(a.raw, b.raw)};
+}
+
+// Signed
+template <size_t N>
+HWY_API Mask128<int8_t, N> operator!=(const Vec128<int8_t, N> a,
+                                      const Vec128<int8_t, N> b) {
+  return Mask128<int8_t, N>{wasm_i8x16_ne(a.raw, b.raw)};
+}
+template <size_t N>
+HWY_API Mask128<int16_t, N> operator!=(Vec128<int16_t, N> a,
+                                       Vec128<int16_t, N> b) {
+  return Mask128<int16_t, N>{wasm_i16x8_ne(a.raw, b.raw)};
+}
+template <size_t N>
+HWY_API Mask128<int32_t, N> operator!=(const Vec128<int32_t, N> a,
+                                       const Vec128<int32_t, N> b) {
+  return Mask128<int32_t, N>{wasm_i32x4_ne(a.raw, b.raw)};
+}
+
+// Float
+template <size_t N>
+HWY_API Mask128<float, N> operator!=(const Vec128<float, N> a,
+                                     const Vec128<float, N> b) {
+  return Mask128<float, N>{wasm_f32x4_ne(a.raw, b.raw)};
 }
 
 // ------------------------------ Strict inequality
@@ -997,12 +1029,12 @@ HWY_API Mask128<int64_t, N> operator>(const Vec128<int64_t, N> a,
 
   // Otherwise, the lower half decides.
   const auto m_eq = a32 == b32;
-  const auto lo_in_hi = wasm_v32x4_shuffle(m_gt, m_gt, 2, 2, 0, 0);
+  const auto lo_in_hi = wasm_i32x4_shuffle(m_gt, m_gt, 2, 2, 0, 0);
   const auto lo_gt = And(m_eq, lo_in_hi);
 
   const auto gt = Or(lo_gt, m_gt);
   // Copy result in upper 32 bits to lower 32 bits.
-  return Mask128<int64_t, N>{wasm_v32x4_shuffle(gt, gt, 3, 3, 1, 1)};
+  return Mask128<int64_t, N>{wasm_i32x4_shuffle(gt, gt, 3, 3, 1, 1)};
 }
 
 template <size_t N>
@@ -1170,8 +1202,7 @@ HWY_API Vec128<T, N> ZeroIfNegative(Vec128<T, N> v) {
 
 template <typename T, size_t N>
 HWY_API Mask128<T, N> Not(const Mask128<T, N> m) {
-  const Simd<T, N> d;
-  return MaskFromVec(Not(VecFromMask(d, m)));
+  return MaskFromVec(Not(VecFromMask(Simd<T, N>(), m)));
 }
 
 template <typename T, size_t N>
@@ -1479,36 +1510,37 @@ template <size_t N>
 HWY_API int32_t GetLane(const Vec128<int32_t, N> v) {
   return wasm_i32x4_extract_lane(v.raw, 0);
 }
+template <size_t N>
+HWY_API uint64_t GetLane(const Vec128<uint64_t, N> v) {
+  return wasm_i64x2_extract_lane(v.raw, 0);
+}
+template <size_t N>
+HWY_API int64_t GetLane(const Vec128<int64_t, N> v) {
+  return wasm_i64x2_extract_lane(v.raw, 0);
+}
+
 template <size_t N>
 HWY_API float GetLane(const Vec128<float, N> v) {
   return wasm_f32x4_extract_lane(v.raw, 0);
 }
 
-// ------------------------------ Extract half
+// ------------------------------ LowerHalf
 
-// Returns upper/lower half of a vector.
 template <typename T, size_t N>
-HWY_API Vec128<T, N / 2> LowerHalf(Vec128<T, N> v) {
+HWY_API Vec128<T, N / 2> LowerHalf(Simd<T, N / 2> /* tag */, Vec128<T, N> v) {
   return Vec128<T, N / 2>{v.raw};
 }
 
-// These copy hi into lo (smaller instruction encoding than shifts).
-template <typename T>
-HWY_API Vec128<T, 8 / sizeof(T)> UpperHalf(Vec128<T> v) {
-  // TODO(eustas): use swizzle?
-  return Vec128<T, 8 / sizeof(T)>{wasm_v32x4_shuffle(v.raw, v.raw, 2, 3, 2, 3)};
-}
-template <>
-HWY_INLINE Vec128<float, 2> UpperHalf(Vec128<float> v) {
-  // TODO(eustas): use swizzle?
-  return Vec128<float, 2>{wasm_v32x4_shuffle(v.raw, v.raw, 2, 3, 2, 3)};
+template <typename T, size_t N>
+HWY_API Vec128<T, N / 2> LowerHalf(Vec128<T, N> v) {
+  return LowerHalf(Simd<T, N / 2>(), v);
 }
 
-// ------------------------------ Shift vector by constant #bytes
+// ------------------------------ ShiftLeftBytes
 
 // 0x01..0F, kBytes = 1 => 0x02..0F00
-template <int kBytes, typename T>
-HWY_API Vec128<T> ShiftLeftBytes(const Vec128<T> v) {
+template <int kBytes, typename T, size_t N>
+HWY_API Vec128<T, N> ShiftLeftBytes(Simd<T, N> /* tag */, Vec128<T, N> v) {
   static_assert(0 <= kBytes && kBytes <= 16, "Invalid kBytes");
   const __i8x16 zero = wasm_i8x16_splat(0);
   switch (kBytes) {
@@ -1516,275 +1548,328 @@ HWY_API Vec128<T> ShiftLeftBytes(const Vec128<T> v) {
       return v;
 
     case 1:
-      return Vec128<T>{wasm_v8x16_shuffle(v.raw, zero, 16, 0, 1, 2, 3, 4, 5, 6,
-                                          7, 8, 9, 10, 11, 12, 13, 14)};
+      return Vec128<T, N>{wasm_i8x16_shuffle(v.raw, zero, 16, 0, 1, 2, 3, 4, 5,
+                                             6, 7, 8, 9, 10, 11, 12, 13, 14)};
 
     case 2:
-      return Vec128<T>{wasm_v8x16_shuffle(v.raw, zero, 16, 16, 0, 1, 2, 3, 4, 5,
-                                          6, 7, 8, 9, 10, 11, 12, 13)};
+      return Vec128<T, N>{wasm_i8x16_shuffle(v.raw, zero, 16, 16, 0, 1, 2, 3, 4,
+                                             5, 6, 7, 8, 9, 10, 11, 12, 13)};
 
     case 3:
-      return Vec128<T>{wasm_v8x16_shuffle(v.raw, zero, 16, 16, 16, 0, 1, 2, 3,
-                                          4, 5, 6, 7, 8, 9, 10, 11, 12)};
+      return Vec128<T, N>{wasm_i8x16_shuffle(v.raw, zero, 16, 16, 16, 0, 1, 2,
+                                             3, 4, 5, 6, 7, 8, 9, 10, 11, 12)};
 
     case 4:
-      return Vec128<T>{wasm_v8x16_shuffle(v.raw, zero, 16, 16, 16, 16, 0, 1, 2,
-                                          3, 4, 5, 6, 7, 8, 9, 10, 11)};
+      return Vec128<T, N>{wasm_i8x16_shuffle(v.raw, zero, 16, 16, 16, 16, 0, 1,
+                                             2, 3, 4, 5, 6, 7, 8, 9, 10, 11)};
 
     case 5:
-      return Vec128<T>{wasm_v8x16_shuffle(v.raw, zero, 16, 16, 16, 16, 16, 0, 1,
-                                          2, 3, 4, 5, 6, 7, 8, 9, 10)};
+      return Vec128<T, N>{wasm_i8x16_shuffle(v.raw, zero, 16, 16, 16, 16, 16, 0,
+                                             1, 2, 3, 4, 5, 6, 7, 8, 9, 10)};
 
     case 6:
-      return Vec128<T>{wasm_v8x16_shuffle(v.raw, zero, 16, 16, 16, 16, 16, 16,
-                                          0, 1, 2, 3, 4, 5, 6, 7, 8, 9)};
+      return Vec128<T, N>{wasm_i8x16_shuffle(v.raw, zero, 16, 16, 16, 16, 16,
+                                             16, 0, 1, 2, 3, 4, 5, 6, 7, 8, 9)};
 
     case 7:
-      return Vec128<T>{wasm_v8x16_shuffle(v.raw, zero, 16, 16, 16, 16, 16, 16,
-                                          16, 0, 1, 2, 3, 4, 5, 6, 7, 8)};
+      return Vec128<T, N>{wasm_i8x16_shuffle(
+          v.raw, zero, 16, 16, 16, 16, 16, 16, 16, 0, 1, 2, 3, 4, 5, 6, 7, 8)};
 
     case 8:
-      return Vec128<T>{wasm_v8x16_shuffle(v.raw, zero, 16, 16, 16, 16, 16, 16,
-                                          16, 16, 0, 1, 2, 3, 4, 5, 6, 7)};
+      return Vec128<T, N>{wasm_i8x16_shuffle(
+          v.raw, zero, 16, 16, 16, 16, 16, 16, 16, 16, 0, 1, 2, 3, 4, 5, 6, 7)};
 
     case 9:
-      return Vec128<T>{wasm_v8x16_shuffle(v.raw, zero, 16, 16, 16, 16, 16, 16,
-                                          16, 16, 16, 0, 1, 2, 3, 4, 5, 6)};
+      return Vec128<T, N>{wasm_i8x16_shuffle(v.raw, zero, 16, 16, 16, 16, 16,
+                                             16, 16, 16, 16, 0, 1, 2, 3, 4, 5,
+                                             6)};
 
     case 10:
-      return Vec128<T>{wasm_v8x16_shuffle(v.raw, zero, 16, 16, 16, 16, 16, 16,
-                                          16, 16, 16, 16, 0, 1, 2, 3, 4, 5)};
+      return Vec128<T, N>{wasm_i8x16_shuffle(v.raw, zero, 16, 16, 16, 16, 16,
+                                             16, 16, 16, 16, 16, 0, 1, 2, 3, 4,
+                                             5)};
 
     case 11:
-      return Vec128<T>{wasm_v8x16_shuffle(v.raw, zero, 16, 16, 16, 16, 16, 16,
-                                          16, 16, 16, 16, 16, 0, 1, 2, 3, 4)};
+      return Vec128<T, N>{wasm_i8x16_shuffle(v.raw, zero, 16, 16, 16, 16, 16,
+                                             16, 16, 16, 16, 16, 16, 0, 1, 2, 3,
+                                             4)};
 
     case 12:
-      return Vec128<T>{wasm_v8x16_shuffle(v.raw, zero, 16, 16, 16, 16, 16, 16,
-                                          16, 16, 16, 16, 16, 16, 0, 1, 2, 3)};
+      return Vec128<T, N>{wasm_i8x16_shuffle(v.raw, zero, 16, 16, 16, 16, 16,
+                                             16, 16, 16, 16, 16, 16, 16, 0, 1,
+                                             2, 3)};
 
     case 13:
-      return Vec128<T>{wasm_v8x16_shuffle(v.raw, zero, 16, 16, 16, 16, 16, 16,
-                                          16, 16, 16, 16, 16, 16, 16, 0, 1, 2)};
+      return Vec128<T, N>{wasm_i8x16_shuffle(v.raw, zero, 16, 16, 16, 16, 16,
+                                             16, 16, 16, 16, 16, 16, 16, 16, 0,
+                                             1, 2)};
 
     case 14:
-      return Vec128<T>{wasm_v8x16_shuffle(v.raw, zero, 16, 16, 16, 16, 16, 16,
-                                          16, 16, 16, 16, 16, 16, 16, 16, 0,
-                                          1)};
+      return Vec128<T, N>{wasm_i8x16_shuffle(v.raw, zero, 16, 16, 16, 16, 16,
+                                             16, 16, 16, 16, 16, 16, 16, 16, 16,
+                                             0, 1)};
 
     case 15:
-      return Vec128<T>{wasm_v8x16_shuffle(v.raw, zero, 16, 16, 16, 16, 16, 16,
-                                          16, 16, 16, 16, 16, 16, 16, 16, 16,
-                                          0)};
+      return Vec128<T, N>{wasm_i8x16_shuffle(v.raw, zero, 16, 16, 16, 16, 16,
+                                             16, 16, 16, 16, 16, 16, 16, 16, 16,
+                                             16, 0)};
   }
-  return Vec128<T>{zero};
+  return Vec128<T, N>{zero};
+}
+
+template <int kBytes, typename T, size_t N>
+HWY_API Vec128<T, N> ShiftLeftBytes(Vec128<T, N> v) {
+  return ShiftLeftBytes<kBytes>(Simd<T, N>(), v);
 }
 
+// ------------------------------ ShiftLeftLanes
+
 template <int kLanes, typename T, size_t N>
-HWY_API Vec128<T, N> ShiftLeftLanes(const Vec128<T, N> v) {
-  const Simd<uint8_t, N * sizeof(T)> d8;
-  const Simd<T, N> d;
+HWY_API Vec128<T, N> ShiftLeftLanes(Simd<T, N> d, const Vec128<T, N> v) {
+  const Repartition<uint8_t, decltype(d)> d8;
   return BitCast(d, ShiftLeftBytes<kLanes * sizeof(T)>(BitCast(d8, v)));
 }
 
-// 0x01..0F, kBytes = 1 => 0x0001..0E
-template <int kBytes, typename T>
-HWY_API Vec128<T> ShiftRightBytes(const Vec128<T> v) {
+template <int kLanes, typename T, size_t N>
+HWY_API Vec128<T, N> ShiftLeftLanes(const Vec128<T, N> v) {
+  return ShiftLeftLanes<kLanes>(Simd<T, N>(), v);
+}
+
+// ------------------------------ ShiftRightBytes
+namespace detail {
+
+// Helper function allows zeroing invalid lanes in caller.
+template <int kBytes, typename T, size_t N>
+HWY_API __i8x16 ShrBytes(const Vec128<T, N> v) {
   static_assert(0 <= kBytes && kBytes <= 16, "Invalid kBytes");
   const __i8x16 zero = wasm_i8x16_splat(0);
+
   switch (kBytes) {
     case 0:
-      return v;
+      return v.raw;
 
     case 1:
-      return Vec128<T>{wasm_v8x16_shuffle(v.raw, zero, 1, 2, 3, 4, 5, 6, 7, 8,
-                                          9, 10, 11, 12, 13, 14, 15, 16)};
+      return wasm_i8x16_shuffle(v.raw, zero, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11,
+                                12, 13, 14, 15, 16);
 
     case 2:
-      return Vec128<T>{wasm_v8x16_shuffle(v.raw, zero, 2, 3, 4, 5, 6, 7, 8, 9,
-                                          10, 11, 12, 13, 14, 15, 16, 16)};
+      return wasm_i8x16_shuffle(v.raw, zero, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12,
+                                13, 14, 15, 16, 16);
 
     case 3:
-      return Vec128<T>{wasm_v8x16_shuffle(v.raw, zero, 3, 4, 5, 6, 7, 8, 9, 10,
-                                          11, 12, 13, 14, 15, 16, 16, 16)};
+      return wasm_i8x16_shuffle(v.raw, zero, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12,
+                                13, 14, 15, 16, 16, 16);
 
     case 4:
-      return Vec128<T>{wasm_v8x16_shuffle(v.raw, zero, 4, 5, 6, 7, 8, 9, 10, 11,
-                                          12, 13, 14, 15, 16, 16, 16, 16)};
+      return wasm_i8x16_shuffle(v.raw, zero, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13,
+                                14, 15, 16, 16, 16, 16);
 
     case 5:
-      return Vec128<T>{wasm_v8x16_shuffle(v.raw, zero, 5, 6, 7, 8, 9, 10, 11,
-                                          12, 13, 14, 15, 16, 16, 16, 16, 16)};
+      return wasm_i8x16_shuffle(v.raw, zero, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14,
+                                15, 16, 16, 16, 16, 16);
 
     case 6:
-      return Vec128<T>{wasm_v8x16_shuffle(v.raw, zero, 6, 7, 8, 9, 10, 11, 12,
-                                          13, 14, 15, 16, 16, 16, 16, 16, 16)};
+      return wasm_i8x16_shuffle(v.raw, zero, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15,
+                                16, 16, 16, 16, 16, 16);
 
     case 7:
-      return Vec128<T>{wasm_v8x16_shuffle(v.raw, zero, 7, 8, 9, 10, 11, 12, 13,
-                                          14, 15, 16, 16, 16, 16, 16, 16, 16)};
+      return wasm_i8x16_shuffle(v.raw, zero, 7, 8, 9, 10, 11, 12, 13, 14, 15,
+                                16, 16, 16, 16, 16, 16, 16);
 
     case 8:
-      return Vec128<T>{wasm_v8x16_shuffle(v.raw, zero, 8, 9, 10, 11, 12, 13, 14,
-                                          15, 16, 16, 16, 16, 16, 16, 16, 16)};
+      return wasm_i8x16_shuffle(v.raw, zero, 8, 9, 10, 11, 12, 13, 14, 15, 16,
+                                16, 16, 16, 16, 16, 16, 16);
 
     case 9:
-      return Vec128<T>{wasm_v8x16_shuffle(v.raw, zero, 9, 10, 11, 12, 13, 14,
-                                          15, 16, 16, 16, 16, 16, 16, 16, 16,
-                                          16)};
+      return wasm_i8x16_shuffle(v.raw, zero, 9, 10, 11, 12, 13, 14, 15, 16, 16,
+                                16, 16, 16, 16, 16, 16, 16);
 
     case 10:
-      return Vec128<T>{wasm_v8x16_shuffle(v.raw, zero, 10, 11, 12, 13, 14, 15,
-                                          16, 16, 16, 16, 16, 16, 16, 16, 16,
-                                          16)};
+      return wasm_i8x16_shuffle(v.raw, zero, 10, 11, 12, 13, 14, 15, 16, 16, 16,
+                                16, 16, 16, 16, 16, 16, 16);
 
     case 11:
-      return Vec128<T>{wasm_v8x16_shuffle(v.raw, zero, 11, 12, 13, 14, 15, 16,
-                                          16, 16, 16, 16, 16, 16, 16, 16, 16,
-                                          16)};
+      return wasm_i8x16_shuffle(v.raw, zero, 11, 12, 13, 14, 15, 16, 16, 16, 16,
+                                16, 16, 16, 16, 16, 16, 16);
 
     case 12:
-      return Vec128<T>{wasm_v8x16_shuffle(v.raw, zero, 12, 13, 14, 15, 16, 16,
-                                          16, 16, 16, 16, 16, 16, 16, 16, 16,
-                                          16)};
+      return wasm_i8x16_shuffle(v.raw, zero, 12, 13, 14, 15, 16, 16, 16, 16, 16,
+                                16, 16, 16, 16, 16, 16, 16);
 
     case 13:
-      return Vec128<T>{wasm_v8x16_shuffle(v.raw, zero, 13, 14, 15, 16, 16, 16,
-                                          16, 16, 16, 16, 16, 16, 16, 16, 16,
-                                          16)};
+      return wasm_i8x16_shuffle(v.raw, zero, 13, 14, 15, 16, 16, 16, 16, 16, 16,
+                                16, 16, 16, 16, 16, 16, 16);
 
     case 14:
-      return Vec128<T>{wasm_v8x16_shuffle(v.raw, zero, 14, 15, 16, 16, 16, 16,
-                                          16, 16, 16, 16, 16, 16, 16, 16, 16,
-                                          16)};
+      return wasm_i8x16_shuffle(v.raw, zero, 14, 15, 16, 16, 16, 16, 16, 16, 16,
+                                16, 16, 16, 16, 16, 16, 16);
 
     case 15:
-      return Vec128<T>{wasm_v8x16_shuffle(v.raw, zero, 15, 16, 16, 16, 16, 16,
-                                          16, 16, 16, 16, 16, 16, 16, 16, 16,
-                                          16)};
+      return wasm_i8x16_shuffle(v.raw, zero, 15, 16, 16, 16, 16, 16, 16, 16, 16,
+                                16, 16, 16, 16, 16, 16, 16);
+    case 16:
+      return zero;
   }
-  return Vec128<T>{zero};
 }
 
+}  // namespace detail
+
+// 0x01..0F, kBytes = 1 => 0x0001..0E
+template <int kBytes, typename T, size_t N>
+HWY_API Vec128<T, N> ShiftRightBytes(Simd<T, N> /* tag */, Vec128<T, N> v) {
+  // For partial vectors, clear upper lanes so we shift in zeros.
+  if (N != 16 / sizeof(T)) {
+    const Vec128<T> vfull{v.raw};
+    v = Vec128<T, N>{IfThenElseZero(FirstN(Full128<T>(), N), vfull).raw};
+  }
+  return Vec128<T, N>{detail::ShrBytes<kBytes>(v)};
+}
+
+// ------------------------------ ShiftRightLanes
 template <int kLanes, typename T, size_t N>
-HWY_API Vec128<T, N> ShiftRightLanes(const Vec128<T, N> v) {
-  const Simd<uint8_t, N * sizeof(T)> d8;
-  const Simd<T, N> d;
+HWY_API Vec128<T, N> ShiftRightLanes(Simd<T, N> d, const Vec128<T, N> v) {
+  const Repartition<uint8_t, decltype(d)> d8;
   return BitCast(d, ShiftRightBytes<kLanes * sizeof(T)>(BitCast(d8, v)));
 }
 
-// ------------------------------ Extract from 2x 128-bit at constant offset
+// ------------------------------ UpperHalf (ShiftRightBytes)
+
+// Full input: copy hi into lo (smaller instruction encoding than shifts).
+template <typename T>
+HWY_API Vec128<T, 8 / sizeof(T)> UpperHalf(Half<Full128<T>> /* tag */,
+                                           const Vec128<T> v) {
+  return Vec128<T, 8 / sizeof(T)>{wasm_i32x4_shuffle(v.raw, v.raw, 2, 3, 2, 3)};
+}
+HWY_API Vec128<float, 2> UpperHalf(Half<Full128<float>> /* tag */,
+                                   const Vec128<float> v) {
+  return Vec128<float, 2>{wasm_i32x4_shuffle(v.raw, v.raw, 2, 3, 2, 3)};
+}
 
-// Extracts 128 bits from <hi, lo> by skipping the least-significant kBytes.
-template <int kBytes, typename T>
-HWY_API Vec128<T> CombineShiftRightBytes(const Vec128<T> hi,
-                                         const Vec128<T> lo) {
+// Partial
+template <typename T, size_t N, HWY_IF_LE64(T, N)>
+HWY_API Vec128<T, (N + 1) / 2> UpperHalf(Half<Simd<T, N>> /* tag */,
+                                         Vec128<T, N> v) {
+  const Simd<T, N> d;
+  const auto vu = BitCast(RebindToUnsigned<decltype(d)>(), v);
+  const auto upper = BitCast(d, ShiftRightBytes<N * sizeof(T) / 2>(vu));
+  return Vec128<T, (N + 1) / 2>{upper.raw};
+}
+
+// ------------------------------ CombineShiftRightBytes
+
+template <int kBytes, typename T, class V = Vec128<T>>
+HWY_API V CombineShiftRightBytes(Full128<T> /* tag */, V hi, V lo) {
   static_assert(0 <= kBytes && kBytes <= 16, "Invalid kBytes");
   switch (kBytes) {
     case 0:
       return lo;
 
     case 1:
-      return Vec128<T>{wasm_v8x16_shuffle(lo.raw, hi.raw, 1, 2, 3, 4, 5, 6, 7,
-                                          8, 9, 10, 11, 12, 13, 14, 15, 16)};
+      return V{wasm_i8x16_shuffle(lo.raw, hi.raw, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10,
+                                  11, 12, 13, 14, 15, 16)};
 
     case 2:
-      return Vec128<T>{wasm_v8x16_shuffle(lo.raw, hi.raw, 2, 3, 4, 5, 6, 7, 8,
-                                          9, 10, 11, 12, 13, 14, 15, 16, 17)};
+      return V{wasm_i8x16_shuffle(lo.raw, hi.raw, 2, 3, 4, 5, 6, 7, 8, 9, 10,
+                                  11, 12, 13, 14, 15, 16, 17)};
 
     case 3:
-      return Vec128<T>{wasm_v8x16_shuffle(lo.raw, hi.raw, 3, 4, 5, 6, 7, 8, 9,
-                                          10, 11, 12, 13, 14, 15, 16, 17, 18)};
+      return V{wasm_i8x16_shuffle(lo.raw, hi.raw, 3, 4, 5, 6, 7, 8, 9, 10, 11,
+                                  12, 13, 14, 15, 16, 17, 18)};
 
     case 4:
-      return Vec128<T>{wasm_v8x16_shuffle(lo.raw, hi.raw, 4, 5, 6, 7, 8, 9, 10,
-                                          11, 12, 13, 14, 15, 16, 17, 18, 19)};
+      return V{wasm_i8x16_shuffle(lo.raw, hi.raw, 4, 5, 6, 7, 8, 9, 10, 11, 12,
+                                  13, 14, 15, 16, 17, 18, 19)};
 
     case 5:
-      return Vec128<T>{wasm_v8x16_shuffle(lo.raw, hi.raw, 5, 6, 7, 8, 9, 10, 11,
-                                          12, 13, 14, 15, 16, 17, 18, 19, 20)};
+      return V{wasm_i8x16_shuffle(lo.raw, hi.raw, 5, 6, 7, 8, 9, 10, 11, 12, 13,
+                                  14, 15, 16, 17, 18, 19, 20)};
 
     case 6:
-      return Vec128<T>{wasm_v8x16_shuffle(lo.raw, hi.raw, 6, 7, 8, 9, 10, 11,
-                                          12, 13, 14, 15, 16, 17, 18, 19, 20,
-                                          21)};
+      return V{wasm_i8x16_shuffle(lo.raw, hi.raw, 6, 7, 8, 9, 10, 11, 12, 13,
+                                  14, 15, 16, 17, 18, 19, 20, 21)};
 
     case 7:
-      return Vec128<T>{wasm_v8x16_shuffle(lo.raw, hi.raw, 7, 8, 9, 10, 11, 12,
-                                          13, 14, 15, 16, 17, 18, 19, 20, 21,
-                                          22)};
+      return V{wasm_i8x16_shuffle(lo.raw, hi.raw, 7, 8, 9, 10, 11, 12, 13, 14,
+                                  15, 16, 17, 18, 19, 20, 21, 22)};
 
     case 8:
-      return Vec128<T>{wasm_v8x16_shuffle(lo.raw, hi.raw, 8, 9, 10, 11, 12, 13,
-                                          14, 15, 16, 17, 18, 19, 20, 21, 22,
-                                          23)};
+      return V{wasm_i8x16_shuffle(lo.raw, hi.raw, 8, 9, 10, 11, 12, 13, 14, 15,
+                                  16, 17, 18, 19, 20, 21, 22, 23)};
 
     case 9:
-      return Vec128<T>{wasm_v8x16_shuffle(lo.raw, hi.raw, 9, 10, 11, 12, 13, 14,
-                                          15, 16, 17, 18, 19, 20, 21, 22, 23,
-                                          24)};
+      return V{wasm_i8x16_shuffle(lo.raw, hi.raw, 9, 10, 11, 12, 13, 14, 15, 16,
+                                  17, 18, 19, 20, 21, 22, 23, 24)};
 
     case 10:
-      return Vec128<T>{wasm_v8x16_shuffle(lo.raw, hi.raw, 10, 11, 12, 13, 14,
-                                          15, 16, 17, 18, 19, 20, 21, 22, 23,
-                                          24, 25)};
+      return V{wasm_i8x16_shuffle(lo.raw, hi.raw, 10, 11, 12, 13, 14, 15, 16,
+                                  17, 18, 19, 20, 21, 22, 23, 24, 25)};
 
     case 11:
-      return Vec128<T>{wasm_v8x16_shuffle(lo.raw, hi.raw, 11, 12, 13, 14, 15,
-                                          16, 17, 18, 19, 20, 21, 22, 23, 24,
-                                          25, 26)};
+      return V{wasm_i8x16_shuffle(lo.raw, hi.raw, 11, 12, 13, 14, 15, 16, 17,
+                                  18, 19, 20, 21, 22, 23, 24, 25, 26)};
 
     case 12:
-      return Vec128<T>{wasm_v8x16_shuffle(lo.raw, hi.raw, 12, 13, 14, 15, 16,
-                                          17, 18, 19, 20, 21, 22, 23, 24, 25,
-                                          26, 27)};
+      return V{wasm_i8x16_shuffle(lo.raw, hi.raw, 12, 13, 14, 15, 16, 17, 18,
+                                  19, 20, 21, 22, 23, 24, 25, 26, 27)};
 
     case 13:
-      return Vec128<T>{wasm_v8x16_shuffle(lo.raw, hi.raw, 13, 14, 15, 16, 17,
-                                          18, 19, 20, 21, 22, 23, 24, 25, 26,
-                                          27, 28)};
+      return V{wasm_i8x16_shuffle(lo.raw, hi.raw, 13, 14, 15, 16, 17, 18, 19,
+                                  20, 21, 22, 23, 24, 25, 26, 27, 28)};
 
     case 14:
-      return Vec128<T>{wasm_v8x16_shuffle(lo.raw, hi.raw, 14, 15, 16, 17, 18,
-                                          19, 20, 21, 22, 23, 24, 25, 26, 27,
-                                          28, 29)};
+      return V{wasm_i8x16_shuffle(lo.raw, hi.raw, 14, 15, 16, 17, 18, 19, 20,
+                                  21, 22, 23, 24, 25, 26, 27, 28, 29)};
 
     case 15:
-      return Vec128<T>{wasm_v8x16_shuffle(lo.raw, hi.raw, 15, 16, 17, 18, 19,
-                                          20, 21, 22, 23, 24, 25, 26, 27, 28,
-                                          29, 30)};
+      return V{wasm_i8x16_shuffle(lo.raw, hi.raw, 15, 16, 17, 18, 19, 20, 21,
+                                  22, 23, 24, 25, 26, 27, 28, 29, 30)};
   }
   return hi;
 }
 
+template <int kBytes, typename T, size_t N, HWY_IF_LE64(T, N),
+          class V = Vec128<T, N>>
+HWY_API V CombineShiftRightBytes(Simd<T, N> d, V hi, V lo) {
+  constexpr size_t kSize = N * sizeof(T);
+  static_assert(0 < kBytes && kBytes < kSize, "kBytes invalid");
+  const Repartition<uint8_t, decltype(d)> d8;
+  const Full128<uint8_t> d_full8;
+  using V8 = VFromD<decltype(d_full8)>;
+  const V8 hi8{BitCast(d8, hi).raw};
+  // Move into most-significant bytes
+  const V8 lo8 = ShiftLeftBytes<16 - kSize>(V8{BitCast(d8, lo).raw});
+  const V8 r = CombineShiftRightBytes<16 - kSize + kBytes>(d_full8, hi8, lo8);
+  return V{BitCast(Full128<T>(), r).raw};
+}
+
 // ------------------------------ Broadcast/splat any lane
 
 // Unsigned
 template <int kLane, size_t N>
 HWY_API Vec128<uint16_t, N> Broadcast(const Vec128<uint16_t, N> v) {
   static_assert(0 <= kLane && kLane < N, "Invalid lane");
-  return Vec128<uint16_t, N>{wasm_v16x8_shuffle(
+  return Vec128<uint16_t, N>{wasm_i16x8_shuffle(
       v.raw, v.raw, kLane, kLane, kLane, kLane, kLane, kLane, kLane, kLane)};
 }
 template <int kLane, size_t N>
 HWY_API Vec128<uint32_t, N> Broadcast(const Vec128<uint32_t, N> v) {
   static_assert(0 <= kLane && kLane < N, "Invalid lane");
   return Vec128<uint32_t, N>{
-      wasm_v32x4_shuffle(v.raw, v.raw, kLane, kLane, kLane, kLane)};
+      wasm_i32x4_shuffle(v.raw, v.raw, kLane, kLane, kLane, kLane)};
 }
 
 // Signed
 template <int kLane, size_t N>
 HWY_API Vec128<int16_t, N> Broadcast(const Vec128<int16_t, N> v) {
   static_assert(0 <= kLane && kLane < N, "Invalid lane");
-  return Vec128<int16_t, N>{wasm_v16x8_shuffle(
+  return Vec128<int16_t, N>{wasm_i16x8_shuffle(
       v.raw, v.raw, kLane, kLane, kLane, kLane, kLane, kLane, kLane, kLane)};
 }
 template <int kLane, size_t N>
 HWY_API Vec128<int32_t, N> Broadcast(const Vec128<int32_t, N> v) {
   static_assert(0 <= kLane && kLane < N, "Invalid lane");
   return Vec128<int32_t, N>{
-      wasm_v32x4_shuffle(v.raw, v.raw, kLane, kLane, kLane, kLane)};
+      wasm_i32x4_shuffle(v.raw, v.raw, kLane, kLane, kLane, kLane)};
 }
 
 // Float
@@ -1792,22 +1877,22 @@ template <int kLane, size_t N>
 HWY_API Vec128<float, N> Broadcast(const Vec128<float, N> v) {
   static_assert(0 <= kLane && kLane < N, "Invalid lane");
   return Vec128<float, N>{
-      wasm_v32x4_shuffle(v.raw, v.raw, kLane, kLane, kLane, kLane)};
+      wasm_i32x4_shuffle(v.raw, v.raw, kLane, kLane, kLane, kLane)};
 }
 
-// ------------------------------ Shuffle bytes with variable indices
+// ------------------------------ TableLookupBytes
 
 // Returns vector of bytes[from[i]]. "from" is also interpreted as bytes, i.e.
 // lane indices in [0, 16).
-template <typename T, size_t N>
-HWY_API Vec128<T, N> TableLookupBytes(const Vec128<T, N> bytes,
-                                      const Vec128<T, N> from) {
+template <typename T, size_t N, typename TI, size_t NI>
+HWY_API Vec128<TI, NI> TableLookupBytes(const Vec128<T, N> bytes,
+                                        const Vec128<TI, NI> from) {
 // Not yet available in all engines, see
 // https://github.com/WebAssembly/simd/blob/bdcc304b2d379f4601c2c44ea9b44ed9484fde7e/proposals/simd/ImplementationStatus.md
 // V8 implementation of this had a bug, fixed on 2021-04-03:
 // https://chromium-review.googlesource.com/c/v8/v8/+/2822951
 #if 0
-  return Vec128<T, N>{wasm_v8x16_swizzle(bytes.raw, from.raw)};
+  return Vec128<TI, NI>{wasm_i8x16_swizzle(bytes.raw, from.raw)};
 #else
   alignas(16) uint8_t control[16];
   alignas(16) uint8_t input[16];
@@ -1817,10 +1902,23 @@ HWY_API Vec128<T, N> TableLookupBytes(const Vec128<T, N> bytes,
   for (size_t i = 0; i < 16; ++i) {
     output[i] = control[i] < 16 ? input[control[i]] : 0;
   }
-  return Vec128<T, N>{wasm_v128_load(output)};
+  return Vec128<TI, NI>{wasm_v128_load(output)};
 #endif
 }
 
+template <typename T, size_t N, typename TI, size_t NI>
+HWY_API Vec128<TI, NI> TableLookupBytesOr0(const Vec128<T, N> bytes,
+                                           const Vec128<TI, NI> from) {
+  const Simd<TI, NI> d;
+  // Mask size must match vector type, so cast everything to this type.
+  Repartition<int8_t, decltype(d)> di8;
+  Repartition<int8_t, Simd<T, N>> d_bytes8;
+  const auto msb = BitCast(di8, from) < Zero(di8);
+  const auto lookup =
+      TableLookupBytes(BitCast(d_bytes8, bytes), BitCast(di8, from));
+  return BitCast(d, IfThenZeroElse(msb, lookup));
+}
+
 // ------------------------------ Hard-coded shuffles
 
 // Notation: let Vec128<int32_t> have lanes 3,2,1,0 (0 is least-significant).
@@ -1830,56 +1928,56 @@ HWY_API Vec128<T, N> TableLookupBytes(const Vec128<T, N> bytes,
 
 // Swap 32-bit halves in 64-bit halves.
 HWY_API Vec128<uint32_t> Shuffle2301(const Vec128<uint32_t> v) {
-  return Vec128<uint32_t>{wasm_v32x4_shuffle(v.raw, v.raw, 1, 0, 3, 2)};
+  return Vec128<uint32_t>{wasm_i32x4_shuffle(v.raw, v.raw, 1, 0, 3, 2)};
 }
 HWY_API Vec128<int32_t> Shuffle2301(const Vec128<int32_t> v) {
-  return Vec128<int32_t>{wasm_v32x4_shuffle(v.raw, v.raw, 1, 0, 3, 2)};
+  return Vec128<int32_t>{wasm_i32x4_shuffle(v.raw, v.raw, 1, 0, 3, 2)};
 }
 HWY_API Vec128<float> Shuffle2301(const Vec128<float> v) {
-  return Vec128<float>{wasm_v32x4_shuffle(v.raw, v.raw, 1, 0, 3, 2)};
+  return Vec128<float>{wasm_i32x4_shuffle(v.raw, v.raw, 1, 0, 3, 2)};
 }
 
 // Swap 64-bit halves
 HWY_API Vec128<uint32_t> Shuffle1032(const Vec128<uint32_t> v) {
-  return Vec128<uint32_t>{wasm_v64x2_shuffle(v.raw, v.raw, 1, 0)};
+  return Vec128<uint32_t>{wasm_i64x2_shuffle(v.raw, v.raw, 1, 0)};
 }
 HWY_API Vec128<int32_t> Shuffle1032(const Vec128<int32_t> v) {
-  return Vec128<int32_t>{wasm_v64x2_shuffle(v.raw, v.raw, 1, 0)};
+  return Vec128<int32_t>{wasm_i64x2_shuffle(v.raw, v.raw, 1, 0)};
 }
 HWY_API Vec128<float> Shuffle1032(const Vec128<float> v) {
-  return Vec128<float>{wasm_v64x2_shuffle(v.raw, v.raw, 1, 0)};
+  return Vec128<float>{wasm_i64x2_shuffle(v.raw, v.raw, 1, 0)};
 }
 
 // Rotate right 32 bits
 HWY_API Vec128<uint32_t> Shuffle0321(const Vec128<uint32_t> v) {
-  return Vec128<uint32_t>{wasm_v32x4_shuffle(v.raw, v.raw, 1, 2, 3, 0)};
+  return Vec128<uint32_t>{wasm_i32x4_shuffle(v.raw, v.raw, 1, 2, 3, 0)};
 }
 HWY_API Vec128<int32_t> Shuffle0321(const Vec128<int32_t> v) {
-  return Vec128<int32_t>{wasm_v32x4_shuffle(v.raw, v.raw, 1, 2, 3, 0)};
+  return Vec128<int32_t>{wasm_i32x4_shuffle(v.raw, v.raw, 1, 2, 3, 0)};
 }
 HWY_API Vec128<float> Shuffle0321(const Vec128<float> v) {
-  return Vec128<float>{wasm_v32x4_shuffle(v.raw, v.raw, 1, 2, 3, 0)};
+  return Vec128<float>{wasm_i32x4_shuffle(v.raw, v.raw, 1, 2, 3, 0)};
 }
 // Rotate left 32 bits
 HWY_API Vec128<uint32_t> Shuffle2103(const Vec128<uint32_t> v) {
-  return Vec128<uint32_t>{wasm_v32x4_shuffle(v.raw, v.raw, 3, 0, 1, 2)};
+  return Vec128<uint32_t>{wasm_i32x4_shuffle(v.raw, v.raw, 3, 0, 1, 2)};
 }
 HWY_API Vec128<int32_t> Shuffle2103(const Vec128<int32_t> v) {
-  return Vec128<int32_t>{wasm_v32x4_shuffle(v.raw, v.raw, 3, 0, 1, 2)};
+  return Vec128<int32_t>{wasm_i32x4_shuffle(v.raw, v.raw, 3, 0, 1, 2)};
 }
 HWY_API Vec128<float> Shuffle2103(const Vec128<float> v) {
-  return Vec128<float>{wasm_v32x4_shuffle(v.raw, v.raw, 3, 0, 1, 2)};
+  return Vec128<float>{wasm_i32x4_shuffle(v.raw, v.raw, 3, 0, 1, 2)};
 }
 
 // Reverse
 HWY_API Vec128<uint32_t> Shuffle0123(const Vec128<uint32_t> v) {
-  return Vec128<uint32_t>{wasm_v32x4_shuffle(v.raw, v.raw, 3, 2, 1, 0)};
+  return Vec128<uint32_t>{wasm_i32x4_shuffle(v.raw, v.raw, 3, 2, 1, 0)};
 }
 HWY_API Vec128<int32_t> Shuffle0123(const Vec128<int32_t> v) {
-  return Vec128<int32_t>{wasm_v32x4_shuffle(v.raw, v.raw, 3, 2, 1, 0)};
+  return Vec128<int32_t>{wasm_i32x4_shuffle(v.raw, v.raw, 3, 2, 1, 0)};
 }
 HWY_API Vec128<float> Shuffle0123(const Vec128<float> v) {
-  return Vec128<float>{wasm_v32x4_shuffle(v.raw, v.raw, 3, 2, 1, 0)};
+  return Vec128<float>{wasm_i32x4_shuffle(v.raw, v.raw, 3, 2, 1, 0)};
 }
 
 // ------------------------------ TableLookupLanes
@@ -1928,176 +2026,274 @@ HWY_API Vec128<float, N> TableLookupLanes(const Vec128<float, N> v,
                  TableLookupBytes(BitCast(di, v), Vec128<int32_t, N>{idx.raw}));
 }
 
-// ------------------------------ Zip lanes
-
-// Same as Interleave*, except that the return lanes are double-width integers;
-// this is necessary because the single-lane scalar cannot return two values.
+// ------------------------------ InterleaveLower
 
 template <size_t N>
-HWY_API Vec128<uint16_t, (N + 1) / 2> ZipLower(const Vec128<uint8_t, N> a,
-                                               const Vec128<uint8_t, N> b) {
-  return Vec128<uint16_t, (N + 1) / 2>{wasm_v8x16_shuffle(
+HWY_API Vec128<uint8_t, N> InterleaveLower(Vec128<uint8_t, N> a,
+                                           Vec128<uint8_t, N> b) {
+  return Vec128<uint8_t, N>{wasm_i8x16_shuffle(
       a.raw, b.raw, 0, 16, 1, 17, 2, 18, 3, 19, 4, 20, 5, 21, 6, 22, 7, 23)};
 }
 template <size_t N>
-HWY_API Vec128<uint32_t, (N + 1) / 2> ZipLower(const Vec128<uint16_t, N> a,
-                                               const Vec128<uint16_t, N> b) {
-  return Vec128<uint32_t, (N + 1) / 2>{
-      wasm_v16x8_shuffle(a.raw, b.raw, 0, 8, 1, 9, 2, 10, 3, 11)};
+HWY_API Vec128<uint16_t, N> InterleaveLower(Vec128<uint16_t, N> a,
+                                            Vec128<uint16_t, N> b) {
+  return Vec128<uint16_t, N>{
+      wasm_i16x8_shuffle(a.raw, b.raw, 0, 8, 1, 9, 2, 10, 3, 11)};
+}
+template <size_t N>
+HWY_API Vec128<uint32_t, N> InterleaveLower(Vec128<uint32_t, N> a,
+                                            Vec128<uint32_t, N> b) {
+  return Vec128<uint32_t, N>{wasm_i32x4_shuffle(a.raw, b.raw, 0, 4, 1, 5)};
+}
+template <size_t N>
+HWY_API Vec128<uint64_t, N> InterleaveLower(Vec128<uint64_t, N> a,
+                                            Vec128<uint64_t, N> b) {
+  return Vec128<uint64_t, N>{wasm_i64x2_shuffle(a.raw, b.raw, 0, 2)};
 }
 
 template <size_t N>
-HWY_API Vec128<int16_t, (N + 1) / 2> ZipLower(const Vec128<int8_t, N> a,
-                                              const Vec128<int8_t, N> b) {
-  return Vec128<int16_t, (N + 1) / 2>{wasm_v8x16_shuffle(
+HWY_API Vec128<int8_t, N> InterleaveLower(Vec128<int8_t, N> a,
+                                          Vec128<int8_t, N> b) {
+  return Vec128<int8_t, N>{wasm_i8x16_shuffle(
       a.raw, b.raw, 0, 16, 1, 17, 2, 18, 3, 19, 4, 20, 5, 21, 6, 22, 7, 23)};
 }
 template <size_t N>
-HWY_API Vec128<int32_t, (N + 1) / 2> ZipLower(const Vec128<int16_t, N> a,
-                                              const Vec128<int16_t, N> b) {
-  return Vec128<int32_t, (N + 1) / 2>{
-      wasm_v16x8_shuffle(a.raw, b.raw, 0, 8, 1, 9, 2, 10, 3, 11)};
+HWY_API Vec128<int16_t, N> InterleaveLower(Vec128<int16_t, N> a,
+                                           Vec128<int16_t, N> b) {
+  return Vec128<int16_t, N>{
+      wasm_i16x8_shuffle(a.raw, b.raw, 0, 8, 1, 9, 2, 10, 3, 11)};
+}
+template <size_t N>
+HWY_API Vec128<int32_t, N> InterleaveLower(Vec128<int32_t, N> a,
+                                           Vec128<int32_t, N> b) {
+  return Vec128<int32_t, N>{wasm_i32x4_shuffle(a.raw, b.raw, 0, 4, 1, 5)};
+}
+template <size_t N>
+HWY_API Vec128<int64_t, N> InterleaveLower(Vec128<int64_t, N> a,
+                                           Vec128<int64_t, N> b) {
+  return Vec128<int64_t, N>{wasm_i64x2_shuffle(a.raw, b.raw, 0, 2)};
 }
 
 template <size_t N>
-HWY_API Vec128<uint16_t, N / 2> ZipUpper(const Vec128<uint8_t, N> a,
-                                         const Vec128<uint8_t, N> b) {
-  return Vec128<uint16_t, N / 2>{wasm_v8x16_shuffle(a.raw, b.raw, 8, 24, 9, 25,
-                                                    10, 26, 11, 27, 12, 28, 13,
-                                                    29, 14, 30, 15, 31)};
+HWY_API Vec128<float, N> InterleaveLower(Vec128<float, N> a,
+                                         Vec128<float, N> b) {
+  return Vec128<float, N>{wasm_i32x4_shuffle(a.raw, b.raw, 0, 4, 1, 5)};
 }
+
+// Additional overload for the optional Simd<> tag.
+template <typename T, size_t N, class V = Vec128<T, N>>
+HWY_API V InterleaveLower(Simd<T, N> /* tag */, V a, V b) {
+  return InterleaveLower(a, b);
+}
+
+// ------------------------------ InterleaveUpper (UpperHalf)
+
+// All functions inside detail lack the required D parameter.
+namespace detail {
+
 template <size_t N>
-HWY_API Vec128<uint32_t, N / 2> ZipUpper(const Vec128<uint16_t, N> a,
-                                         const Vec128<uint16_t, N> b) {
-  return Vec128<uint32_t, N / 2>{
-      wasm_v16x8_shuffle(a.raw, b.raw, 4, 12, 5, 13, 6, 14, 7, 15)};
+HWY_API Vec128<uint8_t, N> InterleaveUpper(Vec128<uint8_t, N> a,
+                                           Vec128<uint8_t, N> b) {
+  return Vec128<uint8_t, N>{wasm_i8x16_shuffle(a.raw, b.raw, 8, 24, 9, 25, 10,
+                                               26, 11, 27, 12, 28, 13, 29, 14,
+                                               30, 15, 31)};
+}
+template <size_t N>
+HWY_API Vec128<uint16_t, N> InterleaveUpper(Vec128<uint16_t, N> a,
+                                            Vec128<uint16_t, N> b) {
+  return Vec128<uint16_t, N>{
+      wasm_i16x8_shuffle(a.raw, b.raw, 4, 12, 5, 13, 6, 14, 7, 15)};
+}
+template <size_t N>
+HWY_API Vec128<uint32_t, N> InterleaveUpper(Vec128<uint32_t, N> a,
+                                            Vec128<uint32_t, N> b) {
+  return Vec128<uint32_t, N>{wasm_i32x4_shuffle(a.raw, b.raw, 2, 6, 3, 7)};
+}
+template <size_t N>
+HWY_API Vec128<uint64_t, N> InterleaveUpper(Vec128<uint64_t, N> a,
+                                            Vec128<uint64_t, N> b) {
+  return Vec128<uint64_t, N>{wasm_i64x2_shuffle(a.raw, b.raw, 1, 3)};
 }
 
 template <size_t N>
-HWY_API Vec128<int16_t, N / 2> ZipUpper(const Vec128<int8_t, N> a,
-                                        const Vec128<int8_t, N> b) {
-  return Vec128<int16_t, N / 2>{wasm_v8x16_shuffle(a.raw, b.raw, 8, 24, 9, 25,
-                                                   10, 26, 11, 27, 12, 28, 13,
-                                                   29, 14, 30, 15, 31)};
+HWY_API Vec128<int8_t, N> InterleaveUpper(Vec128<int8_t, N> a,
+                                          Vec128<int8_t, N> b) {
+  return Vec128<int8_t, N>{wasm_i8x16_shuffle(a.raw, b.raw, 8, 24, 9, 25, 10,
+                                              26, 11, 27, 12, 28, 13, 29, 14,
+                                              30, 15, 31)};
 }
 template <size_t N>
-HWY_API Vec128<int32_t, N / 2> ZipUpper(const Vec128<int16_t, N> a,
-                                        const Vec128<int16_t, N> b) {
-  return Vec128<int32_t, N / 2>{
-      wasm_v16x8_shuffle(a.raw, b.raw, 4, 12, 5, 13, 6, 14, 7, 15)};
+HWY_API Vec128<int16_t, N> InterleaveUpper(Vec128<int16_t, N> a,
+                                           Vec128<int16_t, N> b) {
+  return Vec128<int16_t, N>{
+      wasm_i16x8_shuffle(a.raw, b.raw, 4, 12, 5, 13, 6, 14, 7, 15)};
+}
+template <size_t N>
+HWY_API Vec128<int32_t, N> InterleaveUpper(Vec128<int32_t, N> a,
+                                           Vec128<int32_t, N> b) {
+  return Vec128<int32_t, N>{wasm_i32x4_shuffle(a.raw, b.raw, 2, 6, 3, 7)};
+}
+template <size_t N>
+HWY_API Vec128<int64_t, N> InterleaveUpper(Vec128<int64_t, N> a,
+                                           Vec128<int64_t, N> b) {
+  return Vec128<int64_t, N>{wasm_i64x2_shuffle(a.raw, b.raw, 1, 3)};
 }
 
-// ------------------------------ Interleave lanes
+template <size_t N>
+HWY_API Vec128<float, N> InterleaveUpper(Vec128<float, N> a,
+                                         Vec128<float, N> b) {
+  return Vec128<float, N>{wasm_i32x4_shuffle(a.raw, b.raw, 2, 6, 3, 7)};
+}
 
-// Interleaves lanes from halves of the 128-bit blocks of "a" (which provides
-// the least-significant lane) and "b". To concatenate two half-width integers
-// into one, use ZipLower/Upper instead (also works with scalar).
+}  // namespace detail
 
-template <typename T>
-HWY_API Vec128<T> InterleaveLower(const Vec128<T> a, const Vec128<T> b) {
-  return Vec128<T>{ZipLower(a, b).raw};
+// Full
+template <typename T, class V = Vec128<T>>
+HWY_API V InterleaveUpper(Full128<T> /* tag */, V a, V b) {
+  return detail::InterleaveUpper(a, b);
 }
-template <>
-HWY_INLINE Vec128<uint32_t> InterleaveLower<uint32_t>(
-    const Vec128<uint32_t> a, const Vec128<uint32_t> b) {
-  return Vec128<uint32_t>{wasm_v32x4_shuffle(a.raw, b.raw, 0, 4, 1, 5)};
+
+// Partial
+template <typename T, size_t N, HWY_IF_LE64(T, N), class V = Vec128<T, N>>
+HWY_API V InterleaveUpper(Simd<T, N> d, V a, V b) {
+  const Half<decltype(d)> d2;
+  return InterleaveLower(d, V{UpperHalf(d2, a).raw}, V{UpperHalf(d2, b).raw});
 }
-template <>
-HWY_INLINE Vec128<int32_t> InterleaveLower<int32_t>(const Vec128<int32_t> a,
-                                                    const Vec128<int32_t> b) {
-  return Vec128<int32_t>{wasm_v32x4_shuffle(a.raw, b.raw, 0, 4, 1, 5)};
+
+// ------------------------------ ZipLower/ZipUpper (InterleaveLower)
+
+// Same as Interleave*, except that the return lanes are double-width integers;
+// this is necessary because the single-lane scalar cannot return two values.
+template <typename T, size_t N, class DW = RepartitionToWide<Simd<T, N>>>
+HWY_API VFromD<DW> ZipLower(Vec128<T, N> a, Vec128<T, N> b) {
+  return BitCast(DW(), InterleaveLower(a, b));
 }
-template <>
-HWY_INLINE Vec128<float> InterleaveLower<float>(const Vec128<float> a,
-                                                const Vec128<float> b) {
-  return Vec128<float>{wasm_v32x4_shuffle(a.raw, b.raw, 0, 4, 1, 5)};
+template <typename T, size_t N, class D = Simd<T, N>,
+          class DW = RepartitionToWide<D>>
+HWY_API VFromD<DW> ZipLower(DW dw, Vec128<T, N> a, Vec128<T, N> b) {
+  return BitCast(dw, InterleaveLower(D(), a, b));
 }
 
-template <typename T>
-HWY_API Vec128<T> InterleaveUpper(const Vec128<T> a, const Vec128<T> b) {
-  return Vec128<T>{ZipUpper(a, b).raw};
+template <typename T, size_t N, class D = Simd<T, N>,
+          class DW = RepartitionToWide<D>>
+HWY_API VFromD<DW> ZipUpper(DW dw, Vec128<T, N> a, Vec128<T, N> b) {
+  return BitCast(dw, InterleaveUpper(D(), a, b));
 }
-template <>
-HWY_INLINE Vec128<uint32_t> InterleaveUpper<uint32_t>(
-    const Vec128<uint32_t> a, const Vec128<uint32_t> b) {
-  return Vec128<uint32_t>{wasm_v32x4_shuffle(a.raw, b.raw, 2, 6, 3, 7)};
-}
-template <>
-HWY_INLINE Vec128<int32_t> InterleaveUpper<int32_t>(const Vec128<int32_t> a,
-                                                    const Vec128<int32_t> b) {
-  return Vec128<int32_t>{wasm_v32x4_shuffle(a.raw, b.raw, 2, 6, 3, 7)};
+
+// ================================================== COMBINE
+
+// ------------------------------ Combine (InterleaveLower)
+
+// N = N/2 + N/2 (upper half undefined)
+template <typename T, size_t N>
+HWY_API Vec128<T, N> Combine(Simd<T, N> d, Vec128<T, N / 2> hi_half,
+                             Vec128<T, N / 2> lo_half) {
+  const Half<decltype(d)> d2;
+  const RebindToUnsigned<decltype(d2)> du2;
+  // Treat half-width input as one lane, and expand to two lanes.
+  using VU = Vec128<UnsignedFromSize<N * sizeof(T) / 2>, 2>;
+  const VU lo{BitCast(du2, lo_half).raw};
+  const VU hi{BitCast(du2, hi_half).raw};
+  return BitCast(d, InterleaveLower(lo, hi));
 }
-template <>
-HWY_INLINE Vec128<float> InterleaveUpper<float>(const Vec128<float> a,
-                                                const Vec128<float> b) {
-  return Vec128<float>{wasm_v32x4_shuffle(a.raw, b.raw, 2, 6, 3, 7)};
+
+// ------------------------------ ZeroExtendVector (Combine, IfThenElseZero)
+
+template <typename T, size_t N>
+HWY_API Vec128<T, N> ZeroExtendVector(Simd<T, N> d, Vec128<T, N / 2> lo) {
+  return IfThenElseZero(FirstN(d, N / 2), Vec128<T, N>{lo.raw});
 }
 
-// ------------------------------ Blocks
+// ------------------------------ ConcatLowerLower
 
 // hiH,hiL loH,loL |-> hiL,loL (= lower halves)
 template <typename T>
-HWY_API Vec128<T> ConcatLowerLower(const Vec128<T> hi, const Vec128<T> lo) {
-  return Vec128<T>{wasm_v64x2_shuffle(lo.raw, hi.raw, 0, 2)};
+HWY_API Vec128<T> ConcatLowerLower(Full128<T> /* tag */, const Vec128<T> hi,
+                                   const Vec128<T> lo) {
+  return Vec128<T>{wasm_i64x2_shuffle(lo.raw, hi.raw, 0, 2)};
+}
+template <typename T, size_t N, HWY_IF_LE64(T, N)>
+HWY_API Vec128<T, N> ConcatLowerLower(Simd<T, N> d, const Vec128<T, N> hi,
+                                      const Vec128<T, N> lo) {
+  const Half<decltype(d)> d2;
+  return Combine(LowerHalf(d2, hi), LowerHalf(d2, lo));
 }
 
-// hiH,hiL loH,loL |-> hiH,loH (= upper halves)
+// ------------------------------ ConcatUpperUpper
+
 template <typename T>
-HWY_API Vec128<T> ConcatUpperUpper(const Vec128<T> hi, const Vec128<T> lo) {
-  return Vec128<T>{wasm_v64x2_shuffle(lo.raw, hi.raw, 1, 3)};
+HWY_API Vec128<T> ConcatUpperUpper(Full128<T> /* tag */, const Vec128<T> hi,
+                                   const Vec128<T> lo) {
+  return Vec128<T>{wasm_i64x2_shuffle(lo.raw, hi.raw, 1, 3)};
+}
+template <typename T, size_t N, HWY_IF_LE64(T, N)>
+HWY_API Vec128<T, N> ConcatUpperUpper(Simd<T, N> d, const Vec128<T, N> hi,
+                                      const Vec128<T, N> lo) {
+  const Half<decltype(d)> d2;
+  return Combine(UpperHalf(d2, hi), UpperHalf(d2, lo));
 }
 
-// hiH,hiL loH,loL |-> hiL,loH (= inner halves)
+// ------------------------------ ConcatLowerUpper
+
 template <typename T>
-HWY_API Vec128<T> ConcatLowerUpper(const Vec128<T> hi, const Vec128<T> lo) {
-  return CombineShiftRightBytes<8>(hi, lo);
+HWY_API Vec128<T> ConcatLowerUpper(Full128<T> d, const Vec128<T> hi,
+                                   const Vec128<T> lo) {
+  return CombineShiftRightBytes<8>(d, hi, lo);
+}
+template <typename T, size_t N, HWY_IF_LE64(T, N)>
+HWY_API Vec128<T, N> ConcatLowerUpper(Simd<T, N> d, const Vec128<T, N> hi,
+                                      const Vec128<T, N> lo) {
+  const Half<decltype(d)> d2;
+  return Combine(LowerHalf(d2, hi), UpperHalf(d2, lo));
 }
 
-// hiH,hiL loH,loL |-> hiH,loL (= outer halves)
-template <typename T>
-HWY_API Vec128<T> ConcatUpperLower(const Vec128<T> hi, const Vec128<T> lo) {
-  return Vec128<T>{wasm_v64x2_shuffle(lo.raw, hi.raw, 0, 3)};
+// ------------------------------ ConcatUpperLower
+template <typename T, size_t N>
+HWY_API Vec128<T, N> ConcatUpperLower(Simd<T, N> d, const Vec128<T, N> hi,
+                                      const Vec128<T, N> lo) {
+  return IfThenElse(FirstN(d, Lanes(d) / 2), lo, hi);
 }
 
-// ------------------------------ Odd/even lanes
+// ------------------------------ OddEven
 
-namespace {
+namespace detail {
 
-template <typename T>
-HWY_API Vec128<T> odd_even_impl(hwy::SizeTag<1> /* tag */, const Vec128<T> a,
-                                const Vec128<T> b) {
-  const Full128<T> d;
-  const Full128<uint8_t> d8;
+template <typename T, size_t N>
+HWY_INLINE Vec128<T, N> OddEven(hwy::SizeTag<1> /* tag */, const Vec128<T, N> a,
+                                const Vec128<T, N> b) {
+  const Simd<T, N> d;
+  const Repartition<uint8_t, decltype(d)> d8;
   alignas(16) constexpr uint8_t mask[16] = {0xFF, 0, 0xFF, 0, 0xFF, 0, 0xFF, 0,
                                             0xFF, 0, 0xFF, 0, 0xFF, 0, 0xFF, 0};
   return IfThenElse(MaskFromVec(BitCast(d, Load(d8, mask))), b, a);
 }
-template <typename T>
-HWY_API Vec128<T> odd_even_impl(hwy::SizeTag<2> /* tag */, const Vec128<T> a,
-                                const Vec128<T> b) {
-  return Vec128<T>{wasm_v16x8_shuffle(a.raw, b.raw, 8, 1, 10, 3, 12, 5, 14, 7)};
+template <typename T, size_t N>
+HWY_INLINE Vec128<T, N> OddEven(hwy::SizeTag<2> /* tag */, const Vec128<T, N> a,
+                                const Vec128<T, N> b) {
+  return Vec128<T, N>{
+      wasm_i16x8_shuffle(a.raw, b.raw, 8, 1, 10, 3, 12, 5, 14, 7)};
 }
-template <typename T>
-HWY_API Vec128<T> odd_even_impl(hwy::SizeTag<4> /* tag */, const Vec128<T> a,
-                                const Vec128<T> b) {
-  return Vec128<T>{wasm_v32x4_shuffle(a.raw, b.raw, 4, 1, 6, 3)};
+template <typename T, size_t N>
+HWY_INLINE Vec128<T, N> OddEven(hwy::SizeTag<4> /* tag */, const Vec128<T, N> a,
+                                const Vec128<T, N> b) {
+  return Vec128<T, N>{wasm_i32x4_shuffle(a.raw, b.raw, 4, 1, 6, 3)};
+}
+template <typename T, size_t N>
+HWY_INLINE Vec128<T, N> OddEven(hwy::SizeTag<8> /* tag */, const Vec128<T, N> a,
+                                const Vec128<T, N> b) {
+  return Vec128<T, N>{wasm_i64x2_shuffle(a.raw, b.raw, 2, 1)};
 }
-// TODO(eustas): implement
-// template <typename T>
-// HWY_API Vec128<T> odd_even_impl(hwy::SizeTag<8> /* tag */,
-//                                                 const Vec128<T> a,
-//                                                 const Vec128<T> b)
 
-}  // namespace
+}  // namespace detail
 
-template <typename T>
-HWY_API Vec128<T> OddEven(const Vec128<T> a, const Vec128<T> b) {
-  return odd_even_impl(hwy::SizeTag<sizeof(T)>(), a, b);
+template <typename T, size_t N>
+HWY_API Vec128<T, N> OddEven(const Vec128<T, N> a, const Vec128<T, N> b) {
+  return detail::OddEven(hwy::SizeTag<sizeof(T)>(), a, b);
 }
-template <>
-HWY_INLINE Vec128<float> OddEven<float>(const Vec128<float> a,
-                                        const Vec128<float> b) {
-  return Vec128<float>{wasm_v32x4_shuffle(a.raw, b.raw, 4, 1, 6, 3)};
+template <size_t N>
+HWY_API Vec128<float, N> OddEven(const Vec128<float, N> a,
+                                 const Vec128<float, N> b) {
+  return Vec128<float, N>{wasm_i32x4_shuffle(a.raw, b.raw, 4, 1, 6, 3)};
 }
 
 // ================================================== CONVERT
@@ -2108,69 +2304,63 @@ HWY_INLINE Vec128<float> OddEven<float>(const Vec128<float> a,
 template <size_t N>
 HWY_API Vec128<uint16_t, N> PromoteTo(Simd<uint16_t, N> /* tag */,
                                       const Vec128<uint8_t, N> v) {
-  return Vec128<uint16_t, N>{wasm_i16x8_widen_low_u8x16(v.raw)};
+  return Vec128<uint16_t, N>{wasm_u16x8_extend_low_u8x16(v.raw)};
 }
 template <size_t N>
 HWY_API Vec128<uint32_t, N> PromoteTo(Simd<uint32_t, N> /* tag */,
                                       const Vec128<uint8_t, N> v) {
   return Vec128<uint32_t, N>{
-      wasm_i32x4_widen_low_u16x8(wasm_i16x8_widen_low_u8x16(v.raw))};
+      wasm_u32x4_extend_low_u16x8(wasm_u16x8_extend_low_u8x16(v.raw))};
 }
 template <size_t N>
 HWY_API Vec128<int16_t, N> PromoteTo(Simd<int16_t, N> /* tag */,
                                      const Vec128<uint8_t, N> v) {
-  return Vec128<int16_t, N>{wasm_i16x8_widen_low_u8x16(v.raw)};
+  return Vec128<int16_t, N>{wasm_u16x8_extend_low_u8x16(v.raw)};
 }
 template <size_t N>
 HWY_API Vec128<int32_t, N> PromoteTo(Simd<int32_t, N> /* tag */,
                                      const Vec128<uint8_t, N> v) {
   return Vec128<int32_t, N>{
-      wasm_i32x4_widen_low_u16x8(wasm_i16x8_widen_low_u8x16(v.raw))};
+      wasm_u32x4_extend_low_u16x8(wasm_u16x8_extend_low_u8x16(v.raw))};
 }
 template <size_t N>
 HWY_API Vec128<uint32_t, N> PromoteTo(Simd<uint32_t, N> /* tag */,
                                       const Vec128<uint16_t, N> v) {
-  return Vec128<uint32_t, N>{wasm_i32x4_widen_low_u16x8(v.raw)};
+  return Vec128<uint32_t, N>{wasm_u32x4_extend_low_u16x8(v.raw)};
 }
 template <size_t N>
 HWY_API Vec128<int32_t, N> PromoteTo(Simd<int32_t, N> /* tag */,
                                      const Vec128<uint16_t, N> v) {
-  return Vec128<int32_t, N>{wasm_i32x4_widen_low_u16x8(v.raw)};
+  return Vec128<int32_t, N>{wasm_u32x4_extend_low_u16x8(v.raw)};
 }
 
 // Signed: replicate sign bit.
 template <size_t N>
 HWY_API Vec128<int16_t, N> PromoteTo(Simd<int16_t, N> /* tag */,
                                      const Vec128<int8_t, N> v) {
-  return Vec128<int16_t, N>{wasm_i16x8_widen_low_i8x16(v.raw)};
+  return Vec128<int16_t, N>{wasm_i16x8_extend_low_i8x16(v.raw)};
 }
 template <size_t N>
 HWY_API Vec128<int32_t, N> PromoteTo(Simd<int32_t, N> /* tag */,
                                      const Vec128<int8_t, N> v) {
   return Vec128<int32_t, N>{
-      wasm_i32x4_widen_low_i16x8(wasm_i16x8_widen_low_i8x16(v.raw))};
+      wasm_i32x4_extend_low_i16x8(wasm_i16x8_extend_low_i8x16(v.raw))};
 }
 template <size_t N>
 HWY_API Vec128<int32_t, N> PromoteTo(Simd<int32_t, N> /* tag */,
                                      const Vec128<int16_t, N> v) {
-  return Vec128<int32_t, N>{wasm_i32x4_widen_low_i16x8(v.raw)};
+  return Vec128<int32_t, N>{wasm_i32x4_extend_low_i16x8(v.raw)};
 }
 
 template <size_t N>
-HWY_API Vec128<double, N> PromoteTo(Simd<double, N> df,
+HWY_API Vec128<double, N> PromoteTo(Simd<double, N> /* tag */,
                                     const Vec128<int32_t, N> v) {
-  // TODO(janwas): use https://github.com/WebAssembly/simd/pull/383
-  alignas(16) int32_t lanes[4];
-  Store(v, Simd<int32_t, N>(), lanes);
-  alignas(16) double lanes64[2];
-  lanes64[0] = lanes[0];
-  lanes64[1] = N >= 2 ? lanes[1] : 0.0;
-  return Load(df, lanes64);
+  return Vec128<double, N>{wasm_f64x2_convert_low_i32x4(v.raw)};
 }
 
 template <size_t N>
-HWY_INLINE Vec128<float, N> PromoteTo(Simd<float, N> /* tag */,
-                                      const Vec128<float16_t, N> v) {
+HWY_API Vec128<float, N> PromoteTo(Simd<float, N> /* tag */,
+                                   const Vec128<float16_t, N> v) {
   const Simd<int32_t, N> di32;
   const Simd<uint32_t, N> du32;
   const Simd<float, N> df32;
@@ -2232,19 +2422,14 @@ HWY_API Vec128<int8_t, N> DemoteTo(Simd<int8_t, N> /* tag */,
 }
 
 template <size_t N>
-HWY_API Vec128<int32_t, N> DemoteTo(Simd<int32_t, N> di,
+HWY_API Vec128<int32_t, N> DemoteTo(Simd<int32_t, N> /* di */,
                                     const Vec128<double, N> v) {
-  // TODO(janwas): use https://github.com/WebAssembly/simd/pull/383
-  alignas(16) double lanes64[2];
-  Store(v, Simd<double, N>(), lanes64);
-  alignas(16) int32_t lanes[4] = {static_cast<int32_t>(lanes64[0])};
-  if (N >= 2) lanes[1] = static_cast<int32_t>(lanes64[1]);
-  return Load(di, lanes);
+  return Vec128<int32_t, N>{wasm_i32x4_trunc_sat_f64x2_zero(v.raw)};
 }
 
 template <size_t N>
-HWY_INLINE Vec128<float16_t, N> DemoteTo(Simd<float16_t, N> /* tag */,
-                                         const Vec128<float, N> v) {
+HWY_API Vec128<float16_t, N> DemoteTo(Simd<float16_t, N> /* tag */,
+                                      const Vec128<float, N> v) {
   const Simd<int32_t, N> di;
   const Simd<uint32_t, N> du;
   const Simd<uint16_t, N> du16;
@@ -2291,7 +2476,7 @@ HWY_API Vec128<float, N> ConvertTo(Simd<float, N> /* tag */,
 template <size_t N>
 HWY_API Vec128<int32_t, N> ConvertTo(Simd<int32_t, N> /* tag */,
                                      const Vec128<float, N> v) {
-  return Vec128<int32_t, N>{wasm_i32x4_trunc_saturate_f32x4(v.raw)};
+  return Vec128<int32_t, N>{wasm_i32x4_trunc_sat_f32x4(v.raw)};
 }
 
 template <size_t N>
@@ -2305,9 +2490,10 @@ HWY_API Vec128<int32_t, N> NearestInt(const Vec128<float, N> v) {
 
 namespace detail {
 
-template <typename T, size_t N>
-HWY_API uint64_t BitsFromMask(hwy::SizeTag<1> /*tag*/,
-                              const Mask128<T, N> mask) {
+// Full
+template <typename T>
+HWY_INLINE uint64_t BitsFromMask(hwy::SizeTag<1> /*tag*/,
+                                 const Mask128<T> mask) {
   alignas(16) uint64_t lanes[2];
   wasm_v128_store(lanes, mask.raw);
 
@@ -2317,18 +2503,37 @@ HWY_API uint64_t BitsFromMask(hwy::SizeTag<1> /*tag*/,
   return (hi + lo);
 }
 
+// 64-bit
+template <typename T>
+HWY_INLINE uint64_t BitsFromMask(hwy::SizeTag<1> /*tag*/,
+                                 const Mask128<T, 8> mask) {
+  constexpr uint64_t kMagic = 0x103070F1F3F80ULL;
+  return (wasm_i64x2_extract_lane(mask.raw, 0) * kMagic) >> 56;
+}
+
+// 32-bit or less: need masking
+template <typename T, size_t N, HWY_IF_LE32(T, N)>
+HWY_INLINE uint64_t BitsFromMask(hwy::SizeTag<1> /*tag*/,
+                                 const Mask128<T, N> mask) {
+  uint64_t bytes = wasm_i64x2_extract_lane(mask.raw, 0);
+  // Clear potentially undefined bytes.
+  bytes &= (1ULL << (N * 8)) - 1;
+  constexpr uint64_t kMagic = 0x103070F1F3F80ULL;
+  return (bytes * kMagic) >> 56;
+}
+
 template <typename T, size_t N>
-HWY_API uint64_t BitsFromMask(hwy::SizeTag<2> /*tag*/,
-                              const Mask128<T, N> mask) {
+HWY_INLINE uint64_t BitsFromMask(hwy::SizeTag<2> /*tag*/,
+                                 const Mask128<T, N> mask) {
   // Remove useless lower half of each u16 while preserving the sign bit.
   const __i16x8 zero = wasm_i16x8_splat(0);
-  const Mask128<T> mask8{wasm_i8x16_narrow_i16x8(mask.raw, zero)};
+  const Mask128<uint8_t, N> mask8{wasm_i8x16_narrow_i16x8(mask.raw, zero)};
   return BitsFromMask(hwy::SizeTag<1>(), mask8);
 }
 
 template <typename T, size_t N>
-HWY_API uint64_t BitsFromMask(hwy::SizeTag<4> /*tag*/,
-                              const Mask128<T, N> mask) {
+HWY_INLINE uint64_t BitsFromMask(hwy::SizeTag<4> /*tag*/,
+                                 const Mask128<T, N> mask) {
   const __i32x4 mask_i = static_cast<__i32x4>(mask.raw);
   const __i32x4 slice = wasm_i32x4_make(1, 2, 4, 8);
   const __i32x4 sliced_mask = wasm_v128_and(mask_i, slice);
@@ -2374,22 +2579,22 @@ constexpr __i8x16 BytesAbove() {
 }
 
 template <typename T, size_t N>
-HWY_API uint64_t BitsFromMask(const Mask128<T, N> mask) {
+HWY_INLINE uint64_t BitsFromMask(const Mask128<T, N> mask) {
   return OnlyActive<T, N>(BitsFromMask(hwy::SizeTag<sizeof(T)>(), mask));
 }
 
 template <typename T>
-HWY_API size_t CountTrue(hwy::SizeTag<1> tag, const Mask128<T> m) {
+HWY_INLINE size_t CountTrue(hwy::SizeTag<1> tag, const Mask128<T> m) {
   return PopCount(BitsFromMask(tag, m));
 }
 
 template <typename T>
-HWY_API size_t CountTrue(hwy::SizeTag<2> tag, const Mask128<T> m) {
+HWY_INLINE size_t CountTrue(hwy::SizeTag<2> tag, const Mask128<T> m) {
   return PopCount(BitsFromMask(tag, m));
 }
 
 template <typename T>
-HWY_API size_t CountTrue(hwy::SizeTag<4> /*tag*/, const Mask128<T> m) {
+HWY_INLINE size_t CountTrue(hwy::SizeTag<4> /*tag*/, const Mask128<T> m) {
   const __i32x4 var_shift = wasm_i32x4_make(1, 2, 4, 8);
   const __i32x4 shifted_bits = wasm_v128_and(m.raw, var_shift);
   alignas(16) uint64_t lanes[2];
@@ -2400,76 +2605,85 @@ HWY_API size_t CountTrue(hwy::SizeTag<4> /*tag*/, const Mask128<T> m) {
 }  // namespace detail
 
 template <typename T, size_t N>
-HWY_INLINE size_t StoreMaskBits(const Mask128<T, N> mask, uint8_t* p) {
+HWY_API size_t StoreMaskBits(const Simd<T, N> /* tag */,
+                             const Mask128<T, N> mask, uint8_t* p) {
   const uint64_t bits = detail::BitsFromMask(mask);
   const size_t kNumBytes = (N + 7) / 8;
   CopyBytes<kNumBytes>(&bits, p);
   return kNumBytes;
 }
 
-template <typename T>
-HWY_API size_t CountTrue(const Mask128<T> m) {
+template <typename T, size_t N>
+HWY_API size_t CountTrue(const Simd<T, N> /* tag */, const Mask128<T> m) {
   return detail::CountTrue(hwy::SizeTag<sizeof(T)>(), m);
 }
 
 // Partial vector
 template <typename T, size_t N, HWY_IF_LE64(T, N)>
-HWY_API size_t CountTrue(const Mask128<T, N> m) {
+HWY_API size_t CountTrue(const Simd<T, N> d, const Mask128<T, N> m) {
   // Ensure all undefined bytes are 0.
   const Mask128<T, N> mask{detail::BytesAbove<N * sizeof(T)>()};
-  return CountTrue(Mask128<T>{AndNot(mask, m).raw});
+  return CountTrue(d, Mask128<T>{AndNot(mask, m).raw});
 }
 
-// Full vector, type-independent
+// Full vector
 template <typename T>
-HWY_API bool AllFalse(const Mask128<T> m) {
+HWY_API bool AllFalse(const Full128<T> d, const Mask128<T> m) {
 #if 0
   // Casting followed by wasm_i8x16_any_true results in wasm error:
   // i32.eqz[0] expected type i32, found i8x16.popcnt of type s128
-  const auto v8 = BitCast(Full128<int8_t>(), VecFromMask(Full128<T>(), m));
+  const auto v8 = BitCast(Full128<int8_t>(), VecFromMask(d, m));
   return !wasm_i8x16_any_true(v8.raw);
 #else
+  (void)d;
   return (wasm_i64x2_extract_lane(m.raw, 0) |
           wasm_i64x2_extract_lane(m.raw, 1)) == 0;
 #endif
 }
 
-// Full vector, type-dependent
+// Full vector
 namespace detail {
 template <typename T>
-HWY_API bool AllTrue(hwy::SizeTag<1> /*tag*/, const Mask128<T> m) {
+HWY_INLINE bool AllTrue(hwy::SizeTag<1> /*tag*/, const Mask128<T> m) {
   return wasm_i8x16_all_true(m.raw);
 }
 template <typename T>
-HWY_API bool AllTrue(hwy::SizeTag<2> /*tag*/, const Mask128<T> m) {
+HWY_INLINE bool AllTrue(hwy::SizeTag<2> /*tag*/, const Mask128<T> m) {
   return wasm_i16x8_all_true(m.raw);
 }
 template <typename T>
-HWY_API bool AllTrue(hwy::SizeTag<4> /*tag*/, const Mask128<T> m) {
+HWY_INLINE bool AllTrue(hwy::SizeTag<4> /*tag*/, const Mask128<T> m) {
   return wasm_i32x4_all_true(m.raw);
 }
 
 }  // namespace detail
 
-template <typename T>
-HWY_API bool AllTrue(const Mask128<T> m) {
+template <typename T, size_t N>
+HWY_API bool AllTrue(const Simd<T, N> /* tag */, const Mask128<T> m) {
   return detail::AllTrue(hwy::SizeTag<sizeof(T)>(), m);
 }
 
 // Partial vectors
 
 template <typename T, size_t N, HWY_IF_LE64(T, N)>
-HWY_API bool AllFalse(const Mask128<T, N> m) {
+HWY_API bool AllFalse(Simd<T, N> /* tag */, const Mask128<T, N> m) {
   // Ensure all undefined bytes are 0.
   const Mask128<T, N> mask{detail::BytesAbove<N * sizeof(T)>()};
   return AllFalse(Mask128<T>{AndNot(mask, m).raw});
 }
 
 template <typename T, size_t N, HWY_IF_LE64(T, N)>
-HWY_API bool AllTrue(const Mask128<T, N> m) {
+HWY_API bool AllTrue(const Simd<T, N> d, const Mask128<T, N> m) {
   // Ensure all undefined bytes are FF.
   const Mask128<T, N> mask{detail::BytesAbove<N * sizeof(T)>()};
-  return AllTrue(Mask128<T>{Or(mask, m).raw});
+  return AllTrue(d, Mask128<T>{Or(mask, m).raw});
+}
+
+template <typename T, size_t N>
+HWY_API intptr_t FindFirstTrue(const Simd<T, N> /* tag */,
+                               const Mask128<T, N> mask) {
+  const uint64_t bits = detail::BitsFromMask(mask);
+  return bits ? Num0BitsBelowLS1Bit_Nonzero64(bits) : -1;
 }
 
 // ------------------------------ Compress
@@ -2661,8 +2875,8 @@ HWY_INLINE Vec128<T, N> Idx64x2FromBits(const uint64_t mask_bits) {
 // redundant BitsFromMask in the latter.
 
 template <typename T, size_t N>
-HWY_API Vec128<T, N> Compress(hwy::SizeTag<2> /*tag*/, Vec128<T, N> v,
-                              const uint64_t mask_bits) {
+HWY_INLINE Vec128<T, N> Compress(hwy::SizeTag<2> /*tag*/, Vec128<T, N> v,
+                                 const uint64_t mask_bits) {
   const auto idx = detail::Idx16x8FromBits<T, N>(mask_bits);
   using D = Simd<T, N>;
   const RebindToSigned<D> di;
@@ -2670,8 +2884,8 @@ HWY_API Vec128<T, N> Compress(hwy::SizeTag<2> /*tag*/, Vec128<T, N> v,
 }
 
 template <typename T, size_t N>
-HWY_API Vec128<T, N> Compress(hwy::SizeTag<4> /*tag*/, Vec128<T, N> v,
-                              const uint64_t mask_bits) {
+HWY_INLINE Vec128<T, N> Compress(hwy::SizeTag<4> /*tag*/, Vec128<T, N> v,
+                                 const uint64_t mask_bits) {
   const auto idx = detail::Idx32x4FromBits<T, N>(mask_bits);
   using D = Simd<T, N>;
   const RebindToSigned<D> di;
@@ -2681,9 +2895,9 @@ HWY_API Vec128<T, N> Compress(hwy::SizeTag<4> /*tag*/, Vec128<T, N> v,
 #if HWY_CAP_INTEGER64 || HWY_CAP_FLOAT64
 
 template <typename T, size_t N>
-HWY_API Vec128<uint64_t, N> Compress(hwy::SizeTag<8> /*tag*/,
-                                     Vec128<uint64_t, N> v,
-                                     const uint64_t mask_bits) {
+HWY_INLINE Vec128<uint64_t, N> Compress(hwy::SizeTag<8> /*tag*/,
+                                        Vec128<uint64_t, N> v,
+                                        const uint64_t mask_bits) {
   const auto idx = detail::Idx64x2FromBits<uint64_t, N>(mask_bits);
   using D = Simd<T, N>;
   const RebindToSigned<D> di;
@@ -2730,7 +2944,7 @@ HWY_API void StoreInterleaved3(const Vec128<uint8_t> a, const Vec128<uint8_t> b,
       0x80, 2, 0x80, 0x80, 3, 0x80, 0x80, 4, 0x80, 0x80};
   const auto shuf_r0 = Load(d, tbl_r0);
   const auto shuf_g0 = Load(d, tbl_g0);  // cannot reuse r0 due to 5 in MSB
-  const auto shuf_b0 = CombineShiftRightBytes<15>(shuf_g0, shuf_g0);
+  const auto shuf_b0 = CombineShiftRightBytes<15>(d, shuf_g0, shuf_g0);
   const auto r0 = TableLookupBytes(a, shuf_r0);  // 5..4..3..2..1..0
   const auto g0 = TableLookupBytes(b, shuf_g0);  // ..4..3..2..1..0.
   const auto b0 = TableLookupBytes(c, shuf_b0);  // .4..3..2..1..0..
@@ -2782,7 +2996,7 @@ HWY_API void StoreInterleaved3(const Vec128<uint8_t, 8> a,
       0x80, 2, 0x80, 0x80, 3, 0x80, 0x80, 4, 0x80, 0x80};
   const auto shuf_r0 = Load(d_full, tbl_r0);
   const auto shuf_g0 = Load(d_full, tbl_g0);  // cannot reuse r0 due to 5 in MSB
-  const auto shuf_b0 = CombineShiftRightBytes<15>(shuf_g0, shuf_g0);
+  const auto shuf_b0 = CombineShiftRightBytes<15>(d_full, shuf_g0, shuf_g0);
   const auto r0 = TableLookupBytes(full_a, shuf_r0);  // 5..4..3..2..1..0
   const auto g0 = TableLookupBytes(full_b, shuf_g0);  // ..4..3..2..1..0.
   const auto b0 = TableLookupBytes(full_c, shuf_b0);  // .4..3..2..1..0..
@@ -2820,8 +3034,8 @@ HWY_API void StoreInterleaved3(const Vec128<uint8_t, N> a,
       0,    0x80, 0x80, 1,   0x80, 0x80, 2, 0x80, 0x80, 3, 0x80, 0x80,  //
       0x80, 0x80, 0x80, 0x80};
   const auto shuf_r0 = Load(d_full, tbl_r0);
-  const auto shuf_g0 = CombineShiftRightBytes<15>(shuf_r0, shuf_r0);
-  const auto shuf_b0 = CombineShiftRightBytes<14>(shuf_r0, shuf_r0);
+  const auto shuf_g0 = CombineShiftRightBytes<15>(d_full, shuf_r0, shuf_r0);
+  const auto shuf_b0 = CombineShiftRightBytes<14>(d_full, shuf_r0, shuf_r0);
   const auto r0 = TableLookupBytes(full_a, shuf_r0);  // ......3..2..1..0
   const auto g0 = TableLookupBytes(full_b, shuf_g0);  // .....3..2..1..0.
   const auto b0 = TableLookupBytes(full_c, shuf_b0);  // ....3..2..1..0..
@@ -2837,21 +3051,23 @@ HWY_API void StoreInterleaved3(const Vec128<uint8_t, N> a,
 HWY_API void StoreInterleaved4(const Vec128<uint8_t> v0,
                                const Vec128<uint8_t> v1,
                                const Vec128<uint8_t> v2,
-                               const Vec128<uint8_t> v3, Full128<uint8_t> d,
+                               const Vec128<uint8_t> v3, Full128<uint8_t> d8,
                                uint8_t* HWY_RESTRICT unaligned) {
+  const RepartitionToWide<decltype(d8)> d16;
+  const RepartitionToWide<decltype(d16)> d32;
   // let a,b,c,d denote v0..3.
-  const auto ba0 = ZipLower(v0, v1);  // b7 a7 .. b0 a0
-  const auto dc0 = ZipLower(v2, v3);  // d7 c7 .. d0 c0
-  const auto ba8 = ZipUpper(v0, v1);
-  const auto dc8 = ZipUpper(v2, v3);
-  const auto dcba_0 = ZipLower(ba0, dc0);  // d..a3 d..a0
-  const auto dcba_4 = ZipUpper(ba0, dc0);  // d..a7 d..a4
-  const auto dcba_8 = ZipLower(ba8, dc8);  // d..aB d..a8
-  const auto dcba_C = ZipUpper(ba8, dc8);  // d..aF d..aC
-  StoreU(BitCast(d, dcba_0), d, unaligned + 0 * 16);
-  StoreU(BitCast(d, dcba_4), d, unaligned + 1 * 16);
-  StoreU(BitCast(d, dcba_8), d, unaligned + 2 * 16);
-  StoreU(BitCast(d, dcba_C), d, unaligned + 3 * 16);
+  const auto ba0 = ZipLower(d16, v0, v1);  // b7 a7 .. b0 a0
+  const auto dc0 = ZipLower(d16, v2, v3);  // d7 c7 .. d0 c0
+  const auto ba8 = ZipUpper(d16, v0, v1);
+  const auto dc8 = ZipUpper(d16, v2, v3);
+  const auto dcba_0 = ZipLower(d32, ba0, dc0);  // d..a3 d..a0
+  const auto dcba_4 = ZipUpper(d32, ba0, dc0);  // d..a7 d..a4
+  const auto dcba_8 = ZipLower(d32, ba8, dc8);  // d..aB d..a8
+  const auto dcba_C = ZipUpper(d32, ba8, dc8);  // d..aF d..aC
+  StoreU(BitCast(d8, dcba_0), d8, unaligned + 0 * 16);
+  StoreU(BitCast(d8, dcba_4), d8, unaligned + 1 * 16);
+  StoreU(BitCast(d8, dcba_8), d8, unaligned + 2 * 16);
+  StoreU(BitCast(d8, dcba_C), d8, unaligned + 3 * 16);
 }
 
 // 64 bits
@@ -2859,21 +3075,23 @@ HWY_API void StoreInterleaved4(const Vec128<uint8_t, 8> in0,
                                const Vec128<uint8_t, 8> in1,
                                const Vec128<uint8_t, 8> in2,
                                const Vec128<uint8_t, 8> in3,
-                               Simd<uint8_t, 8> /*tag*/,
+                               Simd<uint8_t, 8> /* tag */,
                                uint8_t* HWY_RESTRICT unaligned) {
   // Use full vectors to reduce the number of stores.
+  const Full128<uint8_t> d_full8;
+  const RepartitionToWide<decltype(d_full8)> d16;
+  const RepartitionToWide<decltype(d16)> d32;
   const Vec128<uint8_t> v0{in0.raw};
   const Vec128<uint8_t> v1{in1.raw};
   const Vec128<uint8_t> v2{in2.raw};
   const Vec128<uint8_t> v3{in3.raw};
   // let a,b,c,d denote v0..3.
-  const auto ba0 = ZipLower(v0, v1);       // b7 a7 .. b0 a0
-  const auto dc0 = ZipLower(v2, v3);       // d7 c7 .. d0 c0
-  const auto dcba_0 = ZipLower(ba0, dc0);  // d..a3 d..a0
-  const auto dcba_4 = ZipUpper(ba0, dc0);  // d..a7 d..a4
-  const Full128<uint8_t> d_full;
-  StoreU(BitCast(d_full, dcba_0), d_full, unaligned + 0 * 16);
-  StoreU(BitCast(d_full, dcba_4), d_full, unaligned + 1 * 16);
+  const auto ba0 = ZipLower(d16, v0, v1);       // b7 a7 .. b0 a0
+  const auto dc0 = ZipLower(d16, v2, v3);       // d7 c7 .. d0 c0
+  const auto dcba_0 = ZipLower(d32, ba0, dc0);  // d..a3 d..a0
+  const auto dcba_4 = ZipUpper(d32, ba0, dc0);  // d..a7 d..a4
+  StoreU(BitCast(d_full8, dcba_0), d_full8, unaligned + 0 * 16);
+  StoreU(BitCast(d_full8, dcba_4), d_full8, unaligned + 1 * 16);
 }
 
 // <= 32 bits
@@ -2885,38 +3103,58 @@ HWY_API void StoreInterleaved4(const Vec128<uint8_t, N> in0,
                                Simd<uint8_t, N> /*tag*/,
                                uint8_t* HWY_RESTRICT unaligned) {
   // Use full vectors to reduce the number of stores.
+  const Full128<uint8_t> d_full8;
+  const RepartitionToWide<decltype(d_full8)> d16;
+  const RepartitionToWide<decltype(d16)> d32;
   const Vec128<uint8_t> v0{in0.raw};
   const Vec128<uint8_t> v1{in1.raw};
   const Vec128<uint8_t> v2{in2.raw};
   const Vec128<uint8_t> v3{in3.raw};
   // let a,b,c,d denote v0..3.
-  const auto ba0 = ZipLower(v0, v1);       // b3 a3 .. b0 a0
-  const auto dc0 = ZipLower(v2, v3);       // d3 c3 .. d0 c0
-  const auto dcba_0 = ZipLower(ba0, dc0);  // d..a3 d..a0
+  const auto ba0 = ZipLower(d16, v0, v1);       // b3 a3 .. b0 a0
+  const auto dc0 = ZipLower(d16, v2, v3);       // d3 c3 .. d0 c0
+  const auto dcba_0 = ZipLower(d32, ba0, dc0);  // d..a3 d..a0
   alignas(16) uint8_t buf[16];
-  const Full128<uint8_t> d_full;
-  StoreU(BitCast(d_full, dcba_0), d_full, buf);
+  StoreU(BitCast(d_full8, dcba_0), d_full8, buf);
   CopyBytes<4 * N>(buf, unaligned);
 }
 
+// ------------------------------ MulEven/Odd (Load)
+
+HWY_INLINE Vec128<uint64_t> MulEven(const Vec128<uint64_t> a,
+                                    const Vec128<uint64_t> b) {
+  alignas(16) uint64_t mul[2];
+  mul[0] = Mul128(wasm_i64x2_extract_lane(a.raw, 0),
+                  wasm_i64x2_extract_lane(b.raw, 0), &mul[1]);
+  return Load(Full128<uint64_t>(), mul);
+}
+
+HWY_INLINE Vec128<uint64_t> MulOdd(const Vec128<uint64_t> a,
+                                   const Vec128<uint64_t> b) {
+  alignas(16) uint64_t mul[2];
+  mul[0] = Mul128(wasm_i64x2_extract_lane(a.raw, 1),
+                  wasm_i64x2_extract_lane(b.raw, 1), &mul[1]);
+  return Load(Full128<uint64_t>(), mul);
+}
+
 // ------------------------------ Reductions
 
 namespace detail {
 
 // N=1 for any T: no-op
 template <typename T>
-HWY_API Vec128<T, 1> SumOfLanes(hwy::SizeTag<sizeof(T)> /* tag */,
-                                const Vec128<T, 1> v) {
+HWY_INLINE Vec128<T, 1> SumOfLanes(hwy::SizeTag<sizeof(T)> /* tag */,
+                                   const Vec128<T, 1> v) {
   return v;
 }
 template <typename T>
-HWY_API Vec128<T, 1> MinOfLanes(hwy::SizeTag<sizeof(T)> /* tag */,
-                                const Vec128<T, 1> v) {
+HWY_INLINE Vec128<T, 1> MinOfLanes(hwy::SizeTag<sizeof(T)> /* tag */,
+                                   const Vec128<T, 1> v) {
   return v;
 }
 template <typename T>
-HWY_API Vec128<T, 1> MaxOfLanes(hwy::SizeTag<sizeof(T)> /* tag */,
-                                const Vec128<T, 1> v) {
+HWY_INLINE Vec128<T, 1> MaxOfLanes(hwy::SizeTag<sizeof(T)> /* tag */,
+                                   const Vec128<T, 1> v) {
   return v;
 }
 
@@ -2924,38 +3162,41 @@ HWY_API Vec128<T, 1> MaxOfLanes(hwy::SizeTag<sizeof(T)> /* tag */,
 
 // N=2
 template <typename T>
-HWY_API Vec128<T, 2> SumOfLanes(hwy::SizeTag<4> /* tag */,
-                                const Vec128<T, 2> v10) {
+HWY_INLINE Vec128<T, 2> SumOfLanes(hwy::SizeTag<4> /* tag */,
+                                   const Vec128<T, 2> v10) {
   return v10 + Vec128<T, 2>{Shuffle2301(Vec128<T>{v10.raw}).raw};
 }
 template <typename T>
-HWY_API Vec128<T, 2> MinOfLanes(hwy::SizeTag<4> /* tag */,
-                                const Vec128<T, 2> v10) {
+HWY_INLINE Vec128<T, 2> MinOfLanes(hwy::SizeTag<4> /* tag */,
+                                   const Vec128<T, 2> v10) {
   return Min(v10, Vec128<T, 2>{Shuffle2301(Vec128<T>{v10.raw}).raw});
 }
 template <typename T>
-HWY_API Vec128<T, 2> MaxOfLanes(hwy::SizeTag<4> /* tag */,
-                                const Vec128<T, 2> v10) {
+HWY_INLINE Vec128<T, 2> MaxOfLanes(hwy::SizeTag<4> /* tag */,
+                                   const Vec128<T, 2> v10) {
   return Max(v10, Vec128<T, 2>{Shuffle2301(Vec128<T>{v10.raw}).raw});
 }
 
 // N=4 (full)
 template <typename T>
-HWY_API Vec128<T> SumOfLanes(hwy::SizeTag<4> /* tag */, const Vec128<T> v3210) {
+HWY_INLINE Vec128<T> SumOfLanes(hwy::SizeTag<4> /* tag */,
+                                const Vec128<T> v3210) {
   const Vec128<T> v1032 = Shuffle1032(v3210);
   const Vec128<T> v31_20_31_20 = v3210 + v1032;
   const Vec128<T> v20_31_20_31 = Shuffle0321(v31_20_31_20);
   return v20_31_20_31 + v31_20_31_20;
 }
 template <typename T>
-HWY_API Vec128<T> MinOfLanes(hwy::SizeTag<4> /* tag */, const Vec128<T> v3210) {
+HWY_INLINE Vec128<T> MinOfLanes(hwy::SizeTag<4> /* tag */,
+                                const Vec128<T> v3210) {
   const Vec128<T> v1032 = Shuffle1032(v3210);
   const Vec128<T> v31_20_31_20 = Min(v3210, v1032);
   const Vec128<T> v20_31_20_31 = Shuffle0321(v31_20_31_20);
   return Min(v20_31_20_31, v31_20_31_20);
 }
 template <typename T>
-HWY_API Vec128<T> MaxOfLanes(hwy::SizeTag<4> /* tag */, const Vec128<T> v3210) {
+HWY_INLINE Vec128<T> MaxOfLanes(hwy::SizeTag<4> /* tag */,
+                                const Vec128<T> v3210) {
   const Vec128<T> v1032 = Shuffle1032(v3210);
   const Vec128<T> v31_20_31_20 = Max(v3210, v1032);
   const Vec128<T> v20_31_20_31 = Shuffle0321(v31_20_31_20);
@@ -2966,17 +3207,20 @@ HWY_API Vec128<T> MaxOfLanes(hwy::SizeTag<4> /* tag */, const Vec128<T> v3210) {
 
 // N=2 (full)
 template <typename T>
-HWY_API Vec128<T> SumOfLanes(hwy::SizeTag<8> /* tag */, const Vec128<T> v10) {
+HWY_INLINE Vec128<T> SumOfLanes(hwy::SizeTag<8> /* tag */,
+                                const Vec128<T> v10) {
   const Vec128<T> v01 = Shuffle01(v10);
   return v10 + v01;
 }
 template <typename T>
-HWY_API Vec128<T> MinOfLanes(hwy::SizeTag<8> /* tag */, const Vec128<T> v10) {
+HWY_INLINE Vec128<T> MinOfLanes(hwy::SizeTag<8> /* tag */,
+                                const Vec128<T> v10) {
   const Vec128<T> v01 = Shuffle01(v10);
   return Min(v10, v01);
 }
 template <typename T>
-HWY_API Vec128<T> MaxOfLanes(hwy::SizeTag<8> /* tag */, const Vec128<T> v10) {
+HWY_INLINE Vec128<T> MaxOfLanes(hwy::SizeTag<8> /* tag */,
+                                const Vec128<T> v10) {
   const Vec128<T> v01 = Shuffle01(v10);
   return Max(v10, v01);
 }
@@ -2985,18 +3229,114 @@ HWY_API Vec128<T> MaxOfLanes(hwy::SizeTag<8> /* tag */, const Vec128<T> v10) {
 
 // Supported for u/i/f 32/64. Returns the same value in each lane.
 template <typename T, size_t N>
-HWY_API Vec128<T, N> SumOfLanes(const Vec128<T, N> v) {
+HWY_API Vec128<T, N> SumOfLanes(Simd<T, N> /* tag */, const Vec128<T, N> v) {
   return detail::SumOfLanes(hwy::SizeTag<sizeof(T)>(), v);
 }
 template <typename T, size_t N>
-HWY_API Vec128<T, N> MinOfLanes(const Vec128<T, N> v) {
+HWY_API Vec128<T, N> MinOfLanes(Simd<T, N> /* tag */, const Vec128<T, N> v) {
   return detail::MinOfLanes(hwy::SizeTag<sizeof(T)>(), v);
 }
 template <typename T, size_t N>
-HWY_API Vec128<T, N> MaxOfLanes(const Vec128<T, N> v) {
+HWY_API Vec128<T, N> MaxOfLanes(Simd<T, N> /* tag */, const Vec128<T, N> v) {
   return detail::MaxOfLanes(hwy::SizeTag<sizeof(T)>(), v);
 }
 
+// ================================================== DEPRECATED
+
+template <typename T, size_t N>
+HWY_API size_t StoreMaskBits(const Mask128<T, N> mask, uint8_t* p) {
+  return StoreMaskBits(Simd<T, N>(), mask, p);
+}
+
+template <typename T, size_t N>
+HWY_API bool AllTrue(const Mask128<T, N> mask) {
+  return AllTrue(Simd<T, N>(), mask);
+}
+
+template <typename T, size_t N>
+HWY_API bool AllFalse(const Mask128<T, N> mask) {
+  return AllFalse(Simd<T, N>(), mask);
+}
+
+template <typename T, size_t N>
+HWY_API size_t CountTrue(const Mask128<T, N> mask) {
+  return CountTrue(Simd<T, N>(), mask);
+}
+
+template <typename T, size_t N>
+HWY_API Vec128<T, N> SumOfLanes(const Vec128<T, N> v) {
+  return SumOfLanes(Simd<T, N>(), v);
+}
+template <typename T, size_t N>
+HWY_API Vec128<T, N> MinOfLanes(const Vec128<T, N> v) {
+  return MinOfLanes(Simd<T, N>(), v);
+}
+template <typename T, size_t N>
+HWY_API Vec128<T, N> MaxOfLanes(const Vec128<T, N> v) {
+  return MaxOfLanes(Simd<T, N>(), v);
+}
+
+template <typename T, size_t N>
+HWY_API Vec128<T, (N + 1) / 2> UpperHalf(Vec128<T, N> v) {
+  return UpperHalf(Half<Simd<T, N>>(), v);
+}
+
+template <int kBytes, typename T, size_t N>
+HWY_API Vec128<T, N> ShiftRightBytes(const Vec128<T, N> v) {
+  return ShiftRightBytes<kBytes>(Simd<T, N>(), v);
+}
+
+template <int kLanes, typename T, size_t N>
+HWY_API Vec128<T, N> ShiftRightLanes(const Vec128<T, N> v) {
+  return ShiftRightLanes<kLanes>(Simd<T, N>(), v);
+}
+
+template <size_t kBytes, typename T, size_t N>
+HWY_API Vec128<T, N> CombineShiftRightBytes(Vec128<T, N> hi, Vec128<T, N> lo) {
+  return CombineShiftRightBytes<kBytes>(Simd<T, N>(), hi, lo);
+}
+
+template <typename T, size_t N>
+HWY_API Vec128<T, N> InterleaveUpper(Vec128<T, N> a, Vec128<T, N> b) {
+  return InterleaveUpper(Simd<T, N>(), a, b);
+}
+
+template <typename T, size_t N, class D = Simd<T, N>>
+HWY_API VFromD<RepartitionToWide<D>> ZipUpper(Vec128<T, N> a, Vec128<T, N> b) {
+  return InterleaveUpper(RepartitionToWide<D>(), a, b);
+}
+
+template <typename T, size_t N2>
+HWY_API Vec128<T, N2 * 2> Combine(Vec128<T, N2> hi2, Vec128<T, N2> lo2) {
+  return Combine(Simd<T, N2 * 2>(), hi2, lo2);
+}
+
+template <typename T, size_t N2, HWY_IF_LE64(T, N2)>
+HWY_API Vec128<T, N2 * 2> ZeroExtendVector(Vec128<T, N2> lo) {
+  return ZeroExtendVector(Simd<T, N2 * 2>(), lo);
+}
+
+template <typename T, size_t N>
+HWY_API Vec128<T, N> ConcatLowerLower(Vec128<T, N> hi, Vec128<T, N> lo) {
+  return ConcatLowerLower(Simd<T, N>(), hi, lo);
+}
+
+template <typename T, size_t N>
+HWY_API Vec128<T, N> ConcatUpperUpper(Vec128<T, N> hi, Vec128<T, N> lo) {
+  return ConcatUpperUpper(Simd<T, N>(), hi, lo);
+}
+
+template <typename T, size_t N>
+HWY_API Vec128<T, N> ConcatLowerUpper(const Vec128<T, N> hi,
+                                      const Vec128<T, N> lo) {
+  return ConcatLowerUpper(Simd<T, N>(), hi, lo);
+}
+
+template <typename T, size_t N>
+HWY_API Vec128<T, N> ConcatUpperLower(Vec128<T, N> hi, Vec128<T, N> lo) {
+  return ConcatUpperLower(Simd<T, N>(), hi, lo);
+}
+
 // ================================================== Operator wrapper
 
 template <class V>
@@ -3031,6 +3371,10 @@ HWY_API auto Eq(V a, V b) -> decltype(a == b) {
   return a == b;
 }
 template <class V>
+HWY_API auto Ne(V a, V b) -> decltype(a == b) {
+  return a != b;
+}
+template <class V>
 HWY_API auto Lt(V a, V b) -> decltype(a == b) {
   return a < b;
 }
diff --git a/third_party/highway/hwy/ops/x86_128-inl.h b/third_party/highway/hwy/ops/x86_128-inl.h
index fc275274596cd..0c7da3930fddf 100644
--- a/third_party/highway/hwy/ops/x86_128-inl.h
+++ b/third_party/highway/hwy/ops/x86_128-inl.h
@@ -17,7 +17,12 @@
 // External include guard in highway.h - see comment there.
 
 #include <emmintrin.h>
+#if HWY_TARGET == HWY_SSSE3
+#include <tmmintrin.h>  // SSSE3
+#else
 #include <smmintrin.h>  // SSE4
+#include <wmmintrin.h>  // CLMUL
+#endif
 #include <stddef.h>
 #include <stdint.h>
 
@@ -37,6 +42,11 @@ HWY_BEFORE_NAMESPACE();
 namespace hwy {
 namespace HWY_NAMESPACE {
 
+template <typename T>
+using Full128 = Simd<T, 16 / sizeof(T)>;
+
+namespace detail {
+
 template <typename T>
 struct Raw128 {
   using type = __m128i;
@@ -50,12 +60,11 @@ struct Raw128<double> {
   using type = __m128d;
 };
 
-template <typename T>
-using Full128 = Simd<T, 16 / sizeof(T)>;
+}  // namespace detail
 
 template <typename T, size_t N = 16 / sizeof(T)>
 class Vec128 {
-  using Raw = typename Raw128<T>::type;
+  using Raw = typename detail::Raw128<T>::type;
 
  public:
   // Compound assignment. Only usable if there is a corresponding non-member
@@ -85,25 +94,53 @@ class Vec128 {
   Raw raw;
 };
 
-// Integer: FF..FF or 0. Float: MSB, all other bits undefined - see README.
+// Forward-declare for use by DeduceD, see below.
+template <typename T>
+class Vec256;
+template <typename T>
+class Vec512;
+
+// FF..FF or 0.
 template <typename T, size_t N = 16 / sizeof(T)>
-class Mask128 {
-  using Raw = typename Raw128<T>::type;
+struct Mask128 {
+  typename detail::Raw128<T>::type raw;
+};
 
- public:
-  Raw raw;
+namespace detail {
+
+// Deduce Simd<T, N> from Vec*<T, N> (pointers because Vec256/512 may be
+// incomplete types at this point; this is simpler than avoiding multiple
+// definitions of DFromV via #if)
+struct DeduceD {
+  template <typename T, size_t N>
+  Simd<T, N> operator()(const Vec128<T, N>*) const {
+    return Simd<T, N>();
+  }
+  template <typename T>
+  Simd<T, 32 / sizeof(T)> operator()(const Vec256<T>*) const {
+    return Simd<T, 32 / sizeof(T)>();
+  }
+  template <typename T>
+  Simd<T, 64 / sizeof(T)> operator()(const Vec512<T>*) const {
+    return Simd<T, 64 / sizeof(T)>();
+  }
 };
 
+}  // namespace detail
+
+template <class V>
+using DFromV = decltype(detail::DeduceD()(static_cast<V*>(nullptr)));
+
 // ------------------------------ BitCast
 
 namespace detail {
 
-HWY_API __m128i BitCastToInteger(__m128i v) { return v; }
-HWY_API __m128i BitCastToInteger(__m128 v) { return _mm_castps_si128(v); }
-HWY_API __m128i BitCastToInteger(__m128d v) { return _mm_castpd_si128(v); }
+HWY_INLINE __m128i BitCastToInteger(__m128i v) { return v; }
+HWY_INLINE __m128i BitCastToInteger(__m128 v) { return _mm_castps_si128(v); }
+HWY_INLINE __m128i BitCastToInteger(__m128d v) { return _mm_castpd_si128(v); }
 
 template <typename T, size_t N>
-HWY_API Vec128<uint8_t, N * sizeof(T)> BitCastToByte(Vec128<T, N> v) {
+HWY_INLINE Vec128<uint8_t, N * sizeof(T)> BitCastToByte(Vec128<T, N> v) {
   return Vec128<uint8_t, N * sizeof(T)>{BitCastToInteger(v.raw)};
 }
 
@@ -122,8 +159,8 @@ struct BitCastFromInteger128<double> {
 };
 
 template <typename T, size_t N>
-HWY_API Vec128<T, N> BitCastFromByte(Simd<T, N> /* tag */,
-                                Vec128<uint8_t, N * sizeof(T)> v) {
+HWY_INLINE Vec128<T, N> BitCastFromByte(Simd<T, N> /* tag */,
+                                        Vec128<uint8_t, N * sizeof(T)> v) {
   return Vec128<T, N>{BitCastFromInteger128<T>()(v.raw)};
 }
 
@@ -135,7 +172,7 @@ HWY_API Vec128<T, N> BitCast(Simd<T, N> d,
   return detail::BitCastFromByte(d, detail::BitCastToByte(v));
 }
 
-// ------------------------------ Set
+// ------------------------------ Zero
 
 // Returns an all-zero vector/part.
 template <typename T, size_t N, HWY_IF_LE128(T, N)>
@@ -151,6 +188,11 @@ HWY_API Vec128<double, N> Zero(Simd<double, N> /* tag */) {
   return Vec128<double, N>{_mm_setzero_pd()};
 }
 
+template <class D>
+using VFromD = decltype(Zero(D()));
+
+// ------------------------------ Set
+
 // Returns a vector/part with all lanes set to "t".
 template <size_t N, HWY_IF_LE128(uint8_t, N)>
 HWY_API Vec128<uint8_t, N> Set(Simd<uint8_t, N> /* tag */, const uint8_t t) {
@@ -254,7 +296,7 @@ HWY_API uint64_t GetLane(const Vec128<uint64_t, N> v) {
   Store(v, Simd<uint64_t, N>(), lanes);
   return lanes[0];
 #else
-  return _mm_cvtsi128_si64(v.raw);
+  return static_cast<uint64_t>(_mm_cvtsi128_si64(v.raw));
 #endif
 }
 template <size_t N>
@@ -350,7 +392,7 @@ HWY_API Vec128<double, N> Xor(const Vec128<double, N> a,
 template <typename T, size_t N>
 HWY_API Vec128<T, N> Not(const Vec128<T, N> v) {
   using TU = MakeUnsigned<T>;
-#if HWY_TARGET == HWY_AVX3
+#if HWY_TARGET <= HWY_AVX3
   const __m128i vu = BitCast(Simd<TU, N>(), v).raw;
   return BitCast(Simd<T, N>(),
                  Vec128<TU, N>{_mm_ternarylogic_epi32(vu, vu, vu, 0x55)});
@@ -376,6 +418,96 @@ HWY_API Vec128<T, N> operator^(const Vec128<T, N> a, const Vec128<T, N> b) {
   return Xor(a, b);
 }
 
+// ------------------------------ PopulationCount
+
+// 8/16 require BITALG, 32/64 require VPOPCNTDQ.
+#if HWY_TARGET == HWY_AVX3_DL
+
+#ifdef HWY_NATIVE_POPCNT
+#undef HWY_NATIVE_POPCNT
+#else
+#define HWY_NATIVE_POPCNT
+#endif
+
+namespace detail {
+
+template <typename T, size_t N>
+HWY_INLINE Vec128<T, N> PopulationCount(hwy::SizeTag<1> /* tag */,
+                                        Vec128<T, N> v) {
+  return Vec128<T, N>{_mm_popcnt_epi8(v.raw)};
+}
+template <typename T, size_t N>
+HWY_INLINE Vec128<T, N> PopulationCount(hwy::SizeTag<2> /* tag */,
+                                        Vec128<T, N> v) {
+  return Vec128<T, N>{_mm_popcnt_epi16(v.raw)};
+}
+template <typename T, size_t N>
+HWY_INLINE Vec128<T, N> PopulationCount(hwy::SizeTag<4> /* tag */,
+                                        Vec128<T, N> v) {
+  return Vec128<T, N>{_mm_popcnt_epi32(v.raw)};
+}
+template <typename T, size_t N>
+HWY_INLINE Vec128<T, N> PopulationCount(hwy::SizeTag<8> /* tag */,
+                                        Vec128<T, N> v) {
+  return Vec128<T, N>{_mm_popcnt_epi64(v.raw)};
+}
+
+}  // namespace detail
+
+template <typename T, size_t N>
+HWY_API Vec128<T, N> PopulationCount(Vec128<T, N> v) {
+  return detail::PopulationCount(hwy::SizeTag<sizeof(T)>(), v);
+}
+
+#endif  // HWY_TARGET == HWY_AVX3_DL
+
+// ================================================== SIGN
+
+// ------------------------------ Neg
+
+template <typename T, size_t N, HWY_IF_FLOAT(T)>
+HWY_API Vec128<T, N> Neg(const Vec128<T, N> v) {
+  return Xor(v, SignBit(Simd<T, N>()));
+}
+
+template <typename T, size_t N, HWY_IF_NOT_FLOAT(T)>
+HWY_API Vec128<T, N> Neg(const Vec128<T, N> v) {
+  return Zero(Simd<T, N>()) - v;
+}
+
+// ------------------------------ Abs
+
+// Returns absolute value, except that LimitsMin() maps to LimitsMax() + 1.
+template <size_t N>
+HWY_API Vec128<int8_t, N> Abs(const Vec128<int8_t, N> v) {
+#if HWY_COMPILER_MSVC
+  // Workaround for incorrect codegen? (reaches breakpoint)
+  const auto zero = Zero(Simd<int8_t, N>());
+  return Vec128<int8_t, N>{_mm_max_epi8(v.raw, (zero - v).raw)};
+#else
+  return Vec128<int8_t, N>{_mm_abs_epi8(v.raw)};
+#endif
+}
+template <size_t N>
+HWY_API Vec128<int16_t, N> Abs(const Vec128<int16_t, N> v) {
+  return Vec128<int16_t, N>{_mm_abs_epi16(v.raw)};
+}
+template <size_t N>
+HWY_API Vec128<int32_t, N> Abs(const Vec128<int32_t, N> v) {
+  return Vec128<int32_t, N>{_mm_abs_epi32(v.raw)};
+}
+// i64 is implemented after BroadcastSignBit.
+template <size_t N>
+HWY_API Vec128<float, N> Abs(const Vec128<float, N> v) {
+  const Vec128<int32_t, N> mask{_mm_set1_epi32(0x7FFFFFFF)};
+  return v & BitCast(Simd<float, N>(), mask);
+}
+template <size_t N>
+HWY_API Vec128<double, N> Abs(const Vec128<double, N> v) {
+  const Vec128<int64_t, N> mask{_mm_set1_epi64x(0x7FFFFFFFFFFFFFFFLL)};
+  return v & BitCast(Simd<double, N>(), mask);
+}
+
 // ------------------------------ CopySign
 
 template <typename T, size_t N>
@@ -386,7 +518,7 @@ HWY_API Vec128<T, N> CopySign(const Vec128<T, N> magn,
   const Simd<T, N> d;
   const auto msb = SignBit(d);
 
-#if HWY_TARGET == HWY_AVX3
+#if HWY_TARGET <= HWY_AVX3
   const Rebind<MakeUnsigned<T>, decltype(d)> du;
   // Truth table for msb, magn, sign | bitwise msb ? sign : mag
   //                  0    0     0   |  0
@@ -409,7 +541,7 @@ HWY_API Vec128<T, N> CopySign(const Vec128<T, N> magn,
 template <typename T, size_t N>
 HWY_API Vec128<T, N> CopySignToAbs(const Vec128<T, N> abs,
                                    const Vec128<T, N> sign) {
-#if HWY_TARGET == HWY_AVX3
+#if HWY_TARGET <= HWY_AVX3
   // AVX3 can also handle abs < 0, so no extra action needed.
   return CopySign(abs, sign);
 #else
@@ -417,6 +549,8 @@ HWY_API Vec128<T, N> CopySignToAbs(const Vec128<T, N> abs,
 #endif
 }
 
+// ================================================== MASK
+
 // ------------------------------ Mask
 
 // Mask and Vec are the same (true = FF..FF).
@@ -436,6 +570,18 @@ HWY_API Vec128<T, N> VecFromMask(const Simd<T, N> /* tag */,
   return Vec128<T, N>{v.raw};
 }
 
+#if HWY_TARGET == HWY_SSSE3
+
+// mask ? yes : no
+template <typename T, size_t N>
+HWY_API Vec128<T, N> IfThenElse(Mask128<T, N> mask, Vec128<T, N> yes,
+                                Vec128<T, N> no) {
+  const auto vmask = VecFromMask(Simd<T, N>(), mask);
+  return Or(And(vmask, yes), AndNot(vmask, no));
+}
+
+#else  // HWY_TARGET == HWY_SSSE3
+
 // mask ? yes : no
 template <typename T, size_t N>
 HWY_API Vec128<T, N> IfThenElse(Mask128<T, N> mask, Vec128<T, N> yes,
@@ -455,6 +601,8 @@ HWY_API Vec128<double, N> IfThenElse(const Mask128<double, N> mask,
   return Vec128<double, N>{_mm_blendv_pd(no.raw, yes.raw, mask.raw)};
 }
 
+#endif  // HWY_TARGET == HWY_SSSE3
+
 // mask ? yes : 0
 template <typename T, size_t N>
 HWY_API Vec128<T, N> IfThenElseZero(Mask128<T, N> mask, Vec128<T, N> yes) {
@@ -467,18 +615,11 @@ HWY_API Vec128<T, N> IfThenZeroElse(Mask128<T, N> mask, Vec128<T, N> no) {
   return AndNot(VecFromMask(Simd<T, N>(), mask), no);
 }
 
-template <typename T, size_t N, HWY_IF_FLOAT(T)>
-HWY_API Vec128<T, N> ZeroIfNegative(Vec128<T, N> v) {
-  const Simd<T, N> d;
-  return IfThenElse(MaskFromVec(v), Zero(d), v);
-}
-
 // ------------------------------ Mask logical
 
 template <typename T, size_t N>
 HWY_API Mask128<T, N> Not(const Mask128<T, N> m) {
-  const Simd<T, N> d;
-  return MaskFromVec(Not(VecFromMask(d, m)));
+  return MaskFromVec(Not(VecFromMask(Simd<T, N>(), m)));
 }
 
 template <typename T, size_t N>
@@ -505,6 +646,84 @@ HWY_API Mask128<T, N> Xor(const Mask128<T, N> a, Mask128<T, N> b) {
   return MaskFromVec(Xor(VecFromMask(d, a), VecFromMask(d, b)));
 }
 
+// ================================================== SWIZZLE (1)
+
+// ------------------------------ Hard-coded shuffles
+
+// Notation: let Vec128<int32_t> have lanes 3,2,1,0 (0 is least-significant).
+// Shuffle0321 rotates one lane to the right (the previous least-significant
+// lane is now most-significant). These could also be implemented via
+// CombineShiftRightBytes but the shuffle_abcd notation is more convenient.
+
+// Swap 32-bit halves in 64-bit halves.
+template <size_t N>
+HWY_API Vec128<uint32_t, N> Shuffle2301(const Vec128<uint32_t, N> v) {
+  static_assert(N == 2 || N == 4, "Does not make sense for N=1");
+  return Vec128<uint32_t, N>{_mm_shuffle_epi32(v.raw, 0xB1)};
+}
+template <size_t N>
+HWY_API Vec128<int32_t, N> Shuffle2301(const Vec128<int32_t, N> v) {
+  static_assert(N == 2 || N == 4, "Does not make sense for N=1");
+  return Vec128<int32_t, N>{_mm_shuffle_epi32(v.raw, 0xB1)};
+}
+template <size_t N>
+HWY_API Vec128<float, N> Shuffle2301(const Vec128<float, N> v) {
+  static_assert(N == 2 || N == 4, "Does not make sense for N=1");
+  return Vec128<float, N>{_mm_shuffle_ps(v.raw, v.raw, 0xB1)};
+}
+
+// Swap 64-bit halves
+HWY_API Vec128<uint32_t> Shuffle1032(const Vec128<uint32_t> v) {
+  return Vec128<uint32_t>{_mm_shuffle_epi32(v.raw, 0x4E)};
+}
+HWY_API Vec128<int32_t> Shuffle1032(const Vec128<int32_t> v) {
+  return Vec128<int32_t>{_mm_shuffle_epi32(v.raw, 0x4E)};
+}
+HWY_API Vec128<float> Shuffle1032(const Vec128<float> v) {
+  return Vec128<float>{_mm_shuffle_ps(v.raw, v.raw, 0x4E)};
+}
+HWY_API Vec128<uint64_t> Shuffle01(const Vec128<uint64_t> v) {
+  return Vec128<uint64_t>{_mm_shuffle_epi32(v.raw, 0x4E)};
+}
+HWY_API Vec128<int64_t> Shuffle01(const Vec128<int64_t> v) {
+  return Vec128<int64_t>{_mm_shuffle_epi32(v.raw, 0x4E)};
+}
+HWY_API Vec128<double> Shuffle01(const Vec128<double> v) {
+  return Vec128<double>{_mm_shuffle_pd(v.raw, v.raw, 1)};
+}
+
+// Rotate right 32 bits
+HWY_API Vec128<uint32_t> Shuffle0321(const Vec128<uint32_t> v) {
+  return Vec128<uint32_t>{_mm_shuffle_epi32(v.raw, 0x39)};
+}
+HWY_API Vec128<int32_t> Shuffle0321(const Vec128<int32_t> v) {
+  return Vec128<int32_t>{_mm_shuffle_epi32(v.raw, 0x39)};
+}
+HWY_API Vec128<float> Shuffle0321(const Vec128<float> v) {
+  return Vec128<float>{_mm_shuffle_ps(v.raw, v.raw, 0x39)};
+}
+// Rotate left 32 bits
+HWY_API Vec128<uint32_t> Shuffle2103(const Vec128<uint32_t> v) {
+  return Vec128<uint32_t>{_mm_shuffle_epi32(v.raw, 0x93)};
+}
+HWY_API Vec128<int32_t> Shuffle2103(const Vec128<int32_t> v) {
+  return Vec128<int32_t>{_mm_shuffle_epi32(v.raw, 0x93)};
+}
+HWY_API Vec128<float> Shuffle2103(const Vec128<float> v) {
+  return Vec128<float>{_mm_shuffle_ps(v.raw, v.raw, 0x93)};
+}
+
+// Reverse
+HWY_API Vec128<uint32_t> Shuffle0123(const Vec128<uint32_t> v) {
+  return Vec128<uint32_t>{_mm_shuffle_epi32(v.raw, 0x1B)};
+}
+HWY_API Vec128<int32_t> Shuffle0123(const Vec128<int32_t> v) {
+  return Vec128<int32_t>{_mm_shuffle_epi32(v.raw, 0x1B)};
+}
+HWY_API Vec128<float> Shuffle0123(const Vec128<float> v) {
+  return Vec128<float>{_mm_shuffle_ps(v.raw, v.raw, 0x1B)};
+}
+
 // ================================================== COMPARE
 
 // Comparisons fill a lane with 1-bits if the condition is true, else 0.
@@ -516,6 +735,12 @@ HWY_API Mask128<TTo, N> RebindMask(Simd<TTo, N> /*tag*/, Mask128<TFrom, N> m) {
   return MaskFromVec(BitCast(Simd<TTo, N>(), VecFromMask(d, m)));
 }
 
+template <typename T, size_t N>
+HWY_API Mask128<T, N> TestBit(Vec128<T, N> v, Vec128<T, N> bit) {
+  static_assert(!hwy::IsFloat<T>(), "Only integer vectors supported");
+  return (v & bit) == bit;
+}
+
 // ------------------------------ Equality
 
 // Unsigned
@@ -537,7 +762,15 @@ HWY_API Mask128<uint32_t, N> operator==(const Vec128<uint32_t, N> a,
 template <size_t N>
 HWY_API Mask128<uint64_t, N> operator==(const Vec128<uint64_t, N> a,
                                         const Vec128<uint64_t, N> b) {
+#if HWY_TARGET == HWY_SSSE3
+  const Simd<uint32_t, N * 2> d32;
+  const Simd<uint64_t, N> d64;
+  const auto cmp32 = VecFromMask(d32, Eq(BitCast(d32, a), BitCast(d32, b)));
+  const auto cmp64 = cmp32 & Shuffle2301(cmp32);
+  return MaskFromVec(BitCast(d64, cmp64));
+#else
   return Mask128<uint64_t, N>{_mm_cmpeq_epi64(a.raw, b.raw)};
+#endif
 }
 
 // Signed
@@ -559,7 +792,9 @@ HWY_API Mask128<int32_t, N> operator==(const Vec128<int32_t, N> a,
 template <size_t N>
 HWY_API Mask128<int64_t, N> operator==(const Vec128<int64_t, N> a,
                                        const Vec128<int64_t, N> b) {
-  return Mask128<int64_t, N>{_mm_cmpeq_epi64(a.raw, b.raw)};
+  // Same as signed ==; avoid duplicating the SSSE3 version.
+  const Simd<uint64_t, N> du;
+  return RebindMask(Simd<int64_t, N>(), BitCast(du, a) == BitCast(du, b));
 }
 
 // Float
@@ -574,10 +809,22 @@ HWY_API Mask128<double, N> operator==(const Vec128<double, N> a,
   return Mask128<double, N>{_mm_cmpeq_pd(a.raw, b.raw)};
 }
 
-template <typename T, size_t N>
-HWY_API Mask128<T, N> TestBit(Vec128<T, N> v, Vec128<T, N> bit) {
-  static_assert(!hwy::IsFloat<T>(), "Only integer vectors supported");
-  return (v & bit) == bit;
+// ------------------------------ Inequality
+
+template <typename T, size_t N, HWY_IF_NOT_FLOAT(T)>
+HWY_API Mask128<T, N> operator!=(const Vec128<T, N> a, const Vec128<T, N> b) {
+  return Not(a == b);
+}
+
+template <size_t N>
+HWY_API Mask128<float, N> operator!=(const Vec128<float, N> a,
+                                     const Vec128<float, N> b) {
+  return Mask128<float, N>{_mm_cmpneq_ps(a.raw, b.raw)};
+}
+template <size_t N>
+HWY_API Mask128<double, N> operator!=(const Vec128<double, N> a,
+                                      const Vec128<double, N> b) {
+  return Mask128<double, N>{_mm_cmpneq_pd(a.raw, b.raw)};
 }
 
 // ------------------------------ Strict inequality
@@ -639,7 +886,7 @@ HWY_API Mask128<double, N> operator>(const Vec128<double, N> a,
 template <size_t N>
 HWY_API Mask128<int64_t, N> operator>(const Vec128<int64_t, N> a,
                                       const Vec128<int64_t, N> b) {
-#if HWY_TARGET == HWY_SSE4  // SSE4.1
+#if HWY_TARGET == HWY_SSSE3
   // If the upper half is less than or greater, this is the answer.
   const __m128i m_gt = _mm_cmpgt_epi32(a.raw, b.raw);
 
@@ -694,37 +941,230 @@ HWY_API Mask128<T, N> FirstN(const Simd<T, N> d, size_t num) {
   return RebindMask(d, Iota(di, 0) < Set(di, static_cast<MakeSigned<T>>(num)));
 }
 
-// ================================================== ARITHMETIC
+// ================================================== MEMORY (1)
 
-// ------------------------------ Addition
+// Clang static analysis claims the memory immediately after a partial vector
+// store is uninitialized, and also flags the input to partial loads (at least
+// for loadl_pd) as "garbage". This is a false alarm because msan does not
+// raise errors. We work around this by using CopyBytes instead of intrinsics,
+// but only for the analyzer to avoid potentially bad code generation.
+// Unfortunately __clang_analyzer__ was not defined for clang-tidy prior to v7.
+#ifndef HWY_SAFE_PARTIAL_LOAD_STORE
+#if defined(__clang_analyzer__) || \
+    (HWY_COMPILER_CLANG != 0 && HWY_COMPILER_CLANG < 700)
+#define HWY_SAFE_PARTIAL_LOAD_STORE 1
+#else
+#define HWY_SAFE_PARTIAL_LOAD_STORE 0
+#endif
+#endif  // HWY_SAFE_PARTIAL_LOAD_STORE
 
-// Unsigned
-template <size_t N>
-HWY_API Vec128<uint8_t, N> operator+(const Vec128<uint8_t, N> a,
-                                     const Vec128<uint8_t, N> b) {
-  return Vec128<uint8_t, N>{_mm_add_epi8(a.raw, b.raw)};
-}
-template <size_t N>
-HWY_API Vec128<uint16_t, N> operator+(const Vec128<uint16_t, N> a,
-                                      const Vec128<uint16_t, N> b) {
-  return Vec128<uint16_t, N>{_mm_add_epi16(a.raw, b.raw)};
+// ------------------------------ Load
+
+template <typename T>
+HWY_API Vec128<T> Load(Full128<T> /* tag */, const T* HWY_RESTRICT aligned) {
+  return Vec128<T>{_mm_load_si128(reinterpret_cast<const __m128i*>(aligned))};
 }
-template <size_t N>
-HWY_API Vec128<uint32_t, N> operator+(const Vec128<uint32_t, N> a,
-                                      const Vec128<uint32_t, N> b) {
-  return Vec128<uint32_t, N>{_mm_add_epi32(a.raw, b.raw)};
+HWY_API Vec128<float> Load(Full128<float> /* tag */,
+                           const float* HWY_RESTRICT aligned) {
+  return Vec128<float>{_mm_load_ps(aligned)};
 }
-template <size_t N>
-HWY_API Vec128<uint64_t, N> operator+(const Vec128<uint64_t, N> a,
-                                      const Vec128<uint64_t, N> b) {
-  return Vec128<uint64_t, N>{_mm_add_epi64(a.raw, b.raw)};
+HWY_API Vec128<double> Load(Full128<double> /* tag */,
+                            const double* HWY_RESTRICT aligned) {
+  return Vec128<double>{_mm_load_pd(aligned)};
 }
 
-// Signed
-template <size_t N>
-HWY_API Vec128<int8_t, N> operator+(const Vec128<int8_t, N> a,
-                                    const Vec128<int8_t, N> b) {
-  return Vec128<int8_t, N>{_mm_add_epi8(a.raw, b.raw)};
+template <typename T>
+HWY_API Vec128<T> LoadU(Full128<T> /* tag */, const T* HWY_RESTRICT p) {
+  return Vec128<T>{_mm_loadu_si128(reinterpret_cast<const __m128i*>(p))};
+}
+HWY_API Vec128<float> LoadU(Full128<float> /* tag */,
+                            const float* HWY_RESTRICT p) {
+  return Vec128<float>{_mm_loadu_ps(p)};
+}
+HWY_API Vec128<double> LoadU(Full128<double> /* tag */,
+                             const double* HWY_RESTRICT p) {
+  return Vec128<double>{_mm_loadu_pd(p)};
+}
+
+template <typename T>
+HWY_API Vec128<T, 8 / sizeof(T)> Load(Simd<T, 8 / sizeof(T)> /* tag */,
+                                      const T* HWY_RESTRICT p) {
+#if HWY_SAFE_PARTIAL_LOAD_STORE
+  __m128i v = _mm_setzero_si128();
+  CopyBytes<8>(p, &v);
+  return Vec128<T, 8 / sizeof(T)>{v};
+#else
+  return Vec128<T, 8 / sizeof(T)>{
+      _mm_loadl_epi64(reinterpret_cast<const __m128i*>(p))};
+#endif
+}
+
+HWY_API Vec128<float, 2> Load(Simd<float, 2> /* tag */,
+                              const float* HWY_RESTRICT p) {
+#if HWY_SAFE_PARTIAL_LOAD_STORE
+  __m128 v = _mm_setzero_ps();
+  CopyBytes<8>(p, &v);
+  return Vec128<float, 2>{v};
+#else
+  const __m128 hi = _mm_setzero_ps();
+  return Vec128<float, 2>{_mm_loadl_pi(hi, reinterpret_cast<const __m64*>(p))};
+#endif
+}
+
+HWY_API Vec128<double, 1> Load(Simd<double, 1> /* tag */,
+                               const double* HWY_RESTRICT p) {
+#if HWY_SAFE_PARTIAL_LOAD_STORE
+  __m128d v = _mm_setzero_pd();
+  CopyBytes<8>(p, &v);
+  return Vec128<double, 1>{v};
+#else
+  return Vec128<double, 1>{_mm_load_sd(p)};
+#endif
+}
+
+HWY_API Vec128<float, 1> Load(Simd<float, 1> /* tag */,
+                              const float* HWY_RESTRICT p) {
+#if HWY_SAFE_PARTIAL_LOAD_STORE
+  __m128 v = _mm_setzero_ps();
+  CopyBytes<4>(p, &v);
+  return Vec128<float, 1>{v};
+#else
+  return Vec128<float, 1>{_mm_load_ss(p)};
+#endif
+}
+
+// Any <= 32 bit except <float, 1>
+template <typename T, size_t N, HWY_IF_LE32(T, N)>
+HWY_API Vec128<T, N> Load(Simd<T, N> /* tag */, const T* HWY_RESTRICT p) {
+  constexpr size_t kSize = sizeof(T) * N;
+#if HWY_SAFE_PARTIAL_LOAD_STORE
+  __m128 v = _mm_setzero_ps();
+  CopyBytes<kSize>(p, &v);
+  return Vec128<T, N>{v};
+#else
+  int32_t bits;
+  CopyBytes<kSize>(p, &bits);
+  return Vec128<T, N>{_mm_cvtsi32_si128(bits)};
+#endif
+}
+
+// For < 128 bit, LoadU == Load.
+template <typename T, size_t N, HWY_IF_LE64(T, N)>
+HWY_API Vec128<T, N> LoadU(Simd<T, N> d, const T* HWY_RESTRICT p) {
+  return Load(d, p);
+}
+
+// 128-bit SIMD => nothing to duplicate, same as an unaligned load.
+template <typename T, size_t N, HWY_IF_LE128(T, N)>
+HWY_API Vec128<T, N> LoadDup128(Simd<T, N> d, const T* HWY_RESTRICT p) {
+  return LoadU(d, p);
+}
+
+// ------------------------------ Store
+
+template <typename T>
+HWY_API void Store(Vec128<T> v, Full128<T> /* tag */, T* HWY_RESTRICT aligned) {
+  _mm_store_si128(reinterpret_cast<__m128i*>(aligned), v.raw);
+}
+HWY_API void Store(const Vec128<float> v, Full128<float> /* tag */,
+                   float* HWY_RESTRICT aligned) {
+  _mm_store_ps(aligned, v.raw);
+}
+HWY_API void Store(const Vec128<double> v, Full128<double> /* tag */,
+                   double* HWY_RESTRICT aligned) {
+  _mm_store_pd(aligned, v.raw);
+}
+
+template <typename T>
+HWY_API void StoreU(Vec128<T> v, Full128<T> /* tag */, T* HWY_RESTRICT p) {
+  _mm_storeu_si128(reinterpret_cast<__m128i*>(p), v.raw);
+}
+HWY_API void StoreU(const Vec128<float> v, Full128<float> /* tag */,
+                    float* HWY_RESTRICT p) {
+  _mm_storeu_ps(p, v.raw);
+}
+HWY_API void StoreU(const Vec128<double> v, Full128<double> /* tag */,
+                    double* HWY_RESTRICT p) {
+  _mm_storeu_pd(p, v.raw);
+}
+
+template <typename T>
+HWY_API void Store(Vec128<T, 8 / sizeof(T)> v, Simd<T, 8 / sizeof(T)> /* tag */,
+                   T* HWY_RESTRICT p) {
+#if HWY_SAFE_PARTIAL_LOAD_STORE
+  CopyBytes<8>(&v, p);
+#else
+  _mm_storel_epi64(reinterpret_cast<__m128i*>(p), v.raw);
+#endif
+}
+HWY_API void Store(const Vec128<float, 2> v, Simd<float, 2> /* tag */,
+                   float* HWY_RESTRICT p) {
+#if HWY_SAFE_PARTIAL_LOAD_STORE
+  CopyBytes<8>(&v, p);
+#else
+  _mm_storel_pi(reinterpret_cast<__m64*>(p), v.raw);
+#endif
+}
+HWY_API void Store(const Vec128<double, 1> v, Simd<double, 1> /* tag */,
+                   double* HWY_RESTRICT p) {
+#if HWY_SAFE_PARTIAL_LOAD_STORE
+  CopyBytes<8>(&v, p);
+#else
+  _mm_storel_pd(p, v.raw);
+#endif
+}
+
+// Any <= 32 bit except <float, 1>
+template <typename T, size_t N, HWY_IF_LE32(T, N)>
+HWY_API void Store(Vec128<T, N> v, Simd<T, N> /* tag */, T* HWY_RESTRICT p) {
+  CopyBytes<sizeof(T) * N>(&v, p);
+}
+HWY_API void Store(const Vec128<float, 1> v, Simd<float, 1> /* tag */,
+                   float* HWY_RESTRICT p) {
+#if HWY_SAFE_PARTIAL_LOAD_STORE
+  CopyBytes<4>(&v, p);
+#else
+  _mm_store_ss(p, v.raw);
+#endif
+}
+
+// For < 128 bit, StoreU == Store.
+template <typename T, size_t N, HWY_IF_LE64(T, N)>
+HWY_API void StoreU(const Vec128<T, N> v, Simd<T, N> d, T* HWY_RESTRICT p) {
+  Store(v, d, p);
+}
+
+// ================================================== ARITHMETIC
+
+// ------------------------------ Addition
+
+// Unsigned
+template <size_t N>
+HWY_API Vec128<uint8_t, N> operator+(const Vec128<uint8_t, N> a,
+                                     const Vec128<uint8_t, N> b) {
+  return Vec128<uint8_t, N>{_mm_add_epi8(a.raw, b.raw)};
+}
+template <size_t N>
+HWY_API Vec128<uint16_t, N> operator+(const Vec128<uint16_t, N> a,
+                                      const Vec128<uint16_t, N> b) {
+  return Vec128<uint16_t, N>{_mm_add_epi16(a.raw, b.raw)};
+}
+template <size_t N>
+HWY_API Vec128<uint32_t, N> operator+(const Vec128<uint32_t, N> a,
+                                      const Vec128<uint32_t, N> b) {
+  return Vec128<uint32_t, N>{_mm_add_epi32(a.raw, b.raw)};
+}
+template <size_t N>
+HWY_API Vec128<uint64_t, N> operator+(const Vec128<uint64_t, N> a,
+                                      const Vec128<uint64_t, N> b) {
+  return Vec128<uint64_t, N>{_mm_add_epi64(a.raw, b.raw)};
+}
+
+// Signed
+template <size_t N>
+HWY_API Vec128<int8_t, N> operator+(const Vec128<int8_t, N> a,
+                                    const Vec128<int8_t, N> b) {
+  return Vec128<int8_t, N>{_mm_add_epi8(a.raw, b.raw)};
 }
 template <size_t N>
 HWY_API Vec128<int16_t, N> operator+(const Vec128<int16_t, N> a,
@@ -884,64 +1324,18 @@ HWY_API Vec128<uint16_t, N> AverageRound(const Vec128<uint16_t, N> a,
   return Vec128<uint16_t, N>{_mm_avg_epu16(a.raw, b.raw)};
 }
 
-// ------------------------------ Abs
-
-// Returns absolute value, except that LimitsMin() maps to LimitsMax() + 1.
-template <size_t N>
-HWY_API Vec128<int8_t, N> Abs(const Vec128<int8_t, N> v) {
-#if HWY_COMPILER_MSVC
-  // Workaround for incorrect codegen? (reaches breakpoint)
-  const auto zero = Zero(Simd<int8_t, N>());
-  return Vec128<int8_t, N>{_mm_max_epi8(v.raw, (zero - v).raw)};
-#else
-  return Vec128<int8_t, N>{_mm_abs_epi8(v.raw)};
-#endif
-}
-template <size_t N>
-HWY_API Vec128<int16_t, N> Abs(const Vec128<int16_t, N> v) {
-  return Vec128<int16_t, N>{_mm_abs_epi16(v.raw)};
-}
-template <size_t N>
-HWY_API Vec128<int32_t, N> Abs(const Vec128<int32_t, N> v) {
-  return Vec128<int32_t, N>{_mm_abs_epi32(v.raw)};
-}
-// i64 is implemented after BroadcastSignBit.
-template <size_t N>
-HWY_API Vec128<float, N> Abs(const Vec128<float, N> v) {
-  const Vec128<int32_t, N> mask{_mm_set1_epi32(0x7FFFFFFF)};
-  return v & BitCast(Simd<float, N>(), mask);
-}
-template <size_t N>
-HWY_API Vec128<double, N> Abs(const Vec128<double, N> v) {
-  const Vec128<int64_t, N> mask{_mm_set1_epi64x(0x7FFFFFFFFFFFFFFFLL)};
-  return v & BitCast(Simd<double, N>(), mask);
-}
-
 // ------------------------------ Integer multiplication
 
-// Unsigned
 template <size_t N>
 HWY_API Vec128<uint16_t, N> operator*(const Vec128<uint16_t, N> a,
                                       const Vec128<uint16_t, N> b) {
   return Vec128<uint16_t, N>{_mm_mullo_epi16(a.raw, b.raw)};
 }
 template <size_t N>
-HWY_API Vec128<uint32_t, N> operator*(const Vec128<uint32_t, N> a,
-                                      const Vec128<uint32_t, N> b) {
-  return Vec128<uint32_t, N>{_mm_mullo_epi32(a.raw, b.raw)};
-}
-
-// Signed
-template <size_t N>
 HWY_API Vec128<int16_t, N> operator*(const Vec128<int16_t, N> a,
                                      const Vec128<int16_t, N> b) {
   return Vec128<int16_t, N>{_mm_mullo_epi16(a.raw, b.raw)};
 }
-template <size_t N>
-HWY_API Vec128<int32_t, N> operator*(const Vec128<int32_t, N> a,
-                                     const Vec128<int32_t, N> b) {
-  return Vec128<int32_t, N>{_mm_mullo_epi32(a.raw, b.raw)};
-}
 
 // Returns the upper 16 bits of a * b in each lane.
 template <size_t N>
@@ -958,36 +1352,93 @@ HWY_API Vec128<int16_t, N> MulHigh(const Vec128<int16_t, N> a,
 // Multiplies even lanes (0, 2 ..) and places the double-wide result into
 // even and the upper half into its odd neighbor lane.
 template <size_t N>
-HWY_API Vec128<int64_t, (N + 1) / 2> MulEven(const Vec128<int32_t, N> a,
-                                             const Vec128<int32_t, N> b) {
-  return Vec128<int64_t, (N + 1) / 2>{_mm_mul_epi32(a.raw, b.raw)};
-}
-template <size_t N>
 HWY_API Vec128<uint64_t, (N + 1) / 2> MulEven(const Vec128<uint32_t, N> a,
                                               const Vec128<uint32_t, N> b) {
   return Vec128<uint64_t, (N + 1) / 2>{_mm_mul_epu32(a.raw, b.raw)};
 }
 
-// ------------------------------ ShiftLeft
+#if HWY_TARGET == HWY_SSSE3
 
-template <int kBits, size_t N>
-HWY_API Vec128<uint16_t, N> ShiftLeft(const Vec128<uint16_t, N> v) {
-  return Vec128<uint16_t, N>{_mm_slli_epi16(v.raw, kBits)};
+template <size_t N, HWY_IF_LE64(int32_t, N)>  // N=1 or 2
+HWY_API Vec128<int64_t, (N + 1) / 2> MulEven(const Vec128<int32_t, N> a,
+                                             const Vec128<int32_t, N> b) {
+  return Set(Simd<int64_t, (N + 1) / 2>(), int64_t(GetLane(a)) * GetLane(b));
 }
-
-template <int kBits, size_t N>
-HWY_API Vec128<uint32_t, N> ShiftLeft(const Vec128<uint32_t, N> v) {
-  return Vec128<uint32_t, N>{_mm_slli_epi32(v.raw, kBits)};
+HWY_API Vec128<int64_t> MulEven(const Vec128<int32_t> a,
+                                const Vec128<int32_t> b) {
+  alignas(16) int32_t a_lanes[4];
+  alignas(16) int32_t b_lanes[4];
+  const Full128<int32_t> di32;
+  Store(a, di32, a_lanes);
+  Store(b, di32, b_lanes);
+  alignas(16) int64_t mul[2];
+  mul[0] = int64_t(a_lanes[0]) * b_lanes[0];
+  mul[1] = int64_t(a_lanes[2]) * b_lanes[2];
+  return Load(Full128<int64_t>(), mul);
 }
 
-template <int kBits, size_t N>
-HWY_API Vec128<uint64_t, N> ShiftLeft(const Vec128<uint64_t, N> v) {
-  return Vec128<uint64_t, N>{_mm_slli_epi64(v.raw, kBits)};
-}
+#else  // HWY_TARGET == HWY_SSSE3
 
-template <int kBits, size_t N>
-HWY_API Vec128<int16_t, N> ShiftLeft(const Vec128<int16_t, N> v) {
-  return Vec128<int16_t, N>{_mm_slli_epi16(v.raw, kBits)};
+template <size_t N>
+HWY_API Vec128<int64_t, (N + 1) / 2> MulEven(const Vec128<int32_t, N> a,
+                                             const Vec128<int32_t, N> b) {
+  return Vec128<int64_t, (N + 1) / 2>{_mm_mul_epi32(a.raw, b.raw)};
+}
+
+#endif  // HWY_TARGET == HWY_SSSE3
+
+template <size_t N>
+HWY_API Vec128<uint32_t, N> operator*(const Vec128<uint32_t, N> a,
+                                      const Vec128<uint32_t, N> b) {
+#if HWY_TARGET == HWY_SSSE3
+  // Not as inefficient as it looks: _mm_mullo_epi32 has 10 cycle latency.
+  // 64-bit right shift would also work but also needs port 5, so no benefit.
+  // Notation: x=don't care, z=0.
+  const __m128i a_x3x1 = _mm_shuffle_epi32(a.raw, _MM_SHUFFLE(3, 3, 1, 1));
+  const auto mullo_x2x0 = MulEven(a, b);
+  const __m128i b_x3x1 = _mm_shuffle_epi32(b.raw, _MM_SHUFFLE(3, 3, 1, 1));
+  const auto mullo_x3x1 =
+      MulEven(Vec128<uint32_t, N>{a_x3x1}, Vec128<uint32_t, N>{b_x3x1});
+  // We could _mm_slli_epi64 by 32 to get 3z1z and OR with z2z0, but generating
+  // the latter requires one more instruction or a constant.
+  const __m128i mul_20 =
+      _mm_shuffle_epi32(mullo_x2x0.raw, _MM_SHUFFLE(2, 0, 2, 0));
+  const __m128i mul_31 =
+      _mm_shuffle_epi32(mullo_x3x1.raw, _MM_SHUFFLE(2, 0, 2, 0));
+  return Vec128<uint32_t, N>{_mm_unpacklo_epi32(mul_20, mul_31)};
+#else
+  return Vec128<uint32_t, N>{_mm_mullo_epi32(a.raw, b.raw)};
+#endif
+}
+
+template <size_t N>
+HWY_API Vec128<int32_t, N> operator*(const Vec128<int32_t, N> a,
+                                     const Vec128<int32_t, N> b) {
+  // Same as unsigned; avoid duplicating the SSSE3 code.
+  const Simd<uint32_t, N> du;
+  return BitCast(Simd<int32_t, N>(), BitCast(du, a) * BitCast(du, b));
+}
+
+// ------------------------------ ShiftLeft
+
+template <int kBits, size_t N>
+HWY_API Vec128<uint16_t, N> ShiftLeft(const Vec128<uint16_t, N> v) {
+  return Vec128<uint16_t, N>{_mm_slli_epi16(v.raw, kBits)};
+}
+
+template <int kBits, size_t N>
+HWY_API Vec128<uint32_t, N> ShiftLeft(const Vec128<uint32_t, N> v) {
+  return Vec128<uint32_t, N>{_mm_slli_epi32(v.raw, kBits)};
+}
+
+template <int kBits, size_t N>
+HWY_API Vec128<uint64_t, N> ShiftLeft(const Vec128<uint64_t, N> v) {
+  return Vec128<uint64_t, N>{_mm_slli_epi64(v.raw, kBits)};
+}
+
+template <int kBits, size_t N>
+HWY_API Vec128<int16_t, N> ShiftLeft(const Vec128<int16_t, N> v) {
+  return Vec128<int16_t, N>{_mm_slli_epi16(v.raw, kBits)};
 }
 template <int kBits, size_t N>
 HWY_API Vec128<int32_t, N> ShiftLeft(const Vec128<int32_t, N> v) {
@@ -1071,13 +1522,13 @@ HWY_API Vec128<int32_t, N> BroadcastSignBit(const Vec128<int32_t, N> v) {
 
 template <size_t N>
 HWY_API Vec128<int64_t, N> BroadcastSignBit(const Vec128<int64_t, N> v) {
-#if HWY_TARGET == HWY_AVX3
+#if HWY_TARGET <= HWY_AVX3
   return Vec128<int64_t, N>{_mm_srai_epi64(v.raw, 63)};
-#elif HWY_TARGET == HWY_AVX2
+#elif HWY_TARGET == HWY_AVX2 || HWY_TARGET == HWY_SSE4
   return VecFromMask(v < Zero(Simd<int64_t, N>()));
 #else
-  // Efficient Gt() requires SSE4.2 but we only have SSE4.1. BLENDVPD requires
-  // two constants and domain crossing. 32-bit shift avoids generating a zero.
+  // Efficient Lt() requires SSE4.2 and BLENDVPD requires SSE4.1. 32-bit shift
+  // avoids generating a zero.
   const Simd<int32_t, N * 2> d32;
   const auto sign = ShiftRight<31>(BitCast(d32, v));
   return Vec128<int64_t, N>{
@@ -1087,17 +1538,17 @@ HWY_API Vec128<int64_t, N> BroadcastSignBit(const Vec128<int64_t, N> v) {
 
 template <size_t N>
 HWY_API Vec128<int64_t, N> Abs(const Vec128<int64_t, N> v) {
-#if HWY_TARGET == HWY_AVX3
+#if HWY_TARGET <= HWY_AVX3
   return Vec128<int64_t, N>{_mm_abs_epi64(v.raw)};
 #else
-  const auto zero = Zero(Simd<int64_t,N>());
+  const auto zero = Zero(Simd<int64_t, N>());
   return IfThenElse(MaskFromVec(BroadcastSignBit(v)), zero - v, v);
 #endif
 }
 
 template <int kBits, size_t N>
 HWY_API Vec128<int64_t, N> ShiftRight(const Vec128<int64_t, N> v) {
-#if HWY_TARGET == HWY_AVX3
+#if HWY_TARGET <= HWY_AVX3
   return Vec128<int64_t, N>{_mm_srai_epi64(v.raw, kBits)};
 #else
   const Simd<int64_t, N> di;
@@ -1108,6 +1559,19 @@ HWY_API Vec128<int64_t, N> ShiftRight(const Vec128<int64_t, N> v) {
 #endif
 }
 
+// ------------------------------ ZeroIfNegative (BroadcastSignBit)
+template <typename T, size_t N, HWY_IF_FLOAT(T)>
+HWY_API Vec128<T, N> ZeroIfNegative(Vec128<T, N> v) {
+  const Simd<T, N> d;
+#if HWY_TARGET == HWY_SSSE3
+  const RebindToSigned<decltype(d)> di;
+  const auto mask = MaskFromVec(BitCast(d, BroadcastSignBit(BitCast(di, v))));
+#else
+  const auto mask = MaskFromVec(v);  // MSB is sufficient for BLENDVPS
+#endif
+  return IfThenElse(mask, Zero(d), v);
+}
+
 // ------------------------------ ShiftLeftSame
 
 template <size_t N>
@@ -1195,7 +1659,7 @@ HWY_API Vec128<int32_t, N> ShiftRightSame(const Vec128<int32_t, N> v,
 template <size_t N>
 HWY_API Vec128<int64_t, N> ShiftRightSame(const Vec128<int64_t, N> v,
                                           const int bits) {
-#if HWY_TARGET == HWY_AVX3
+#if HWY_TARGET <= HWY_AVX3
   return Vec128<int64_t, N>{_mm_sra_epi64(v.raw, _mm_cvtsi32_si128(bits))};
 #else
   const Simd<int64_t, N> di;
@@ -1215,18 +1679,6 @@ HWY_API Vec128<int8_t, N> ShiftRightSame(Vec128<int8_t, N> v, const int bits) {
   return (shifted ^ shifted_sign) - shifted_sign;
 }
 
-// ------------------------------ Negate
-
-template <typename T, size_t N, HWY_IF_FLOAT(T)>
-HWY_API Vec128<T, N> Neg(const Vec128<T, N> v) {
-  return Xor(v, SignBit(Simd<T, N>()));
-}
-
-template <typename T, size_t N, HWY_IF_NOT_FLOAT(T)>
-HWY_API Vec128<T, N> Neg(const Vec128<T, N> v) {
-  return Zero(Simd<T, N>()) - v;
-}
-
 // ------------------------------ Floating-point mul / div
 
 template <size_t N>
@@ -1289,7 +1741,7 @@ template <size_t N>
 HWY_API Vec128<float, N> MulAdd(const Vec128<float, N> mul,
                                 const Vec128<float, N> x,
                                 const Vec128<float, N> add) {
-#if HWY_TARGET == HWY_SSE4
+#if HWY_TARGET == HWY_SSSE3 || HWY_TARGET == HWY_SSE4
   return mul * x + add;
 #else
   return Vec128<float, N>{_mm_fmadd_ps(mul.raw, x.raw, add.raw)};
@@ -1299,7 +1751,7 @@ template <size_t N>
 HWY_API Vec128<double, N> MulAdd(const Vec128<double, N> mul,
                                  const Vec128<double, N> x,
                                  const Vec128<double, N> add) {
-#if HWY_TARGET == HWY_SSE4
+#if HWY_TARGET == HWY_SSSE3 || HWY_TARGET == HWY_SSE4
   return mul * x + add;
 #else
   return Vec128<double, N>{_mm_fmadd_pd(mul.raw, x.raw, add.raw)};
@@ -1311,7 +1763,7 @@ template <size_t N>
 HWY_API Vec128<float, N> NegMulAdd(const Vec128<float, N> mul,
                                    const Vec128<float, N> x,
                                    const Vec128<float, N> add) {
-#if HWY_TARGET == HWY_SSE4
+#if HWY_TARGET == HWY_SSSE3 || HWY_TARGET == HWY_SSE4
   return add - mul * x;
 #else
   return Vec128<float, N>{_mm_fnmadd_ps(mul.raw, x.raw, add.raw)};
@@ -1321,7 +1773,7 @@ template <size_t N>
 HWY_API Vec128<double, N> NegMulAdd(const Vec128<double, N> mul,
                                     const Vec128<double, N> x,
                                     const Vec128<double, N> add) {
-#if HWY_TARGET == HWY_SSE4
+#if HWY_TARGET == HWY_SSSE3 || HWY_TARGET == HWY_SSE4
   return add - mul * x;
 #else
   return Vec128<double, N>{_mm_fnmadd_pd(mul.raw, x.raw, add.raw)};
@@ -1333,7 +1785,7 @@ template <size_t N>
 HWY_API Vec128<float, N> MulSub(const Vec128<float, N> mul,
                                 const Vec128<float, N> x,
                                 const Vec128<float, N> sub) {
-#if HWY_TARGET == HWY_SSE4
+#if HWY_TARGET == HWY_SSSE3 || HWY_TARGET == HWY_SSE4
   return mul * x - sub;
 #else
   return Vec128<float, N>{_mm_fmsub_ps(mul.raw, x.raw, sub.raw)};
@@ -1343,7 +1795,7 @@ template <size_t N>
 HWY_API Vec128<double, N> MulSub(const Vec128<double, N> mul,
                                  const Vec128<double, N> x,
                                  const Vec128<double, N> sub) {
-#if HWY_TARGET == HWY_SSE4
+#if HWY_TARGET == HWY_SSSE3 || HWY_TARGET == HWY_SSE4
   return mul * x - sub;
 #else
   return Vec128<double, N>{_mm_fmsub_pd(mul.raw, x.raw, sub.raw)};
@@ -1355,7 +1807,7 @@ template <size_t N>
 HWY_API Vec128<float, N> NegMulSub(const Vec128<float, N> mul,
                                    const Vec128<float, N> x,
                                    const Vec128<float, N> sub) {
-#if HWY_TARGET == HWY_SSE4
+#if HWY_TARGET == HWY_SSSE3 || HWY_TARGET == HWY_SSE4
   return Neg(mul) * x - sub;
 #else
   return Vec128<float, N>{_mm_fnmsub_ps(mul.raw, x.raw, sub.raw)};
@@ -1365,7 +1817,7 @@ template <size_t N>
 HWY_API Vec128<double, N> NegMulSub(const Vec128<double, N> mul,
                                     const Vec128<double, N> x,
                                     const Vec128<double, N> sub) {
-#if HWY_TARGET == HWY_SSE4
+#if HWY_TARGET == HWY_SSSE3 || HWY_TARGET == HWY_SSE4
   return Neg(mul) * x - sub;
 #else
   return Vec128<double, N>{_mm_fnmsub_pd(mul.raw, x.raw, sub.raw)};
@@ -1399,57 +1851,21 @@ HWY_API Vec128<float, 1> ApproximateReciprocalSqrt(const Vec128<float, 1> v) {
   return Vec128<float, 1>{_mm_rsqrt_ss(v.raw)};
 }
 
-// ------------------------------ Floating-point rounding
-
-// Toward nearest integer, ties to even
-template <size_t N>
-HWY_API Vec128<float, N> Round(const Vec128<float, N> v) {
-  return Vec128<float, N>{
-      _mm_round_ps(v.raw, _MM_FROUND_TO_NEAREST_INT | _MM_FROUND_NO_EXC)};
-}
-template <size_t N>
-HWY_API Vec128<double, N> Round(const Vec128<double, N> v) {
-  return Vec128<double, N>{
-      _mm_round_pd(v.raw, _MM_FROUND_TO_NEAREST_INT | _MM_FROUND_NO_EXC)};
-}
-
-// Toward zero, aka truncate
-template <size_t N>
-HWY_API Vec128<float, N> Trunc(const Vec128<float, N> v) {
-  return Vec128<float, N>{
-      _mm_round_ps(v.raw, _MM_FROUND_TO_ZERO | _MM_FROUND_NO_EXC)};
-}
-template <size_t N>
-HWY_API Vec128<double, N> Trunc(const Vec128<double, N> v) {
-  return Vec128<double, N>{
-      _mm_round_pd(v.raw, _MM_FROUND_TO_ZERO | _MM_FROUND_NO_EXC)};
-}
+// ------------------------------ Min (Gt, IfThenElse)
 
-// Toward +infinity, aka ceiling
-template <size_t N>
-HWY_API Vec128<float, N> Ceil(const Vec128<float, N> v) {
-  return Vec128<float, N>{
-      _mm_round_ps(v.raw, _MM_FROUND_TO_POS_INF | _MM_FROUND_NO_EXC)};
-}
-template <size_t N>
-HWY_API Vec128<double, N> Ceil(const Vec128<double, N> v) {
-  return Vec128<double, N>{
-      _mm_round_pd(v.raw, _MM_FROUND_TO_POS_INF | _MM_FROUND_NO_EXC)};
-}
+namespace detail {
 
-// Toward -infinity, aka floor
-template <size_t N>
-HWY_API Vec128<float, N> Floor(const Vec128<float, N> v) {
-  return Vec128<float, N>{
-      _mm_round_ps(v.raw, _MM_FROUND_TO_NEG_INF | _MM_FROUND_NO_EXC)};
-}
-template <size_t N>
-HWY_API Vec128<double, N> Floor(const Vec128<double, N> v) {
-  return Vec128<double, N>{
-      _mm_round_pd(v.raw, _MM_FROUND_TO_NEG_INF | _MM_FROUND_NO_EXC)};
+template <typename T, size_t N>
+HWY_INLINE HWY_MAYBE_UNUSED Vec128<T, N> MinU(const Vec128<T, N> a,
+                                              const Vec128<T, N> b) {
+  const Simd<T, N> du;
+  const RebindToSigned<decltype(du)> di;
+  const auto msb = Set(du, static_cast<T>(T(1) << (sizeof(T) * 8 - 1)));
+  const auto gt = RebindMask(du, BitCast(di, a ^ msb) > BitCast(di, b ^ msb));
+  return IfThenElse(gt, b, a);
 }
 
-// ------------------------------ Min (Gt, IfThenElse)
+}  // namespace detail
 
 // Unsigned
 template <size_t N>
@@ -1460,24 +1876,28 @@ HWY_API Vec128<uint8_t, N> Min(const Vec128<uint8_t, N> a,
 template <size_t N>
 HWY_API Vec128<uint16_t, N> Min(const Vec128<uint16_t, N> a,
                                 const Vec128<uint16_t, N> b) {
+#if HWY_TARGET == HWY_SSSE3
+  return detail::MinU(a, b);
+#else
   return Vec128<uint16_t, N>{_mm_min_epu16(a.raw, b.raw)};
+#endif
 }
 template <size_t N>
 HWY_API Vec128<uint32_t, N> Min(const Vec128<uint32_t, N> a,
                                 const Vec128<uint32_t, N> b) {
+#if HWY_TARGET == HWY_SSSE3
+  return detail::MinU(a, b);
+#else
   return Vec128<uint32_t, N>{_mm_min_epu32(a.raw, b.raw)};
+#endif
 }
 template <size_t N>
 HWY_API Vec128<uint64_t, N> Min(const Vec128<uint64_t, N> a,
                                 const Vec128<uint64_t, N> b) {
-#if HWY_TARGET == HWY_AVX3
+#if HWY_TARGET <= HWY_AVX3
   return Vec128<uint64_t, N>{_mm_min_epu64(a.raw, b.raw)};
 #else
-  const Simd<uint64_t, N> du;
-  const Simd<int64_t, N> di;
-  const auto msb = Set(du, 1ull << 63);
-  const auto gt = RebindMask(du, BitCast(di, a ^ msb) > BitCast(di, b ^ msb));
-  return IfThenElse(gt, b, a);
+  return detail::MinU(a, b);
 #endif
 }
 
@@ -1485,7 +1905,11 @@ HWY_API Vec128<uint64_t, N> Min(const Vec128<uint64_t, N> a,
 template <size_t N>
 HWY_API Vec128<int8_t, N> Min(const Vec128<int8_t, N> a,
                               const Vec128<int8_t, N> b) {
+#if HWY_TARGET == HWY_SSSE3
+  return IfThenElse(a < b, a, b);
+#else
   return Vec128<int8_t, N>{_mm_min_epi8(a.raw, b.raw)};
+#endif
 }
 template <size_t N>
 HWY_API Vec128<int16_t, N> Min(const Vec128<int16_t, N> a,
@@ -1495,12 +1919,16 @@ HWY_API Vec128<int16_t, N> Min(const Vec128<int16_t, N> a,
 template <size_t N>
 HWY_API Vec128<int32_t, N> Min(const Vec128<int32_t, N> a,
                                const Vec128<int32_t, N> b) {
+#if HWY_TARGET == HWY_SSSE3
+  return IfThenElse(a < b, a, b);
+#else
   return Vec128<int32_t, N>{_mm_min_epi32(a.raw, b.raw)};
+#endif
 }
 template <size_t N>
 HWY_API Vec128<int64_t, N> Min(const Vec128<int64_t, N> a,
                                const Vec128<int64_t, N> b) {
-#if HWY_TARGET == HWY_AVX3
+#if HWY_TARGET <= HWY_AVX3
   return Vec128<int64_t, N>{_mm_min_epi64(a.raw, b.raw)};
 #else
   return IfThenElse(a < b, a, b);
@@ -1521,6 +1949,19 @@ HWY_API Vec128<double, N> Min(const Vec128<double, N> a,
 
 // ------------------------------ Max (Gt, IfThenElse)
 
+namespace detail {
+template <typename T, size_t N>
+HWY_INLINE HWY_MAYBE_UNUSED Vec128<T, N> MaxU(const Vec128<T, N> a,
+                                              const Vec128<T, N> b) {
+  const Simd<T, N> du;
+  const RebindToSigned<decltype(du)> di;
+  const auto msb = Set(du, static_cast<T>(T(1) << (sizeof(T) * 8 - 1)));
+  const auto gt = RebindMask(du, BitCast(di, a ^ msb) > BitCast(di, b ^ msb));
+  return IfThenElse(gt, a, b);
+}
+
+}  // namespace detail
+
 // Unsigned
 template <size_t N>
 HWY_API Vec128<uint8_t, N> Max(const Vec128<uint8_t, N> a,
@@ -1530,24 +1971,28 @@ HWY_API Vec128<uint8_t, N> Max(const Vec128<uint8_t, N> a,
 template <size_t N>
 HWY_API Vec128<uint16_t, N> Max(const Vec128<uint16_t, N> a,
                                 const Vec128<uint16_t, N> b) {
+#if HWY_TARGET == HWY_SSSE3
+  return detail::MaxU(a, b);
+#else
   return Vec128<uint16_t, N>{_mm_max_epu16(a.raw, b.raw)};
+#endif
 }
 template <size_t N>
 HWY_API Vec128<uint32_t, N> Max(const Vec128<uint32_t, N> a,
                                 const Vec128<uint32_t, N> b) {
+#if HWY_TARGET == HWY_SSSE3
+  return detail::MaxU(a, b);
+#else
   return Vec128<uint32_t, N>{_mm_max_epu32(a.raw, b.raw)};
+#endif
 }
 template <size_t N>
 HWY_API Vec128<uint64_t, N> Max(const Vec128<uint64_t, N> a,
                                 const Vec128<uint64_t, N> b) {
-#if HWY_TARGET == HWY_AVX3
+#if HWY_TARGET <= HWY_AVX3
   return Vec128<uint64_t, N>{_mm_max_epu64(a.raw, b.raw)};
 #else
-  const Simd<uint64_t, N> du;
-  const Simd<int64_t, N> di;
-  const auto msb = Set(du, 1ull << 63);
-  const auto gt = RebindMask(du, BitCast(di, a ^ msb) > BitCast(di, b ^ msb));
-  return IfThenElse(gt, a, b);
+  return detail::MaxU(a, b);
 #endif
 }
 
@@ -1555,235 +2000,50 @@ HWY_API Vec128<uint64_t, N> Max(const Vec128<uint64_t, N> a,
 template <size_t N>
 HWY_API Vec128<int8_t, N> Max(const Vec128<int8_t, N> a,
                               const Vec128<int8_t, N> b) {
-  return Vec128<int8_t, N>{_mm_max_epi8(a.raw, b.raw)};
-}
-template <size_t N>
-HWY_API Vec128<int16_t, N> Max(const Vec128<int16_t, N> a,
-                               const Vec128<int16_t, N> b) {
-  return Vec128<int16_t, N>{_mm_max_epi16(a.raw, b.raw)};
-}
-template <size_t N>
-HWY_API Vec128<int32_t, N> Max(const Vec128<int32_t, N> a,
-                               const Vec128<int32_t, N> b) {
-  return Vec128<int32_t, N>{_mm_max_epi32(a.raw, b.raw)};
-}
-template <size_t N>
-HWY_API Vec128<int64_t, N> Max(const Vec128<int64_t, N> a,
-                               const Vec128<int64_t, N> b) {
-#if HWY_TARGET == HWY_AVX3
-  return Vec128<int64_t, N>{_mm_max_epi64(a.raw, b.raw)};
-#else
+#if HWY_TARGET == HWY_SSSE3
   return IfThenElse(a < b, b, a);
-#endif
-}
-
-// Float
-template <size_t N>
-HWY_API Vec128<float, N> Max(const Vec128<float, N> a,
-                             const Vec128<float, N> b) {
-  return Vec128<float, N>{_mm_max_ps(a.raw, b.raw)};
-}
-template <size_t N>
-HWY_API Vec128<double, N> Max(const Vec128<double, N> a,
-                              const Vec128<double, N> b) {
-  return Vec128<double, N>{_mm_max_pd(a.raw, b.raw)};
-}
-
-
-// ================================================== MEMORY
-
-// Clang static analysis claims the memory immediately after a partial vector
-// store is uninitialized, and also flags the input to partial loads (at least
-// for loadl_pd) as "garbage". This is a false alarm because msan does not
-// raise errors. We work around this by using CopyBytes instead of intrinsics,
-// but only for the analyzer to avoid potentially bad code generation.
-// Unfortunately __clang_analyzer__ was not defined for clang-tidy prior to v7.
-#ifndef HWY_SAFE_PARTIAL_LOAD_STORE
-#if defined(__clang_analyzer__) || \
-    (HWY_COMPILER_CLANG != 0 && HWY_COMPILER_CLANG < 700)
-#define HWY_SAFE_PARTIAL_LOAD_STORE 1
-#else
-#define HWY_SAFE_PARTIAL_LOAD_STORE 0
-#endif
-#endif  // HWY_SAFE_PARTIAL_LOAD_STORE
-
-// ------------------------------ Load
-
-template <typename T>
-HWY_API Vec128<T> Load(Full128<T> /* tag */, const T* HWY_RESTRICT aligned) {
-  return Vec128<T>{_mm_load_si128(reinterpret_cast<const __m128i*>(aligned))};
-}
-HWY_API Vec128<float> Load(Full128<float> /* tag */,
-                           const float* HWY_RESTRICT aligned) {
-  return Vec128<float>{_mm_load_ps(aligned)};
-}
-HWY_API Vec128<double> Load(Full128<double> /* tag */,
-                            const double* HWY_RESTRICT aligned) {
-  return Vec128<double>{_mm_load_pd(aligned)};
-}
-
-template <typename T>
-HWY_API Vec128<T> LoadU(Full128<T> /* tag */, const T* HWY_RESTRICT p) {
-  return Vec128<T>{_mm_loadu_si128(reinterpret_cast<const __m128i*>(p))};
-}
-HWY_API Vec128<float> LoadU(Full128<float> /* tag */,
-                            const float* HWY_RESTRICT p) {
-  return Vec128<float>{_mm_loadu_ps(p)};
-}
-HWY_API Vec128<double> LoadU(Full128<double> /* tag */,
-                             const double* HWY_RESTRICT p) {
-  return Vec128<double>{_mm_loadu_pd(p)};
-}
-
-template <typename T>
-HWY_API Vec128<T, 8 / sizeof(T)> Load(Simd<T, 8 / sizeof(T)> /* tag */,
-                                      const T* HWY_RESTRICT p) {
-#if HWY_SAFE_PARTIAL_LOAD_STORE
-  __m128i v = _mm_setzero_si128();
-  CopyBytes<8>(p, &v);
-  return Vec128<T, 8 / sizeof(T)>{v};
-#else
-  return Vec128<T, 8 / sizeof(T)>{
-      _mm_loadl_epi64(reinterpret_cast<const __m128i*>(p))};
-#endif
-}
-
-HWY_API Vec128<float, 2> Load(Simd<float, 2> /* tag */,
-                              const float* HWY_RESTRICT p) {
-#if HWY_SAFE_PARTIAL_LOAD_STORE
-  __m128 v = _mm_setzero_ps();
-  CopyBytes<8>(p, &v);
-  return Vec128<float, 2>{v};
-#else
-  const __m128 hi = _mm_setzero_ps();
-  return Vec128<float, 2>{_mm_loadl_pi(hi, reinterpret_cast<const __m64*>(p))};
-#endif
-}
-
-HWY_API Vec128<double, 1> Load(Simd<double, 1> /* tag */,
-                               const double* HWY_RESTRICT p) {
-#if HWY_SAFE_PARTIAL_LOAD_STORE
-  __m128d v = _mm_setzero_pd();
-  CopyBytes<8>(p, &v);
-  return Vec128<double, 1>{v};
-#else
-  return Vec128<double, 1>{_mm_load_sd(p)};
-#endif
-}
-
-HWY_API Vec128<float, 1> Load(Simd<float, 1> /* tag */,
-                              const float* HWY_RESTRICT p) {
-#if HWY_SAFE_PARTIAL_LOAD_STORE
-  __m128 v = _mm_setzero_ps();
-  CopyBytes<4>(p, &v);
-  return Vec128<float, 1>{v};
-#else
-  return Vec128<float, 1>{_mm_load_ss(p)};
-#endif
-}
-
-// Any <= 32 bit except <float, 1>
-template <typename T, size_t N, HWY_IF_LE32(T, N)>
-HWY_API Vec128<T, N> Load(Simd<T, N> /* tag */, const T* HWY_RESTRICT p) {
-  constexpr size_t kSize = sizeof(T) * N;
-#if HWY_SAFE_PARTIAL_LOAD_STORE
-  __m128 v = _mm_setzero_ps();
-  CopyBytes<kSize>(p, &v);
-  return Vec128<T, N>{v};
-#else
-  // TODO(janwas): load_ss?
-  int32_t bits;
-  CopyBytes<kSize>(p, &bits);
-  return Vec128<T, N>{_mm_cvtsi32_si128(bits)};
-#endif
-}
-
-// For < 128 bit, LoadU == Load.
-template <typename T, size_t N, HWY_IF_LE64(T, N)>
-HWY_API Vec128<T, N> LoadU(Simd<T, N> d, const T* HWY_RESTRICT p) {
-  return Load(d, p);
-}
-
-// 128-bit SIMD => nothing to duplicate, same as an unaligned load.
-template <typename T, size_t N, HWY_IF_LE128(T, N)>
-HWY_API Vec128<T, N> LoadDup128(Simd<T, N> d, const T* HWY_RESTRICT p) {
-  return LoadU(d, p);
-}
-
-// ------------------------------ Store
-
-template <typename T>
-HWY_API void Store(Vec128<T> v, Full128<T> /* tag */, T* HWY_RESTRICT aligned) {
-  _mm_store_si128(reinterpret_cast<__m128i*>(aligned), v.raw);
-}
-HWY_API void Store(const Vec128<float> v, Full128<float> /* tag */,
-                   float* HWY_RESTRICT aligned) {
-  _mm_store_ps(aligned, v.raw);
-}
-HWY_API void Store(const Vec128<double> v, Full128<double> /* tag */,
-                   double* HWY_RESTRICT aligned) {
-  _mm_store_pd(aligned, v.raw);
-}
-
-template <typename T>
-HWY_API void StoreU(Vec128<T> v, Full128<T> /* tag */, T* HWY_RESTRICT p) {
-  _mm_storeu_si128(reinterpret_cast<__m128i*>(p), v.raw);
-}
-HWY_API void StoreU(const Vec128<float> v, Full128<float> /* tag */,
-                    float* HWY_RESTRICT p) {
-  _mm_storeu_ps(p, v.raw);
-}
-HWY_API void StoreU(const Vec128<double> v, Full128<double> /* tag */,
-                    double* HWY_RESTRICT p) {
-  _mm_storeu_pd(p, v.raw);
-}
-
-template <typename T>
-HWY_API void Store(Vec128<T, 8 / sizeof(T)> v, Simd<T, 8 / sizeof(T)> /* tag */,
-                   T* HWY_RESTRICT p) {
-#if HWY_SAFE_PARTIAL_LOAD_STORE
-  CopyBytes<8>(&v, p);
-#else
-  _mm_storel_epi64(reinterpret_cast<__m128i*>(p), v.raw);
-#endif
-}
-HWY_API void Store(const Vec128<float, 2> v, Simd<float, 2> /* tag */,
-                   float* HWY_RESTRICT p) {
-#if HWY_SAFE_PARTIAL_LOAD_STORE
-  CopyBytes<8>(&v, p);
 #else
-  _mm_storel_pi(reinterpret_cast<__m64*>(p), v.raw);
+  return Vec128<int8_t, N>{_mm_max_epi8(a.raw, b.raw)};
 #endif
 }
-HWY_API void Store(const Vec128<double, 1> v, Simd<double, 1> /* tag */,
-                   double* HWY_RESTRICT p) {
-#if HWY_SAFE_PARTIAL_LOAD_STORE
-  CopyBytes<8>(&v, p);
+template <size_t N>
+HWY_API Vec128<int16_t, N> Max(const Vec128<int16_t, N> a,
+                               const Vec128<int16_t, N> b) {
+  return Vec128<int16_t, N>{_mm_max_epi16(a.raw, b.raw)};
+}
+template <size_t N>
+HWY_API Vec128<int32_t, N> Max(const Vec128<int32_t, N> a,
+                               const Vec128<int32_t, N> b) {
+#if HWY_TARGET == HWY_SSSE3
+  return IfThenElse(a < b, b, a);
 #else
-  _mm_storel_pd(p, v.raw);
+  return Vec128<int32_t, N>{_mm_max_epi32(a.raw, b.raw)};
 #endif
 }
-
-// Any <= 32 bit except <float, 1>
-template <typename T, size_t N, HWY_IF_LE32(T, N)>
-HWY_API void Store(Vec128<T, N> v, Simd<T, N> /* tag */, T* HWY_RESTRICT p) {
-  CopyBytes<sizeof(T) * N>(&v, p);
-}
-HWY_API void Store(const Vec128<float, 1> v, Simd<float, 1> /* tag */,
-                   float* HWY_RESTRICT p) {
-#if HWY_SAFE_PARTIAL_LOAD_STORE
-  CopyBytes<4>(&v, p);
+template <size_t N>
+HWY_API Vec128<int64_t, N> Max(const Vec128<int64_t, N> a,
+                               const Vec128<int64_t, N> b) {
+#if HWY_TARGET <= HWY_AVX3
+  return Vec128<int64_t, N>{_mm_max_epi64(a.raw, b.raw)};
 #else
-  _mm_store_ss(p, v.raw);
+  return IfThenElse(a < b, b, a);
 #endif
 }
 
-// For < 128 bit, StoreU == Store.
-template <typename T, size_t N, HWY_IF_LE64(T, N)>
-HWY_API void StoreU(const Vec128<T, N> v, Simd<T, N> d, T* HWY_RESTRICT p) {
-  Store(v, d, p);
+// Float
+template <size_t N>
+HWY_API Vec128<float, N> Max(const Vec128<float, N> a,
+                             const Vec128<float, N> b) {
+  return Vec128<float, N>{_mm_max_ps(a.raw, b.raw)};
+}
+template <size_t N>
+HWY_API Vec128<double, N> Max(const Vec128<double, N> a,
+                              const Vec128<double, N> b) {
+  return Vec128<double, N>{_mm_max_pd(a.raw, b.raw)};
 }
 
+// ================================================== MEMORY (2)
+
 // ------------------------------ Non-temporal stores
 
 // On clang6, we see incorrect code generated for _mm_stream_pi, so
@@ -1814,13 +2074,13 @@ HWY_DIAGNOSTICS_OFF(disable : 4245 4365, ignored "-Wsign-conversion")
 using GatherIndex64 = long long int;  // NOLINT(google-runtime-int)
 static_assert(sizeof(GatherIndex64) == 8, "Must be 64-bit type");
 
-#if HWY_TARGET == HWY_AVX3
+#if HWY_TARGET <= HWY_AVX3
 namespace detail {
 
 template <typename T, size_t N>
-HWY_API void ScatterOffset(hwy::SizeTag<4> /* tag */, Vec128<T, N> v,
-                           Simd<T, N> /* tag */, T* HWY_RESTRICT base,
-                           const Vec128<int32_t, N> offset) {
+HWY_INLINE void ScatterOffset(hwy::SizeTag<4> /* tag */, Vec128<T, N> v,
+                              Simd<T, N> /* tag */, T* HWY_RESTRICT base,
+                              const Vec128<int32_t, N> offset) {
   if (N == 4) {
     _mm_i32scatter_epi32(base, offset.raw, v.raw, 1);
   } else {
@@ -1829,9 +2089,9 @@ HWY_API void ScatterOffset(hwy::SizeTag<4> /* tag */, Vec128<T, N> v,
   }
 }
 template <typename T, size_t N>
-HWY_API void ScatterIndex(hwy::SizeTag<4> /* tag */, Vec128<T, N> v,
-                          Simd<T, N> /* tag */, T* HWY_RESTRICT base,
-                          const Vec128<int32_t, N> index) {
+HWY_INLINE void ScatterIndex(hwy::SizeTag<4> /* tag */, Vec128<T, N> v,
+                             Simd<T, N> /* tag */, T* HWY_RESTRICT base,
+                             const Vec128<int32_t, N> index) {
   if (N == 4) {
     _mm_i32scatter_epi32(base, index.raw, v.raw, 4);
   } else {
@@ -1841,9 +2101,9 @@ HWY_API void ScatterIndex(hwy::SizeTag<4> /* tag */, Vec128<T, N> v,
 }
 
 template <typename T, size_t N>
-HWY_API void ScatterOffset(hwy::SizeTag<8> /* tag */, Vec128<T, N> v,
-                           Simd<T, N> /* tag */, T* HWY_RESTRICT base,
-                           const Vec128<int64_t, N> offset) {
+HWY_INLINE void ScatterOffset(hwy::SizeTag<8> /* tag */, Vec128<T, N> v,
+                              Simd<T, N> /* tag */, T* HWY_RESTRICT base,
+                              const Vec128<int64_t, N> offset) {
   if (N == 2) {
     _mm_i64scatter_epi64(base, offset.raw, v.raw, 1);
   } else {
@@ -1852,9 +2112,9 @@ HWY_API void ScatterOffset(hwy::SizeTag<8> /* tag */, Vec128<T, N> v,
   }
 }
 template <typename T, size_t N>
-HWY_API void ScatterIndex(hwy::SizeTag<8> /* tag */, Vec128<T, N> v,
-                          Simd<T, N> /* tag */, T* HWY_RESTRICT base,
-                          const Vec128<int64_t, N> index) {
+HWY_INLINE void ScatterIndex(hwy::SizeTag<8> /* tag */, Vec128<T, N> v,
+                             Simd<T, N> /* tag */, T* HWY_RESTRICT base,
+                             const Vec128<int64_t, N> index) {
   if (N == 2) {
     _mm_i64scatter_epi64(base, index.raw, v.raw, 8);
   } else {
@@ -1879,9 +2139,9 @@ HWY_API void ScatterIndex(Vec128<T, N> v, Simd<T, N> d, T* HWY_RESTRICT base,
 }
 
 template <size_t N>
-HWY_INLINE void ScatterOffset(Vec128<float, N> v, Simd<float, N> /* tag */,
-                              float* HWY_RESTRICT base,
-                              const Vec128<int32_t, N> offset) {
+HWY_API void ScatterOffset(Vec128<float, N> v, Simd<float, N> /* tag */,
+                           float* HWY_RESTRICT base,
+                           const Vec128<int32_t, N> offset) {
   if (N == 4) {
     _mm_i32scatter_ps(base, offset.raw, v.raw, 1);
   } else {
@@ -1890,9 +2150,9 @@ HWY_INLINE void ScatterOffset(Vec128<float, N> v, Simd<float, N> /* tag */,
   }
 }
 template <size_t N>
-HWY_INLINE void ScatterIndex(Vec128<float, N> v, Simd<float, N> /* tag */,
-                             float* HWY_RESTRICT base,
-                             const Vec128<int32_t, N> index) {
+HWY_API void ScatterIndex(Vec128<float, N> v, Simd<float, N> /* tag */,
+                          float* HWY_RESTRICT base,
+                          const Vec128<int32_t, N> index) {
   if (N == 4) {
     _mm_i32scatter_ps(base, index.raw, v.raw, 4);
   } else {
@@ -1902,9 +2162,9 @@ HWY_INLINE void ScatterIndex(Vec128<float, N> v, Simd<float, N> /* tag */,
 }
 
 template <size_t N>
-HWY_INLINE void ScatterOffset(Vec128<double, N> v, Simd<double, N> /* tag */,
-                              double* HWY_RESTRICT base,
-                              const Vec128<int64_t, N> offset) {
+HWY_API void ScatterOffset(Vec128<double, N> v, Simd<double, N> /* tag */,
+                           double* HWY_RESTRICT base,
+                           const Vec128<int64_t, N> offset) {
   if (N == 2) {
     _mm_i64scatter_pd(base, offset.raw, v.raw, 1);
   } else {
@@ -1913,9 +2173,9 @@ HWY_INLINE void ScatterOffset(Vec128<double, N> v, Simd<double, N> /* tag */,
   }
 }
 template <size_t N>
-HWY_INLINE void ScatterIndex(Vec128<double, N> v, Simd<double, N> /* tag */,
-                             double* HWY_RESTRICT base,
-                             const Vec128<int64_t, N> index) {
+HWY_API void ScatterIndex(Vec128<double, N> v, Simd<double, N> /* tag */,
+                          double* HWY_RESTRICT base,
+                          const Vec128<int64_t, N> index) {
   if (N == 2) {
     _mm_i64scatter_pd(base, index.raw, v.raw, 8);
   } else {
@@ -1923,7 +2183,7 @@ HWY_INLINE void ScatterIndex(Vec128<double, N> v, Simd<double, N> /* tag */,
     _mm_mask_i64scatter_pd(base, mask, index.raw, v.raw, 8);
   }
 }
-#else  // HWY_TARGET == HWY_AVX3
+#else  // HWY_TARGET <= HWY_AVX3
 
 template <typename T, size_t N, typename Offset, HWY_IF_LE128(T, N)>
 HWY_API void ScatterOffset(Vec128<T, N> v, Simd<T, N> d, T* HWY_RESTRICT base,
@@ -1962,7 +2222,7 @@ HWY_API void ScatterIndex(Vec128<T, N> v, Simd<T, N> d, T* HWY_RESTRICT base,
 
 // ------------------------------ Gather (Load/Store)
 
-#if HWY_TARGET == HWY_SSE4
+#if HWY_TARGET == HWY_SSSE3 || HWY_TARGET == HWY_SSE4
 
 template <typename T, size_t N, typename Offset>
 HWY_API Vec128<T, N> GatherOffset(const Simd<T, N> d,
@@ -2001,31 +2261,35 @@ HWY_API Vec128<T, N> GatherIndex(const Simd<T, N> d, const T* HWY_RESTRICT base,
 namespace detail {
 
 template <typename T, size_t N>
-HWY_API Vec128<T, N> GatherOffset(hwy::SizeTag<4> /* tag */, Simd<T, N> /* d */,
-                                  const T* HWY_RESTRICT base,
-                                  const Vec128<int32_t, N> offset) {
+HWY_INLINE Vec128<T, N> GatherOffset(hwy::SizeTag<4> /* tag */,
+                                     Simd<T, N> /* d */,
+                                     const T* HWY_RESTRICT base,
+                                     const Vec128<int32_t, N> offset) {
   return Vec128<T, N>{_mm_i32gather_epi32(
       reinterpret_cast<const int32_t*>(base), offset.raw, 1)};
 }
 template <typename T, size_t N>
-HWY_API Vec128<T, N> GatherIndex(hwy::SizeTag<4> /* tag */, Simd<T, N> /* d */,
-                                 const T* HWY_RESTRICT base,
-                                 const Vec128<int32_t, N> index) {
+HWY_INLINE Vec128<T, N> GatherIndex(hwy::SizeTag<4> /* tag */,
+                                    Simd<T, N> /* d */,
+                                    const T* HWY_RESTRICT base,
+                                    const Vec128<int32_t, N> index) {
   return Vec128<T, N>{_mm_i32gather_epi32(
       reinterpret_cast<const int32_t*>(base), index.raw, 4)};
 }
 
 template <typename T, size_t N>
-HWY_API Vec128<T, N> GatherOffset(hwy::SizeTag<8> /* tag */, Simd<T, N> /* d */,
-                                  const T* HWY_RESTRICT base,
-                                  const Vec128<int64_t, N> offset) {
+HWY_INLINE Vec128<T, N> GatherOffset(hwy::SizeTag<8> /* tag */,
+                                     Simd<T, N> /* d */,
+                                     const T* HWY_RESTRICT base,
+                                     const Vec128<int64_t, N> offset) {
   return Vec128<T, N>{_mm_i64gather_epi64(
       reinterpret_cast<const GatherIndex64*>(base), offset.raw, 1)};
 }
 template <typename T, size_t N>
-HWY_API Vec128<T, N> GatherIndex(hwy::SizeTag<8> /* tag */, Simd<T, N> /* d */,
-                                 const T* HWY_RESTRICT base,
-                                 const Vec128<int64_t, N> index) {
+HWY_INLINE Vec128<T, N> GatherIndex(hwy::SizeTag<8> /* tag */,
+                                    Simd<T, N> /* d */,
+                                    const T* HWY_RESTRICT base,
+                                    const Vec128<int64_t, N> index) {
   return Vec128<T, N>{_mm_i64gather_epi64(
       reinterpret_cast<const GatherIndex64*>(base), index.raw, 8)};
 }
@@ -2069,74 +2333,118 @@ HWY_API Vec128<double, N> GatherIndex(Simd<double, N> /* tag */,
   return Vec128<double, N>{_mm_i64gather_pd(base, index.raw, 8)};
 }
 
-#endif  // HWY_TARGET != HWY_SSE4
+#endif  // HWY_TARGET == HWY_SSSE3 || HWY_TARGET == HWY_SSE4
 
 HWY_DIAGNOSTICS(pop)
 
-// ================================================== SWIZZLE
+// ================================================== SWIZZLE (2)
 
-// ------------------------------ Extract half
+// ------------------------------ LowerHalf
 
 // Returns upper/lower half of a vector.
 template <typename T, size_t N>
-HWY_API Vec128<T, N / 2> LowerHalf(Vec128<T, N> v) {
+HWY_API Vec128<T, N / 2> LowerHalf(Simd<T, N / 2> /* tag */, Vec128<T, N> v) {
   return Vec128<T, N / 2>{v.raw};
 }
 
-// These copy hi into lo (smaller instruction encoding than shifts).
-template <typename T>
-HWY_API Vec128<T, 8 / sizeof(T)> UpperHalf(Vec128<T> v) {
-  return Vec128<T, 8 / sizeof(T)>{_mm_unpackhi_epi64(v.raw, v.raw)};
-}
-template <>
-HWY_INLINE Vec128<float, 2> UpperHalf(Vec128<float> v) {
-  return Vec128<float, 2>{_mm_movehl_ps(v.raw, v.raw)};
-}
-template <>
-HWY_INLINE Vec128<double, 1> UpperHalf(Vec128<double> v) {
-  return Vec128<double, 1>{_mm_unpackhi_pd(v.raw, v.raw)};
+template <typename T, size_t N>
+HWY_API Vec128<T, N / 2> LowerHalf(Vec128<T, N> v) {
+  return LowerHalf(Simd<T, N / 2>(), v);
 }
 
-// ------------------------------ Shift vector by constant #bytes
+// ------------------------------ ShiftLeftBytes
 
-// 0x01..0F, kBytes = 1 => 0x02..0F00
 template <int kBytes, typename T, size_t N>
-HWY_API Vec128<T, N> ShiftLeftBytes(const Vec128<T, N> v) {
+HWY_API Vec128<T, N> ShiftLeftBytes(Simd<T, N> /* tag */, Vec128<T, N> v) {
   static_assert(0 <= kBytes && kBytes <= 16, "Invalid kBytes");
   return Vec128<T, N>{_mm_slli_si128(v.raw, kBytes)};
 }
 
+template <int kBytes, typename T, size_t N>
+HWY_API Vec128<T, N> ShiftLeftBytes(const Vec128<T, N> v) {
+  return ShiftLeftBytes<kBytes>(Simd<T, N>(), v);
+}
+
+// ------------------------------ ShiftLeftLanes
+
 template <int kLanes, typename T, size_t N>
-HWY_API Vec128<T, N> ShiftLeftLanes(const Vec128<T, N> v) {
-  const Simd<uint8_t, N * sizeof(T)> d8;
-  const Simd<T, N> d;
+HWY_API Vec128<T, N> ShiftLeftLanes(Simd<T, N> d, const Vec128<T, N> v) {
+  const Repartition<uint8_t, decltype(d)> d8;
   return BitCast(d, ShiftLeftBytes<kLanes * sizeof(T)>(BitCast(d8, v)));
 }
 
-// 0x01..0F, kBytes = 1 => 0x0001..0E
+template <int kLanes, typename T, size_t N>
+HWY_API Vec128<T, N> ShiftLeftLanes(const Vec128<T, N> v) {
+  return ShiftLeftLanes<kLanes>(Simd<T, N>(), v);
+}
+
+// ------------------------------ ShiftRightBytes
 template <int kBytes, typename T, size_t N>
-HWY_API Vec128<T, N> ShiftRightBytes(const Vec128<T, N> v) {
+HWY_API Vec128<T, N> ShiftRightBytes(Simd<T, N> /* tag */, Vec128<T, N> v) {
   static_assert(0 <= kBytes && kBytes <= 16, "Invalid kBytes");
+  // For partial vectors, clear upper lanes so we shift in zeros.
+  if (N != 16 / sizeof(T)) {
+    const Vec128<T> vfull{v.raw};
+    v = Vec128<T, N>{IfThenElseZero(FirstN(Full128<T>(), N), vfull).raw};
+  }
   return Vec128<T, N>{_mm_srli_si128(v.raw, kBytes)};
 }
 
+// ------------------------------ ShiftRightLanes
 template <int kLanes, typename T, size_t N>
-HWY_API Vec128<T, N> ShiftRightLanes(const Vec128<T, N> v) {
-  const Simd<uint8_t, N * sizeof(T)> d8;
-  const Simd<T, N> d;
+HWY_API Vec128<T, N> ShiftRightLanes(Simd<T, N> d, const Vec128<T, N> v) {
+  const Repartition<uint8_t, decltype(d)> d8;
   return BitCast(d, ShiftRightBytes<kLanes * sizeof(T)>(BitCast(d8, v)));
 }
 
-// ------------------------------ Extract from 2x 128-bit at constant offset
+// ------------------------------ UpperHalf (ShiftRightBytes)
+
+// Full input: copy hi into lo (smaller instruction encoding than shifts).
+template <typename T>
+HWY_API Vec128<T, 8 / sizeof(T)> UpperHalf(Half<Full128<T>> /* tag */,
+                                           Vec128<T> v) {
+  return Vec128<T, 8 / sizeof(T)>{_mm_unpackhi_epi64(v.raw, v.raw)};
+}
+HWY_API Vec128<float, 2> UpperHalf(Simd<float, 2> /* tag */, Vec128<float> v) {
+  return Vec128<float, 2>{_mm_movehl_ps(v.raw, v.raw)};
+}
+HWY_API Vec128<double, 1> UpperHalf(Simd<double, 1> /* tag */,
+                                    Vec128<double> v) {
+  return Vec128<double, 1>{_mm_unpackhi_pd(v.raw, v.raw)};
+}
+
+// Partial
+template <typename T, size_t N, HWY_IF_LE64(T, N)>
+HWY_API Vec128<T, (N + 1) / 2> UpperHalf(Half<Simd<T, N>> /* tag */,
+                                         Vec128<T, N> v) {
+  const Simd<T, N> d;
+  const auto vu = BitCast(RebindToUnsigned<decltype(d)>(), v);
+  const auto upper = BitCast(d, ShiftRightBytes<N * sizeof(T) / 2>(vu));
+  return Vec128<T, (N + 1) / 2>{upper.raw};
+}
+
+// ------------------------------ CombineShiftRightBytes
 
-// Extracts 128 bits from <hi, lo> by skipping the least-significant kBytes.
-template <int kBytes, typename T>
-HWY_API Vec128<T> CombineShiftRightBytes(const Vec128<T> hi,
-                                         const Vec128<T> lo) {
-  const Full128<uint8_t> d8;
-  const Vec128<uint8_t> extracted_bytes{
-      _mm_alignr_epi8(BitCast(d8, hi).raw, BitCast(d8, lo).raw, kBytes)};
-  return BitCast(Full128<T>(), extracted_bytes);
+template <int kBytes, typename T, class V = Vec128<T>>
+HWY_API V CombineShiftRightBytes(Full128<T> d, V hi, V lo) {
+  const Repartition<uint8_t, decltype(d)> d8;
+  return BitCast(d, Vec128<uint8_t>{_mm_alignr_epi8(
+                        BitCast(d8, hi).raw, BitCast(d8, lo).raw, kBytes)});
+}
+
+template <int kBytes, typename T, size_t N, HWY_IF_LE64(T, N),
+          class V = Vec128<T, N>>
+HWY_API V CombineShiftRightBytes(Simd<T, N> d, V hi, V lo) {
+  constexpr size_t kSize = N * sizeof(T);
+  static_assert(0 < kBytes && kBytes < kSize, "kBytes invalid");
+  const Repartition<uint8_t, decltype(d)> d8;
+  const Full128<uint8_t> d_full8;
+  using V8 = VFromD<decltype(d_full8)>;
+  const V8 hi8{BitCast(d8, hi).raw};
+  // Move into most-significant bytes
+  const V8 lo8 = ShiftLeftBytes<16 - kSize>(V8{BitCast(d8, lo).raw});
+  const V8 r = CombineShiftRightBytes<16 - kSize + kBytes>(d_full8, hi8, lo8);
+  return V{BitCast(Full128<T>(), r).raw};
 }
 
 // ------------------------------ Broadcast/splat any lane
@@ -2199,84 +2507,18 @@ HWY_API Vec128<double, N> Broadcast(const Vec128<double, N> v) {
   return Vec128<double, N>{_mm_shuffle_pd(v.raw, v.raw, 3 * kLane)};
 }
 
-// ------------------------------ Shuffle bytes with variable indices
-
-// Returns vector of bytes[from[i]]. "from" is also interpreted as bytes, i.e.
-// lane indices in [0, 16).
-template <typename T, size_t N>
-HWY_API Vec128<T, N> TableLookupBytes(const Vec128<T, N> bytes,
-                                      const Vec128<T, N> from) {
-  return Vec128<T, N>{_mm_shuffle_epi8(bytes.raw, from.raw)};
-}
-
-// ------------------------------ Hard-coded shuffles
-
-// Notation: let Vec128<int32_t> have lanes 3,2,1,0 (0 is least-significant).
-// Shuffle0321 rotates one lane to the right (the previous least-significant
-// lane is now most-significant). These could also be implemented via
-// CombineShiftRightBytes but the shuffle_abcd notation is more convenient.
-
-// Swap 32-bit halves in 64-bit halves.
-HWY_API Vec128<uint32_t> Shuffle2301(const Vec128<uint32_t> v) {
-  return Vec128<uint32_t>{_mm_shuffle_epi32(v.raw, 0xB1)};
-}
-HWY_API Vec128<int32_t> Shuffle2301(const Vec128<int32_t> v) {
-  return Vec128<int32_t>{_mm_shuffle_epi32(v.raw, 0xB1)};
-}
-HWY_API Vec128<float> Shuffle2301(const Vec128<float> v) {
-  return Vec128<float>{_mm_shuffle_ps(v.raw, v.raw, 0xB1)};
-}
-
-// Swap 64-bit halves
-HWY_API Vec128<uint32_t> Shuffle1032(const Vec128<uint32_t> v) {
-  return Vec128<uint32_t>{_mm_shuffle_epi32(v.raw, 0x4E)};
-}
-HWY_API Vec128<int32_t> Shuffle1032(const Vec128<int32_t> v) {
-  return Vec128<int32_t>{_mm_shuffle_epi32(v.raw, 0x4E)};
-}
-HWY_API Vec128<float> Shuffle1032(const Vec128<float> v) {
-  return Vec128<float>{_mm_shuffle_ps(v.raw, v.raw, 0x4E)};
-}
-HWY_API Vec128<uint64_t> Shuffle01(const Vec128<uint64_t> v) {
-  return Vec128<uint64_t>{_mm_shuffle_epi32(v.raw, 0x4E)};
-}
-HWY_API Vec128<int64_t> Shuffle01(const Vec128<int64_t> v) {
-  return Vec128<int64_t>{_mm_shuffle_epi32(v.raw, 0x4E)};
-}
-HWY_API Vec128<double> Shuffle01(const Vec128<double> v) {
-  return Vec128<double>{_mm_shuffle_pd(v.raw, v.raw, 1)};
-}
-
-// Rotate right 32 bits
-HWY_API Vec128<uint32_t> Shuffle0321(const Vec128<uint32_t> v) {
-  return Vec128<uint32_t>{_mm_shuffle_epi32(v.raw, 0x39)};
-}
-HWY_API Vec128<int32_t> Shuffle0321(const Vec128<int32_t> v) {
-  return Vec128<int32_t>{_mm_shuffle_epi32(v.raw, 0x39)};
-}
-HWY_API Vec128<float> Shuffle0321(const Vec128<float> v) {
-  return Vec128<float>{_mm_shuffle_ps(v.raw, v.raw, 0x39)};
-}
-// Rotate left 32 bits
-HWY_API Vec128<uint32_t> Shuffle2103(const Vec128<uint32_t> v) {
-  return Vec128<uint32_t>{_mm_shuffle_epi32(v.raw, 0x93)};
-}
-HWY_API Vec128<int32_t> Shuffle2103(const Vec128<int32_t> v) {
-  return Vec128<int32_t>{_mm_shuffle_epi32(v.raw, 0x93)};
-}
-HWY_API Vec128<float> Shuffle2103(const Vec128<float> v) {
-  return Vec128<float>{_mm_shuffle_ps(v.raw, v.raw, 0x93)};
+// ------------------------------ TableLookupBytes
+template <typename T, size_t N, typename TI, size_t NI>
+HWY_API Vec128<TI, NI> TableLookupBytes(const Vec128<T, N> bytes,
+                                        const Vec128<TI, NI> from) {
+  return Vec128<TI, NI>{_mm_shuffle_epi8(bytes.raw, from.raw)};
 }
 
-// Reverse
-HWY_API Vec128<uint32_t> Shuffle0123(const Vec128<uint32_t> v) {
-  return Vec128<uint32_t>{_mm_shuffle_epi32(v.raw, 0x1B)};
-}
-HWY_API Vec128<int32_t> Shuffle0123(const Vec128<int32_t> v) {
-  return Vec128<int32_t>{_mm_shuffle_epi32(v.raw, 0x1B)};
-}
-HWY_API Vec128<float> Shuffle0123(const Vec128<float> v) {
-  return Vec128<float>{_mm_shuffle_ps(v.raw, v.raw, 0x1B)};
+// ------------------------------ TableLookupBytesOr0
+// For all vector widths; x86 anyway zeroes if >= 0x80.
+template <class V, class VI>
+HWY_API VI TableLookupBytesOr0(const V bytes, const VI from) {
+  return TableLookupBytes(bytes, from);
 }
 
 // ------------------------------ TableLookupLanes
@@ -2325,55 +2567,76 @@ HWY_API Vec128<float, N> TableLookupLanes(const Vec128<float, N> v,
                  TableLookupBytes(BitCast(di, v), Vec128<int32_t, N>{idx.raw}));
 }
 
-// ------------------------------ Interleave lanes
+// ------------------------------ InterleaveLower
 
 // Interleaves lanes from halves of the 128-bit blocks of "a" (which provides
 // the least-significant lane) and "b". To concatenate two half-width integers
 // into one, use ZipLower/Upper instead (also works with scalar).
 
-HWY_API Vec128<uint8_t> InterleaveLower(const Vec128<uint8_t> a,
-                                        const Vec128<uint8_t> b) {
-  return Vec128<uint8_t>{_mm_unpacklo_epi8(a.raw, b.raw)};
+template <size_t N, HWY_IF_LE128(uint8_t, N)>
+HWY_API Vec128<uint8_t, N> InterleaveLower(const Vec128<uint8_t, N> a,
+                                           const Vec128<uint8_t, N> b) {
+  return Vec128<uint8_t, N>{_mm_unpacklo_epi8(a.raw, b.raw)};
 }
-HWY_API Vec128<uint16_t> InterleaveLower(const Vec128<uint16_t> a,
-                                         const Vec128<uint16_t> b) {
-  return Vec128<uint16_t>{_mm_unpacklo_epi16(a.raw, b.raw)};
+template <size_t N, HWY_IF_LE128(uint16_t, N)>
+HWY_API Vec128<uint16_t, N> InterleaveLower(const Vec128<uint16_t, N> a,
+                                            const Vec128<uint16_t, N> b) {
+  return Vec128<uint16_t, N>{_mm_unpacklo_epi16(a.raw, b.raw)};
 }
-HWY_API Vec128<uint32_t> InterleaveLower(const Vec128<uint32_t> a,
-                                         const Vec128<uint32_t> b) {
-  return Vec128<uint32_t>{_mm_unpacklo_epi32(a.raw, b.raw)};
+template <size_t N, HWY_IF_LE128(uint32_t, N)>
+HWY_API Vec128<uint32_t, N> InterleaveLower(const Vec128<uint32_t, N> a,
+                                            const Vec128<uint32_t, N> b) {
+  return Vec128<uint32_t, N>{_mm_unpacklo_epi32(a.raw, b.raw)};
 }
-HWY_API Vec128<uint64_t> InterleaveLower(const Vec128<uint64_t> a,
-                                         const Vec128<uint64_t> b) {
-  return Vec128<uint64_t>{_mm_unpacklo_epi64(a.raw, b.raw)};
+template <size_t N, HWY_IF_LE128(uint64_t, N)>
+HWY_API Vec128<uint64_t, N> InterleaveLower(const Vec128<uint64_t, N> a,
+                                            const Vec128<uint64_t, N> b) {
+  return Vec128<uint64_t, N>{_mm_unpacklo_epi64(a.raw, b.raw)};
 }
 
-HWY_API Vec128<int8_t> InterleaveLower(const Vec128<int8_t> a,
-                                       const Vec128<int8_t> b) {
-  return Vec128<int8_t>{_mm_unpacklo_epi8(a.raw, b.raw)};
+template <size_t N, HWY_IF_LE128(int8_t, N)>
+HWY_API Vec128<int8_t, N> InterleaveLower(const Vec128<int8_t, N> a,
+                                          const Vec128<int8_t, N> b) {
+  return Vec128<int8_t, N>{_mm_unpacklo_epi8(a.raw, b.raw)};
 }
-HWY_API Vec128<int16_t> InterleaveLower(const Vec128<int16_t> a,
-                                        const Vec128<int16_t> b) {
-  return Vec128<int16_t>{_mm_unpacklo_epi16(a.raw, b.raw)};
+template <size_t N, HWY_IF_LE128(int16_t, N)>
+HWY_API Vec128<int16_t, N> InterleaveLower(const Vec128<int16_t, N> a,
+                                           const Vec128<int16_t, N> b) {
+  return Vec128<int16_t, N>{_mm_unpacklo_epi16(a.raw, b.raw)};
 }
-HWY_API Vec128<int32_t> InterleaveLower(const Vec128<int32_t> a,
-                                        const Vec128<int32_t> b) {
-  return Vec128<int32_t>{_mm_unpacklo_epi32(a.raw, b.raw)};
+template <size_t N, HWY_IF_LE128(int32_t, N)>
+HWY_API Vec128<int32_t, N> InterleaveLower(const Vec128<int32_t, N> a,
+                                           const Vec128<int32_t, N> b) {
+  return Vec128<int32_t, N>{_mm_unpacklo_epi32(a.raw, b.raw)};
 }
-HWY_API Vec128<int64_t> InterleaveLower(const Vec128<int64_t> a,
-                                        const Vec128<int64_t> b) {
-  return Vec128<int64_t>{_mm_unpacklo_epi64(a.raw, b.raw)};
+template <size_t N, HWY_IF_LE128(int64_t, N)>
+HWY_API Vec128<int64_t, N> InterleaveLower(const Vec128<int64_t, N> a,
+                                           const Vec128<int64_t, N> b) {
+  return Vec128<int64_t, N>{_mm_unpacklo_epi64(a.raw, b.raw)};
 }
 
-HWY_API Vec128<float> InterleaveLower(const Vec128<float> a,
-                                      const Vec128<float> b) {
-  return Vec128<float>{_mm_unpacklo_ps(a.raw, b.raw)};
+template <size_t N, HWY_IF_LE128(float, N)>
+HWY_API Vec128<float, N> InterleaveLower(const Vec128<float, N> a,
+                                         const Vec128<float, N> b) {
+  return Vec128<float, N>{_mm_unpacklo_ps(a.raw, b.raw)};
 }
-HWY_API Vec128<double> InterleaveLower(const Vec128<double> a,
-                                       const Vec128<double> b) {
-  return Vec128<double>{_mm_unpacklo_pd(a.raw, b.raw)};
+template <size_t N, HWY_IF_LE128(double, N)>
+HWY_API Vec128<double, N> InterleaveLower(const Vec128<double, N> a,
+                                          const Vec128<double, N> b) {
+  return Vec128<double, N>{_mm_unpacklo_pd(a.raw, b.raw)};
 }
 
+// Additional overload for the optional Simd<> tag.
+template <typename T, size_t N, HWY_IF_LE128(T, N), class V = Vec128<T, N>>
+HWY_API V InterleaveLower(Simd<T, N> /* tag */, V a, V b) {
+  return InterleaveLower(a, b);
+}
+
+// ------------------------------ InterleaveUpper (UpperHalf)
+
+// All functions inside detail lack the required D parameter.
+namespace detail {
+
 HWY_API Vec128<uint8_t> InterleaveUpper(const Vec128<uint8_t> a,
                                         const Vec128<uint8_t> b) {
   return Vec128<uint8_t>{_mm_unpackhi_epi8(a.raw, b.raw)};
@@ -2417,113 +2680,151 @@ HWY_API Vec128<double> InterleaveUpper(const Vec128<double> a,
   return Vec128<double>{_mm_unpackhi_pd(a.raw, b.raw)};
 }
 
-// ------------------------------ Zip lanes
-
-// Same as interleave_*, except that the return lanes are double-width integers;
-// this is necessary because the single-lane scalar cannot return two values.
+}  // namespace detail
 
-template <size_t N>
-HWY_API Vec128<uint16_t, (N + 1) / 2> ZipLower(const Vec128<uint8_t, N> a,
-                                               const Vec128<uint8_t, N> b) {
-  return Vec128<uint16_t, (N + 1) / 2>{_mm_unpacklo_epi8(a.raw, b.raw)};
-}
-template <size_t N>
-HWY_API Vec128<uint32_t, (N + 1) / 2> ZipLower(const Vec128<uint16_t, N> a,
-                                               const Vec128<uint16_t, N> b) {
-  return Vec128<uint32_t, (N + 1) / 2>{_mm_unpacklo_epi16(a.raw, b.raw)};
-}
-template <size_t N>
-HWY_API Vec128<uint64_t, (N + 1) / 2> ZipLower(const Vec128<uint32_t, N> a,
-                                               const Vec128<uint32_t, N> b) {
-  return Vec128<uint64_t, (N + 1) / 2>{_mm_unpacklo_epi32(a.raw, b.raw)};
+// Full
+template <typename T, class V = Vec128<T>>
+HWY_API V InterleaveUpper(Full128<T> /* tag */, V a, V b) {
+  return detail::InterleaveUpper(a, b);
 }
 
-template <size_t N>
-HWY_API Vec128<int16_t, (N + 1) / 2> ZipLower(const Vec128<int8_t, N> a,
-                                              const Vec128<int8_t, N> b) {
-  return Vec128<int16_t, (N + 1) / 2>{_mm_unpacklo_epi8(a.raw, b.raw)};
+// Partial
+template <typename T, size_t N, HWY_IF_LE64(T, N), class V = Vec128<T, N>>
+HWY_API V InterleaveUpper(Simd<T, N> d, V a, V b) {
+  const Half<decltype(d)> d2;
+  return InterleaveLower(d, V{UpperHalf(d2, a).raw}, V{UpperHalf(d2, b).raw});
 }
-template <size_t N>
-HWY_API Vec128<int32_t, (N + 1) / 2> ZipLower(const Vec128<int16_t, N> a,
-                                              const Vec128<int16_t, N> b) {
-  return Vec128<int32_t, (N + 1) / 2>{_mm_unpacklo_epi16(a.raw, b.raw)};
+
+// ------------------------------ ZipLower/ZipUpper (InterleaveLower)
+
+// Same as Interleave*, except that the return lanes are double-width integers;
+// this is necessary because the single-lane scalar cannot return two values.
+template <typename T, size_t N, class DW = RepartitionToWide<Simd<T, N>>>
+HWY_API VFromD<DW> ZipLower(Vec128<T, N> a, Vec128<T, N> b) {
+  return BitCast(DW(), InterleaveLower(a, b));
 }
-template <size_t N>
-HWY_API Vec128<int64_t, (N + 1) / 2> ZipLower(const Vec128<int32_t, N> a,
-                                              const Vec128<int32_t, N> b) {
-  return Vec128<int64_t, (N + 1) / 2>{_mm_unpacklo_epi32(a.raw, b.raw)};
+template <typename T, size_t N, class D = Simd<T, N>,
+          class DW = RepartitionToWide<D>>
+HWY_API VFromD<DW> ZipLower(DW dw, Vec128<T, N> a, Vec128<T, N> b) {
+  return BitCast(dw, InterleaveLower(D(), a, b));
 }
 
-template <size_t N>
-HWY_API Vec128<uint16_t, (N + 1) / 2> ZipUpper(const Vec128<uint8_t, N> a,
-                                               const Vec128<uint8_t, N> b) {
-  return Vec128<uint16_t, (N + 1) / 2>{_mm_unpackhi_epi8(a.raw, b.raw)};
+template <typename T, size_t N, class D = Simd<T, N>,
+          class DW = RepartitionToWide<D>>
+HWY_API VFromD<DW> ZipUpper(DW dw, Vec128<T, N> a, Vec128<T, N> b) {
+  return BitCast(dw, InterleaveUpper(D(), a, b));
 }
-template <size_t N>
-HWY_API Vec128<uint32_t, (N + 1) / 2> ZipUpper(const Vec128<uint16_t, N> a,
-                                               const Vec128<uint16_t, N> b) {
-  return Vec128<uint32_t, (N + 1) / 2>{_mm_unpackhi_epi16(a.raw, b.raw)};
-}
-template <size_t N>
-HWY_API Vec128<uint64_t, (N + 1) / 2> ZipUpper(const Vec128<uint32_t, N> a,
-                                               const Vec128<uint32_t, N> b) {
-  return Vec128<uint64_t, (N + 1) / 2>{_mm_unpackhi_epi32(a.raw, b.raw)};
+
+// ================================================== COMBINE
+
+// ------------------------------ Combine (InterleaveLower)
+
+// N = N/2 + N/2 (upper half undefined)
+template <typename T, size_t N, HWY_IF_LE128(T, N)>
+HWY_API Vec128<T, N> Combine(Simd<T, N> d, Vec128<T, N / 2> hi_half,
+                             Vec128<T, N / 2> lo_half) {
+  const Half<decltype(d)> d2;
+  const RebindToUnsigned<decltype(d2)> du2;
+  // Treat half-width input as one lane, and expand to two lanes.
+  using VU = Vec128<UnsignedFromSize<N * sizeof(T) / 2>, 2>;
+  const VU lo{BitCast(du2, lo_half).raw};
+  const VU hi{BitCast(du2, hi_half).raw};
+  return BitCast(d, InterleaveLower(lo, hi));
 }
 
-template <size_t N>
-HWY_API Vec128<int16_t, (N + 1) / 2> ZipUpper(const Vec128<int8_t, N> a,
-                                              const Vec128<int8_t, N> b) {
-  return Vec128<int16_t, (N + 1) / 2>{_mm_unpackhi_epi8(a.raw, b.raw)};
+// ------------------------------ ZeroExtendVector (Combine, IfThenElseZero)
+
+template <typename T, HWY_IF_NOT_FLOAT(T)>
+HWY_API Vec128<T> ZeroExtendVector(Full128<T> /* tag */,
+                                   Vec128<T, 8 / sizeof(T)> lo) {
+  return Vec128<T>{_mm_move_epi64(lo.raw)};
 }
-template <size_t N>
-HWY_API Vec128<int32_t, (N + 1) / 2> ZipUpper(const Vec128<int16_t, N> a,
-                                              const Vec128<int16_t, N> b) {
-  return Vec128<int32_t, (N + 1) / 2>{_mm_unpackhi_epi16(a.raw, b.raw)};
+
+template <typename T, HWY_IF_FLOAT(T)>
+HWY_API Vec128<T> ZeroExtendVector(Full128<T> d, Vec128<T, 8 / sizeof(T)> lo) {
+  const RebindToUnsigned<decltype(d)> du;
+  return BitCast(d, ZeroExtendVector(du, BitCast(Half<decltype(du)>(), lo)));
 }
-template <size_t N>
-HWY_API Vec128<int64_t, (N + 1) / 2> ZipUpper(const Vec128<int32_t, N> a,
-                                              const Vec128<int32_t, N> b) {
-  return Vec128<int64_t, (N + 1) / 2>{_mm_unpackhi_epi32(a.raw, b.raw)};
+
+template <typename T, size_t N, HWY_IF_LE64(T, N)>
+HWY_API Vec128<T, N> ZeroExtendVector(Simd<T, N> d, Vec128<T, N / 2> lo) {
+  return IfThenElseZero(FirstN(d, N / 2), Vec128<T, N>{lo.raw});
 }
 
-// ------------------------------ Blocks
+// ------------------------------ Concat full (InterleaveLower)
 
 // hiH,hiL loH,loL |-> hiL,loL (= lower halves)
 template <typename T>
-HWY_API Vec128<T> ConcatLowerLower(const Vec128<T> hi, const Vec128<T> lo) {
-  const Full128<uint64_t> d64;
-  return BitCast(Full128<T>(),
-                 InterleaveLower(BitCast(d64, lo), BitCast(d64, hi)));
+HWY_API Vec128<T> ConcatLowerLower(Full128<T> d, Vec128<T> hi, Vec128<T> lo) {
+  const Repartition<uint64_t, decltype(d)> d64;
+  return BitCast(d, InterleaveLower(BitCast(d64, lo), BitCast(d64, hi)));
 }
 
 // hiH,hiL loH,loL |-> hiH,loH (= upper halves)
 template <typename T>
-HWY_API Vec128<T> ConcatUpperUpper(const Vec128<T> hi, const Vec128<T> lo) {
-  const Full128<uint64_t> d64;
-  return BitCast(Full128<T>(),
-                 InterleaveUpper(BitCast(d64, lo), BitCast(d64, hi)));
+HWY_API Vec128<T> ConcatUpperUpper(Full128<T> d, Vec128<T> hi, Vec128<T> lo) {
+  const Repartition<uint64_t, decltype(d)> d64;
+  return BitCast(d, InterleaveUpper(d64, BitCast(d64, lo), BitCast(d64, hi)));
 }
 
 // hiH,hiL loH,loL |-> hiL,loH (= inner halves)
 template <typename T>
-HWY_API Vec128<T> ConcatLowerUpper(const Vec128<T> hi, const Vec128<T> lo) {
-  return CombineShiftRightBytes<8>(hi, lo);
+HWY_API Vec128<T> ConcatLowerUpper(Full128<T> d, const Vec128<T> hi,
+                                   const Vec128<T> lo) {
+  return CombineShiftRightBytes<8>(d, hi, lo);
 }
 
 // hiH,hiL loH,loL |-> hiH,loL (= outer halves)
 template <typename T>
-HWY_API Vec128<T> ConcatUpperLower(const Vec128<T> hi, const Vec128<T> lo) {
+HWY_API Vec128<T> ConcatUpperLower(Full128<T> d, Vec128<T> hi, Vec128<T> lo) {
+#if HWY_TARGET == HWY_SSSE3
+  const Full128<double> dd;
+  const __m128d concat = _mm_move_sd(BitCast(dd, hi).raw, BitCast(dd, lo).raw);
+  return BitCast(d, Vec128<double>{concat});
+#else
+  (void)d;
   return Vec128<T>{_mm_blend_epi16(hi.raw, lo.raw, 0x0F)};
+#endif
 }
-template <>
-HWY_INLINE Vec128<float> ConcatUpperLower(const Vec128<float> hi,
-                                          const Vec128<float> lo) {
-  return Vec128<float>{_mm_blend_ps(hi.raw, lo.raw, 3)};
+HWY_API Vec128<float> ConcatUpperLower(Full128<float> /* tag */,
+                                       const Vec128<float> hi,
+                                       const Vec128<float> lo) {
+  return Vec128<float>{_mm_shuffle_ps(lo.raw, hi.raw, _MM_SHUFFLE(3, 2, 1, 0))};
 }
-template <>
-HWY_INLINE Vec128<double> ConcatUpperLower(const Vec128<double> hi,
-                                           const Vec128<double> lo) {
-  return Vec128<double>{_mm_blend_pd(hi.raw, lo.raw, 1)};
+HWY_API Vec128<double> ConcatUpperLower(Full128<double> /* tag */,
+                                        const Vec128<double> hi,
+                                        const Vec128<double> lo) {
+  return Vec128<double>{_mm_shuffle_pd(lo.raw, hi.raw, _MM_SHUFFLE2(1, 0))};
+}
+
+// ------------------------------ Concat partial (Combine, LowerHalf)
+
+template <typename T, size_t N, HWY_IF_LE64(T, N)>
+HWY_API Vec128<T, N> ConcatLowerLower(Simd<T, N> d, Vec128<T, N> hi,
+                                      Vec128<T, N> lo) {
+  const Half<decltype(d)> d2;
+  return Combine(LowerHalf(d2, hi), LowerHalf(d2, lo));
+}
+
+template <typename T, size_t N, HWY_IF_LE64(T, N)>
+HWY_API Vec128<T, N> ConcatUpperUpper(Simd<T, N> d, Vec128<T, N> hi,
+                                      Vec128<T, N> lo) {
+  const Half<decltype(d)> d2;
+  return Combine(UpperHalf(d2, hi), UpperHalf(d2, lo));
+}
+
+template <typename T, size_t N, HWY_IF_LE64(T, N)>
+HWY_API Vec128<T, N> ConcatLowerUpper(Simd<T, N> d, const Vec128<T, N> hi,
+                                      const Vec128<T, N> lo) {
+  const Half<decltype(d)> d2;
+  return Combine(LowerHalf(d2, hi), UpperHalf(d2, lo));
+}
+
+template <typename T, size_t N, HWY_IF_LE64(T, N)>
+HWY_API Vec128<T, N> ConcatUpperLower(Simd<T, N> d, Vec128<T, N> hi,
+                                      Vec128<T, N> lo) {
+  const Half<decltype(d)> d2;
+  return Combine(UpperHalf(d2, hi), LowerHalf(d2, lo));
 }
 
 // ------------------------------ OddEven (IfThenElse)
@@ -2531,8 +2832,8 @@ HWY_INLINE Vec128<double> ConcatUpperLower(const Vec128<double> hi,
 namespace detail {
 
 template <typename T, size_t N>
-HWY_API Vec128<T, N> OddEven(hwy::SizeTag<1> /* tag */, const Vec128<T, N> a,
-                             const Vec128<T, N> b) {
+HWY_INLINE Vec128<T, N> OddEven(hwy::SizeTag<1> /* tag */, const Vec128<T, N> a,
+                                const Vec128<T, N> b) {
   const Simd<T, N> d;
   const Repartition<uint8_t, decltype(d)> d8;
   alignas(16) constexpr uint8_t mask[16] = {0xFF, 0, 0xFF, 0, 0xFF, 0, 0xFF, 0,
@@ -2540,19 +2841,39 @@ HWY_API Vec128<T, N> OddEven(hwy::SizeTag<1> /* tag */, const Vec128<T, N> a,
   return IfThenElse(MaskFromVec(BitCast(d, Load(d8, mask))), b, a);
 }
 template <typename T, size_t N>
-HWY_API Vec128<T, N> OddEven(hwy::SizeTag<2> /* tag */, const Vec128<T, N> a,
-                             const Vec128<T, N> b) {
+HWY_INLINE Vec128<T, N> OddEven(hwy::SizeTag<2> /* tag */, const Vec128<T, N> a,
+                                const Vec128<T, N> b) {
+#if HWY_TARGET == HWY_SSSE3
+  const Simd<T, N> d;
+  const Repartition<uint8_t, decltype(d)> d8;
+  alignas(16) constexpr uint8_t mask[16] = {0xFF, 0xFF, 0, 0, 0xFF, 0xFF, 0, 0,
+                                            0xFF, 0xFF, 0, 0, 0xFF, 0xFF, 0, 0};
+  return IfThenElse(MaskFromVec(BitCast(d, Load(d8, mask))), b, a);
+#else
   return Vec128<T, N>{_mm_blend_epi16(a.raw, b.raw, 0x55)};
+#endif
 }
 template <typename T, size_t N>
-HWY_API Vec128<T, N> OddEven(hwy::SizeTag<4> /* tag */, const Vec128<T, N> a,
-                             const Vec128<T, N> b) {
+HWY_INLINE Vec128<T, N> OddEven(hwy::SizeTag<4> /* tag */, const Vec128<T, N> a,
+                                const Vec128<T, N> b) {
+#if HWY_TARGET == HWY_SSSE3
+  const __m128i odd = _mm_shuffle_epi32(a.raw, _MM_SHUFFLE(3, 1, 3, 1));
+  const __m128i even = _mm_shuffle_epi32(b.raw, _MM_SHUFFLE(2, 0, 2, 0));
+  return Vec128<T, N>{_mm_unpacklo_epi32(even, odd)};
+#else
   return Vec128<T, N>{_mm_blend_epi16(a.raw, b.raw, 0x33)};
+#endif
 }
 template <typename T, size_t N>
-HWY_API Vec128<T, N> OddEven(hwy::SizeTag<8> /* tag */, const Vec128<T, N> a,
-                             const Vec128<T, N> b) {
+HWY_INLINE Vec128<T, N> OddEven(hwy::SizeTag<8> /* tag */, const Vec128<T, N> a,
+                                const Vec128<T, N> b) {
+#if HWY_TARGET == HWY_SSSE3
+  const Full128<double> dd;
+  const __m128d concat = _mm_move_sd(BitCast(dd, a).raw, BitCast(dd, b).raw);
+  return BitCast(Full128<T>(), Vec128<double>{concat});
+#else
   return Vec128<T, N>{_mm_blend_epi16(a.raw, b.raw, 0x0F)};
+#endif
 }
 
 }  // namespace detail
@@ -2562,15 +2883,23 @@ HWY_API Vec128<T, N> OddEven(const Vec128<T, N> a, const Vec128<T, N> b) {
   return detail::OddEven(hwy::SizeTag<sizeof(T)>(), a, b);
 }
 template <size_t N>
-HWY_INLINE Vec128<float, N> OddEven(const Vec128<float, N> a,
-                                    const Vec128<float, N> b) {
+HWY_API Vec128<float, N> OddEven(const Vec128<float, N> a,
+                                 const Vec128<float, N> b) {
+#if HWY_TARGET == HWY_SSSE3
+  // SHUFPS must fill the lower half of the output from one register, so we
+  // need another shuffle. Unpack avoids another immediate byte.
+  const __m128 odd = _mm_shuffle_ps(a.raw, a.raw, _MM_SHUFFLE(3, 1, 3, 1));
+  const __m128 even = _mm_shuffle_ps(b.raw, b.raw, _MM_SHUFFLE(2, 0, 2, 0));
+  return Vec128<float, N>{_mm_unpacklo_ps(even, odd)};
+#else
   return Vec128<float, N>{_mm_blend_ps(a.raw, b.raw, 5)};
+#endif
 }
 
 template <size_t N>
-HWY_INLINE Vec128<double, N> OddEven(const Vec128<double, N> a,
-                                     const Vec128<double, N> b) {
-  return Vec128<double, N>{_mm_blend_pd(a.raw, b.raw, 1)};
+HWY_API Vec128<double, N> OddEven(const Vec128<double, N> a,
+                                  const Vec128<double, N> b) {
+  return Vec128<double>{_mm_shuffle_pd(b.raw, a.raw, _MM_SHUFFLE2(1, 0))};
 }
 
 // ------------------------------ Shl (ZipLower, Mul)
@@ -2579,21 +2908,22 @@ HWY_INLINE Vec128<double, N> OddEven(const Vec128<double, N> a,
 // two from loading float exponents, which is considerably faster (according
 // to LLVM-MCA) than scalar or testing bits: https://gcc.godbolt.org/z/9G7Y9v.
 
-#if HWY_TARGET != HWY_AVX3
+#if HWY_TARGET > HWY_AVX3  // AVX2 or older
 namespace detail {
 
 // Returns 2^v for use as per-lane multipliers to emulate 16-bit shifts.
 template <typename T, size_t N, HWY_IF_LANE_SIZE(T, 2)>
-HWY_API Vec128<MakeUnsigned<T>, N> Pow2(const Vec128<T, N> v) {
+HWY_INLINE Vec128<MakeUnsigned<T>, N> Pow2(const Vec128<T, N> v) {
   const Simd<T, N> d;
-  const Repartition<float, decltype(d)> df;
+  const RepartitionToWide<decltype(d)> dw;
+  const Rebind<float, decltype(dw)> df;
   const auto zero = Zero(d);
   // Move into exponent (this u16 will become the upper half of an f32)
   const auto exp = ShiftLeft<23 - 16>(v);
   const auto upper = exp + Set(d, 0x3F80);  // upper half of 1.0f
   // Insert 0 into lower halves for reinterpreting as binary32.
-  const auto f0 = ZipLower(zero, upper);
-  const auto f1 = ZipUpper(zero, upper);
+  const auto f0 = ZipLower(dw, zero, upper);
+  const auto f1 = ZipUpper(dw, zero, upper);
   // See comment below.
   const Vec128<int32_t, N> bits0{_mm_cvtps_epi32(BitCast(df, f0).raw)};
   const Vec128<int32_t, N> bits1{_mm_cvtps_epi32(BitCast(df, f1).raw)};
@@ -2602,7 +2932,7 @@ HWY_API Vec128<MakeUnsigned<T>, N> Pow2(const Vec128<T, N> v) {
 
 // Same, for 32-bit shifts.
 template <typename T, size_t N, HWY_IF_LANE_SIZE(T, 4)>
-HWY_API Vec128<MakeUnsigned<T>, N> Pow2(const Vec128<T, N> v) {
+HWY_INLINE Vec128<MakeUnsigned<T>, N> Pow2(const Vec128<T, N> v) {
   const Simd<T, N> d;
   const auto exp = ShiftLeft<23>(v);
   const auto f = exp + Set(d, 0x3F800000);  // 1.0f
@@ -2613,41 +2943,52 @@ HWY_API Vec128<MakeUnsigned<T>, N> Pow2(const Vec128<T, N> v) {
 }
 
 }  // namespace detail
-#endif  // HWY_TARGET != HWY_AVX3
+#endif  // HWY_TARGET > HWY_AVX3
 
 template <size_t N>
 HWY_API Vec128<uint16_t, N> operator<<(const Vec128<uint16_t, N> v,
                                        const Vec128<uint16_t, N> bits) {
-#if HWY_TARGET == HWY_AVX3
+#if HWY_TARGET <= HWY_AVX3
   return Vec128<uint16_t, N>{_mm_sllv_epi16(v.raw, bits.raw)};
 #else
   return v * detail::Pow2(bits);
 #endif
 }
+HWY_API Vec128<uint16_t, 1> operator<<(const Vec128<uint16_t, 1> v,
+                                       const Vec128<uint16_t, 1> bits) {
+  return Vec128<uint16_t, 1>{_mm_sll_epi16(v.raw, bits.raw)};
+}
 
 template <size_t N>
 HWY_API Vec128<uint32_t, N> operator<<(const Vec128<uint32_t, N> v,
                                        const Vec128<uint32_t, N> bits) {
-#if HWY_TARGET == HWY_SSE4
+#if HWY_TARGET == HWY_SSSE3 || HWY_TARGET == HWY_SSE4
   return v * detail::Pow2(bits);
 #else
   return Vec128<uint32_t, N>{_mm_sllv_epi32(v.raw, bits.raw)};
 #endif
 }
+HWY_API Vec128<uint32_t, 1> operator<<(const Vec128<uint32_t, 1> v,
+                                       const Vec128<uint32_t, 1> bits) {
+  return Vec128<uint32_t, 1>{_mm_sll_epi32(v.raw, bits.raw)};
+}
 
-template <size_t N>
-HWY_API Vec128<uint64_t, N> operator<<(const Vec128<uint64_t, N> v,
-                                       const Vec128<uint64_t, N> bits) {
-#if HWY_TARGET == HWY_SSE4
+HWY_API Vec128<uint64_t> operator<<(const Vec128<uint64_t> v,
+                                    const Vec128<uint64_t> bits) {
+#if HWY_TARGET == HWY_SSSE3 || HWY_TARGET == HWY_SSE4
   // Individual shifts and combine
-  const __m128i out0 = _mm_sll_epi64(v.raw, bits.raw);
+  const Vec128<uint64_t> out0{_mm_sll_epi64(v.raw, bits.raw)};
   const __m128i bits1 = _mm_unpackhi_epi64(bits.raw, bits.raw);
-  const __m128i out1 = _mm_sll_epi64(v.raw, bits1);
-  return Vec128<uint64_t, N>{_mm_blend_epi16(out0, out1, 0xF0)};
+  const Vec128<uint64_t> out1{_mm_sll_epi64(v.raw, bits1)};
+  return ConcatUpperLower(Full128<uint64_t>(), out1, out0);
 #else
-  return Vec128<uint64_t, N>{_mm_sllv_epi64(v.raw, bits.raw)};
+  return Vec128<uint64_t>{_mm_sllv_epi64(v.raw, bits.raw)};
 #endif
 }
+HWY_API Vec128<uint64_t, 1> operator<<(const Vec128<uint64_t, 1> v,
+                                       const Vec128<uint64_t, 1> bits) {
+  return Vec128<uint64_t, 1>{_mm_sll_epi64(v.raw, bits.raw)};
+}
 
 // Signed left shift is the same as unsigned.
 template <typename T, size_t N, HWY_IF_SIGNED(T)>
@@ -2659,7 +3000,7 @@ HWY_API Vec128<T, N> operator<<(const Vec128<T, N> v, const Vec128<T, N> bits) {
 
 // ------------------------------ Shr (mul, mask, BroadcastSignBit)
 
-// Use AVX2+ variable shifts except for the SSE4 target or 16-bit. There, we use
+// Use AVX2+ variable shifts except for SSSE3/SSE4 or 16-bit. There, we use
 // widening multiplication by powers of two obtained by loading float exponents,
 // followed by a constant right-shift. This is still faster than a scalar or
 // bit-test approach: https://gcc.godbolt.org/z/9G7Y9v.
@@ -2667,7 +3008,7 @@ HWY_API Vec128<T, N> operator<<(const Vec128<T, N> v, const Vec128<T, N> bits) {
 template <size_t N>
 HWY_API Vec128<uint16_t, N> operator>>(const Vec128<uint16_t, N> in,
                                        const Vec128<uint16_t, N> bits) {
-#if HWY_TARGET == HWY_AVX3
+#if HWY_TARGET <= HWY_AVX3
   return Vec128<uint16_t, N>{_mm_srlv_epi16(in.raw, bits.raw)};
 #else
   const Simd<uint16_t, N> d;
@@ -2677,11 +3018,15 @@ HWY_API Vec128<uint16_t, N> operator>>(const Vec128<uint16_t, N> in,
   return IfThenElse(bits == Zero(d), in, out);
 #endif
 }
+HWY_API Vec128<uint16_t, 1> operator>>(const Vec128<uint16_t, 1> in,
+                                       const Vec128<uint16_t, 1> bits) {
+  return Vec128<uint16_t, 1>{_mm_srl_epi16(in.raw, bits.raw)};
+}
 
 template <size_t N>
 HWY_API Vec128<uint32_t, N> operator>>(const Vec128<uint32_t, N> in,
                                        const Vec128<uint32_t, N> bits) {
-#if HWY_TARGET == HWY_SSE4
+#if HWY_TARGET == HWY_SSSE3 || HWY_TARGET == HWY_SSE4
   // 32x32 -> 64 bit mul, then shift right by 32.
   const Simd<uint32_t, N> d32;
   // Move odd lanes into position for the second mul. Shuffle more gracefully
@@ -2692,36 +3037,42 @@ HWY_API Vec128<uint32_t, N> operator>>(const Vec128<uint32_t, N> in,
   const auto out20 = ShiftRight<32>(MulEven(in, mul));  // z 2 z 0
   const Vec128<uint32_t, N> mul31{_mm_shuffle_epi32(mul.raw, 0x31)};
   // No need to shift right, already in the correct position.
-  const auto out31 = MulEven(in31, mul31);  // 3 ? 1 ?
-  // OddEven is defined below, avoid the dependency.
-  const Vec128<uint32_t, N> out{_mm_blend_epi16(out31.raw, out20.raw, 0x33)};
+  const auto out31 = BitCast(d32, MulEven(in31, mul31));  // 3 ? 1 ?
+  const Vec128<uint32_t, N> out = OddEven(out31, BitCast(d32, out20));
   // Replace output with input where bits == 0.
   return IfThenElse(bits == Zero(d32), in, out);
 #else
   return Vec128<uint32_t, N>{_mm_srlv_epi32(in.raw, bits.raw)};
 #endif
 }
+HWY_API Vec128<uint32_t, 1> operator>>(const Vec128<uint32_t, 1> in,
+                                       const Vec128<uint32_t, 1> bits) {
+  return Vec128<uint32_t, 1>{_mm_srl_epi32(in.raw, bits.raw)};
+}
 
-template <size_t N>
-HWY_API Vec128<uint64_t, N> operator>>(const Vec128<uint64_t, N> v,
-                                       const Vec128<uint64_t, N> bits) {
-#if HWY_TARGET == HWY_SSE4
+HWY_API Vec128<uint64_t> operator>>(const Vec128<uint64_t> v,
+                                    const Vec128<uint64_t> bits) {
+#if HWY_TARGET == HWY_SSSE3 || HWY_TARGET == HWY_SSE4
   // Individual shifts and combine
-  const __m128i out0 = _mm_srl_epi64(v.raw, bits.raw);
+  const Vec128<uint64_t> out0{_mm_srl_epi64(v.raw, bits.raw)};
   const __m128i bits1 = _mm_unpackhi_epi64(bits.raw, bits.raw);
-  const __m128i out1 = _mm_srl_epi64(v.raw, bits1);
-  return Vec128<uint64_t, N>{_mm_blend_epi16(out0, out1, 0xF0)};
+  const Vec128<uint64_t> out1{_mm_srl_epi64(v.raw, bits1)};
+  return ConcatUpperLower(Full128<uint64_t>(), out1, out0);
 #else
-  return Vec128<uint64_t, N>{_mm_srlv_epi64(v.raw, bits.raw)};
+  return Vec128<uint64_t>{_mm_srlv_epi64(v.raw, bits.raw)};
 #endif
 }
+HWY_API Vec128<uint64_t, 1> operator>>(const Vec128<uint64_t, 1> v,
+                                       const Vec128<uint64_t, 1> bits) {
+  return Vec128<uint64_t, 1>{_mm_srl_epi64(v.raw, bits.raw)};
+}
 
-#if HWY_TARGET != HWY_AVX3
+#if HWY_TARGET > HWY_AVX3  // AVX2 or older
 namespace detail {
 
 // Also used in x86_256-inl.h.
 template <class DI, class V>
-HWY_API V SignedShr(const DI di, const V v, const V count_i) {
+HWY_INLINE V SignedShr(const DI di, const V v, const V count_i) {
   const RebindToUnsigned<DI> du;
   const auto count = BitCast(du, count_i);  // same type as value to shift
   // Clear sign and restore afterwards. This is preferable to shifting the MSB
@@ -2732,38 +3083,64 @@ HWY_API V SignedShr(const DI di, const V v, const V count_i) {
 }
 
 }  // namespace detail
-#endif  // HWY_TARGET != HWY_AVX3
+#endif  // HWY_TARGET > HWY_AVX3
 
 template <size_t N>
 HWY_API Vec128<int16_t, N> operator>>(const Vec128<int16_t, N> v,
                                       const Vec128<int16_t, N> bits) {
-#if HWY_TARGET == HWY_AVX3
+#if HWY_TARGET <= HWY_AVX3
   return Vec128<int16_t, N>{_mm_srav_epi16(v.raw, bits.raw)};
 #else
   return detail::SignedShr(Simd<int16_t, N>(), v, bits);
 #endif
 }
+HWY_API Vec128<int16_t, 1> operator>>(const Vec128<int16_t, 1> v,
+                                      const Vec128<int16_t, 1> bits) {
+  return Vec128<int16_t, 1>{_mm_sra_epi16(v.raw, bits.raw)};
+}
 
 template <size_t N>
 HWY_API Vec128<int32_t, N> operator>>(const Vec128<int32_t, N> v,
                                       const Vec128<int32_t, N> bits) {
-#if HWY_TARGET == HWY_AVX3
+#if HWY_TARGET <= HWY_AVX3
   return Vec128<int32_t, N>{_mm_srav_epi32(v.raw, bits.raw)};
 #else
   return detail::SignedShr(Simd<int32_t, N>(), v, bits);
 #endif
 }
+HWY_API Vec128<int32_t, 1> operator>>(const Vec128<int32_t, 1> v,
+                                      const Vec128<int32_t, 1> bits) {
+  return Vec128<int32_t, 1>{_mm_sra_epi32(v.raw, bits.raw)};
+}
 
 template <size_t N>
 HWY_API Vec128<int64_t, N> operator>>(const Vec128<int64_t, N> v,
                                       const Vec128<int64_t, N> bits) {
-#if HWY_TARGET == HWY_AVX3
+#if HWY_TARGET <= HWY_AVX3
   return Vec128<int64_t, N>{_mm_srav_epi64(v.raw, bits.raw)};
 #else
   return detail::SignedShr(Simd<int64_t, N>(), v, bits);
 #endif
 }
 
+// ------------------------------ MulEven/Odd 64x64 (UpperHalf)
+
+HWY_INLINE Vec128<uint64_t> MulEven(const Vec128<uint64_t> a,
+                                    const Vec128<uint64_t> b) {
+  alignas(16) uint64_t mul[2];
+  mul[0] = Mul128(GetLane(a), GetLane(b), &mul[1]);
+  return Load(Full128<uint64_t>(), mul);
+}
+
+HWY_INLINE Vec128<uint64_t> MulOdd(const Vec128<uint64_t> a,
+                                   const Vec128<uint64_t> b) {
+  alignas(16) uint64_t mul[2];
+  const Half<Full128<uint64_t>> d2;
+  mul[0] =
+      Mul128(GetLane(UpperHalf(d2, a)), GetLane(UpperHalf(d2, b)), &mul[1]);
+  return Load(Full128<uint64_t>(), mul);
+}
+
 // ================================================== CONVERT
 
 // ------------------------------ Promotions (part w/ narrow lanes -> full)
@@ -2772,59 +3149,98 @@ HWY_API Vec128<int64_t, N> operator>>(const Vec128<int64_t, N> v,
 template <size_t N>
 HWY_API Vec128<uint16_t, N> PromoteTo(Simd<uint16_t, N> /* tag */,
                                       const Vec128<uint8_t, N> v) {
+#if HWY_TARGET == HWY_SSSE3
+  const __m128i zero = _mm_setzero_si128();
+  return Vec128<uint16_t, N>{_mm_unpacklo_epi8(v.raw, zero)};
+#else
   return Vec128<uint16_t, N>{_mm_cvtepu8_epi16(v.raw)};
+#endif
 }
 template <size_t N>
 HWY_API Vec128<uint32_t, N> PromoteTo(Simd<uint32_t, N> /* tag */,
-                                      const Vec128<uint8_t, N> v) {
-  return Vec128<uint32_t, N>{_mm_cvtepu8_epi32(v.raw)};
+                                      const Vec128<uint16_t, N> v) {
+#if HWY_TARGET == HWY_SSSE3
+  return Vec128<uint32_t, N>{_mm_unpacklo_epi16(v.raw, _mm_setzero_si128())};
+#else
+  return Vec128<uint32_t, N>{_mm_cvtepu16_epi32(v.raw)};
+#endif
 }
 template <size_t N>
-HWY_API Vec128<int16_t, N> PromoteTo(Simd<int16_t, N> /* tag */,
-                                     const Vec128<uint8_t, N> v) {
-  return Vec128<int16_t, N>{_mm_cvtepu8_epi16(v.raw)};
+HWY_API Vec128<uint64_t, N> PromoteTo(Simd<uint64_t, N> /* tag */,
+                                      const Vec128<uint32_t, N> v) {
+#if HWY_TARGET == HWY_SSSE3
+  return Vec128<uint64_t, N>{_mm_unpacklo_epi32(v.raw, _mm_setzero_si128())};
+#else
+  return Vec128<uint64_t, N>{_mm_cvtepu32_epi64(v.raw)};
+#endif
 }
 template <size_t N>
-HWY_API Vec128<int32_t, N> PromoteTo(Simd<int32_t, N> /* tag */,
-                                     const Vec128<uint8_t, N> v) {
-  return Vec128<int32_t, N>{_mm_cvtepu8_epi32(v.raw)};
+HWY_API Vec128<uint32_t, N> PromoteTo(Simd<uint32_t, N> /* tag */,
+                                      const Vec128<uint8_t, N> v) {
+#if HWY_TARGET == HWY_SSSE3
+  const __m128i zero = _mm_setzero_si128();
+  const __m128i u16 = _mm_unpacklo_epi8(v.raw, zero);
+  return Vec128<uint32_t, N>{_mm_unpacklo_epi16(u16, zero)};
+#else
+  return Vec128<uint32_t, N>{_mm_cvtepu8_epi32(v.raw)};
+#endif
 }
+
+// Unsigned to signed: same plus cast.
 template <size_t N>
-HWY_API Vec128<uint32_t, N> PromoteTo(Simd<uint32_t, N> /* tag */,
-                                      const Vec128<uint16_t, N> v) {
-  return Vec128<uint32_t, N>{_mm_cvtepu16_epi32(v.raw)};
+HWY_API Vec128<int16_t, N> PromoteTo(Simd<int16_t, N> di,
+                                     const Vec128<uint8_t, N> v) {
+  return BitCast(di, PromoteTo(Simd<uint16_t, N>(), v));
 }
 template <size_t N>
-HWY_API Vec128<int32_t, N> PromoteTo(Simd<int32_t, N> /* tag */,
+HWY_API Vec128<int32_t, N> PromoteTo(Simd<int32_t, N> di,
                                      const Vec128<uint16_t, N> v) {
-  return Vec128<int32_t, N>{_mm_cvtepu16_epi32(v.raw)};
+  return BitCast(di, PromoteTo(Simd<uint32_t, N>(), v));
 }
 template <size_t N>
-HWY_API Vec128<uint64_t, N> PromoteTo(Simd<uint64_t, N> /* tag */,
-                                      const Vec128<uint32_t, N> v) {
-  return Vec128<uint64_t, N>{_mm_cvtepu32_epi64(v.raw)};
+HWY_API Vec128<int32_t, N> PromoteTo(Simd<int32_t, N> di,
+                                     const Vec128<uint8_t, N> v) {
+  return BitCast(di, PromoteTo(Simd<uint32_t, N>(), v));
 }
 
 // Signed: replicate sign bit.
 template <size_t N>
 HWY_API Vec128<int16_t, N> PromoteTo(Simd<int16_t, N> /* tag */,
                                      const Vec128<int8_t, N> v) {
+#if HWY_TARGET == HWY_SSSE3
+  return ShiftRight<8>(Vec128<int16_t, N>{_mm_unpacklo_epi8(v.raw, v.raw)});
+#else
   return Vec128<int16_t, N>{_mm_cvtepi8_epi16(v.raw)};
-}
-template <size_t N>
-HWY_API Vec128<int32_t, N> PromoteTo(Simd<int32_t, N> /* tag */,
-                                     const Vec128<int8_t, N> v) {
-  return Vec128<int32_t, N>{_mm_cvtepi8_epi32(v.raw)};
+#endif
 }
 template <size_t N>
 HWY_API Vec128<int32_t, N> PromoteTo(Simd<int32_t, N> /* tag */,
                                      const Vec128<int16_t, N> v) {
+#if HWY_TARGET == HWY_SSSE3
+  return ShiftRight<16>(Vec128<int32_t, N>{_mm_unpacklo_epi16(v.raw, v.raw)});
+#else
   return Vec128<int32_t, N>{_mm_cvtepi16_epi32(v.raw)};
+#endif
 }
 template <size_t N>
 HWY_API Vec128<int64_t, N> PromoteTo(Simd<int64_t, N> /* tag */,
                                      const Vec128<int32_t, N> v) {
+#if HWY_TARGET == HWY_SSSE3
+  return ShiftRight<32>(Vec128<int64_t, N>{_mm_unpacklo_epi32(v.raw, v.raw)});
+#else
   return Vec128<int64_t, N>{_mm_cvtepi32_epi64(v.raw)};
+#endif
+}
+template <size_t N>
+HWY_API Vec128<int32_t, N> PromoteTo(Simd<int32_t, N> /* tag */,
+                                     const Vec128<int8_t, N> v) {
+#if HWY_TARGET == HWY_SSSE3
+  const __m128i x2 = _mm_unpacklo_epi8(v.raw, v.raw);
+  const __m128i x4 = _mm_unpacklo_epi16(x2, x2);
+  return ShiftRight<24>(Vec128<int32_t, N>{x4});
+#else
+  return Vec128<int32_t, N>{_mm_cvtepi8_epi32(v.raw)};
+#endif
 }
 
 // Workaround for origin tracking bug in Clang msan prior to 11.0
@@ -2836,12 +3252,11 @@ HWY_API Vec128<int64_t, N> PromoteTo(Simd<int64_t, N> /* tag */,
 #define HWY_INLINE_F16 HWY_INLINE
 #endif
 template <size_t N>
-HWY_INLINE_F16 Vec128<float, N> PromoteTo(Simd<float, N> /* tag */,
+HWY_INLINE_F16 Vec128<float, N> PromoteTo(Simd<float, N> df32,
                                           const Vec128<float16_t, N> v) {
-#if HWY_TARGET == HWY_SSE4
-  const Simd<int32_t, N> di32;
-  const Simd<uint32_t, N> du32;
-  const Simd<float, N> df32;
+#if HWY_TARGET >= HWY_SSE4 || defined(HWY_DISABLE_F16C)
+  const RebindToSigned<decltype(df32)> di32;
+  const RebindToUnsigned<decltype(df32)> du32;
   // Expand to u32 so we can shift.
   const auto bits16 = PromoteTo(du32, Vec128<uint16_t, N>{v.raw});
   const auto sign = ShiftRight<15>(bits16);
@@ -2857,6 +3272,7 @@ HWY_INLINE_F16 Vec128<float, N> PromoteTo(Simd<float, N> /* tag */,
   const auto bits32 = IfThenElse(biased_exp == Zero(du32), subnormal, normal);
   return BitCast(df32, ShiftLeft<31>(sign) | bits32);
 #else
+  (void)df32;
   return Vec128<float, N>{_mm_cvtph_ps(v.raw)};
 #endif
 }
@@ -2878,7 +3294,20 @@ HWY_API Vec128<double, N> PromoteTo(Simd<double, N> /* tag */,
 template <size_t N>
 HWY_API Vec128<uint16_t, N> DemoteTo(Simd<uint16_t, N> /* tag */,
                                      const Vec128<int32_t, N> v) {
+#if HWY_TARGET == HWY_SSSE3
+  const Simd<int32_t, N> di32;
+  const Simd<uint16_t, N * 2> du16;
+  const auto zero_if_neg = AndNot(ShiftRight<31>(v), v);
+  const auto too_big = VecFromMask(di32, Gt(v, Set(di32, 0xFFFF)));
+  const auto clamped = Or(zero_if_neg, too_big);
+  // Lower 2 bytes from each 32-bit lane; same as return type for fewer casts.
+  alignas(16) constexpr uint16_t kLower2Bytes[16] = {
+      0x0100, 0x0504, 0x0908, 0x0D0C, 0x8080, 0x8080, 0x8080, 0x8080};
+  const auto lo2 = Load(du16, kLower2Bytes);
+  return Vec128<uint16_t, N>{TableLookupBytes(BitCast(du16, clamped), lo2).raw};
+#else
   return Vec128<uint16_t, N>{_mm_packus_epi32(v.raw, v.raw)};
+#endif
 }
 
 template <size_t N>
@@ -2890,10 +3319,7 @@ HWY_API Vec128<int16_t, N> DemoteTo(Simd<int16_t, N> /* tag */,
 template <size_t N>
 HWY_API Vec128<uint8_t, N> DemoteTo(Simd<uint8_t, N> /* tag */,
                                     const Vec128<int32_t, N> v) {
-  const __m128i u16 = _mm_packus_epi32(v.raw, v.raw);
-  // packus treats the input as signed; we want unsigned. Clear the MSB to get
-  // unsigned saturation to u8.
-  const __m128i i16 = _mm_and_si128(u16, _mm_set1_epi16(0x7FFF));
+  const __m128i i16 = _mm_packs_epi32(v.raw, v.raw);
   return Vec128<uint8_t, N>{_mm_packus_epi16(i16, i16)};
 }
 
@@ -2917,13 +3343,12 @@ HWY_API Vec128<int8_t, N> DemoteTo(Simd<int8_t, N> /* tag */,
 }
 
 template <size_t N>
-HWY_INLINE Vec128<float16_t, N> DemoteTo(Simd<float16_t, N> /* tag */,
-                                         const Vec128<float, N> v) {
-#if HWY_TARGET == HWY_SSE4
-  const Simd<int32_t, N> di;
-  const Simd<uint32_t, N> du;
-  const Simd<uint16_t, N> du16;
-  const Simd<float16_t, N> df16;
+HWY_API Vec128<float16_t, N> DemoteTo(Simd<float16_t, N> df16,
+                                      const Vec128<float, N> v) {
+#if HWY_TARGET >= HWY_SSE4 || defined(HWY_DISABLE_F16C)
+  const RebindToUnsigned<decltype(df16)> du16;
+  const Rebind<uint32_t, decltype(df16)> du;
+  const RebindToSigned<decltype(du)> di;
   const auto bits32 = BitCast(du, v);
   const auto sign = ShiftRight<31>(bits32);
   const auto biased_exp32 = ShiftRight<23>(bits32) & Set(du, 0xFF);
@@ -2947,13 +3372,14 @@ HWY_INLINE Vec128<float16_t, N> DemoteTo(Simd<float16_t, N> /* tag */,
   const auto bits16 = IfThenZeroElse(is_tiny, BitCast(di, normal16));
   return BitCast(df16, DemoteTo(du16, bits16));
 #else
+  (void)df16;
   return Vec128<float16_t, N>{_mm_cvtps_ph(v.raw, _MM_FROUND_NO_EXC)};
 #endif
 }
 
 template <size_t N>
-HWY_INLINE Vec128<float, N> DemoteTo(Simd<float, N> /* tag */,
-                                     const Vec128<double, N> v) {
+HWY_API Vec128<float, N> DemoteTo(Simd<float, N> /* tag */,
+                                  const Vec128<double, N> v) {
   return Vec128<float, N>{_mm_cvtpd_ps(v.raw)};
 }
 
@@ -2962,7 +3388,7 @@ namespace detail {
 // For well-defined float->int demotion in all x86_*-inl.h.
 
 template <size_t N>
-HWY_API auto ClampF64ToI32Max(Simd<double, N> d, decltype(Zero(d)) v)
+HWY_INLINE auto ClampF64ToI32Max(Simd<double, N> d, decltype(Zero(d)) v)
     -> decltype(Zero(d)) {
   // The max can be exactly represented in binary64, so clamping beforehand
   // prevents x86 conversion from raising an exception and returning 80..00.
@@ -2973,9 +3399,9 @@ HWY_API auto ClampF64ToI32Max(Simd<double, N> d, decltype(Zero(d)) v)
 // change the result because the max integer value is not exactly representable.
 // Instead detect the overflow result after conversion and fix it.
 template <typename TI, size_t N, class DF = Simd<MakeFloat<TI>, N>>
-HWY_API auto FixConversionOverflow(Simd<TI, N> di,
-                                   decltype(Zero(DF())) original,
-                                   decltype(Zero(di).raw) converted_raw)
+HWY_INLINE auto FixConversionOverflow(Simd<TI, N> di,
+                                      decltype(Zero(DF())) original,
+                                      decltype(Zero(di).raw) converted_raw)
     -> decltype(Zero(di)) {
   // Combinations of original and output sign:
   //   --: normal <0 or -huge_val to 80..00: OK
@@ -2990,8 +3416,8 @@ HWY_API auto FixConversionOverflow(Simd<TI, N> di,
 }  // namespace detail
 
 template <size_t N>
-HWY_INLINE Vec128<int32_t, N> DemoteTo(Simd<int32_t, N> /* tag */,
-                                       const Vec128<double, N> v) {
+HWY_API Vec128<int32_t, N> DemoteTo(Simd<int32_t, N> /* tag */,
+                                    const Vec128<double, N> v) {
   const auto clamped = detail::ClampF64ToI32Max(Simd<double, N>(), v);
   return Vec128<int32_t, N>{_mm_cvttpd_epi32(clamped.raw)};
 }
@@ -3019,7 +3445,7 @@ HWY_API Vec128<float, N> ConvertTo(Simd<float, N> /* tag */,
 template <size_t N>
 HWY_API Vec128<double, N> ConvertTo(Simd<double, N> dd,
                                     const Vec128<int64_t, N> v) {
-#if HWY_TARGET == HWY_AVX3
+#if HWY_TARGET <= HWY_AVX3
   (void)dd;
   return Vec128<double, N>{_mm_cvtepi64_pd(v.raw)};
 #else
@@ -3047,25 +3473,61 @@ HWY_API Vec128<int32_t, N> ConvertTo(const Simd<int32_t, N> di,
   return detail::FixConversionOverflow(di, v, _mm_cvttps_epi32(v.raw));
 }
 
-template <size_t N>
-HWY_API Vec128<int64_t, N> ConvertTo(Simd<int64_t, N> di,
-                                     const Vec128<double, N> v) {
-#if HWY_TARGET == HWY_AVX3
+// Full (partial handled below)
+HWY_API Vec128<int64_t> ConvertTo(Full128<int64_t> di, const Vec128<double> v) {
+#if HWY_TARGET <= HWY_AVX3 && HWY_ARCH_X86_64
   return detail::FixConversionOverflow(di, v, _mm_cvttpd_epi64(v.raw));
+#elif HWY_ARCH_X86_64
+  const __m128i i0 = _mm_cvtsi64_si128(_mm_cvttsd_si64(v.raw));
+  const Half<Full128<double>> dd2;
+  const __m128i i1 = _mm_cvtsi64_si128(_mm_cvttsd_si64(UpperHalf(dd2, v).raw));
+  return detail::FixConversionOverflow(di, v, _mm_unpacklo_epi64(i0, i1));
 #else
-  alignas(16) double lanes_d[2];
-  Store(v, Simd<double, N>(), lanes_d);
-  alignas(16) int64_t lanes_i[2];
-  for (size_t i = 0; i < N; ++i) {
-    if (lanes_d[i] >= static_cast<double>(LimitsMax<int64_t>())) {
-      lanes_i[i] = LimitsMax<int64_t>();
-    } else if (lanes_d[i] <= static_cast<double>(LimitsMin<int64_t>())) {
-      lanes_i[i] = LimitsMin<int64_t>();
-    } else {
-      lanes_i[i] = static_cast<int64_t>(lanes_d[i]);
-    }
-  }
-  return Load(di, lanes_i);
+  using VI = decltype(Zero(di));
+  const VI k0 = Zero(di);
+  const VI k1 = Set(di, 1);
+  const VI k51 = Set(di, 51);
+
+  // Exponent indicates whether the number can be represented as int64_t.
+  const VI biased_exp = ShiftRight<52>(BitCast(di, v)) & Set(di, 0x7FF);
+  const VI exp = biased_exp - Set(di, 0x3FF);
+  const auto in_range = exp < Set(di, 63);
+
+  // If we were to cap the exponent at 51 and add 2^52, the number would be in
+  // [2^52, 2^53) and mantissa bits could be read out directly. We need to
+  // round-to-0 (truncate), but changing rounding mode in MXCSR hits a
+  // compiler reordering bug: https://gcc.godbolt.org/z/4hKj6c6qc . We instead
+  // manually shift the mantissa into place (we already have many of the
+  // inputs anyway).
+  const VI shift_mnt = Max(k51 - exp, k0);
+  const VI shift_int = Max(exp - k51, k0);
+  const VI mantissa = BitCast(di, v) & Set(di, (1ULL << 52) - 1);
+  // Include implicit 1-bit; shift by one more to ensure it's in the mantissa.
+  const VI int52 = (mantissa | Set(di, 1ULL << 52)) >> (shift_mnt + k1);
+  // For inputs larger than 2^52, insert zeros at the bottom.
+  const VI shifted = int52 << shift_int;
+  // Restore the one bit lost when shifting in the implicit 1-bit.
+  const VI restored = shifted | ((mantissa & k1) << (shift_int - k1));
+
+  // Saturate to LimitsMin (unchanged when negating below) or LimitsMax.
+  const VI sign_mask = BroadcastSignBit(BitCast(di, v));
+  const VI limit = Set(di, LimitsMax<int64_t>()) - sign_mask;
+  const VI magnitude = IfThenElse(in_range, restored, limit);
+
+  // If the input was negative, negate the integer (two's complement).
+  return (magnitude ^ sign_mask) - sign_mask;
+#endif
+}
+HWY_API Vec128<int64_t, 1> ConvertTo(Simd<int64_t, 1> di,
+                                     const Vec128<double, 1> v) {
+  // Only need to specialize for non-AVX3, 64-bit (single scalar op)
+#if HWY_TARGET > HWY_AVX3 && HWY_ARCH_X86_64
+  const Vec128<int64_t, 1> i0{_mm_cvtsi64_si128(_mm_cvttsd_si64(v.raw))};
+  return detail::FixConversionOverflow(di, v, i0.raw);
+#else
+  (void)di;
+  const auto full = ConvertTo(Full128<int64_t>(), Vec128<double>{v.raw});
+  return Vec128<int64_t, 1>{full.raw};
 #endif
 }
 
@@ -3075,11 +3537,166 @@ HWY_API Vec128<int32_t, N> NearestInt(const Vec128<float, N> v) {
   return detail::FixConversionOverflow(di, v, _mm_cvtps_epi32(v.raw));
 }
 
+// ------------------------------ Floating-point rounding (ConvertTo)
+
+#if HWY_TARGET == HWY_SSSE3
+
+// Toward nearest integer, ties to even
+template <typename T, size_t N, HWY_IF_FLOAT(T)>
+HWY_API Vec128<T, N> Round(const Vec128<T, N> v) {
+  // Rely on rounding after addition with a large value such that no mantissa
+  // bits remain (assuming the current mode is nearest-even). We may need a
+  // compiler flag for precise floating-point to prevent "optimizing" this out.
+  const Simd<T, N> df;
+  const auto max = Set(df, MantissaEnd<T>());
+  const auto large = CopySignToAbs(max, v);
+  const auto added = large + v;
+  const auto rounded = added - large;
+  // Keep original if NaN or the magnitude is large (already an int).
+  return IfThenElse(Abs(v) < max, rounded, v);
+}
+
+namespace detail {
+
+// Truncating to integer and converting back to float is correct except when the
+// input magnitude is large, in which case the input was already an integer
+// (because mantissa >> exponent is zero).
+template <typename T, size_t N, HWY_IF_FLOAT(T)>
+HWY_INLINE Mask128<T, N> UseInt(const Vec128<T, N> v) {
+  return Abs(v) < Set(Simd<T, N>(), MantissaEnd<T>());
+}
+
+}  // namespace detail
+
+// Toward zero, aka truncate
+template <typename T, size_t N, HWY_IF_FLOAT(T)>
+HWY_API Vec128<T, N> Trunc(const Vec128<T, N> v) {
+  const Simd<T, N> df;
+  const RebindToSigned<decltype(df)> di;
+
+  const auto integer = ConvertTo(di, v);  // round toward 0
+  const auto int_f = ConvertTo(df, integer);
+
+  return IfThenElse(detail::UseInt(v), CopySign(int_f, v), v);
+}
+
+// Toward +infinity, aka ceiling
+template <typename T, size_t N, HWY_IF_FLOAT(T)>
+HWY_API Vec128<T, N> Ceil(const Vec128<T, N> v) {
+  const Simd<T, N> df;
+  const RebindToSigned<decltype(df)> di;
+
+  const auto integer = ConvertTo(di, v);  // round toward 0
+  const auto int_f = ConvertTo(df, integer);
+
+  // Truncating a positive non-integer ends up smaller; if so, add 1.
+  const auto neg1 = ConvertTo(df, VecFromMask(di, RebindMask(di, int_f < v)));
+
+  return IfThenElse(detail::UseInt(v), int_f - neg1, v);
+}
+
+// Toward -infinity, aka floor
+template <typename T, size_t N, HWY_IF_FLOAT(T)>
+HWY_API Vec128<T, N> Floor(const Vec128<T, N> v) {
+  const Simd<T, N> df;
+  const RebindToSigned<decltype(df)> di;
+
+  const auto integer = ConvertTo(di, v);  // round toward 0
+  const auto int_f = ConvertTo(df, integer);
+
+  // Truncating a negative non-integer ends up larger; if so, subtract 1.
+  const auto neg1 = ConvertTo(df, VecFromMask(di, RebindMask(di, int_f > v)));
+
+  return IfThenElse(detail::UseInt(v), int_f + neg1, v);
+}
+
+#else
+
+// Toward nearest integer, ties to even
+template <size_t N>
+HWY_API Vec128<float, N> Round(const Vec128<float, N> v) {
+  return Vec128<float, N>{
+      _mm_round_ps(v.raw, _MM_FROUND_TO_NEAREST_INT | _MM_FROUND_NO_EXC)};
+}
+template <size_t N>
+HWY_API Vec128<double, N> Round(const Vec128<double, N> v) {
+  return Vec128<double, N>{
+      _mm_round_pd(v.raw, _MM_FROUND_TO_NEAREST_INT | _MM_FROUND_NO_EXC)};
+}
+
+// Toward zero, aka truncate
+template <size_t N>
+HWY_API Vec128<float, N> Trunc(const Vec128<float, N> v) {
+  return Vec128<float, N>{
+      _mm_round_ps(v.raw, _MM_FROUND_TO_ZERO | _MM_FROUND_NO_EXC)};
+}
+template <size_t N>
+HWY_API Vec128<double, N> Trunc(const Vec128<double, N> v) {
+  return Vec128<double, N>{
+      _mm_round_pd(v.raw, _MM_FROUND_TO_ZERO | _MM_FROUND_NO_EXC)};
+}
+
+// Toward +infinity, aka ceiling
+template <size_t N>
+HWY_API Vec128<float, N> Ceil(const Vec128<float, N> v) {
+  return Vec128<float, N>{
+      _mm_round_ps(v.raw, _MM_FROUND_TO_POS_INF | _MM_FROUND_NO_EXC)};
+}
+template <size_t N>
+HWY_API Vec128<double, N> Ceil(const Vec128<double, N> v) {
+  return Vec128<double, N>{
+      _mm_round_pd(v.raw, _MM_FROUND_TO_POS_INF | _MM_FROUND_NO_EXC)};
+}
+
+// Toward -infinity, aka floor
+template <size_t N>
+HWY_API Vec128<float, N> Floor(const Vec128<float, N> v) {
+  return Vec128<float, N>{
+      _mm_round_ps(v.raw, _MM_FROUND_TO_NEG_INF | _MM_FROUND_NO_EXC)};
+}
+template <size_t N>
+HWY_API Vec128<double, N> Floor(const Vec128<double, N> v) {
+  return Vec128<double, N>{
+      _mm_round_pd(v.raw, _MM_FROUND_TO_NEG_INF | _MM_FROUND_NO_EXC)};
+}
+
+#endif  // !HWY_SSSE3
+
+// ================================================== CRYPTO
+
+#if !defined(HWY_DISABLE_PCLMUL_AES) && HWY_TARGET != HWY_SSSE3
+
+// Per-target flag to prevent generic_ops-inl.h from defining AESRound.
+#ifdef HWY_NATIVE_AES
+#undef HWY_NATIVE_AES
+#else
+#define HWY_NATIVE_AES
+#endif
+
+HWY_API Vec128<uint8_t> AESRound(Vec128<uint8_t> state,
+                                 Vec128<uint8_t> round_key) {
+  return Vec128<uint8_t>{_mm_aesenc_si128(state.raw, round_key.raw)};
+}
+
+template <size_t N, HWY_IF_LE128(uint64_t, N)>
+HWY_API Vec128<uint64_t, N> CLMulLower(Vec128<uint64_t, N> a,
+                                       Vec128<uint64_t, N> b) {
+  return Vec128<uint64_t, N>{_mm_clmulepi64_si128(a.raw, b.raw, 0x00)};
+}
+
+template <size_t N, HWY_IF_LE128(uint64_t, N)>
+HWY_API Vec128<uint64_t, N> CLMulUpper(Vec128<uint64_t, N> a,
+                                       Vec128<uint64_t, N> b) {
+  return Vec128<uint64_t, N>{_mm_clmulepi64_si128(a.raw, b.raw, 0x11)};
+}
+
+#endif  // !defined(HWY_DISABLE_PCLMUL_AES) && HWY_TARGET != HWY_SSSE3
+
 // ================================================== MISC
 
 // Returns a vector with lane i=[0, N) set to "first" + i.
 template <typename T, size_t N, typename T2, HWY_IF_LE128(T, N)>
-Vec128<T, N> Iota(const Simd<T, N> d, const T2 first) {
+HWY_API Vec128<T, N> Iota(const Simd<T, N> d, const T2 first) {
   HWY_ALIGN T lanes[16 / sizeof(T)];
   for (size_t i = 0; i < 16 / sizeof(T); ++i) {
     lanes[i] = static_cast<T>(first + static_cast<T2>(i));
@@ -3096,24 +3713,24 @@ constexpr HWY_INLINE uint64_t U64FromInt(int bits) {
 }
 
 template <typename T, size_t N>
-HWY_API uint64_t BitsFromMask(hwy::SizeTag<1> /*tag*/,
-                              const Mask128<T, N> mask) {
+HWY_INLINE uint64_t BitsFromMask(hwy::SizeTag<1> /*tag*/,
+                                 const Mask128<T, N> mask) {
   const Simd<T, N> d;
   const auto sign_bits = BitCast(d, VecFromMask(d, mask)).raw;
   return U64FromInt(_mm_movemask_epi8(sign_bits));
 }
 
 template <typename T, size_t N>
-HWY_API uint64_t BitsFromMask(hwy::SizeTag<2> /*tag*/,
-                              const Mask128<T, N> mask) {
+HWY_INLINE uint64_t BitsFromMask(hwy::SizeTag<2> /*tag*/,
+                                 const Mask128<T, N> mask) {
   // Remove useless lower half of each u16 while preserving the sign bit.
   const auto sign_bits = _mm_packs_epi16(mask.raw, _mm_setzero_si128());
   return U64FromInt(_mm_movemask_epi8(sign_bits));
 }
 
 template <typename T, size_t N>
-HWY_API uint64_t BitsFromMask(hwy::SizeTag<4> /*tag*/,
-                              const Mask128<T, N> mask) {
+HWY_INLINE uint64_t BitsFromMask(hwy::SizeTag<4> /*tag*/,
+                                 const Mask128<T, N> mask) {
   const Simd<T, N> d;
   const Simd<float, N> df;
   const auto sign_bits = BitCast(df, VecFromMask(d, mask));
@@ -3121,8 +3738,8 @@ HWY_API uint64_t BitsFromMask(hwy::SizeTag<4> /*tag*/,
 }
 
 template <typename T, size_t N>
-HWY_API uint64_t BitsFromMask(hwy::SizeTag<8> /*tag*/,
-                              const Mask128<T, N> mask) {
+HWY_INLINE uint64_t BitsFromMask(hwy::SizeTag<8> /*tag*/,
+                                 const Mask128<T, N> mask) {
   const Simd<T, N> d;
   const Simd<double, N> df;
   const auto sign_bits = BitCast(df, VecFromMask(d, mask));
@@ -3136,38 +3753,46 @@ constexpr uint64_t OnlyActive(uint64_t bits) {
 }
 
 template <typename T, size_t N>
-HWY_API uint64_t BitsFromMask(const Mask128<T, N> mask) {
+HWY_INLINE uint64_t BitsFromMask(const Mask128<T, N> mask) {
   return OnlyActive<T, N>(BitsFromMask(hwy::SizeTag<sizeof(T)>(), mask));
 }
 
 }  // namespace detail
 
 template <typename T, size_t N>
-HWY_INLINE size_t StoreMaskBits(const Mask128<T, N> mask, uint8_t* p) {
+HWY_API size_t StoreMaskBits(const Simd<T, N> /* tag */,
+                             const Mask128<T, N> mask, uint8_t* p) {
   const uint64_t bits = detail::BitsFromMask(mask);
-  const size_t kNumBytes = (N + 7)/8;
+  const size_t kNumBytes = (N + 7) / 8;
   CopyBytes<kNumBytes>(&bits, p);
   return kNumBytes;
 }
 
 template <typename T, size_t N>
-HWY_API bool AllFalse(const Mask128<T, N> mask) {
+HWY_API bool AllFalse(const Simd<T, N> /* tag */, const Mask128<T, N> mask) {
   // Cheaper than PTEST, which is 2 uop / 3L.
   return detail::BitsFromMask(mask) == 0;
 }
 
 template <typename T, size_t N>
-HWY_API bool AllTrue(const Mask128<T, N> mask) {
+HWY_API bool AllTrue(const Simd<T, N> /* tag */, const Mask128<T, N> mask) {
   constexpr uint64_t kAllBits =
       detail::OnlyActive<T, N>((1ull << (16 / sizeof(T))) - 1);
   return detail::BitsFromMask(mask) == kAllBits;
 }
 
 template <typename T, size_t N>
-HWY_API size_t CountTrue(const Mask128<T, N> mask) {
+HWY_API size_t CountTrue(const Simd<T, N> /* tag */, const Mask128<T, N> mask) {
   return PopCount(detail::BitsFromMask(mask));
 }
 
+template <typename T, size_t N>
+HWY_API intptr_t FindFirstTrue(const Simd<T, N> /* tag */,
+                               const Mask128<T, N> mask) {
+  const uint64_t bits = detail::BitsFromMask(mask);
+  return bits ? Num0BitsBelowLS1Bit_Nonzero64(bits) : -1;
+}
+
 // ------------------------------ Compress
 
 namespace detail {
@@ -3356,8 +3981,8 @@ HWY_INLINE Vec128<T, N> Idx64x2FromBits(const uint64_t mask_bits) {
 // redundant BitsFromMask in the latter.
 
 template <typename T, size_t N>
-HWY_API Vec128<T, N> Compress(hwy::SizeTag<2> /*tag*/, Vec128<T, N> v,
-                              const uint64_t mask_bits) {
+HWY_INLINE Vec128<T, N> Compress(hwy::SizeTag<2> /*tag*/, Vec128<T, N> v,
+                                 const uint64_t mask_bits) {
   const auto idx = detail::Idx16x8FromBits<T, N>(mask_bits);
   using D = Simd<T, N>;
   const RebindToSigned<D> di;
@@ -3365,12 +3990,12 @@ HWY_API Vec128<T, N> Compress(hwy::SizeTag<2> /*tag*/, Vec128<T, N> v,
 }
 
 template <typename T, size_t N>
-HWY_API Vec128<T, N> Compress(hwy::SizeTag<4> /*tag*/, Vec128<T, N> v,
-                              const uint64_t mask_bits) {
+HWY_INLINE Vec128<T, N> Compress(hwy::SizeTag<4> /*tag*/, Vec128<T, N> v,
+                                 const uint64_t mask_bits) {
   using D = Simd<T, N>;
   using TI = MakeSigned<T>;
   const Rebind<TI, D> di;
-#if HWY_TARGET == HWY_AVX3
+#if HWY_TARGET <= HWY_AVX3
   return BitCast(D(), Vec128<TI, N>{_mm_maskz_compress_epi32(
                           mask_bits, BitCast(di, v).raw)});
 #else
@@ -3380,12 +4005,12 @@ HWY_API Vec128<T, N> Compress(hwy::SizeTag<4> /*tag*/, Vec128<T, N> v,
 }
 
 template <typename T, size_t N>
-HWY_API Vec128<T, N> Compress(hwy::SizeTag<8> /*tag*/, Vec128<T, N> v,
-                              const uint64_t mask_bits) {
+HWY_INLINE Vec128<T, N> Compress(hwy::SizeTag<8> /*tag*/, Vec128<T, N> v,
+                                 const uint64_t mask_bits) {
   using D = Simd<T, N>;
   using TI = MakeSigned<T>;
   const Rebind<TI, D> di;
-#if HWY_TARGET == HWY_AVX3
+#if HWY_TARGET <= HWY_AVX3
   return BitCast(D(), Vec128<TI, N>{_mm_maskz_compress_epi64(
                           mask_bits, BitCast(di, v).raw)});
 #else
@@ -3434,7 +4059,7 @@ HWY_API void StoreInterleaved3(const Vec128<uint8_t> v0,
       0x80, 2, 0x80, 0x80, 3, 0x80, 0x80, 4, 0x80, 0x80};
   const auto shuf_r0 = Load(d, tbl_r0);
   const auto shuf_g0 = Load(d, tbl_g0);  // cannot reuse r0 due to 5 in MSB
-  const auto shuf_b0 = CombineShiftRightBytes<15>(shuf_g0, shuf_g0);
+  const auto shuf_b0 = CombineShiftRightBytes<15>(d, shuf_g0, shuf_g0);
   const auto r0 = TableLookupBytes(v0, shuf_r0);  // 5..4..3..2..1..0
   const auto g0 = TableLookupBytes(v1, shuf_g0);  // ..4..3..2..1..0.
   const auto b0 = TableLookupBytes(v2, shuf_b0);  // .4..3..2..1..0..
@@ -3486,7 +4111,7 @@ HWY_API void StoreInterleaved3(const Vec128<uint8_t, 8> v0,
       0x80, 2, 0x80, 0x80, 3, 0x80, 0x80, 4, 0x80, 0x80};
   const auto shuf_r0 = Load(d_full, tbl_r0);
   const auto shuf_g0 = Load(d_full, tbl_g0);  // cannot reuse r0 due to 5 in MSB
-  const auto shuf_b0 = CombineShiftRightBytes<15>(shuf_g0, shuf_g0);
+  const auto shuf_b0 = CombineShiftRightBytes<15>(d_full, shuf_g0, shuf_g0);
   const auto r0 = TableLookupBytes(full_a, shuf_r0);  // 5..4..3..2..1..0
   const auto g0 = TableLookupBytes(full_b, shuf_g0);  // ..4..3..2..1..0.
   const auto b0 = TableLookupBytes(full_c, shuf_b0);  // .4..3..2..1..0..
@@ -3524,8 +4149,8 @@ HWY_API void StoreInterleaved3(const Vec128<uint8_t, N> v0,
       0,    0x80, 0x80, 1,   0x80, 0x80, 2, 0x80, 0x80, 3, 0x80, 0x80,  //
       0x80, 0x80, 0x80, 0x80};
   const auto shuf_r0 = Load(d_full, tbl_r0);
-  const auto shuf_g0 = CombineShiftRightBytes<15>(shuf_r0, shuf_r0);
-  const auto shuf_b0 = CombineShiftRightBytes<14>(shuf_r0, shuf_r0);
+  const auto shuf_g0 = CombineShiftRightBytes<15>(d_full, shuf_r0, shuf_r0);
+  const auto shuf_b0 = CombineShiftRightBytes<14>(d_full, shuf_r0, shuf_r0);
   const auto r0 = TableLookupBytes(full_a, shuf_r0);  // ......3..2..1..0
   const auto g0 = TableLookupBytes(full_b, shuf_g0);  // .....3..2..1..0.
   const auto b0 = TableLookupBytes(full_c, shuf_b0);  // ....3..2..1..0..
@@ -3541,21 +4166,23 @@ HWY_API void StoreInterleaved3(const Vec128<uint8_t, N> v0,
 HWY_API void StoreInterleaved4(const Vec128<uint8_t> v0,
                                const Vec128<uint8_t> v1,
                                const Vec128<uint8_t> v2,
-                               const Vec128<uint8_t> v3, Full128<uint8_t> d,
+                               const Vec128<uint8_t> v3, Full128<uint8_t> d8,
                                uint8_t* HWY_RESTRICT unaligned) {
+  const RepartitionToWide<decltype(d8)> d16;
+  const RepartitionToWide<decltype(d16)> d32;
   // let a,b,c,d denote v0..3.
-  const auto ba0 = ZipLower(v0, v1);  // b7 a7 .. b0 a0
-  const auto dc0 = ZipLower(v2, v3);  // d7 c7 .. d0 c0
-  const auto ba8 = ZipUpper(v0, v1);
-  const auto dc8 = ZipUpper(v2, v3);
-  const auto dcba_0 = ZipLower(ba0, dc0);  // d..a3 d..a0
-  const auto dcba_4 = ZipUpper(ba0, dc0);  // d..a7 d..a4
-  const auto dcba_8 = ZipLower(ba8, dc8);  // d..aB d..a8
-  const auto dcba_C = ZipUpper(ba8, dc8);  // d..aF d..aC
-  StoreU(BitCast(d, dcba_0), d, unaligned + 0 * 16);
-  StoreU(BitCast(d, dcba_4), d, unaligned + 1 * 16);
-  StoreU(BitCast(d, dcba_8), d, unaligned + 2 * 16);
-  StoreU(BitCast(d, dcba_C), d, unaligned + 3 * 16);
+  const auto ba0 = ZipLower(d16, v0, v1);  // b7 a7 .. b0 a0
+  const auto dc0 = ZipLower(d16, v2, v3);  // d7 c7 .. d0 c0
+  const auto ba8 = ZipUpper(d16, v0, v1);
+  const auto dc8 = ZipUpper(d16, v2, v3);
+  const auto dcba_0 = ZipLower(d32, ba0, dc0);  // d..a3 d..a0
+  const auto dcba_4 = ZipUpper(d32, ba0, dc0);  // d..a7 d..a4
+  const auto dcba_8 = ZipLower(d32, ba8, dc8);  // d..aB d..a8
+  const auto dcba_C = ZipUpper(d32, ba8, dc8);  // d..aF d..aC
+  StoreU(BitCast(d8, dcba_0), d8, unaligned + 0 * 16);
+  StoreU(BitCast(d8, dcba_4), d8, unaligned + 1 * 16);
+  StoreU(BitCast(d8, dcba_8), d8, unaligned + 2 * 16);
+  StoreU(BitCast(d8, dcba_C), d8, unaligned + 3 * 16);
 }
 
 // 64 bits
@@ -3566,18 +4193,20 @@ HWY_API void StoreInterleaved4(const Vec128<uint8_t, 8> in0,
                                Simd<uint8_t, 8> /*tag*/,
                                uint8_t* HWY_RESTRICT unaligned) {
   // Use full vectors to reduce the number of stores.
+  const Full128<uint8_t> d_full8;
+  const RepartitionToWide<decltype(d_full8)> d16;
+  const RepartitionToWide<decltype(d16)> d32;
   const Vec128<uint8_t> v0{in0.raw};
   const Vec128<uint8_t> v1{in1.raw};
   const Vec128<uint8_t> v2{in2.raw};
   const Vec128<uint8_t> v3{in3.raw};
   // let a,b,c,d denote v0..3.
-  const auto ba0 = ZipLower(v0, v1);       // b7 a7 .. b0 a0
-  const auto dc0 = ZipLower(v2, v3);       // d7 c7 .. d0 c0
-  const auto dcba_0 = ZipLower(ba0, dc0);  // d..a3 d..a0
-  const auto dcba_4 = ZipUpper(ba0, dc0);  // d..a7 d..a4
-  const Full128<uint8_t> d_full;
-  StoreU(BitCast(d_full, dcba_0), d_full, unaligned + 0 * 16);
-  StoreU(BitCast(d_full, dcba_4), d_full, unaligned + 1 * 16);
+  const auto ba0 = ZipLower(d16, v0, v1);       // b7 a7 .. b0 a0
+  const auto dc0 = ZipLower(d16, v2, v3);       // d7 c7 .. d0 c0
+  const auto dcba_0 = ZipLower(d32, ba0, dc0);  // d..a3 d..a0
+  const auto dcba_4 = ZipUpper(d32, ba0, dc0);  // d..a7 d..a4
+  StoreU(BitCast(d_full8, dcba_0), d_full8, unaligned + 0 * 16);
+  StoreU(BitCast(d_full8, dcba_4), d_full8, unaligned + 1 * 16);
 }
 
 // <= 32 bits
@@ -3589,17 +4218,19 @@ HWY_API void StoreInterleaved4(const Vec128<uint8_t, N> in0,
                                Simd<uint8_t, N> /*tag*/,
                                uint8_t* HWY_RESTRICT unaligned) {
   // Use full vectors to reduce the number of stores.
+  const Full128<uint8_t> d_full8;
+  const RepartitionToWide<decltype(d_full8)> d16;
+  const RepartitionToWide<decltype(d16)> d32;
   const Vec128<uint8_t> v0{in0.raw};
   const Vec128<uint8_t> v1{in1.raw};
   const Vec128<uint8_t> v2{in2.raw};
   const Vec128<uint8_t> v3{in3.raw};
   // let a,b,c,d denote v0..3.
-  const auto ba0 = ZipLower(v0, v1);       // b3 a3 .. b0 a0
-  const auto dc0 = ZipLower(v2, v3);       // d3 c3 .. d0 c0
-  const auto dcba_0 = ZipLower(ba0, dc0);  // d..a3 d..a0
+  const auto ba0 = ZipLower(d16, v0, v1);       // b3 a3 .. b0 a0
+  const auto dc0 = ZipLower(d16, v2, v3);       // d3 c3 .. d0 c0
+  const auto dcba_0 = ZipLower(d32, ba0, dc0);  // d..a3 d..a0
   alignas(16) uint8_t buf[16];
-  const Full128<uint8_t> d_full;
-  StoreU(BitCast(d_full, dcba_0), d_full, buf);
+  StoreU(BitCast(d_full8, dcba_0), d_full8, buf);
   CopyBytes<4 * N>(buf, unaligned);
 }
 
@@ -3609,18 +4240,18 @@ namespace detail {
 
 // N=1 for any T: no-op
 template <typename T>
-HWY_API Vec128<T, 1> SumOfLanes(hwy::SizeTag<sizeof(T)> /* tag */,
-                                const Vec128<T, 1> v) {
+HWY_INLINE Vec128<T, 1> SumOfLanes(hwy::SizeTag<sizeof(T)> /* tag */,
+                                   const Vec128<T, 1> v) {
   return v;
 }
 template <typename T>
-HWY_API Vec128<T, 1> MinOfLanes(hwy::SizeTag<sizeof(T)> /* tag */,
-                                const Vec128<T, 1> v) {
+HWY_INLINE Vec128<T, 1> MinOfLanes(hwy::SizeTag<sizeof(T)> /* tag */,
+                                   const Vec128<T, 1> v) {
   return v;
 }
 template <typename T>
-HWY_API Vec128<T, 1> MaxOfLanes(hwy::SizeTag<sizeof(T)> /* tag */,
-                                const Vec128<T, 1> v) {
+HWY_INLINE Vec128<T, 1> MaxOfLanes(hwy::SizeTag<sizeof(T)> /* tag */,
+                                   const Vec128<T, 1> v) {
   return v;
 }
 
@@ -3628,38 +4259,41 @@ HWY_API Vec128<T, 1> MaxOfLanes(hwy::SizeTag<sizeof(T)> /* tag */,
 
 // N=2
 template <typename T>
-HWY_API Vec128<T, 2> SumOfLanes(hwy::SizeTag<4> /* tag */,
-                                const Vec128<T, 2> v10) {
-  return v10 + Vec128<T, 2>{Shuffle2301(Vec128<T>{v10.raw}).raw};
+HWY_INLINE Vec128<T, 2> SumOfLanes(hwy::SizeTag<4> /* tag */,
+                                   const Vec128<T, 2> v10) {
+  return v10 + Shuffle2301(v10);
 }
 template <typename T>
-HWY_API Vec128<T, 2> MinOfLanes(hwy::SizeTag<4> /* tag */,
-                                const Vec128<T, 2> v10) {
-  return Min(v10, Vec128<T, 2>{Shuffle2301(Vec128<T>{v10.raw}).raw});
+HWY_INLINE Vec128<T, 2> MinOfLanes(hwy::SizeTag<4> /* tag */,
+                                   const Vec128<T, 2> v10) {
+  return Min(v10, Shuffle2301(v10));
 }
 template <typename T>
-HWY_API Vec128<T, 2> MaxOfLanes(hwy::SizeTag<4> /* tag */,
-                                const Vec128<T, 2> v10) {
-  return Max(v10, Vec128<T, 2>{Shuffle2301(Vec128<T>{v10.raw}).raw});
+HWY_INLINE Vec128<T, 2> MaxOfLanes(hwy::SizeTag<4> /* tag */,
+                                   const Vec128<T, 2> v10) {
+  return Max(v10, Shuffle2301(v10));
 }
 
 // N=4 (full)
 template <typename T>
-HWY_API Vec128<T> SumOfLanes(hwy::SizeTag<4> /* tag */, const Vec128<T> v3210) {
+HWY_INLINE Vec128<T> SumOfLanes(hwy::SizeTag<4> /* tag */,
+                                const Vec128<T> v3210) {
   const Vec128<T> v1032 = Shuffle1032(v3210);
   const Vec128<T> v31_20_31_20 = v3210 + v1032;
   const Vec128<T> v20_31_20_31 = Shuffle0321(v31_20_31_20);
   return v20_31_20_31 + v31_20_31_20;
 }
 template <typename T>
-HWY_API Vec128<T> MinOfLanes(hwy::SizeTag<4> /* tag */, const Vec128<T> v3210) {
+HWY_INLINE Vec128<T> MinOfLanes(hwy::SizeTag<4> /* tag */,
+                                const Vec128<T> v3210) {
   const Vec128<T> v1032 = Shuffle1032(v3210);
   const Vec128<T> v31_20_31_20 = Min(v3210, v1032);
   const Vec128<T> v20_31_20_31 = Shuffle0321(v31_20_31_20);
   return Min(v20_31_20_31, v31_20_31_20);
 }
 template <typename T>
-HWY_API Vec128<T> MaxOfLanes(hwy::SizeTag<4> /* tag */, const Vec128<T> v3210) {
+HWY_INLINE Vec128<T> MaxOfLanes(hwy::SizeTag<4> /* tag */,
+                                const Vec128<T> v3210) {
   const Vec128<T> v1032 = Shuffle1032(v3210);
   const Vec128<T> v31_20_31_20 = Max(v3210, v1032);
   const Vec128<T> v20_31_20_31 = Shuffle0321(v31_20_31_20);
@@ -3670,17 +4304,20 @@ HWY_API Vec128<T> MaxOfLanes(hwy::SizeTag<4> /* tag */, const Vec128<T> v3210) {
 
 // N=2 (full)
 template <typename T>
-HWY_API Vec128<T> SumOfLanes(hwy::SizeTag<8> /* tag */, const Vec128<T> v10) {
+HWY_INLINE Vec128<T> SumOfLanes(hwy::SizeTag<8> /* tag */,
+                                const Vec128<T> v10) {
   const Vec128<T> v01 = Shuffle01(v10);
   return v10 + v01;
 }
 template <typename T>
-HWY_API Vec128<T> MinOfLanes(hwy::SizeTag<8> /* tag */, const Vec128<T> v10) {
+HWY_INLINE Vec128<T> MinOfLanes(hwy::SizeTag<8> /* tag */,
+                                const Vec128<T> v10) {
   const Vec128<T> v01 = Shuffle01(v10);
   return Min(v10, v01);
 }
 template <typename T>
-HWY_API Vec128<T> MaxOfLanes(hwy::SizeTag<8> /* tag */, const Vec128<T> v10) {
+HWY_INLINE Vec128<T> MaxOfLanes(hwy::SizeTag<8> /* tag */,
+                                const Vec128<T> v10) {
   const Vec128<T> v01 = Shuffle01(v10);
   return Max(v10, v01);
 }
@@ -3689,18 +4326,114 @@ HWY_API Vec128<T> MaxOfLanes(hwy::SizeTag<8> /* tag */, const Vec128<T> v10) {
 
 // Supported for u/i/f 32/64. Returns the same value in each lane.
 template <typename T, size_t N>
-HWY_API Vec128<T, N> SumOfLanes(const Vec128<T, N> v) {
+HWY_API Vec128<T, N> SumOfLanes(Simd<T, N> /* tag */, const Vec128<T, N> v) {
   return detail::SumOfLanes(hwy::SizeTag<sizeof(T)>(), v);
 }
 template <typename T, size_t N>
-HWY_API Vec128<T, N> MinOfLanes(const Vec128<T, N> v) {
+HWY_API Vec128<T, N> MinOfLanes(Simd<T, N> /* tag */, const Vec128<T, N> v) {
   return detail::MinOfLanes(hwy::SizeTag<sizeof(T)>(), v);
 }
 template <typename T, size_t N>
-HWY_API Vec128<T, N> MaxOfLanes(const Vec128<T, N> v) {
+HWY_API Vec128<T, N> MaxOfLanes(Simd<T, N> /* tag */, const Vec128<T, N> v) {
   return detail::MaxOfLanes(hwy::SizeTag<sizeof(T)>(), v);
 }
 
+// ================================================== DEPRECATED
+
+template <typename T, size_t N>
+HWY_API size_t StoreMaskBits(const Mask128<T, N> mask, uint8_t* p) {
+  return StoreMaskBits(Simd<T, N>(), mask, p);
+}
+
+template <typename T, size_t N>
+HWY_API bool AllTrue(const Mask128<T, N> mask) {
+  return AllTrue(Simd<T, N>(), mask);
+}
+
+template <typename T, size_t N>
+HWY_API bool AllFalse(const Mask128<T, N> mask) {
+  return AllFalse(Simd<T, N>(), mask);
+}
+
+template <typename T, size_t N>
+HWY_API size_t CountTrue(const Mask128<T, N> mask) {
+  return CountTrue(Simd<T, N>(), mask);
+}
+
+template <typename T, size_t N>
+HWY_API Vec128<T, N> SumOfLanes(const Vec128<T, N> v) {
+  return SumOfLanes(Simd<T, N>(), v);
+}
+template <typename T, size_t N>
+HWY_API Vec128<T, N> MinOfLanes(const Vec128<T, N> v) {
+  return MinOfLanes(Simd<T, N>(), v);
+}
+template <typename T, size_t N>
+HWY_API Vec128<T, N> MaxOfLanes(const Vec128<T, N> v) {
+  return MaxOfLanes(Simd<T, N>(), v);
+}
+
+template <typename T, size_t N>
+HWY_API Vec128<T, (N + 1) / 2> UpperHalf(Vec128<T, N> v) {
+  return UpperHalf(Half<Simd<T, N>>(), v);
+}
+
+template <int kBytes, typename T, size_t N>
+HWY_API Vec128<T, N> ShiftRightBytes(const Vec128<T, N> v) {
+  return ShiftRightBytes<kBytes>(Simd<T, N>(), v);
+}
+
+template <int kLanes, typename T, size_t N>
+HWY_API Vec128<T, N> ShiftRightLanes(const Vec128<T, N> v) {
+  return ShiftRightLanes<kLanes>(Simd<T, N>(), v);
+}
+
+template <size_t kBytes, typename T, size_t N>
+HWY_API Vec128<T, N> CombineShiftRightBytes(Vec128<T, N> hi, Vec128<T, N> lo) {
+  return CombineShiftRightBytes<kBytes>(Simd<T, N>(), hi, lo);
+}
+
+template <typename T, size_t N>
+HWY_API Vec128<T, N> InterleaveUpper(Vec128<T, N> a, Vec128<T, N> b) {
+  return InterleaveUpper(Simd<T, N>(), a, b);
+}
+
+template <typename T, size_t N, class D = Simd<T, N>>
+HWY_API VFromD<RepartitionToWide<D>> ZipUpper(Vec128<T, N> a, Vec128<T, N> b) {
+  return InterleaveUpper(RepartitionToWide<D>(), a, b);
+}
+
+template <typename T, size_t N2>
+HWY_API Vec128<T, N2 * 2> Combine(Vec128<T, N2> hi2, Vec128<T, N2> lo2) {
+  return Combine(Simd<T, N2 * 2>(), hi2, lo2);
+}
+
+template <typename T, size_t N2, HWY_IF_LE64(T, N2)>
+HWY_API Vec128<T, N2 * 2> ZeroExtendVector(Vec128<T, N2> lo) {
+  return ZeroExtendVector(Simd<T, N2 * 2>(), lo);
+}
+
+template <typename T, size_t N>
+HWY_API Vec128<T, N> ConcatLowerLower(Vec128<T, N> hi, Vec128<T, N> lo) {
+  return ConcatLowerLower(Simd<T, N>(), hi, lo);
+}
+
+template <typename T, size_t N>
+HWY_API Vec128<T, N> ConcatUpperUpper(Vec128<T, N> hi, Vec128<T, N> lo) {
+  return ConcatUpperUpper(Simd<T, N>(), hi, lo);
+}
+
+template <typename T, size_t N>
+HWY_API Vec128<T, N> ConcatLowerUpper(const Vec128<T, N> hi,
+                                      const Vec128<T, N> lo) {
+  return ConcatLowerUpper(Simd<T, N>(), hi, lo);
+}
+
+template <typename T, size_t N>
+HWY_API Vec128<T, N> ConcatUpperLower(Vec128<T, N> hi, Vec128<T, N> lo) {
+  return ConcatUpperLower(Simd<T, N>(), hi, lo);
+}
+
 // ================================================== Operator wrapper
 
 // These apply to all x86_*-inl.h because there are no restrictions on V.
@@ -3737,6 +4470,10 @@ HWY_API auto Eq(V a, V b) -> decltype(a == b) {
   return a == b;
 }
 template <class V>
+HWY_API auto Ne(V a, V b) -> decltype(a == b) {
+  return a != b;
+}
+template <class V>
 HWY_API auto Lt(V a, V b) -> decltype(a == b) {
   return a < b;
 }
diff --git a/third_party/highway/hwy/ops/x86_256-inl.h b/third_party/highway/hwy/ops/x86_256-inl.h
index b934140f0c72f..4ff7aabca350a 100644
--- a/third_party/highway/hwy/ops/x86_256-inl.h
+++ b/third_party/highway/hwy/ops/x86_256-inl.h
@@ -20,7 +20,6 @@
 // particular, "Broadcast", pack and zip behavior may be surprising.
 
 #include <immintrin.h>  // AVX2+
-
 #if defined(_MSC_VER) && defined(__clang__)
 // Including <immintrin.h> should be enough, but Clang's headers helpfully skip
 // including these headers when _MSC_VER is defined, like when using clang-cl.
@@ -44,6 +43,11 @@ HWY_BEFORE_NAMESPACE();
 namespace hwy {
 namespace HWY_NAMESPACE {
 
+template <typename T>
+using Full256 = Simd<T, 32 / sizeof(T)>;
+
+namespace detail {
+
 template <typename T>
 struct Raw256 {
   using type = __m256i;
@@ -57,12 +61,11 @@ struct Raw256<double> {
   using type = __m256d;
 };
 
-template <typename T>
-using Full256 = Simd<T, 32 / sizeof(T)>;
+}  // namespace detail
 
 template <typename T>
 class Vec256 {
-  using Raw = typename Raw256<T>::type;
+  using Raw = typename detail::Raw256<T>::type;
 
  public:
   // Compound assignment. Only usable if there is a corresponding non-member
@@ -92,25 +95,24 @@ class Vec256 {
   Raw raw;
 };
 
-// Integer: FF..FF or 0. Float: MSB, all other bits undefined - see README.
+// FF..FF or 0.
 template <typename T>
-class Mask256 {
-  using Raw = typename Raw256<T>::type;
-
- public:
-  Raw raw;
+struct Mask256 {
+  typename detail::Raw256<T>::type raw;
 };
 
 // ------------------------------ BitCast
 
 namespace detail {
 
-HWY_API __m256i BitCastToInteger(__m256i v) { return v; }
-HWY_API __m256i BitCastToInteger(__m256 v) { return _mm256_castps_si256(v); }
-HWY_API __m256i BitCastToInteger(__m256d v) { return _mm256_castpd_si256(v); }
+HWY_INLINE __m256i BitCastToInteger(__m256i v) { return v; }
+HWY_INLINE __m256i BitCastToInteger(__m256 v) { return _mm256_castps_si256(v); }
+HWY_INLINE __m256i BitCastToInteger(__m256d v) {
+  return _mm256_castpd_si256(v);
+}
 
 template <typename T>
-HWY_API Vec256<uint8_t> BitCastToByte(Vec256<T> v) {
+HWY_INLINE Vec256<uint8_t> BitCastToByte(Vec256<T> v) {
   return Vec256<uint8_t>{BitCastToInteger(v.raw)};
 }
 
@@ -129,7 +131,7 @@ struct BitCastFromInteger256<double> {
 };
 
 template <typename T>
-HWY_API Vec256<T> BitCastFromByte(Full256<T> /* tag */, Vec256<uint8_t> v) {
+HWY_INLINE Vec256<T> BitCastFromByte(Full256<T> /* tag */, Vec256<uint8_t> v) {
   return Vec256<T>{BitCastFromInteger256<T>()(v.raw)};
 }
 
@@ -272,7 +274,7 @@ HWY_API Vec256<double> Xor(const Vec256<double> a, const Vec256<double> b) {
 template <typename T>
 HWY_API Vec256<T> Not(const Vec256<T> v) {
   using TU = MakeUnsigned<T>;
-#if HWY_TARGET == HWY_AVX3
+#if HWY_TARGET <= HWY_AVX3
   const __m256i vu = BitCast(Full256<TU>(), v).raw;
   return BitCast(Full256<T>(),
                  Vec256<TU>{_mm256_ternarylogic_epi32(vu, vu, vu, 0x55)});
@@ -298,6 +300,47 @@ HWY_API Vec256<T> operator^(const Vec256<T> a, const Vec256<T> b) {
   return Xor(a, b);
 }
 
+// ------------------------------ PopulationCount
+
+// 8/16 require BITALG, 32/64 require VPOPCNTDQ.
+#if HWY_TARGET == HWY_AVX3_DL
+
+#ifdef HWY_NATIVE_POPCNT
+#undef HWY_NATIVE_POPCNT
+#else
+#define HWY_NATIVE_POPCNT
+#endif
+
+namespace detail {
+
+template <typename T>
+HWY_INLINE Vec256<T> PopulationCount(hwy::SizeTag<1> /* tag */, Vec256<T> v) {
+  return Vec256<T>{_mm256_popcnt_epi8(v.raw)};
+}
+template <typename T>
+HWY_INLINE Vec256<T> PopulationCount(hwy::SizeTag<2> /* tag */, Vec256<T> v) {
+  return Vec256<T>{_mm256_popcnt_epi16(v.raw)};
+}
+template <typename T>
+HWY_INLINE Vec256<T> PopulationCount(hwy::SizeTag<4> /* tag */, Vec256<T> v) {
+  return Vec256<T>{_mm256_popcnt_epi32(v.raw)};
+}
+template <typename T>
+HWY_INLINE Vec256<T> PopulationCount(hwy::SizeTag<8> /* tag */, Vec256<T> v) {
+  return Vec256<T>{_mm256_popcnt_epi64(v.raw)};
+}
+
+}  // namespace detail
+
+template <typename T>
+HWY_API Vec256<T> PopulationCount(Vec256<T> v) {
+  return detail::PopulationCount(hwy::SizeTag<sizeof(T)>(), v);
+}
+
+#endif  // HWY_TARGET == HWY_AVX3_DL
+
+// ================================================== SIGN
+
 // ------------------------------ CopySign
 
 template <typename T>
@@ -307,7 +350,7 @@ HWY_API Vec256<T> CopySign(const Vec256<T> magn, const Vec256<T> sign) {
   const Full256<T> d;
   const auto msb = SignBit(d);
 
-#if HWY_TARGET == HWY_AVX3
+#if HWY_TARGET <= HWY_AVX3
   const Rebind<MakeUnsigned<T>, decltype(d)> du;
   // Truth table for msb, magn, sign | bitwise msb ? sign : mag
   //                  0    0     0   |  0
@@ -329,7 +372,7 @@ HWY_API Vec256<T> CopySign(const Vec256<T> magn, const Vec256<T> sign) {
 
 template <typename T>
 HWY_API Vec256<T> CopySignToAbs(const Vec256<T> abs, const Vec256<T> sign) {
-#if HWY_TARGET == HWY_AVX3
+#if HWY_TARGET <= HWY_AVX3
   // AVX3 can also handle abs < 0, so no extra action needed.
   return CopySign(abs, sign);
 #else
@@ -337,6 +380,8 @@ HWY_API Vec256<T> CopySignToAbs(const Vec256<T> abs, const Vec256<T> sign) {
 #endif
 }
 
+// ================================================== MASK
+
 // ------------------------------ Mask
 
 // Mask and Vec are the same (true = FF..FF).
@@ -396,8 +441,7 @@ HWY_API Vec256<T> ZeroIfNegative(Vec256<T> v) {
 
 template <typename T>
 HWY_API Mask256<T> Not(const Mask256<T> m) {
-  const Full256<T> d;
-  return MaskFromVec(Not(VecFromMask(d, m)));
+  return MaskFromVec(Not(VecFromMask(Full256<T>(), m)));
 }
 
 template <typename T>
@@ -434,6 +478,12 @@ HWY_API Mask256<TTo> RebindMask(Full256<TTo> d_to, Mask256<TFrom> m) {
   return MaskFromVec(BitCast(d_to, VecFromMask(Full256<TFrom>(), m)));
 }
 
+template <typename T>
+HWY_API Mask256<T> TestBit(const Vec256<T> v, const Vec256<T> bit) {
+  static_assert(!hwy::IsFloat<T>(), "Only integer vectors supported");
+  return (v & bit) == bit;
+}
+
 // ------------------------------ Equality
 
 // Unsigned
@@ -482,10 +532,20 @@ HWY_API Mask256<double> operator==(const Vec256<double> a,
   return Mask256<double>{_mm256_cmp_pd(a.raw, b.raw, _CMP_EQ_OQ)};
 }
 
-template <typename T>
-HWY_API Mask256<T> TestBit(const Vec256<T> v, const Vec256<T> bit) {
-  static_assert(!hwy::IsFloat<T>(), "Only integer vectors supported");
-  return (v & bit) == bit;
+// ------------------------------ Inequality
+
+template <typename T, HWY_IF_NOT_FLOAT(T)>
+HWY_API Mask256<T> operator!=(const Vec256<T> a, const Vec256<T> b) {
+  return Not(a == b);
+}
+
+HWY_API Mask256<float> operator!=(const Vec256<float> a,
+                                  const Vec256<float> b) {
+  return Mask256<float>{_mm256_cmp_ps(a.raw, b.raw, _CMP_NEQ_OQ)};
+}
+HWY_API Mask256<double> operator!=(const Vec256<double> a,
+                                   const Vec256<double> b) {
+  return Mask256<double>{_mm256_cmp_pd(a.raw, b.raw, _CMP_NEQ_OQ)};
 }
 
 // ------------------------------ Strict inequality
@@ -597,7 +657,7 @@ HWY_API Vec256<uint32_t> Min(const Vec256<uint32_t> a,
 }
 HWY_API Vec256<uint64_t> Min(const Vec256<uint64_t> a,
                              const Vec256<uint64_t> b) {
-#if HWY_TARGET == HWY_AVX3
+#if HWY_TARGET <= HWY_AVX3
   return Vec256<uint64_t>{_mm256_min_epu64(a.raw, b.raw)};
 #else
   const Full256<uint64_t> du;
@@ -619,7 +679,7 @@ HWY_API Vec256<int32_t> Min(const Vec256<int32_t> a, const Vec256<int32_t> b) {
   return Vec256<int32_t>{_mm256_min_epi32(a.raw, b.raw)};
 }
 HWY_API Vec256<int64_t> Min(const Vec256<int64_t> a, const Vec256<int64_t> b) {
-#if HWY_TARGET == HWY_AVX3
+#if HWY_TARGET <= HWY_AVX3
   return Vec256<int64_t>{_mm256_min_epi64(a.raw, b.raw)};
 #else
   return IfThenElse(a < b, a, b);
@@ -650,7 +710,7 @@ HWY_API Vec256<uint32_t> Max(const Vec256<uint32_t> a,
 }
 HWY_API Vec256<uint64_t> Max(const Vec256<uint64_t> a,
                              const Vec256<uint64_t> b) {
-#if HWY_TARGET == HWY_AVX3
+#if HWY_TARGET <= HWY_AVX3
   return Vec256<uint64_t>{_mm256_max_epu64(a.raw, b.raw)};
 #else
   const Full256<uint64_t> du;
@@ -672,7 +732,7 @@ HWY_API Vec256<int32_t> Max(const Vec256<int32_t> a, const Vec256<int32_t> b) {
   return Vec256<int32_t>{_mm256_max_epi32(a.raw, b.raw)};
 }
 HWY_API Vec256<int64_t> Max(const Vec256<int64_t> a, const Vec256<int64_t> b) {
-#if HWY_TARGET == HWY_AVX3
+#if HWY_TARGET <= HWY_AVX3
   return Vec256<int64_t>{_mm256_max_epi64(a.raw, b.raw)};
 #else
   return IfThenElse(a < b, b, a);
@@ -853,7 +913,7 @@ HWY_API Vec256<uint16_t> AverageRound(const Vec256<uint16_t> a,
   return Vec256<uint16_t>{_mm256_avg_epu16(a.raw, b.raw)};
 }
 
-// ------------------------------ Absolute value
+// ------------------------------ Abs (Sub)
 
 // Returns absolute value, except that LimitsMin() maps to LimitsMax() + 1.
 HWY_API Vec256<int8_t> Abs(const Vec256<int8_t> v) {
@@ -1037,7 +1097,7 @@ HWY_API Vec256<int64_t> BroadcastSignBit(const Vec256<int64_t> v) {
 
 template <int kBits>
 HWY_API Vec256<int64_t> ShiftRight(const Vec256<int64_t> v) {
-#if HWY_TARGET == HWY_AVX3
+#if HWY_TARGET <= HWY_AVX3
   return Vec256<int64_t>{_mm256_srai_epi64(v.raw, kBits)};
 #else
   const Full256<int64_t> di;
@@ -1049,7 +1109,7 @@ HWY_API Vec256<int64_t> ShiftRight(const Vec256<int64_t> v) {
 }
 
 HWY_API Vec256<int64_t> Abs(const Vec256<int64_t> v) {
-#if HWY_TARGET == HWY_AVX3
+#if HWY_TARGET <= HWY_AVX3
   return Vec256<int64_t>{_mm256_abs_epi64(v.raw)};
 #else
   const auto zero = Zero(Full256<int64_t>());
@@ -1125,7 +1185,7 @@ HWY_API Vec256<int32_t> ShiftRightSame(const Vec256<int32_t> v,
 }
 HWY_API Vec256<int64_t> ShiftRightSame(const Vec256<int64_t> v,
                                        const int bits) {
-#if HWY_TARGET == HWY_AVX3
+#if HWY_TARGET <= HWY_AVX3
   return Vec256<int64_t>{_mm256_sra_epi64(v.raw, _mm_cvtsi32_si128(bits))};
 #else
   const Full256<int64_t> di;
@@ -1144,7 +1204,7 @@ HWY_API Vec256<int8_t> ShiftRightSame(Vec256<int8_t> v, const int bits) {
   return (shifted ^ shifted_sign) - shifted_sign;
 }
 
-// ------------------------------ Negate
+// ------------------------------ Neg (Xor, Sub)
 
 template <typename T, HWY_IF_FLOAT(T)>
 HWY_API Vec256<T> Neg(const Vec256<T> v) {
@@ -1356,6 +1416,13 @@ HWY_API Vec256<T> LoadDup128(Full256<T> /* tag */, const T* HWY_RESTRICT p) {
   __m256i out;
   asm("vbroadcasti128 %1, %[reg]" : [ reg ] "=x"(out) : "m"(p[0]));
   return Vec256<T>{out};
+#elif HWY_COMPILER_MSVC && !HWY_COMPILER_CLANG
+  // Workaround for incorrect results with _mm256_broadcastsi128_si256. Note
+  // that MSVC also lacks _mm256_zextsi128_si256, but cast (which leaves the
+  // upper half undefined) is fine because we're overwriting that anyway.
+  const __m128i v128 = LoadU(Full128<T>(), p).raw;
+  return Vec256<T>{
+      _mm256_inserti128_si256(_mm256_castsi128_si256(v128), v128, 1)};
 #else
   return Vec256<T>{_mm256_broadcastsi128_si256(LoadU(Full128<T>(), p).raw)};
 #endif
@@ -1366,6 +1433,10 @@ HWY_API Vec256<float> LoadDup128(Full256<float> /* tag */,
   __m256 out;
   asm("vbroadcastf128 %1, %[reg]" : [ reg ] "=x"(out) : "m"(p[0]));
   return Vec256<float>{out};
+#elif HWY_COMPILER_MSVC && !HWY_COMPILER_CLANG
+  const __m128 v128 = LoadU(Full128<float>(), p).raw;
+  return Vec256<float>{
+      _mm256_insertf128_ps(_mm256_castps128_ps256(v128), v128, 1)};
 #else
   return Vec256<float>{_mm256_broadcast_ps(reinterpret_cast<const __m128*>(p))};
 #endif
@@ -1376,6 +1447,10 @@ HWY_API Vec256<double> LoadDup128(Full256<double> /* tag */,
   __m256d out;
   asm("vbroadcastf128 %1, %[reg]" : [ reg ] "=x"(out) : "m"(p[0]));
   return Vec256<double>{out};
+#elif HWY_COMPILER_MSVC && !HWY_COMPILER_CLANG
+  const __m128d v128 = LoadU(Full128<double>(), p).raw;
+  return Vec256<double>{
+      _mm256_insertf128_pd(_mm256_castpd128_pd256(v128), v128, 1)};
 #else
   return Vec256<double>{
       _mm256_broadcast_pd(reinterpret_cast<const __m128d*>(p))};
@@ -1432,32 +1507,32 @@ HWY_API void Stream(const Vec256<double> v, Full256<double> /* tag */,
 HWY_DIAGNOSTICS(push)
 HWY_DIAGNOSTICS_OFF(disable : 4245 4365, ignored "-Wsign-conversion")
 
-#if HWY_TARGET == HWY_AVX3
+#if HWY_TARGET <= HWY_AVX3
 namespace detail {
 
 template <typename T>
-HWY_API void ScatterOffset(hwy::SizeTag<4> /* tag */, Vec256<T> v,
-                           Full256<T> /* tag */, T* HWY_RESTRICT base,
-                           const Vec256<int32_t> offset) {
+HWY_INLINE void ScatterOffset(hwy::SizeTag<4> /* tag */, Vec256<T> v,
+                              Full256<T> /* tag */, T* HWY_RESTRICT base,
+                              const Vec256<int32_t> offset) {
   _mm256_i32scatter_epi32(base, offset.raw, v.raw, 1);
 }
 template <typename T>
-HWY_API void ScatterIndex(hwy::SizeTag<4> /* tag */, Vec256<T> v,
-                          Full256<T> /* tag */, T* HWY_RESTRICT base,
-                          const Vec256<int32_t> index) {
+HWY_INLINE void ScatterIndex(hwy::SizeTag<4> /* tag */, Vec256<T> v,
+                             Full256<T> /* tag */, T* HWY_RESTRICT base,
+                             const Vec256<int32_t> index) {
   _mm256_i32scatter_epi32(base, index.raw, v.raw, 4);
 }
 
 template <typename T>
-HWY_API void ScatterOffset(hwy::SizeTag<8> /* tag */, Vec256<T> v,
-                           Full256<T> /* tag */, T* HWY_RESTRICT base,
-                           const Vec256<int64_t> offset) {
+HWY_INLINE void ScatterOffset(hwy::SizeTag<8> /* tag */, Vec256<T> v,
+                              Full256<T> /* tag */, T* HWY_RESTRICT base,
+                              const Vec256<int64_t> offset) {
   _mm256_i64scatter_epi64(base, offset.raw, v.raw, 1);
 }
 template <typename T>
-HWY_API void ScatterIndex(hwy::SizeTag<8> /* tag */, Vec256<T> v,
-                          Full256<T> /* tag */, T* HWY_RESTRICT base,
-                          const Vec256<int64_t> index) {
+HWY_INLINE void ScatterIndex(hwy::SizeTag<8> /* tag */, Vec256<T> v,
+                             Full256<T> /* tag */, T* HWY_RESTRICT base,
+                             const Vec256<int64_t> index) {
   _mm256_i64scatter_epi64(base, index.raw, v.raw, 8);
 }
 
@@ -1476,31 +1551,25 @@ HWY_API void ScatterIndex(Vec256<T> v, Full256<T> d, T* HWY_RESTRICT base,
   return detail::ScatterIndex(hwy::SizeTag<sizeof(T)>(), v, d, base, index);
 }
 
-template <>
-HWY_INLINE void ScatterOffset<float>(Vec256<float> v, Full256<float> /* tag */,
-                                     float* HWY_RESTRICT base,
-                                     const Vec256<int32_t> offset) {
+HWY_API void ScatterOffset(Vec256<float> v, Full256<float> /* tag */,
+                           float* HWY_RESTRICT base,
+                           const Vec256<int32_t> offset) {
   _mm256_i32scatter_ps(base, offset.raw, v.raw, 1);
 }
-template <>
-HWY_INLINE void ScatterIndex<float>(Vec256<float> v, Full256<float> /* tag */,
-                                    float* HWY_RESTRICT base,
-                                    const Vec256<int32_t> index) {
+HWY_API void ScatterIndex(Vec256<float> v, Full256<float> /* tag */,
+                          float* HWY_RESTRICT base,
+                          const Vec256<int32_t> index) {
   _mm256_i32scatter_ps(base, index.raw, v.raw, 4);
 }
 
-template <>
-HWY_INLINE void ScatterOffset<double>(Vec256<double> v,
-                                      Full256<double> /* tag */,
-                                      double* HWY_RESTRICT base,
-                                      const Vec256<int64_t> offset) {
+HWY_API void ScatterOffset(Vec256<double> v, Full256<double> /* tag */,
+                           double* HWY_RESTRICT base,
+                           const Vec256<int64_t> offset) {
   _mm256_i64scatter_pd(base, offset.raw, v.raw, 1);
 }
-template <>
-HWY_INLINE void ScatterIndex<double>(Vec256<double> v,
-                                     Full256<double> /* tag */,
-                                     double* HWY_RESTRICT base,
-                                     const Vec256<int64_t> index) {
+HWY_API void ScatterIndex(Vec256<double> v, Full256<double> /* tag */,
+                          double* HWY_RESTRICT base,
+                          const Vec256<int64_t> index) {
   _mm256_i64scatter_pd(base, index.raw, v.raw, 8);
 }
 
@@ -1548,31 +1617,35 @@ HWY_API void ScatterIndex(Vec256<T> v, Full256<T> d, T* HWY_RESTRICT base,
 namespace detail {
 
 template <typename T>
-HWY_API Vec256<T> GatherOffset(hwy::SizeTag<4> /* tag */, Full256<T> /* tag */,
-                               const T* HWY_RESTRICT base,
-                               const Vec256<int32_t> offset) {
+HWY_INLINE Vec256<T> GatherOffset(hwy::SizeTag<4> /* tag */,
+                                  Full256<T> /* tag */,
+                                  const T* HWY_RESTRICT base,
+                                  const Vec256<int32_t> offset) {
   return Vec256<T>{_mm256_i32gather_epi32(
       reinterpret_cast<const int32_t*>(base), offset.raw, 1)};
 }
 template <typename T>
-HWY_API Vec256<T> GatherIndex(hwy::SizeTag<4> /* tag */, Full256<T> /* tag */,
-                              const T* HWY_RESTRICT base,
-                              const Vec256<int32_t> index) {
+HWY_INLINE Vec256<T> GatherIndex(hwy::SizeTag<4> /* tag */,
+                                 Full256<T> /* tag */,
+                                 const T* HWY_RESTRICT base,
+                                 const Vec256<int32_t> index) {
   return Vec256<T>{_mm256_i32gather_epi32(
       reinterpret_cast<const int32_t*>(base), index.raw, 4)};
 }
 
 template <typename T>
-HWY_API Vec256<T> GatherOffset(hwy::SizeTag<8> /* tag */, Full256<T> /* tag */,
-                               const T* HWY_RESTRICT base,
-                               const Vec256<int64_t> offset) {
+HWY_INLINE Vec256<T> GatherOffset(hwy::SizeTag<8> /* tag */,
+                                  Full256<T> /* tag */,
+                                  const T* HWY_RESTRICT base,
+                                  const Vec256<int64_t> offset) {
   return Vec256<T>{_mm256_i64gather_epi64(
       reinterpret_cast<const GatherIndex64*>(base), offset.raw, 1)};
 }
 template <typename T>
-HWY_API Vec256<T> GatherIndex(hwy::SizeTag<8> /* tag */, Full256<T> /* tag */,
-                              const T* HWY_RESTRICT base,
-                              const Vec256<int64_t> index) {
+HWY_INLINE Vec256<T> GatherIndex(hwy::SizeTag<8> /* tag */,
+                                 Full256<T> /* tag */,
+                                 const T* HWY_RESTRICT base,
+                                 const Vec256<int64_t> index) {
   return Vec256<T>{_mm256_i64gather_epi64(
       reinterpret_cast<const GatherIndex64*>(base), index.raw, 8)};
 }
@@ -1592,29 +1665,25 @@ HWY_API Vec256<T> GatherIndex(Full256<T> d, const T* HWY_RESTRICT base,
   return detail::GatherIndex(hwy::SizeTag<sizeof(T)>(), d, base, index);
 }
 
-template <>
-HWY_INLINE Vec256<float> GatherOffset<float>(Full256<float> /* tag */,
-                                             const float* HWY_RESTRICT base,
-                                             const Vec256<int32_t> offset) {
+HWY_API Vec256<float> GatherOffset(Full256<float> /* tag */,
+                                   const float* HWY_RESTRICT base,
+                                   const Vec256<int32_t> offset) {
   return Vec256<float>{_mm256_i32gather_ps(base, offset.raw, 1)};
 }
-template <>
-HWY_INLINE Vec256<float> GatherIndex<float>(Full256<float> /* tag */,
-                                            const float* HWY_RESTRICT base,
-                                            const Vec256<int32_t> index) {
+HWY_API Vec256<float> GatherIndex(Full256<float> /* tag */,
+                                  const float* HWY_RESTRICT base,
+                                  const Vec256<int32_t> index) {
   return Vec256<float>{_mm256_i32gather_ps(base, index.raw, 4)};
 }
 
-template <>
-HWY_INLINE Vec256<double> GatherOffset<double>(Full256<double> /* tag */,
-                                               const double* HWY_RESTRICT base,
-                                               const Vec256<int64_t> offset) {
+HWY_API Vec256<double> GatherOffset(Full256<double> /* tag */,
+                                    const double* HWY_RESTRICT base,
+                                    const Vec256<int64_t> offset) {
   return Vec256<double>{_mm256_i64gather_pd(base, offset.raw, 1)};
 }
-template <>
-HWY_INLINE Vec256<double> GatherIndex<double>(Full256<double> /* tag */,
-                                              const double* HWY_RESTRICT base,
-                                              const Vec256<int64_t> index) {
+HWY_API Vec256<double> GatherIndex(Full256<double> /* tag */,
+                                   const double* HWY_RESTRICT base,
+                                   const Vec256<int64_t> index) {
   return Vec256<double>{_mm256_i64gather_pd(base, index.raw, 8)};
 }
 
@@ -1622,39 +1691,43 @@ HWY_DIAGNOSTICS(pop)
 
 // ================================================== SWIZZLE
 
-template <typename T>
-HWY_API T GetLane(const Vec256<T> v) {
-  return GetLane(LowerHalf(v));
-}
-
-// ------------------------------ Extract half
+// ------------------------------ LowerHalf
 
 template <typename T>
-HWY_API Vec128<T> LowerHalf(Vec256<T> v) {
+HWY_API Vec128<T> LowerHalf(Full128<T> /* tag */, Vec256<T> v) {
   return Vec128<T>{_mm256_castsi256_si128(v.raw)};
 }
-template <>
-HWY_INLINE Vec128<float> LowerHalf(Vec256<float> v) {
+HWY_API Vec128<float> LowerHalf(Full128<float> /* tag */, Vec256<float> v) {
   return Vec128<float>{_mm256_castps256_ps128(v.raw)};
 }
-template <>
-HWY_INLINE Vec128<double> LowerHalf(Vec256<double> v) {
+HWY_API Vec128<double> LowerHalf(Full128<double> /* tag */, Vec256<double> v) {
   return Vec128<double>{_mm256_castpd256_pd128(v.raw)};
 }
 
 template <typename T>
-HWY_API Vec128<T> UpperHalf(Vec256<T> v) {
+HWY_API Vec128<T> LowerHalf(Vec256<T> v) {
+  return LowerHalf(Full128<T>(), v);
+}
+
+// ------------------------------ UpperHalf
+
+template <typename T>
+HWY_API Vec128<T> UpperHalf(Full128<T> /* tag */, Vec256<T> v) {
   return Vec128<T>{_mm256_extracti128_si256(v.raw, 1)};
 }
-template <>
-HWY_INLINE Vec128<float> UpperHalf(Vec256<float> v) {
+HWY_API Vec128<float> UpperHalf(Full128<float> /* tag */, Vec256<float> v) {
   return Vec128<float>{_mm256_extractf128_ps(v.raw, 1)};
 }
-template <>
-HWY_INLINE Vec128<double> UpperHalf(Vec256<double> v) {
+HWY_API Vec128<double> UpperHalf(Full128<double> /* tag */, Vec256<double> v) {
   return Vec128<double>{_mm256_extractf128_pd(v.raw, 1)};
 }
 
+// ------------------------------ GetLane (LowerHalf)
+template <typename T>
+HWY_API T GetLane(const Vec256<T> v) {
+  return GetLane(LowerHalf(v));
+}
+
 // ------------------------------ ZeroExtendVector
 
 // Unfortunately the initial _mm256_castsi128_si256 intrinsic leaves the upper
@@ -1663,29 +1736,29 @@ HWY_INLINE Vec128<double> UpperHalf(Vec256<double> v) {
 // compiler could decide to optimize out code that relies on this.
 //
 // The newer _mm256_zextsi128_si256 intrinsic fixes this by specifying the
-// zeroing, but it is not available on GCC until 10.1. For older GCC, we can
-// still obtain the desired code thanks to pattern recognition; note that the
-// expensive insert instruction is not actually generated, see
+// zeroing, but it is not available on MSVC nor GCC until 10.1. For older GCC,
+// we can still obtain the desired code thanks to pattern recognition; note that
+// the expensive insert instruction is not actually generated, see
 // https://gcc.godbolt.org/z/1MKGaP.
 
 template <typename T>
-HWY_API Vec256<T> ZeroExtendVector(Vec128<T> lo) {
+HWY_API Vec256<T> ZeroExtendVector(Full256<T> /* tag */, Vec128<T> lo) {
 #if !HWY_COMPILER_CLANG && HWY_COMPILER_GCC && (HWY_COMPILER_GCC < 1000)
   return Vec256<T>{_mm256_inserti128_si256(_mm256_setzero_si256(), lo.raw, 0)};
 #else
   return Vec256<T>{_mm256_zextsi128_si256(lo.raw)};
 #endif
 }
-template <>
-HWY_INLINE Vec256<float> ZeroExtendVector(Vec128<float> lo) {
+HWY_API Vec256<float> ZeroExtendVector(Full256<float> /* tag */,
+                                       Vec128<float> lo) {
 #if !HWY_COMPILER_CLANG && HWY_COMPILER_GCC && (HWY_COMPILER_GCC < 1000)
   return Vec256<float>{_mm256_insertf128_ps(_mm256_setzero_ps(), lo.raw, 0)};
 #else
   return Vec256<float>{_mm256_zextps128_ps256(lo.raw)};
 #endif
 }
-template <>
-HWY_INLINE Vec256<double> ZeroExtendVector(Vec128<double> lo) {
+HWY_API Vec256<double> ZeroExtendVector(Full256<double> /* tag */,
+                                        Vec128<double> lo) {
 #if !HWY_COMPILER_CLANG && HWY_COMPILER_GCC && (HWY_COMPILER_GCC < 1000)
   return Vec256<double>{_mm256_insertf128_pd(_mm256_setzero_pd(), lo.raw, 0)};
 #else
@@ -1696,63 +1769,72 @@ HWY_INLINE Vec256<double> ZeroExtendVector(Vec128<double> lo) {
 // ------------------------------ Combine
 
 template <typename T>
-HWY_API Vec256<T> Combine(Vec128<T> hi, Vec128<T> lo) {
-  const auto lo256 = ZeroExtendVector(lo);
+HWY_API Vec256<T> Combine(Full256<T> d, Vec128<T> hi, Vec128<T> lo) {
+  const auto lo256 = ZeroExtendVector(d, lo);
   return Vec256<T>{_mm256_inserti128_si256(lo256.raw, hi.raw, 1)};
 }
-template <>
-HWY_INLINE Vec256<float> Combine(Vec128<float> hi, Vec128<float> lo) {
-  const auto lo256 = ZeroExtendVector(lo);
+HWY_API Vec256<float> Combine(Full256<float> d, Vec128<float> hi,
+                              Vec128<float> lo) {
+  const auto lo256 = ZeroExtendVector(d, lo);
   return Vec256<float>{_mm256_insertf128_ps(lo256.raw, hi.raw, 1)};
 }
-template <>
-HWY_INLINE Vec256<double> Combine(Vec128<double> hi, Vec128<double> lo) {
-  const auto lo256 = ZeroExtendVector(lo);
+HWY_API Vec256<double> Combine(Full256<double> d, Vec128<double> hi,
+                               Vec128<double> lo) {
+  const auto lo256 = ZeroExtendVector(d, lo);
   return Vec256<double>{_mm256_insertf128_pd(lo256.raw, hi.raw, 1)};
 }
 
-// ------------------------------ Shift vector by constant #bytes
+// ------------------------------ ShiftLeftBytes
 
-// 0x01..0F, kBytes = 1 => 0x02..0F00
 template <int kBytes, typename T>
-HWY_API Vec256<T> ShiftLeftBytes(const Vec256<T> v) {
+HWY_API Vec256<T> ShiftLeftBytes(Full256<T> /* tag */, const Vec256<T> v) {
   static_assert(0 <= kBytes && kBytes <= 16, "Invalid kBytes");
   // This is the same operation as _mm256_bslli_epi128.
   return Vec256<T>{_mm256_slli_si256(v.raw, kBytes)};
 }
 
+template <int kBytes, typename T>
+HWY_API Vec256<T> ShiftLeftBytes(const Vec256<T> v) {
+  return ShiftLeftBytes<kBytes>(Full256<T>(), v);
+}
+
+// ------------------------------ ShiftLeftLanes
+
 template <int kLanes, typename T>
-HWY_API Vec256<T> ShiftLeftLanes(const Vec256<T> v) {
-  const Full256<uint8_t> d8;
-  const Full256<T> d;
+HWY_API Vec256<T> ShiftLeftLanes(Full256<T> d, const Vec256<T> v) {
+  const Repartition<uint8_t, decltype(d)> d8;
   return BitCast(d, ShiftLeftBytes<kLanes * sizeof(T)>(BitCast(d8, v)));
 }
 
-// 0x01..0F, kBytes = 1 => 0x0001..0E
+template <int kLanes, typename T>
+HWY_API Vec256<T> ShiftLeftLanes(const Vec256<T> v) {
+  return ShiftLeftLanes<kLanes>(Full256<T>(), v);
+}
+
+// ------------------------------ ShiftRightBytes
+
 template <int kBytes, typename T>
-HWY_API Vec256<T> ShiftRightBytes(const Vec256<T> v) {
+HWY_API Vec256<T> ShiftRightBytes(Full256<T> /* tag */, const Vec256<T> v) {
   static_assert(0 <= kBytes && kBytes <= 16, "Invalid kBytes");
   // This is the same operation as _mm256_bsrli_epi128.
   return Vec256<T>{_mm256_srli_si256(v.raw, kBytes)};
 }
 
+// ------------------------------ ShiftRightLanes
 template <int kLanes, typename T>
-HWY_API Vec256<T> ShiftRightLanes(const Vec256<T> v) {
-  const Full256<uint8_t> d8;
-  const Full256<T> d;
+HWY_API Vec256<T> ShiftRightLanes(Full256<T> d, const Vec256<T> v) {
+  const Repartition<uint8_t, decltype(d)> d8;
   return BitCast(d, ShiftRightBytes<kLanes * sizeof(T)>(BitCast(d8, v)));
 }
 
-// ------------------------------ Extract from 2x 128-bit at constant offset
+// ------------------------------ CombineShiftRightBytes
 
 // Extracts 128 bits from <hi, lo> by skipping the least-significant kBytes.
-template <int kBytes, typename T>
-HWY_API Vec256<T> CombineShiftRightBytes(const Vec256<T> hi,
-                                         const Vec256<T> lo) {
-  const Full256<uint8_t> d8;
-  const Vec256<uint8_t> extracted_bytes{
-      _mm256_alignr_epi8(BitCast(d8, hi).raw, BitCast(d8, lo).raw, kBytes)};
-  return BitCast(Full256<T>(), extracted_bytes);
+template <int kBytes, typename T, class V = Vec256<T>>
+HWY_API V CombineShiftRightBytes(Full256<T> d, V hi, V lo) {
+  const Repartition<uint8_t, decltype(d)> d8;
+  return BitCast(d, Vec256<uint8_t>{_mm256_alignr_epi8(
+                        BitCast(d8, hi).raw, BitCast(d8, lo).raw, kBytes)});
 }
 
 // ------------------------------ Broadcast/splat any lane
@@ -1922,7 +2004,7 @@ HWY_API Vec256<float> TableLookupLanes(const Vec256<float> v,
   return Vec256<float>{_mm256_permutevar8x32_ps(v.raw, idx.raw)};
 }
 
-// ------------------------------ Interleave lanes
+// ------------------------------ InterleaveLower
 
 // Interleaves lanes from halves of the 128-bit blocks of "a" (which provides
 // the least-significant lane) and "b". To concatenate two half-width integers
@@ -1971,6 +2053,17 @@ HWY_API Vec256<double> InterleaveLower(const Vec256<double> a,
   return Vec256<double>{_mm256_unpacklo_pd(a.raw, b.raw)};
 }
 
+// Additional overload for the optional Simd<> tag.
+template <typename T, class V = Vec256<T>>
+HWY_API V InterleaveLower(Full256<T> /* tag */, V a, V b) {
+  return InterleaveLower(a, b);
+}
+
+// ------------------------------ InterleaveUpper
+
+// All functions inside detail lack the required D parameter.
+namespace detail {
+
 HWY_API Vec256<uint8_t> InterleaveUpper(const Vec256<uint8_t> a,
                                         const Vec256<uint8_t> b) {
   return Vec256<uint8_t>{_mm256_unpackhi_epi8(a.raw, b.raw)};
@@ -2014,61 +2107,29 @@ HWY_API Vec256<double> InterleaveUpper(const Vec256<double> a,
   return Vec256<double>{_mm256_unpackhi_pd(a.raw, b.raw)};
 }
 
-// ------------------------------ Zip lanes
-
-// Same as interleave_*, except that the return lanes are double-width integers;
-// this is necessary because the single-lane scalar cannot return two values.
+}  // namespace detail
 
-HWY_API Vec256<uint16_t> ZipLower(const Vec256<uint8_t> a,
-                                  const Vec256<uint8_t> b) {
-  return Vec256<uint16_t>{_mm256_unpacklo_epi8(a.raw, b.raw)};
-}
-HWY_API Vec256<uint32_t> ZipLower(const Vec256<uint16_t> a,
-                                  const Vec256<uint16_t> b) {
-  return Vec256<uint32_t>{_mm256_unpacklo_epi16(a.raw, b.raw)};
-}
-HWY_API Vec256<uint64_t> ZipLower(const Vec256<uint32_t> a,
-                                  const Vec256<uint32_t> b) {
-  return Vec256<uint64_t>{_mm256_unpacklo_epi32(a.raw, b.raw)};
+template <typename T, class V = Vec256<T>>
+HWY_API V InterleaveUpper(Full256<T> /* tag */, V a, V b) {
+  return detail::InterleaveUpper(a, b);
 }
 
-HWY_API Vec256<int16_t> ZipLower(const Vec256<int8_t> a,
-                                 const Vec256<int8_t> b) {
-  return Vec256<int16_t>{_mm256_unpacklo_epi8(a.raw, b.raw)};
-}
-HWY_API Vec256<int32_t> ZipLower(const Vec256<int16_t> a,
-                                 const Vec256<int16_t> b) {
-  return Vec256<int32_t>{_mm256_unpacklo_epi16(a.raw, b.raw)};
-}
-HWY_API Vec256<int64_t> ZipLower(const Vec256<int32_t> a,
-                                 const Vec256<int32_t> b) {
-  return Vec256<int64_t>{_mm256_unpacklo_epi32(a.raw, b.raw)};
-}
+// ------------------------------ ZipLower/ZipUpper (InterleaveLower)
 
-HWY_API Vec256<uint16_t> ZipUpper(const Vec256<uint8_t> a,
-                                  const Vec256<uint8_t> b) {
-  return Vec256<uint16_t>{_mm256_unpackhi_epi8(a.raw, b.raw)};
-}
-HWY_API Vec256<uint32_t> ZipUpper(const Vec256<uint16_t> a,
-                                  const Vec256<uint16_t> b) {
-  return Vec256<uint32_t>{_mm256_unpackhi_epi16(a.raw, b.raw)};
+// Same as Interleave*, except that the return lanes are double-width integers;
+// this is necessary because the single-lane scalar cannot return two values.
+template <typename T, typename TW = MakeWide<T>>
+HWY_API Vec256<TW> ZipLower(Vec256<T> a, Vec256<T> b) {
+  return BitCast(Full256<TW>(), InterleaveLower(Full256<T>(), a, b));
 }
-HWY_API Vec256<uint64_t> ZipUpper(const Vec256<uint32_t> a,
-                                  const Vec256<uint32_t> b) {
-  return Vec256<uint64_t>{_mm256_unpackhi_epi32(a.raw, b.raw)};
+template <typename T, typename TW = MakeWide<T>>
+HWY_API Vec256<TW> ZipLower(Full256<TW> dw, Vec256<T> a, Vec256<T> b) {
+  return BitCast(dw, InterleaveLower(Full256<T>(), a, b));
 }
 
-HWY_API Vec256<int16_t> ZipUpper(const Vec256<int8_t> a,
-                                 const Vec256<int8_t> b) {
-  return Vec256<int16_t>{_mm256_unpackhi_epi8(a.raw, b.raw)};
-}
-HWY_API Vec256<int32_t> ZipUpper(const Vec256<int16_t> a,
-                                 const Vec256<int16_t> b) {
-  return Vec256<int32_t>{_mm256_unpackhi_epi16(a.raw, b.raw)};
-}
-HWY_API Vec256<int64_t> ZipUpper(const Vec256<int32_t> a,
-                                 const Vec256<int32_t> b) {
-  return Vec256<int64_t>{_mm256_unpackhi_epi32(a.raw, b.raw)};
+template <typename T, typename TW = MakeWide<T>>
+HWY_API Vec256<TW> ZipUpper(Full256<TW> dw, Vec256<T> a, Vec256<T> b) {
+  return BitCast(dw, InterleaveUpper(Full256<T>(), a, b));
 }
 
 // ------------------------------ Blocks (LowerHalf, ZeroExtendVector)
@@ -2079,56 +2140,63 @@ HWY_API Vec256<int64_t> ZipUpper(const Vec256<int32_t> a,
 
 // hiH,hiL loH,loL |-> hiL,loL (= lower halves)
 template <typename T>
-HWY_API Vec256<T> ConcatLowerLower(const Vec256<T> hi, const Vec256<T> lo) {
-  return Vec256<T>{_mm256_inserti128_si256(lo.raw, LowerHalf(hi).raw, 1)};
+HWY_API Vec256<T> ConcatLowerLower(Full256<T> d, const Vec256<T> hi,
+                                   const Vec256<T> lo) {
+  const Half<decltype(d)> d2;
+  return Vec256<T>{_mm256_inserti128_si256(lo.raw, LowerHalf(d2, hi).raw, 1)};
 }
-template <>
-HWY_INLINE Vec256<float> ConcatLowerLower(const Vec256<float> hi,
-                                          const Vec256<float> lo) {
-  return Vec256<float>{_mm256_insertf128_ps(lo.raw, LowerHalf(hi).raw, 1)};
+HWY_API Vec256<float> ConcatLowerLower(Full256<float> d, const Vec256<float> hi,
+                                       const Vec256<float> lo) {
+  const Half<decltype(d)> d2;
+  return Vec256<float>{_mm256_insertf128_ps(lo.raw, LowerHalf(d2, hi).raw, 1)};
 }
-template <>
-HWY_INLINE Vec256<double> ConcatLowerLower(const Vec256<double> hi,
-                                           const Vec256<double> lo) {
-  return Vec256<double>{_mm256_insertf128_pd(lo.raw, LowerHalf(hi).raw, 1)};
+HWY_API Vec256<double> ConcatLowerLower(Full256<double> d,
+                                        const Vec256<double> hi,
+                                        const Vec256<double> lo) {
+  const Half<decltype(d)> d2;
+  return Vec256<double>{_mm256_insertf128_pd(lo.raw, LowerHalf(d2, hi).raw, 1)};
 }
 
 // hiH,hiL loH,loL |-> hiL,loH (= inner halves / swap blocks)
 template <typename T>
-HWY_API Vec256<T> ConcatLowerUpper(const Vec256<T> hi, const Vec256<T> lo) {
+HWY_API Vec256<T> ConcatLowerUpper(Full256<T> /* tag */, const Vec256<T> hi,
+                                   const Vec256<T> lo) {
   return Vec256<T>{_mm256_permute2x128_si256(lo.raw, hi.raw, 0x21)};
 }
-template <>
-HWY_INLINE Vec256<float> ConcatLowerUpper(const Vec256<float> hi,
-                                          const Vec256<float> lo) {
+HWY_API Vec256<float> ConcatLowerUpper(Full256<float> /* tag */,
+                                       const Vec256<float> hi,
+                                       const Vec256<float> lo) {
   return Vec256<float>{_mm256_permute2f128_ps(lo.raw, hi.raw, 0x21)};
 }
-template <>
-HWY_INLINE Vec256<double> ConcatLowerUpper(const Vec256<double> hi,
-                                           const Vec256<double> lo) {
+HWY_API Vec256<double> ConcatLowerUpper(Full256<double> /* tag */,
+                                        const Vec256<double> hi,
+                                        const Vec256<double> lo) {
   return Vec256<double>{_mm256_permute2f128_pd(lo.raw, hi.raw, 0x21)};
 }
 
 // hiH,hiL loH,loL |-> hiH,loL (= outer halves)
 template <typename T>
-HWY_API Vec256<T> ConcatUpperLower(const Vec256<T> hi, const Vec256<T> lo) {
+HWY_API Vec256<T> ConcatUpperLower(Full256<T> /* tag */, const Vec256<T> hi,
+                                   const Vec256<T> lo) {
   return Vec256<T>{_mm256_blend_epi32(hi.raw, lo.raw, 0x0F)};
 }
-template <>
-HWY_INLINE Vec256<float> ConcatUpperLower(const Vec256<float> hi,
-                                          const Vec256<float> lo) {
+HWY_API Vec256<float> ConcatUpperLower(Full256<float> /* tag */,
+                                       const Vec256<float> hi,
+                                       const Vec256<float> lo) {
   return Vec256<float>{_mm256_blend_ps(hi.raw, lo.raw, 0x0F)};
 }
-template <>
-HWY_INLINE Vec256<double> ConcatUpperLower(const Vec256<double> hi,
-                                           const Vec256<double> lo) {
+HWY_API Vec256<double> ConcatUpperLower(Full256<double> /* tag */,
+                                        const Vec256<double> hi,
+                                        const Vec256<double> lo) {
   return Vec256<double>{_mm256_blend_pd(hi.raw, lo.raw, 3)};
 }
 
 // hiH,hiL loH,loL |-> hiH,loH (= upper halves)
 template <typename T>
-HWY_API Vec256<T> ConcatUpperUpper(const Vec256<T> hi, const Vec256<T> lo) {
-  return ConcatUpperLower(hi, ZeroExtendVector(UpperHalf(lo)));
+HWY_API Vec256<T> ConcatUpperUpper(Full256<T> d, const Vec256<T> hi,
+                                   const Vec256<T> lo) {
+  const Half<decltype(d)> d2;
+  return ConcatUpperLower(d, hi, ZeroExtendVector(d, UpperHalf(d2, lo)));
 }
 
 // ------------------------------ Odd/even lanes
@@ -2136,8 +2204,8 @@ HWY_API Vec256<T> ConcatUpperUpper(const Vec256<T> hi, const Vec256<T> lo) {
 namespace detail {
 
 template <typename T>
-HWY_API Vec256<T> OddEven(hwy::SizeTag<1> /* tag */, const Vec256<T> a,
-                          const Vec256<T> b) {
+HWY_INLINE Vec256<T> OddEven(hwy::SizeTag<1> /* tag */, const Vec256<T> a,
+                             const Vec256<T> b) {
   const Full256<T> d;
   const Full256<uint8_t> d8;
   alignas(32) constexpr uint8_t mask[16] = {0xFF, 0, 0xFF, 0, 0xFF, 0, 0xFF, 0,
@@ -2145,18 +2213,18 @@ HWY_API Vec256<T> OddEven(hwy::SizeTag<1> /* tag */, const Vec256<T> a,
   return IfThenElse(MaskFromVec(BitCast(d, LoadDup128(d8, mask))), b, a);
 }
 template <typename T>
-HWY_API Vec256<T> OddEven(hwy::SizeTag<2> /* tag */, const Vec256<T> a,
-                          const Vec256<T> b) {
+HWY_INLINE Vec256<T> OddEven(hwy::SizeTag<2> /* tag */, const Vec256<T> a,
+                             const Vec256<T> b) {
   return Vec256<T>{_mm256_blend_epi16(a.raw, b.raw, 0x55)};
 }
 template <typename T>
-HWY_API Vec256<T> OddEven(hwy::SizeTag<4> /* tag */, const Vec256<T> a,
-                          const Vec256<T> b) {
+HWY_INLINE Vec256<T> OddEven(hwy::SizeTag<4> /* tag */, const Vec256<T> a,
+                             const Vec256<T> b) {
   return Vec256<T>{_mm256_blend_epi32(a.raw, b.raw, 0x55)};
 }
 template <typename T>
-HWY_API Vec256<T> OddEven(hwy::SizeTag<8> /* tag */, const Vec256<T> a,
-                          const Vec256<T> b) {
+HWY_INLINE Vec256<T> OddEven(hwy::SizeTag<8> /* tag */, const Vec256<T> a,
+                             const Vec256<T> b) {
   return Vec256<T>{_mm256_blend_epi32(a.raw, b.raw, 0x33)};
 }
 
@@ -2166,45 +2234,63 @@ template <typename T>
 HWY_API Vec256<T> OddEven(const Vec256<T> a, const Vec256<T> b) {
   return detail::OddEven(hwy::SizeTag<sizeof(T)>(), a, b);
 }
-template <>
-HWY_INLINE Vec256<float> OddEven<float>(const Vec256<float> a,
-                                        const Vec256<float> b) {
+HWY_API Vec256<float> OddEven(const Vec256<float> a, const Vec256<float> b) {
   return Vec256<float>{_mm256_blend_ps(a.raw, b.raw, 0x55)};
 }
 
-template <>
-HWY_INLINE Vec256<double> OddEven<double>(const Vec256<double> a,
-                                          const Vec256<double> b) {
+HWY_API Vec256<double> OddEven(const Vec256<double> a, const Vec256<double> b) {
   return Vec256<double>{_mm256_blend_pd(a.raw, b.raw, 5)};
 }
 
-// ------------------------------ Shuffle bytes with variable indices
+// ------------------------------ TableLookupBytes (ZeroExtendVector)
 
-// Returns vector of bytes[from[i]]. "from" is also interpreted as bytes, i.e.
-// lane indices in [0, 16).
-template <typename T>
-HWY_API Vec256<T> TableLookupBytes(const Vec256<T> bytes,
-                                   const Vec256<T> from) {
-  return Vec256<T>{_mm256_shuffle_epi8(bytes.raw, from.raw)};
+// Both full
+template <typename T, typename TI>
+HWY_API Vec256<TI> TableLookupBytes(const Vec256<T> bytes,
+                                    const Vec256<TI> from) {
+  return Vec256<TI>{_mm256_shuffle_epi8(bytes.raw, from.raw)};
 }
 
+// Partial index vector
+template <typename T, typename TI, size_t NI>
+HWY_API Vec128<TI, NI> TableLookupBytes(const Vec256<T> bytes,
+                                        const Vec128<TI, NI> from) {
+  // First expand to full 128, then 256.
+  const auto from_256 = ZeroExtendVector(Full256<TI>(), Vec128<TI>{from.raw});
+  const auto tbl_full = TableLookupBytes(bytes, from_256);
+  // Shrink to 128, then partial.
+  return Vec128<TI, NI>{LowerHalf(Full128<TI>(), tbl_full).raw};
+}
+
+// Partial table vector
+template <typename T, size_t N, typename TI>
+HWY_API Vec256<TI> TableLookupBytes(const Vec128<T, N> bytes,
+                                    const Vec256<TI> from) {
+  // First expand to full 128, then 256.
+  const auto bytes_256 = ZeroExtendVector(Full256<T>(), Vec128<T>{bytes.raw});
+  return TableLookupBytes(bytes_256, from);
+}
+
+// Partial both are handled by x86_128.
+
 // ------------------------------ Shl (Mul, ZipLower)
 
-#if HWY_TARGET != HWY_AVX3
+#if HWY_TARGET > HWY_AVX3  // AVX2 or older
 namespace detail {
 
 // Returns 2^v for use as per-lane multipliers to emulate 16-bit shifts.
 template <typename T, HWY_IF_LANE_SIZE(T, 2)>
-HWY_API Vec256<MakeUnsigned<T>> Pow2(const Vec256<T> v) {
+HWY_INLINE Vec256<MakeUnsigned<T>> Pow2(const Vec256<T> v) {
   const Full256<T> d;
-  const Full256<float> df;
+  const RepartitionToWide<decltype(d)> dw;
+  const Rebind<float, decltype(dw)> df;
   const auto zero = Zero(d);
   // Move into exponent (this u16 will become the upper half of an f32)
   const auto exp = ShiftLeft<23 - 16>(v);
   const auto upper = exp + Set(d, 0x3F80);  // upper half of 1.0f
   // Insert 0 into lower halves for reinterpreting as binary32.
-  const auto f0 = ZipLower(zero, upper);
-  const auto f1 = ZipUpper(zero, upper);
+  const auto f0 = ZipLower(dw, zero, upper);
+  const auto f1 = ZipUpper(dw, zero, upper);
   // Do not use ConvertTo because it checks for overflow, which is redundant
   // because we only care about v in [0, 16).
   const Vec256<int32_t> bits0{_mm256_cvttps_epi32(BitCast(df, f0).raw)};
@@ -2213,11 +2299,11 @@ HWY_API Vec256<MakeUnsigned<T>> Pow2(const Vec256<T> v) {
 }
 
 }  // namespace detail
-#endif  // HWY_TARGET != HWY_AVX3
+#endif  // HWY_TARGET > HWY_AVX3
 
 HWY_API Vec256<uint16_t> operator<<(const Vec256<uint16_t> v,
                                     const Vec256<uint16_t> bits) {
-#if HWY_TARGET == HWY_AVX3
+#if HWY_TARGET <= HWY_AVX3
   return Vec256<uint16_t>{_mm256_sllv_epi16(v.raw, bits.raw)};
 #else
   return v * detail::Pow2(bits);
@@ -2246,7 +2332,7 @@ HWY_API Vec256<T> operator<<(const Vec256<T> v, const Vec256<T> bits) {
 
 HWY_API Vec256<uint16_t> operator>>(const Vec256<uint16_t> v,
                                     const Vec256<uint16_t> bits) {
-#if HWY_TARGET == HWY_AVX3
+#if HWY_TARGET <= HWY_AVX3
   return Vec256<uint16_t>{_mm256_srlv_epi16(v.raw, bits.raw)};
 #else
   const Full256<uint16_t> d;
@@ -2269,7 +2355,7 @@ HWY_API Vec256<uint64_t> operator>>(const Vec256<uint64_t> v,
 
 HWY_API Vec256<int16_t> operator>>(const Vec256<int16_t> v,
                                    const Vec256<int16_t> bits) {
-#if HWY_TARGET == HWY_AVX3
+#if HWY_TARGET <= HWY_AVX3
   return Vec256<int16_t>{_mm256_srav_epi16(v.raw, bits.raw)};
 #else
   return detail::SignedShr(Full256<int16_t>(), v, bits);
@@ -2283,22 +2369,73 @@ HWY_API Vec256<int32_t> operator>>(const Vec256<int32_t> v,
 
 HWY_API Vec256<int64_t> operator>>(const Vec256<int64_t> v,
                                    const Vec256<int64_t> bits) {
-#if HWY_TARGET == HWY_AVX3
+#if HWY_TARGET <= HWY_AVX3
   return Vec256<int64_t>{_mm256_srav_epi64(v.raw, bits.raw)};
 #else
   return detail::SignedShr(Full256<int64_t>(), v, bits);
 #endif
 }
 
-// ================================================== CONVERT
+HWY_INLINE Vec256<uint64_t> MulEven(const Vec256<uint64_t> a,
+                                    const Vec256<uint64_t> b) {
+  const DFromV<decltype(a)> du64;
+  const RepartitionToNarrow<decltype(du64)> du32;
+  const auto maskL = Set(du64, 0xFFFFFFFFULL);
+  const auto a32 = BitCast(du32, a);
+  const auto b32 = BitCast(du32, b);
+  // Inputs for MulEven: we only need the lower 32 bits
+  const auto aH = Shuffle2301(a32);
+  const auto bH = Shuffle2301(b32);
 
-// ------------------------------ Promotions (part w/ narrow lanes -> full)
+  // Knuth double-word multiplication. We use 32x32 = 64 MulEven and only need
+  // the even (lower 64 bits of every 128-bit block) results. See
+  // https://github.com/hcs0/Hackers-Delight/blob/master/muldwu.c.tat
+  const auto aLbL = MulEven(a32, b32);
+  const auto w3 = aLbL & maskL;
 
-HWY_API Vec256<float> PromoteTo(Full256<float> /* tag */,
-                                const Vec128<float16_t, 8> v) {
-  return Vec256<float>{_mm256_cvtph_ps(v.raw)};
+  const auto t2 = MulEven(aH, b32) + ShiftRight<32>(aLbL);
+  const auto w2 = t2 & maskL;
+  const auto w1 = ShiftRight<32>(t2);
+
+  const auto t = MulEven(a32, bH) + w2;
+  const auto k = ShiftRight<32>(t);
+
+  const auto mulH = MulEven(aH, bH) + w1 + k;
+  const auto mulL = ShiftLeft<32>(t) + w3;
+  return InterleaveLower(mulL, mulH);
 }
 
+HWY_INLINE Vec256<uint64_t> MulOdd(const Vec256<uint64_t> a,
+                                   const Vec256<uint64_t> b) {
+  const DFromV<decltype(a)> du64;
+  const RepartitionToNarrow<decltype(du64)> du32;
+  const auto maskL = Set(du64, 0xFFFFFFFFULL);
+  const auto a32 = BitCast(du32, a);
+  const auto b32 = BitCast(du32, b);
+  // Inputs for MulEven: we only need bits [95:64] (= upper half of input)
+  const auto aH = Shuffle2301(a32);
+  const auto bH = Shuffle2301(b32);
+
+  // Same as above, but we're using the odd results (upper 64 bits per block).
+  const auto aLbL = MulEven(a32, b32);
+  const auto w3 = aLbL & maskL;
+
+  const auto t2 = MulEven(aH, b32) + ShiftRight<32>(aLbL);
+  const auto w2 = t2 & maskL;
+  const auto w1 = ShiftRight<32>(t2);
+
+  const auto t = MulEven(a32, bH) + w2;
+  const auto k = ShiftRight<32>(t);
+
+  const auto mulH = MulEven(aH, bH) + w1 + k;
+  const auto mulL = ShiftLeft<32>(t) + w3;
+  return InterleaveUpper(du64, mulL, mulH);
+}
+
+// ================================================== CONVERT
+
+// ------------------------------ Promotions (part w/ narrow lanes -> full)
+
 HWY_API Vec256<double> PromoteTo(Full256<double> /* tag */,
                                  const Vec128<float, 4> v) {
   return Vec256<double>{_mm256_cvtps_pd(v.raw)};
@@ -2420,9 +2557,38 @@ HWY_API Vec128<int8_t> DemoteTo(Full128<int8_t> /* tag */,
 HWY_DIAGNOSTICS(push)
 HWY_DIAGNOSTICS_OFF(disable : 4556, ignored "-Wsign-conversion")
 
-HWY_API Vec128<float16_t> DemoteTo(Full128<float16_t> /* tag */,
+HWY_API Vec128<float16_t> DemoteTo(Full128<float16_t> df16,
                                    const Vec256<float> v) {
+#ifdef HWY_DISABLE_F16C
+  const RebindToUnsigned<decltype(df16)> du16;
+  const Rebind<uint32_t, decltype(df16)> du;
+  const RebindToSigned<decltype(du)> di;
+  const auto bits32 = BitCast(du, v);
+  const auto sign = ShiftRight<31>(bits32);
+  const auto biased_exp32 = ShiftRight<23>(bits32) & Set(du, 0xFF);
+  const auto mantissa32 = bits32 & Set(du, 0x7FFFFF);
+
+  const auto k15 = Set(di, 15);
+  const auto exp = Min(BitCast(di, biased_exp32) - Set(di, 127), k15);
+  const auto is_tiny = exp < Set(di, -24);
+
+  const auto is_subnormal = exp < Set(di, -14);
+  const auto biased_exp16 =
+      BitCast(du, IfThenZeroElse(is_subnormal, exp + k15));
+  const auto sub_exp = BitCast(du, Set(di, -14) - exp);  // [1, 11)
+  const auto sub_m = (Set(du, 1) << (Set(du, 10) - sub_exp)) +
+                     (mantissa32 >> (Set(du, 13) + sub_exp));
+  const auto mantissa16 = IfThenElse(RebindMask(du, is_subnormal), sub_m,
+                                     ShiftRight<13>(mantissa32));  // <1024
+
+  const auto sign16 = ShiftLeft<15>(sign);
+  const auto normal16 = sign16 | ShiftLeft<10>(biased_exp16) | mantissa16;
+  const auto bits16 = IfThenZeroElse(is_tiny, BitCast(di, normal16));
+  return BitCast(df16, DemoteTo(du16, bits16));
+#else
+  (void)df16;
   return Vec128<float16_t>{_mm256_cvtps_ph(v.raw, _MM_FROUND_NO_EXC)};
+#endif
 }
 
 HWY_DIAGNOSTICS(pop)
@@ -2447,7 +2613,7 @@ HWY_API Vec128<uint8_t, 8> U8FromU32(const Vec256<uint32_t> v) {
   const auto quad = TableLookupBytes(v, Load(d32, k8From32));
   // Interleave both quadruplets - OR instead of unpack reduces port5 pressure.
   const auto lo = LowerHalf(quad);
-  const auto hi = UpperHalf(quad);
+  const auto hi = UpperHalf(Full128<uint32_t>(), quad);
   const auto pair = LowerHalf(lo | hi);
   return BitCast(Simd<uint8_t, 8>(), pair);
 }
@@ -2460,7 +2626,7 @@ HWY_API Vec256<float> ConvertTo(Full256<float> /* tag */,
 }
 
 HWY_API Vec256<double> ConvertTo(Full256<double> dd, const Vec256<int64_t> v) {
-#if HWY_TARGET == HWY_AVX3
+#if HWY_TARGET <= HWY_AVX3
   (void)dd;
   return Vec256<double>{_mm256_cvtepi64_pd(v.raw)};
 #else
@@ -2487,22 +2653,42 @@ HWY_API Vec256<int32_t> ConvertTo(Full256<int32_t> d, const Vec256<float> v) {
 }
 
 HWY_API Vec256<int64_t> ConvertTo(Full256<int64_t> di, const Vec256<double> v) {
-#if HWY_TARGET == HWY_AVX3
+#if HWY_TARGET <= HWY_AVX3
   return detail::FixConversionOverflow(di, v, _mm256_cvttpd_epi64(v.raw));
 #else
-  alignas(32) double lanes_d[4];
-  Store(v, Full256<double>(), lanes_d);
-  alignas(32) int64_t lanes_i[4];
-  for (size_t i = 0; i < 4; ++i) {
-    if (lanes_d[i] >= static_cast<double>(LimitsMax<int64_t>())) {
-      lanes_i[i] = LimitsMax<int64_t>();
-    } else if (lanes_d[i] <= static_cast<double>(LimitsMin<int64_t>())) {
-      lanes_i[i] = LimitsMin<int64_t>();
-    } else {
-      lanes_i[i] = static_cast<int64_t>(lanes_d[i]);
-    }
-  }
-  return Load(di, lanes_i);
+  using VI = decltype(Zero(di));
+  const VI k0 = Zero(di);
+  const VI k1 = Set(di, 1);
+  const VI k51 = Set(di, 51);
+
+  // Exponent indicates whether the number can be represented as int64_t.
+  const VI biased_exp = ShiftRight<52>(BitCast(di, v)) & Set(di, 0x7FF);
+  const VI exp = biased_exp - Set(di, 0x3FF);
+  const auto in_range = exp < Set(di, 63);
+
+  // If we were to cap the exponent at 51 and add 2^52, the number would be in
+  // [2^52, 2^53) and mantissa bits could be read out directly. We need to
+  // round-to-0 (truncate), but changing rounding mode in MXCSR hits a
+  // compiler reordering bug: https://gcc.godbolt.org/z/4hKj6c6qc . We instead
+  // manually shift the mantissa into place (we already have many of the
+  // inputs anyway).
+  const VI shift_mnt = Max(k51 - exp, k0);
+  const VI shift_int = Max(exp - k51, k0);
+  const VI mantissa = BitCast(di, v) & Set(di, (1ULL << 52) - 1);
+  // Include implicit 1-bit; shift by one more to ensure it's in the mantissa.
+  const VI int52 = (mantissa | Set(di, 1ULL << 52)) >> (shift_mnt + k1);
+  // For inputs larger than 2^52, insert zeros at the bottom.
+  const VI shifted = int52 << shift_int;
+  // Restore the one bit lost when shifting in the implicit 1-bit.
+  const VI restored = shifted | ((mantissa & k1) << (shift_int - k1));
+
+  // Saturate to LimitsMin (unchanged when negating below) or LimitsMax.
+  const VI sign_mask = BroadcastSignBit(BitCast(di, v));
+  const VI limit = Set(di, LimitsMax<int64_t>()) - sign_mask;
+  const VI magnitude = IfThenElse(in_range, restored, limit);
+
+  // If the input was negative, negate the integer (two's complement).
+  return (magnitude ^ sign_mask) - sign_mask;
 #endif
 }
 
@@ -2511,11 +2697,84 @@ HWY_API Vec256<int32_t> NearestInt(const Vec256<float> v) {
   return detail::FixConversionOverflow(di, v, _mm256_cvtps_epi32(v.raw));
 }
 
+
+HWY_API Vec256<float> PromoteTo(Full256<float> df32,
+                                const Vec128<float16_t> v) {
+#ifdef HWY_DISABLE_F16C
+  const RebindToSigned<decltype(df32)> di32;
+  const RebindToUnsigned<decltype(df32)> du32;
+  // Expand to u32 so we can shift.
+  const auto bits16 = PromoteTo(du32, Vec128<uint16_t>{v.raw});
+  const auto sign = ShiftRight<15>(bits16);
+  const auto biased_exp = ShiftRight<10>(bits16) & Set(du32, 0x1F);
+  const auto mantissa = bits16 & Set(du32, 0x3FF);
+  const auto subnormal =
+      BitCast(du32, ConvertTo(df32, BitCast(di32, mantissa)) *
+                        Set(df32, 1.0f / 16384 / 1024));
+
+  const auto biased_exp32 = biased_exp + Set(du32, 127 - 15);
+  const auto mantissa32 = ShiftLeft<23 - 10>(mantissa);
+  const auto normal = ShiftLeft<23>(biased_exp32) | mantissa32;
+  const auto bits32 = IfThenElse(biased_exp == Zero(du32), subnormal, normal);
+  return BitCast(df32, ShiftLeft<31>(sign) | bits32);
+#else
+  (void)df32;
+  return Vec256<float>{_mm256_cvtph_ps(v.raw)};
+#endif
+}
+
+// ================================================== CRYPTO
+
+#if !defined(HWY_DISABLE_PCLMUL_AES)
+
+// Per-target flag to prevent generic_ops-inl.h from defining AESRound.
+#ifdef HWY_NATIVE_AES
+#undef HWY_NATIVE_AES
+#else
+#define HWY_NATIVE_AES
+#endif
+
+HWY_API Vec256<uint8_t> AESRound(Vec256<uint8_t> state,
+                                 Vec256<uint8_t> round_key) {
+#if HWY_TARGET == HWY_AVX3_DL
+  return Vec256<uint8_t>{_mm256_aesenc_epi128(state.raw, round_key.raw)};
+#else
+  const Full256<uint8_t> d;
+  const Half<decltype(d)> d2;
+  return Combine(d, AESRound(UpperHalf(d2, state), UpperHalf(d2, round_key)),
+                 AESRound(LowerHalf(state), LowerHalf(round_key)));
+#endif
+}
+
+HWY_API Vec256<uint64_t> CLMulLower(Vec256<uint64_t> a, Vec256<uint64_t> b) {
+#if HWY_TARGET == HWY_AVX3_DL
+  return Vec256<uint64_t>{_mm256_clmulepi64_epi128(a.raw, b.raw, 0x00)};
+#else
+  const Full256<uint64_t> d;
+  const Half<decltype(d)> d2;
+  return Combine(d, CLMulLower(UpperHalf(d2, a), UpperHalf(d2, b)),
+                 CLMulLower(LowerHalf(a), LowerHalf(b)));
+#endif
+}
+
+HWY_API Vec256<uint64_t> CLMulUpper(Vec256<uint64_t> a, Vec256<uint64_t> b) {
+#if HWY_TARGET == HWY_AVX3_DL
+  return Vec256<uint64_t>{_mm256_clmulepi64_epi128(a.raw, b.raw, 0x11)};
+#else
+  const Full256<uint64_t> d;
+  const Half<decltype(d)> d2;
+  return Combine(d, CLMulUpper(UpperHalf(d2, a), UpperHalf(d2, b)),
+                 CLMulUpper(LowerHalf(a), LowerHalf(b)));
+#endif
+}
+
+#endif  // HWY_DISABLE_PCLMUL_AES
+
 // ================================================== MISC
 
 // Returns a vector with lane i=[0, N) set to "first" + i.
 template <typename T, typename T2>
-Vec256<T> Iota(const Full256<T> d, const T2 first) {
+HWY_API Vec256<T> Iota(const Full256<T> d, const T2 first) {
   HWY_ALIGN T lanes[32 / sizeof(T)];
   for (size_t i = 0; i < 32 / sizeof(T); ++i) {
     lanes[i] = static_cast<T>(first + static_cast<T2>(i));
@@ -2528,7 +2787,8 @@ Vec256<T> Iota(const Full256<T> d, const T2 first) {
 namespace detail {
 
 template <typename T>
-HWY_API uint64_t BitsFromMask(hwy::SizeTag<1> /*tag*/, const Mask256<T> mask) {
+HWY_INLINE uint64_t BitsFromMask(hwy::SizeTag<1> /*tag*/,
+                                 const Mask256<T> mask) {
   const Full256<T> d;
   const Full256<uint8_t> d8;
   const auto sign_bits = BitCast(d8, VecFromMask(d, mask)).raw;
@@ -2537,7 +2797,8 @@ HWY_API uint64_t BitsFromMask(hwy::SizeTag<1> /*tag*/, const Mask256<T> mask) {
 }
 
 template <typename T>
-HWY_API uint64_t BitsFromMask(hwy::SizeTag<2> /*tag*/, const Mask256<T> mask) {
+HWY_INLINE uint64_t BitsFromMask(hwy::SizeTag<2> /*tag*/,
+                                 const Mask256<T> mask) {
 #if HWY_ARCH_X86_64
   const uint64_t sign_bits8 = BitsFromMask(hwy::SizeTag<1>(), mask);
   // Skip the bits from the lower byte of each u16 (better not to use the
@@ -2556,7 +2817,8 @@ HWY_API uint64_t BitsFromMask(hwy::SizeTag<2> /*tag*/, const Mask256<T> mask) {
 }
 
 template <typename T>
-HWY_API uint64_t BitsFromMask(hwy::SizeTag<4> /*tag*/, const Mask256<T> mask) {
+HWY_INLINE uint64_t BitsFromMask(hwy::SizeTag<4> /*tag*/,
+                                 const Mask256<T> mask) {
   const Full256<T> d;
   const Full256<float> df;
   const auto sign_bits = BitCast(df, VecFromMask(d, mask)).raw;
@@ -2564,7 +2826,8 @@ HWY_API uint64_t BitsFromMask(hwy::SizeTag<4> /*tag*/, const Mask256<T> mask) {
 }
 
 template <typename T>
-HWY_API uint64_t BitsFromMask(hwy::SizeTag<8> /*tag*/, const Mask256<T> mask) {
+HWY_INLINE uint64_t BitsFromMask(hwy::SizeTag<8> /*tag*/,
+                                 const Mask256<T> mask) {
   const Full256<T> d;
   const Full256<double> df;
   const auto sign_bits = BitCast(df, VecFromMask(d, mask)).raw;
@@ -2572,14 +2835,15 @@ HWY_API uint64_t BitsFromMask(hwy::SizeTag<8> /*tag*/, const Mask256<T> mask) {
 }
 
 template <typename T>
-HWY_API uint64_t BitsFromMask(const Mask256<T> mask) {
+HWY_INLINE uint64_t BitsFromMask(const Mask256<T> mask) {
   return BitsFromMask(hwy::SizeTag<sizeof(T)>(), mask);
 }
 
 }  // namespace detail
 
 template <typename T>
-HWY_INLINE size_t StoreMaskBits(const Mask256<T> mask, uint8_t* p) {
+HWY_API size_t StoreMaskBits(const Full256<T> /* tag */, const Mask256<T> mask,
+                             uint8_t* p) {
   const uint64_t bits = detail::BitsFromMask(mask);
   const size_t kNumBytes = (4 + sizeof(T) - 1) / sizeof(T);
   CopyBytes<kNumBytes>(&bits, p);
@@ -2587,22 +2851,29 @@ HWY_INLINE size_t StoreMaskBits(const Mask256<T> mask, uint8_t* p) {
 }
 
 template <typename T>
-HWY_API bool AllFalse(const Mask256<T> mask) {
+HWY_API bool AllFalse(const Full256<T> /* tag */, const Mask256<T> mask) {
   // Cheaper than PTEST, which is 2 uop / 3L.
   return detail::BitsFromMask(mask) == 0;
 }
 
 template <typename T>
-HWY_API bool AllTrue(const Mask256<T> mask) {
+HWY_API bool AllTrue(const Full256<T> /* tag */, const Mask256<T> mask) {
   constexpr uint64_t kAllBits = (1ull << (32 / sizeof(T))) - 1;
   return detail::BitsFromMask(mask) == kAllBits;
 }
 
 template <typename T>
-HWY_API size_t CountTrue(const Mask256<T> mask) {
+HWY_API size_t CountTrue(const Full256<T> /* tag */, const Mask256<T> mask) {
   return PopCount(detail::BitsFromMask(mask));
 }
 
+template <typename T>
+HWY_API intptr_t FindFirstTrue(const Full256<T> /* tag */,
+                               const Mask256<T> mask) {
+  const uint64_t bits = detail::BitsFromMask(mask);
+  return bits ? Num0BitsBelowLS1Bit_Nonzero64(bits) : -1;
+}
+
 // ------------------------------ Compress
 
 namespace detail {
@@ -2694,10 +2965,10 @@ HWY_INLINE Vec256<uint32_t> Idx64x4FromBits(const uint64_t mask_bits) {
 // redundant BitsFromMask in the latter.
 
 template <typename T>
-HWY_API Vec256<T> Compress(hwy::SizeTag<4> /*tag*/, Vec256<T> v,
-                           const uint64_t mask_bits) {
+HWY_INLINE Vec256<T> Compress(hwy::SizeTag<4> /*tag*/, Vec256<T> v,
+                              const uint64_t mask_bits) {
   const auto vu = BitCast(Full256<uint32_t>(), v);
-#if HWY_TARGET == HWY_AVX3
+#if HWY_TARGET <= HWY_AVX3
   const __m256i ret =
       _mm256_maskz_compress_epi32(static_cast<__mmask8>(mask_bits), vu.raw);
 #else
@@ -2708,10 +2979,10 @@ HWY_API Vec256<T> Compress(hwy::SizeTag<4> /*tag*/, Vec256<T> v,
 }
 
 template <typename T>
-HWY_API Vec256<T> Compress(hwy::SizeTag<8> /*tag*/, Vec256<T> v,
-                           const uint64_t mask_bits) {
+HWY_INLINE Vec256<T> Compress(hwy::SizeTag<8> /*tag*/, Vec256<T> v,
+                              const uint64_t mask_bits) {
   const auto vu = BitCast(Full256<uint64_t>(), v);
-#if HWY_TARGET == HWY_AVX3
+#if HWY_TARGET <= HWY_AVX3
   const __m256i ret =
       _mm256_maskz_compress_epi64(static_cast<__mmask8>(mask_bits), vu.raw);
 #else
@@ -2722,19 +2993,19 @@ HWY_API Vec256<T> Compress(hwy::SizeTag<8> /*tag*/, Vec256<T> v,
 }
 
 // Otherwise, defined in x86_512-inl.h so it can use wider vectors.
-#if HWY_TARGET != HWY_AVX3
+#if HWY_TARGET > HWY_AVX3  // AVX2 or older
 
 // LUTs are infeasible for 2^16 possible masks. Promoting to 32-bit and using
 // the native Compress is probably more efficient than 2 LUTs.
 template <typename T>
-HWY_API Vec256<T> Compress(hwy::SizeTag<2> /*tag*/, Vec256<T> v,
-                           const uint64_t mask_bits) {
+HWY_INLINE Vec256<T> Compress(hwy::SizeTag<2> /*tag*/, Vec256<T> v,
+                              const uint64_t mask_bits) {
   using D = Full256<T>;
   const Rebind<uint16_t, D> du;
   const Repartition<int32_t, D> dw;
   const auto vu16 = BitCast(du, v);  // (required for float16_t inputs)
   const auto promoted0 = PromoteTo(dw, LowerHalf(vu16));
-  const auto promoted1 = PromoteTo(dw, UpperHalf(vu16));
+  const auto promoted1 = PromoteTo(dw, UpperHalf(Half<decltype(du)>(), vu16));
 
   const uint64_t mask_bits0 = mask_bits & 0xFF;
   const uint64_t mask_bits1 = mask_bits >> 8;
@@ -2762,7 +3033,8 @@ HWY_API Vec256<T> Compress(hwy::SizeTag<2> /*tag*/, Vec256<T> v,
       Vec256<uint16_t>{_mm256_alignr_epi8(shift1_multiple4.raw, lo_zz, 14)};
 
   // Make the shift conditional on the lower bit of count0.
-  const auto m_odd = TestBit(Set(du, count0), Set(du, 1));
+  const auto m_odd =
+      TestBit(Set(du, static_cast<uint16_t>(count0)), Set(du, 1));
   const auto shifted1 = IfThenElse(m_odd, shift1_multiple2, shift1_multiple4);
 
   // Blend the lower and shifted upper parts.
@@ -2773,12 +3045,12 @@ HWY_API Vec256<T> Compress(hwy::SizeTag<2> /*tag*/, Vec256<T> v,
   return BitCast(D(), IfThenElse(m_lower, demoted0, shifted1));
 }
 
-#endif  // HWY_TARGET != HWY_AVX3
+#endif  // HWY_TARGET > HWY_AVX3
 
 }  // namespace detail
 
 // Otherwise, defined in x86_512-inl.h after detail::Compress.
-#if HWY_TARGET != HWY_AVX3
+#if HWY_TARGET > HWY_AVX3  // AVX2 or older
 
 template <typename T>
 HWY_API Vec256<T> Compress(Vec256<T> v, const Mask256<T> mask) {
@@ -2799,7 +3071,7 @@ HWY_API size_t CompressStore(Vec256<T> v, const Mask256<T> mask, Full256<T> d,
   return PopCount(mask_bits);
 }
 
-#endif  // HWY_TARGET != HWY_AVX3
+#endif  // HWY_TARGET > HWY_AVX3
 
 // ------------------------------ StoreInterleaved3 (CombineShiftRightBytes,
 // TableLookupBytes, ConcatUpperLower)
@@ -2821,7 +3093,7 @@ HWY_API void StoreInterleaved3(const Vec256<uint8_t> v0,
       0x80, 2, 0x80, 0x80, 3, 0x80, 0x80, 4, 0x80, 0x80};
   const auto shuf_r0 = LoadDup128(d, tbl_r0);
   const auto shuf_g0 = LoadDup128(d, tbl_g0);  // cannot reuse r0 due to 5
-  const auto shuf_b0 = CombineShiftRightBytes<15>(shuf_g0, shuf_g0);
+  const auto shuf_b0 = CombineShiftRightBytes<15>(d, shuf_g0, shuf_g0);
   const auto r0 = TableLookupBytes(v0, shuf_r0);  // 5..4..3..2..1..0
   const auto g0 = TableLookupBytes(v1, shuf_g0);  // ..4..3..2..1..0.
   const auto b0 = TableLookupBytes(v2, shuf_b0);  // .4..3..2..1..0..
@@ -2840,7 +3112,7 @@ HWY_API void StoreInterleaved3(const Vec256<uint8_t> v0,
   // upper halves. We could obtain 10_05 and 15_0A via ConcatUpperLower, but
   // that would require two ununaligned stores. For the lower halves, we can
   // merge two 128-bit stores for the same swizzling cost:
-  const auto out0 = ConcatLowerLower(interleaved_15_05, interleaved_10_00);
+  const auto out0 = ConcatLowerLower(d, interleaved_15_05, interleaved_10_00);
   StoreU(out0, d, unaligned + 0 * 32);
 
   // Third vector: bgr[15:11], b10
@@ -2852,10 +3124,10 @@ HWY_API void StoreInterleaved3(const Vec256<uint8_t> v0,
   const auto b2 = TableLookupBytes(v2, shuf_b2);
   const auto interleaved_1A_0A = r2 | g2 | b2;
 
-  const auto out1 = ConcatUpperLower(interleaved_10_00, interleaved_1A_0A);
+  const auto out1 = ConcatUpperLower(d, interleaved_10_00, interleaved_1A_0A);
   StoreU(out1, d, unaligned + 1 * 32);
 
-  const auto out2 = ConcatUpperUpper(interleaved_1A_0A, interleaved_15_05);
+  const auto out2 = ConcatUpperUpper(d, interleaved_1A_0A, interleaved_15_05);
   StoreU(out2, d, unaligned + 2 * 32);
 }
 
@@ -2864,27 +3136,29 @@ HWY_API void StoreInterleaved3(const Vec256<uint8_t> v0,
 HWY_API void StoreInterleaved4(const Vec256<uint8_t> v0,
                                const Vec256<uint8_t> v1,
                                const Vec256<uint8_t> v2,
-                               const Vec256<uint8_t> v3, Full256<uint8_t> d,
+                               const Vec256<uint8_t> v3, Full256<uint8_t> d8,
                                uint8_t* HWY_RESTRICT unaligned) {
+  const RepartitionToWide<decltype(d8)> d16;
+  const RepartitionToWide<decltype(d16)> d32;
   // let a,b,c,d denote v0..3.
-  const auto ba0 = ZipLower(v0, v1);  // b7 a7 .. b0 a0
-  const auto dc0 = ZipLower(v2, v3);  // d7 c7 .. d0 c0
-  const auto ba8 = ZipUpper(v0, v1);
-  const auto dc8 = ZipUpper(v2, v3);
-  const auto dcba_0 = ZipLower(ba0, dc0);  // d..a13 d..a10 | d..a03 d..a00
-  const auto dcba_4 = ZipUpper(ba0, dc0);  // d..a17 d..a14 | d..a07 d..a04
-  const auto dcba_8 = ZipLower(ba8, dc8);  // d..a1B d..a18 | d..a0B d..a08
-  const auto dcba_C = ZipUpper(ba8, dc8);  // d..a1F d..a1C | d..a0F d..a0C
+  const auto ba0 = ZipLower(d16, v0, v1);  // b7 a7 .. b0 a0
+  const auto dc0 = ZipLower(d16, v2, v3);  // d7 c7 .. d0 c0
+  const auto ba8 = ZipUpper(d16, v0, v1);
+  const auto dc8 = ZipUpper(d16, v2, v3);
+  const auto dcba_0 = ZipLower(d32, ba0, dc0);  // d..a13 d..a10 | d..a03 d..a00
+  const auto dcba_4 = ZipUpper(d32, ba0, dc0);  // d..a17 d..a14 | d..a07 d..a04
+  const auto dcba_8 = ZipLower(d32, ba8, dc8);  // d..a1B d..a18 | d..a0B d..a08
+  const auto dcba_C = ZipUpper(d32, ba8, dc8);  // d..a1F d..a1C | d..a0F d..a0C
   // Write lower halves, then upper. vperm2i128 is slow on Zen1 but we can
   // efficiently combine two lower halves into 256 bits:
-  const auto out0 = BitCast(d, ConcatLowerLower(dcba_4, dcba_0));
-  const auto out1 = BitCast(d, ConcatLowerLower(dcba_C, dcba_8));
-  StoreU(out0, d, unaligned + 0 * 32);
-  StoreU(out1, d, unaligned + 1 * 32);
-  const auto out2 = BitCast(d, ConcatUpperUpper(dcba_4, dcba_0));
-  const auto out3 = BitCast(d, ConcatUpperUpper(dcba_C, dcba_8));
-  StoreU(out2, d, unaligned + 2 * 32);
-  StoreU(out3, d, unaligned + 3 * 32);
+  const auto out0 = BitCast(d8, ConcatLowerLower(d32, dcba_4, dcba_0));
+  const auto out1 = BitCast(d8, ConcatLowerLower(d32, dcba_C, dcba_8));
+  StoreU(out0, d8, unaligned + 0 * 32);
+  StoreU(out1, d8, unaligned + 1 * 32);
+  const auto out2 = BitCast(d8, ConcatUpperUpper(d32, dcba_4, dcba_0));
+  const auto out3 = BitCast(d8, ConcatUpperUpper(d32, dcba_C, dcba_8));
+  StoreU(out2, d8, unaligned + 2 * 32);
+  StoreU(out3, d8, unaligned + 3 * 32);
 }
 
 // ------------------------------ Reductions
@@ -2894,21 +3168,24 @@ namespace detail {
 // Returns sum{lane[i]} in each lane. "v3210" is a replicated 128-bit block.
 // Same logic as x86/128.h, but with Vec256 arguments.
 template <typename T>
-HWY_API Vec256<T> SumOfLanes(hwy::SizeTag<4> /* tag */, const Vec256<T> v3210) {
+HWY_INLINE Vec256<T> SumOfLanes(hwy::SizeTag<4> /* tag */,
+                                const Vec256<T> v3210) {
   const auto v1032 = Shuffle1032(v3210);
   const auto v31_20_31_20 = v3210 + v1032;
   const auto v20_31_20_31 = Shuffle0321(v31_20_31_20);
   return v20_31_20_31 + v31_20_31_20;
 }
 template <typename T>
-HWY_API Vec256<T> MinOfLanes(hwy::SizeTag<4> /* tag */, const Vec256<T> v3210) {
+HWY_INLINE Vec256<T> MinOfLanes(hwy::SizeTag<4> /* tag */,
+                                const Vec256<T> v3210) {
   const auto v1032 = Shuffle1032(v3210);
   const auto v31_20_31_20 = Min(v3210, v1032);
   const auto v20_31_20_31 = Shuffle0321(v31_20_31_20);
   return Min(v20_31_20_31, v31_20_31_20);
 }
 template <typename T>
-HWY_API Vec256<T> MaxOfLanes(hwy::SizeTag<4> /* tag */, const Vec256<T> v3210) {
+HWY_INLINE Vec256<T> MaxOfLanes(hwy::SizeTag<4> /* tag */,
+                                const Vec256<T> v3210) {
   const auto v1032 = Shuffle1032(v3210);
   const auto v31_20_31_20 = Max(v3210, v1032);
   const auto v20_31_20_31 = Shuffle0321(v31_20_31_20);
@@ -2916,17 +3193,20 @@ HWY_API Vec256<T> MaxOfLanes(hwy::SizeTag<4> /* tag */, const Vec256<T> v3210) {
 }
 
 template <typename T>
-HWY_API Vec256<T> SumOfLanes(hwy::SizeTag<8> /* tag */, const Vec256<T> v10) {
+HWY_INLINE Vec256<T> SumOfLanes(hwy::SizeTag<8> /* tag */,
+                                const Vec256<T> v10) {
   const auto v01 = Shuffle01(v10);
   return v10 + v01;
 }
 template <typename T>
-HWY_API Vec256<T> MinOfLanes(hwy::SizeTag<8> /* tag */, const Vec256<T> v10) {
+HWY_INLINE Vec256<T> MinOfLanes(hwy::SizeTag<8> /* tag */,
+                                const Vec256<T> v10) {
   const auto v01 = Shuffle01(v10);
   return Min(v10, v01);
 }
 template <typename T>
-HWY_API Vec256<T> MaxOfLanes(hwy::SizeTag<8> /* tag */, const Vec256<T> v10) {
+HWY_INLINE Vec256<T> MaxOfLanes(hwy::SizeTag<8> /* tag */,
+                                const Vec256<T> v10) {
   const auto v01 = Shuffle01(v10);
   return Max(v10, v01);
 }
@@ -2935,21 +3215,116 @@ HWY_API Vec256<T> MaxOfLanes(hwy::SizeTag<8> /* tag */, const Vec256<T> v10) {
 
 // Supported for {uif}32x8, {uif}64x4. Returns the sum in each lane.
 template <typename T>
-HWY_API Vec256<T> SumOfLanes(const Vec256<T> vHL) {
-  const Vec256<T> vLH = ConcatLowerUpper(vHL, vHL);
+HWY_API Vec256<T> SumOfLanes(Full256<T> d, const Vec256<T> vHL) {
+  const Vec256<T> vLH = ConcatLowerUpper(d, vHL, vHL);
   return detail::SumOfLanes(hwy::SizeTag<sizeof(T)>(), vLH + vHL);
 }
 template <typename T>
-HWY_API Vec256<T> MinOfLanes(const Vec256<T> vHL) {
-  const Vec256<T> vLH = ConcatLowerUpper(vHL, vHL);
+HWY_API Vec256<T> MinOfLanes(Full256<T> d, const Vec256<T> vHL) {
+  const Vec256<T> vLH = ConcatLowerUpper(d, vHL, vHL);
   return detail::MinOfLanes(hwy::SizeTag<sizeof(T)>(), Min(vLH, vHL));
 }
 template <typename T>
-HWY_API Vec256<T> MaxOfLanes(const Vec256<T> vHL) {
-  const Vec256<T> vLH = ConcatLowerUpper(vHL, vHL);
+HWY_API Vec256<T> MaxOfLanes(Full256<T> d, const Vec256<T> vHL) {
+  const Vec256<T> vLH = ConcatLowerUpper(d, vHL, vHL);
   return detail::MaxOfLanes(hwy::SizeTag<sizeof(T)>(), Max(vLH, vHL));
 }
 
+// ================================================== DEPRECATED
+
+template <typename T>
+HWY_API size_t StoreMaskBits(const Mask256<T> mask, uint8_t* p) {
+  return StoreMaskBits(Full256<T>(), mask, p);
+}
+
+template <typename T>
+HWY_API bool AllTrue(const Mask256<T> mask) {
+  return AllTrue(Full256<T>(), mask);
+}
+
+template <typename T>
+HWY_API bool AllFalse(const Mask256<T> mask) {
+  return AllFalse(Full256<T>(), mask);
+}
+
+template <typename T>
+HWY_API size_t CountTrue(const Mask256<T> mask) {
+  return CountTrue(Full256<T>(), mask);
+}
+
+template <typename T>
+HWY_API Vec256<T> SumOfLanes(const Vec256<T> vHL) {
+  return SumOfLanes(Full256<T>(), vHL);
+}
+template <typename T>
+HWY_API Vec256<T> MinOfLanes(const Vec256<T> vHL) {
+  return MinOfLanes(Full256<T>(), vHL);
+}
+template <typename T>
+HWY_API Vec256<T> MaxOfLanes(const Vec256<T> vHL) {
+  return MaxOfLanes(Full256<T>(), vHL);
+}
+
+template <typename T>
+HWY_API Vec128<T> UpperHalf(Vec256<T> v) {
+  return UpperHalf(Full128<T>(), v);
+}
+
+template <int kBytes, typename T>
+HWY_API Vec256<T> ShiftRightBytes(const Vec256<T> v) {
+  return ShiftRightBytes<kBytes>(Full256<T>(), v);
+}
+
+template <int kLanes, typename T>
+HWY_API Vec256<T> ShiftRightLanes(const Vec256<T> v) {
+  return ShiftRightLanes<kLanes>(Full256<T>(), v);
+}
+
+template <size_t kBytes, typename T>
+HWY_API Vec256<T> CombineShiftRightBytes(Vec256<T> hi, Vec256<T> lo) {
+  return CombineShiftRightBytes<kBytes>(Full256<T>(), hi, lo);
+}
+
+template <typename T>
+HWY_API Vec256<T> InterleaveUpper(Vec256<T> a, Vec256<T> b) {
+  return InterleaveUpper(Full256<T>(), a, b);
+}
+
+template <typename T>
+HWY_API Vec256<MakeWide<T>> ZipUpper(Vec256<T> a, Vec256<T> b) {
+  return InterleaveUpper(Full256<MakeWide<T>>(), a, b);
+}
+
+template <typename T>
+HWY_API Vec256<T> Combine(Vec128<T> hi, Vec128<T> lo) {
+  return Combine(Full256<T>(), hi, lo);
+}
+
+template <typename T>
+HWY_API Vec256<T> ZeroExtendVector(Vec128<T> lo) {
+  return ZeroExtendVector(Full256<T>(), lo);
+}
+
+template <typename T>
+HWY_API Vec256<T> ConcatLowerLower(Vec256<T> hi, Vec256<T> lo) {
+  return ConcatLowerLower(Full256<T>(), hi, lo);
+}
+
+template <typename T>
+HWY_API Vec256<T> ConcatLowerUpper(Vec256<T> hi, Vec256<T> lo) {
+  return ConcatLowerUpper(Full256<T>(), hi, lo);
+}
+
+template <typename T>
+HWY_API Vec256<T> ConcatUpperLower(Vec256<T> hi, Vec256<T> lo) {
+  return ConcatUpperLower(Full256<T>(), hi, lo);
+}
+
+template <typename T>
+HWY_API Vec256<T> ConcatUpperUpper(Vec256<T> hi, Vec256<T> lo) {
+  return ConcatUpperUpper(Full256<T>(), hi, lo);
+}
+
 // NOLINTNEXTLINE(google-readability-namespace-comments)
 }  // namespace HWY_NAMESPACE
 }  // namespace hwy
diff --git a/third_party/highway/hwy/ops/x86_512-inl.h b/third_party/highway/hwy/ops/x86_512-inl.h
index fe34146fa972f..3d5792f307e20 100644
--- a/third_party/highway/hwy/ops/x86_512-inl.h
+++ b/third_party/highway/hwy/ops/x86_512-inl.h
@@ -23,17 +23,25 @@
 // Including <immintrin.h> should be enough, but Clang's headers helpfully skip
 // including these headers when _MSC_VER is defined, like when using clang-cl.
 // Include these directly here.
+// clang-format off
 #include <smmintrin.h>
+
 #include <avxintrin.h>
 #include <avx2intrin.h>
 #include <f16cintrin.h>
 #include <fmaintrin.h>
+
 #include <avx512fintrin.h>
 #include <avx512vlintrin.h>
 #include <avx512bwintrin.h>
 #include <avx512dqintrin.h>
 #include <avx512vlbwintrin.h>
 #include <avx512vldqintrin.h>
+#include <avx512bitalgintrin.h>
+#include <avx512vlbitalgintrin.h>
+#include <avx512vpopcntdqintrin.h>
+#include <avx512vpopcntdqvlintrin.h>
+// clang-format on
 #endif
 
 #include <stddef.h>
@@ -46,6 +54,11 @@ HWY_BEFORE_NAMESPACE();
 namespace hwy {
 namespace HWY_NAMESPACE {
 
+template <typename T>
+using Full512 = Simd<T, 64 / sizeof(T)>;
+
+namespace detail {
+
 template <typename T>
 struct Raw512 {
   using type = __m512i;
@@ -59,12 +72,31 @@ struct Raw512<double> {
   using type = __m512d;
 };
 
-template <typename T>
-using Full512 = Simd<T, 64 / sizeof(T)>;
+// Template arg: sizeof(lane type)
+template <size_t size>
+struct RawMask512 {};
+template <>
+struct RawMask512<1> {
+  using type = __mmask64;
+};
+template <>
+struct RawMask512<2> {
+  using type = __mmask32;
+};
+template <>
+struct RawMask512<4> {
+  using type = __mmask16;
+};
+template <>
+struct RawMask512<8> {
+  using type = __mmask8;
+};
+
+}  // namespace detail
 
 template <typename T>
 class Vec512 {
-  using Raw = typename Raw512<T>::type;
+  using Raw = typename detail::Raw512<T>::type;
 
  public:
   // Compound assignment. Only usable if there is a corresponding non-member
@@ -94,44 +126,24 @@ class Vec512 {
   Raw raw;
 };
 
-// Template arg: sizeof(lane type)
-template <size_t size>
-struct RawMask512 {};
-template <>
-struct RawMask512<1> {
-  using type = __mmask64;
-};
-template <>
-struct RawMask512<2> {
-  using type = __mmask32;
-};
-template <>
-struct RawMask512<4> {
-  using type = __mmask16;
-};
-template <>
-struct RawMask512<8> {
-  using type = __mmask8;
-};
-
 // Mask register: one bit per lane.
 template <typename T>
-class Mask512 {
- public:
-  using Raw = typename RawMask512<sizeof(T)>::type;
-  Raw raw;
+struct Mask512 {
+  typename detail::RawMask512<sizeof(T)>::type raw;
 };
 
 // ------------------------------ BitCast
 
 namespace detail {
 
-HWY_API __m512i BitCastToInteger(__m512i v) { return v; }
-HWY_API __m512i BitCastToInteger(__m512 v) { return _mm512_castps_si512(v); }
-HWY_API __m512i BitCastToInteger(__m512d v) { return _mm512_castpd_si512(v); }
+HWY_INLINE __m512i BitCastToInteger(__m512i v) { return v; }
+HWY_INLINE __m512i BitCastToInteger(__m512 v) { return _mm512_castps_si512(v); }
+HWY_INLINE __m512i BitCastToInteger(__m512d v) {
+  return _mm512_castpd_si512(v);
+}
 
 template <typename T>
-HWY_API Vec512<uint8_t> BitCastToByte(Vec512<T> v) {
+HWY_INLINE Vec512<uint8_t> BitCastToByte(Vec512<T> v) {
   return Vec512<uint8_t>{BitCastToInteger(v.raw)};
 }
 
@@ -150,7 +162,7 @@ struct BitCastFromInteger512<double> {
 };
 
 template <typename T>
-HWY_API Vec512<T> BitCastFromByte(Full512<T> /* tag */, Vec512<uint8_t> v) {
+HWY_INLINE Vec512<T> BitCastFromByte(Full512<T> /* tag */, Vec512<uint8_t> v) {
   return Vec512<T>{BitCastFromInteger512<T>()(v.raw)};
 }
 
@@ -315,6 +327,47 @@ HWY_API Vec512<T> operator^(const Vec512<T> a, const Vec512<T> b) {
   return Xor(a, b);
 }
 
+// ------------------------------ PopulationCount
+
+// 8/16 require BITALG, 32/64 require VPOPCNTDQ.
+#if HWY_TARGET == HWY_AVX3_DL
+
+#ifdef HWY_NATIVE_POPCNT
+#undef HWY_NATIVE_POPCNT
+#else
+#define HWY_NATIVE_POPCNT
+#endif
+
+namespace detail {
+
+template <typename T>
+HWY_INLINE Vec512<T> PopulationCount(hwy::SizeTag<1> /* tag */, Vec512<T> v) {
+  return Vec512<T>{_mm512_popcnt_epi8(v.raw)};
+}
+template <typename T>
+HWY_INLINE Vec512<T> PopulationCount(hwy::SizeTag<2> /* tag */, Vec512<T> v) {
+  return Vec512<T>{_mm512_popcnt_epi16(v.raw)};
+}
+template <typename T>
+HWY_INLINE Vec512<T> PopulationCount(hwy::SizeTag<4> /* tag */, Vec512<T> v) {
+  return Vec512<T>{_mm512_popcnt_epi32(v.raw)};
+}
+template <typename T>
+HWY_INLINE Vec512<T> PopulationCount(hwy::SizeTag<8> /* tag */, Vec512<T> v) {
+  return Vec512<T>{_mm512_popcnt_epi64(v.raw)};
+}
+
+}  // namespace detail
+
+template <typename T>
+HWY_API Vec512<T> PopulationCount(Vec512<T> v) {
+  return detail::PopulationCount(hwy::SizeTag<sizeof(T)>(), v);
+}
+
+#endif  // HWY_TARGET == HWY_AVX3_DL
+
+// ================================================== SIGN
+
 // ------------------------------ CopySign
 
 template <typename T>
@@ -346,6 +399,8 @@ HWY_API Vec512<T> CopySignToAbs(const Vec512<T> abs, const Vec512<T> sign) {
   return CopySign(abs, sign);
 }
 
+// ================================================== MASK
+
 // ------------------------------ FirstN
 
 // Possibilities for constructing a bitmask of N ones:
@@ -360,13 +415,14 @@ namespace detail {
 
 // 32 bit mask is sufficient for lane size >= 2.
 template <typename T, HWY_IF_NOT_LANE_SIZE(T, 1)>
-HWY_API Mask512<T> FirstN(size_t n) {
-  using Bits = typename Mask512<T>::Raw;
-  return Mask512<T>{static_cast<Bits>(_bzhi_u32(~uint32_t(0), n))};
+HWY_INLINE Mask512<T> FirstN(size_t n) {
+  Mask512<T> m;
+  m.raw = static_cast<decltype(m.raw)>(_bzhi_u32(~uint32_t(0), n));
+  return m;
 }
 
 template <typename T, HWY_IF_LANE_SIZE(T, 1)>
-HWY_API Mask512<T> FirstN(size_t n) {
+HWY_INLINE Mask512<T> FirstN(size_t n) {
   const uint64_t bits = n < 64 ? ((1ULL << n) - 1) : ~uint64_t(0);
   return Mask512<T>{static_cast<__mmask64>(bits)};
 }
@@ -377,8 +433,9 @@ HWY_API Mask512<T> FirstN(size_t n) {
 template <typename T>
 HWY_API Mask512<T> FirstN(const Full512<T> /*tag*/, size_t n) {
 #if HWY_ARCH_X86_64
-  using Bits = typename Mask512<T>::Raw;
-  return Mask512<T>{static_cast<Bits>(_bzhi_u64(~uint64_t(0), n))};
+  Mask512<T> m;
+  m.raw = static_cast<decltype(m.raw)>(_bzhi_u64(~uint64_t(0), n));
+  return m;
 #else
   return detail::FirstN<T>(n);
 #endif  // HWY_ARCH_X86_64
@@ -392,23 +449,27 @@ namespace detail {
 
 // Templates for signed/unsigned integer of a particular size.
 template <typename T>
-HWY_API Vec512<T> IfThenElse(hwy::SizeTag<1> /* tag */, const Mask512<T> mask,
-                             const Vec512<T> yes, const Vec512<T> no) {
+HWY_INLINE Vec512<T> IfThenElse(hwy::SizeTag<1> /* tag */,
+                                const Mask512<T> mask, const Vec512<T> yes,
+                                const Vec512<T> no) {
   return Vec512<T>{_mm512_mask_mov_epi8(no.raw, mask.raw, yes.raw)};
 }
 template <typename T>
-HWY_API Vec512<T> IfThenElse(hwy::SizeTag<2> /* tag */, const Mask512<T> mask,
-                             const Vec512<T> yes, const Vec512<T> no) {
+HWY_INLINE Vec512<T> IfThenElse(hwy::SizeTag<2> /* tag */,
+                                const Mask512<T> mask, const Vec512<T> yes,
+                                const Vec512<T> no) {
   return Vec512<T>{_mm512_mask_mov_epi16(no.raw, mask.raw, yes.raw)};
 }
 template <typename T>
-HWY_API Vec512<T> IfThenElse(hwy::SizeTag<4> /* tag */, const Mask512<T> mask,
-                             const Vec512<T> yes, const Vec512<T> no) {
+HWY_INLINE Vec512<T> IfThenElse(hwy::SizeTag<4> /* tag */,
+                                const Mask512<T> mask, const Vec512<T> yes,
+                                const Vec512<T> no) {
   return Vec512<T>{_mm512_mask_mov_epi32(no.raw, mask.raw, yes.raw)};
 }
 template <typename T>
-HWY_API Vec512<T> IfThenElse(hwy::SizeTag<8> /* tag */, const Mask512<T> mask,
-                             const Vec512<T> yes, const Vec512<T> no) {
+HWY_INLINE Vec512<T> IfThenElse(hwy::SizeTag<8> /* tag */,
+                                const Mask512<T> mask, const Vec512<T> yes,
+                                const Vec512<T> no) {
   return Vec512<T>{_mm512_mask_mov_epi64(no.raw, mask.raw, yes.raw)};
 }
 
@@ -419,39 +480,41 @@ HWY_API Vec512<T> IfThenElse(const Mask512<T> mask, const Vec512<T> yes,
                              const Vec512<T> no) {
   return detail::IfThenElse(hwy::SizeTag<sizeof(T)>(), mask, yes, no);
 }
-template <>
-HWY_INLINE Vec512<float> IfThenElse(const Mask512<float> mask,
-                                    const Vec512<float> yes,
-                                    const Vec512<float> no) {
+HWY_API Vec512<float> IfThenElse(const Mask512<float> mask,
+                                 const Vec512<float> yes,
+                                 const Vec512<float> no) {
   return Vec512<float>{_mm512_mask_mov_ps(no.raw, mask.raw, yes.raw)};
 }
-template <>
-HWY_INLINE Vec512<double> IfThenElse(const Mask512<double> mask,
-                                     const Vec512<double> yes,
-                                     const Vec512<double> no) {
+HWY_API Vec512<double> IfThenElse(const Mask512<double> mask,
+                                  const Vec512<double> yes,
+                                  const Vec512<double> no) {
   return Vec512<double>{_mm512_mask_mov_pd(no.raw, mask.raw, yes.raw)};
 }
 
 namespace detail {
 
 template <typename T>
-HWY_API Vec512<T> IfThenElseZero(hwy::SizeTag<1> /* tag */,
-                                 const Mask512<T> mask, const Vec512<T> yes) {
+HWY_INLINE Vec512<T> IfThenElseZero(hwy::SizeTag<1> /* tag */,
+                                    const Mask512<T> mask,
+                                    const Vec512<T> yes) {
   return Vec512<T>{_mm512_maskz_mov_epi8(mask.raw, yes.raw)};
 }
 template <typename T>
-HWY_API Vec512<T> IfThenElseZero(hwy::SizeTag<2> /* tag */,
-                                 const Mask512<T> mask, const Vec512<T> yes) {
+HWY_INLINE Vec512<T> IfThenElseZero(hwy::SizeTag<2> /* tag */,
+                                    const Mask512<T> mask,
+                                    const Vec512<T> yes) {
   return Vec512<T>{_mm512_maskz_mov_epi16(mask.raw, yes.raw)};
 }
 template <typename T>
-HWY_API Vec512<T> IfThenElseZero(hwy::SizeTag<4> /* tag */,
-                                 const Mask512<T> mask, const Vec512<T> yes) {
+HWY_INLINE Vec512<T> IfThenElseZero(hwy::SizeTag<4> /* tag */,
+                                    const Mask512<T> mask,
+                                    const Vec512<T> yes) {
   return Vec512<T>{_mm512_maskz_mov_epi32(mask.raw, yes.raw)};
 }
 template <typename T>
-HWY_API Vec512<T> IfThenElseZero(hwy::SizeTag<8> /* tag */,
-                                 const Mask512<T> mask, const Vec512<T> yes) {
+HWY_INLINE Vec512<T> IfThenElseZero(hwy::SizeTag<8> /* tag */,
+                                    const Mask512<T> mask,
+                                    const Vec512<T> yes) {
   return Vec512<T>{_mm512_maskz_mov_epi64(mask.raw, yes.raw)};
 }
 
@@ -461,38 +524,36 @@ template <typename T>
 HWY_API Vec512<T> IfThenElseZero(const Mask512<T> mask, const Vec512<T> yes) {
   return detail::IfThenElseZero(hwy::SizeTag<sizeof(T)>(), mask, yes);
 }
-template <>
-HWY_INLINE Vec512<float> IfThenElseZero(const Mask512<float> mask,
-                                        const Vec512<float> yes) {
+HWY_API Vec512<float> IfThenElseZero(const Mask512<float> mask,
+                                     const Vec512<float> yes) {
   return Vec512<float>{_mm512_maskz_mov_ps(mask.raw, yes.raw)};
 }
-template <>
-HWY_INLINE Vec512<double> IfThenElseZero(const Mask512<double> mask,
-                                         const Vec512<double> yes) {
+HWY_API Vec512<double> IfThenElseZero(const Mask512<double> mask,
+                                      const Vec512<double> yes) {
   return Vec512<double>{_mm512_maskz_mov_pd(mask.raw, yes.raw)};
 }
 
 namespace detail {
 
 template <typename T>
-HWY_API Vec512<T> IfThenZeroElse(hwy::SizeTag<1> /* tag */,
-                                 const Mask512<T> mask, const Vec512<T> no) {
+HWY_INLINE Vec512<T> IfThenZeroElse(hwy::SizeTag<1> /* tag */,
+                                    const Mask512<T> mask, const Vec512<T> no) {
   // xor_epi8/16 are missing, but we have sub, which is just as fast for u8/16.
   return Vec512<T>{_mm512_mask_sub_epi8(no.raw, mask.raw, no.raw, no.raw)};
 }
 template <typename T>
-HWY_API Vec512<T> IfThenZeroElse(hwy::SizeTag<2> /* tag */,
-                                 const Mask512<T> mask, const Vec512<T> no) {
+HWY_INLINE Vec512<T> IfThenZeroElse(hwy::SizeTag<2> /* tag */,
+                                    const Mask512<T> mask, const Vec512<T> no) {
   return Vec512<T>{_mm512_mask_sub_epi16(no.raw, mask.raw, no.raw, no.raw)};
 }
 template <typename T>
-HWY_API Vec512<T> IfThenZeroElse(hwy::SizeTag<4> /* tag */,
-                                 const Mask512<T> mask, const Vec512<T> no) {
+HWY_INLINE Vec512<T> IfThenZeroElse(hwy::SizeTag<4> /* tag */,
+                                    const Mask512<T> mask, const Vec512<T> no) {
   return Vec512<T>{_mm512_mask_xor_epi32(no.raw, mask.raw, no.raw, no.raw)};
 }
 template <typename T>
-HWY_API Vec512<T> IfThenZeroElse(hwy::SizeTag<8> /* tag */,
-                                 const Mask512<T> mask, const Vec512<T> no) {
+HWY_INLINE Vec512<T> IfThenZeroElse(hwy::SizeTag<8> /* tag */,
+                                    const Mask512<T> mask, const Vec512<T> no) {
   return Vec512<T>{_mm512_mask_xor_epi64(no.raw, mask.raw, no.raw, no.raw)};
 }
 
@@ -502,14 +563,12 @@ template <typename T>
 HWY_API Vec512<T> IfThenZeroElse(const Mask512<T> mask, const Vec512<T> no) {
   return detail::IfThenZeroElse(hwy::SizeTag<sizeof(T)>(), mask, no);
 }
-template <>
-HWY_INLINE Vec512<float> IfThenZeroElse(const Mask512<float> mask,
-                                        const Vec512<float> no) {
+HWY_API Vec512<float> IfThenZeroElse(const Mask512<float> mask,
+                                     const Vec512<float> no) {
   return Vec512<float>{_mm512_mask_xor_ps(no.raw, mask.raw, no.raw, no.raw)};
 }
-template <>
-HWY_INLINE Vec512<double> IfThenZeroElse(const Mask512<double> mask,
-                                         const Vec512<double> no) {
+HWY_API Vec512<double> IfThenZeroElse(const Mask512<double> mask,
+                                      const Vec512<double> no) {
   return Vec512<double>{_mm512_mask_xor_pd(no.raw, mask.raw, no.raw, no.raw)};
 }
 
@@ -677,7 +736,7 @@ HWY_API Vec512<uint16_t> AverageRound(const Vec512<uint16_t> a,
   return Vec512<uint16_t>{_mm512_avg_epu16(a.raw, b.raw)};
 }
 
-// ------------------------------ Absolute value
+// ------------------------------ Abs (Sub)
 
 // Returns absolute value, except that LimitsMin() maps to LimitsMax() + 1.
 HWY_API Vec512<int8_t> Abs(const Vec512<int8_t> v) {
@@ -706,7 +765,6 @@ HWY_API Vec512<float> Abs(const Vec512<float> v) {
 HWY_API Vec512<double> Abs(const Vec512<double> v) {
   return Vec512<double>{_mm512_abs_pd(v.raw)};
 }
-
 // ------------------------------ ShiftLeft
 
 template <int kBits>
@@ -1059,7 +1117,7 @@ HWY_API Vec512<uint64_t> MulEven(const Vec512<uint32_t> a,
   return Vec512<uint64_t>{_mm512_mul_epu32(a.raw, b.raw)};
 }
 
-// ------------------------------ Negate
+// ------------------------------ Neg (Sub)
 
 template <typename T, HWY_IF_FLOAT(T)>
 HWY_API Vec512<T> Neg(const Vec512<T> v) {
@@ -1219,23 +1277,23 @@ HWY_API Mask512<TTo> RebindMask(Full512<TTo> /*tag*/, Mask512<TFrom> m) {
 namespace detail {
 
 template <typename T>
-HWY_API Mask512<T> TestBit(hwy::SizeTag<1> /*tag*/, const Vec512<T> v,
-                           const Vec512<T> bit) {
+HWY_INLINE Mask512<T> TestBit(hwy::SizeTag<1> /*tag*/, const Vec512<T> v,
+                              const Vec512<T> bit) {
   return Mask512<T>{_mm512_test_epi8_mask(v.raw, bit.raw)};
 }
 template <typename T>
-HWY_API Mask512<T> TestBit(hwy::SizeTag<2> /*tag*/, const Vec512<T> v,
-                           const Vec512<T> bit) {
+HWY_INLINE Mask512<T> TestBit(hwy::SizeTag<2> /*tag*/, const Vec512<T> v,
+                              const Vec512<T> bit) {
   return Mask512<T>{_mm512_test_epi16_mask(v.raw, bit.raw)};
 }
 template <typename T>
-HWY_API Mask512<T> TestBit(hwy::SizeTag<4> /*tag*/, const Vec512<T> v,
-                           const Vec512<T> bit) {
+HWY_INLINE Mask512<T> TestBit(hwy::SizeTag<4> /*tag*/, const Vec512<T> v,
+                              const Vec512<T> bit) {
   return Mask512<T>{_mm512_test_epi32_mask(v.raw, bit.raw)};
 }
 template <typename T>
-HWY_API Mask512<T> TestBit(hwy::SizeTag<8> /*tag*/, const Vec512<T> v,
-                           const Vec512<T> bit) {
+HWY_INLINE Mask512<T> TestBit(hwy::SizeTag<8> /*tag*/, const Vec512<T> v,
+                              const Vec512<T> bit) {
   return Mask512<T>{_mm512_test_epi64_mask(v.raw, bit.raw)};
 }
 
@@ -1295,6 +1353,54 @@ HWY_API Mask512<double> operator==(const Vec512<double> a,
   return Mask512<double>{_mm512_cmp_pd_mask(a.raw, b.raw, _CMP_EQ_OQ)};
 }
 
+// ------------------------------ Inequality
+
+// Unsigned
+HWY_API Mask512<uint8_t> operator!=(const Vec512<uint8_t> a,
+                                    const Vec512<uint8_t> b) {
+  return Mask512<uint8_t>{_mm512_cmpneq_epi8_mask(a.raw, b.raw)};
+}
+HWY_API Mask512<uint16_t> operator!=(const Vec512<uint16_t> a,
+                                     const Vec512<uint16_t> b) {
+  return Mask512<uint16_t>{_mm512_cmpneq_epi16_mask(a.raw, b.raw)};
+}
+HWY_API Mask512<uint32_t> operator!=(const Vec512<uint32_t> a,
+                                     const Vec512<uint32_t> b) {
+  return Mask512<uint32_t>{_mm512_cmpneq_epi32_mask(a.raw, b.raw)};
+}
+HWY_API Mask512<uint64_t> operator!=(const Vec512<uint64_t> a,
+                                     const Vec512<uint64_t> b) {
+  return Mask512<uint64_t>{_mm512_cmpneq_epi64_mask(a.raw, b.raw)};
+}
+
+// Signed
+HWY_API Mask512<int8_t> operator!=(const Vec512<int8_t> a,
+                                   const Vec512<int8_t> b) {
+  return Mask512<int8_t>{_mm512_cmpneq_epi8_mask(a.raw, b.raw)};
+}
+HWY_API Mask512<int16_t> operator!=(const Vec512<int16_t> a,
+                                    const Vec512<int16_t> b) {
+  return Mask512<int16_t>{_mm512_cmpneq_epi16_mask(a.raw, b.raw)};
+}
+HWY_API Mask512<int32_t> operator!=(const Vec512<int32_t> a,
+                                    const Vec512<int32_t> b) {
+  return Mask512<int32_t>{_mm512_cmpneq_epi32_mask(a.raw, b.raw)};
+}
+HWY_API Mask512<int64_t> operator!=(const Vec512<int64_t> a,
+                                    const Vec512<int64_t> b) {
+  return Mask512<int64_t>{_mm512_cmpneq_epi64_mask(a.raw, b.raw)};
+}
+
+// Float
+HWY_API Mask512<float> operator!=(const Vec512<float> a,
+                                  const Vec512<float> b) {
+  return Mask512<float>{_mm512_cmp_ps_mask(a.raw, b.raw, _CMP_NEQ_OQ)};
+}
+HWY_API Mask512<double> operator!=(const Vec512<double> a,
+                                   const Vec512<double> b) {
+  return Mask512<double>{_mm512_cmp_pd_mask(a.raw, b.raw, _CMP_NEQ_OQ)};
+}
+
 // ------------------------------ Strict inequality
 
 // Signed/float <
@@ -1372,19 +1478,19 @@ HWY_API Mask512<double> operator>=(const Vec512<double> a,
 namespace detail {
 
 template <typename T>
-HWY_API Mask512<T> MaskFromVec(hwy::SizeTag<1> /*tag*/, const Vec512<T> v) {
+HWY_INLINE Mask512<T> MaskFromVec(hwy::SizeTag<1> /*tag*/, const Vec512<T> v) {
   return Mask512<T>{_mm512_movepi8_mask(v.raw)};
 }
 template <typename T>
-HWY_API Mask512<T> MaskFromVec(hwy::SizeTag<2> /*tag*/, const Vec512<T> v) {
+HWY_INLINE Mask512<T> MaskFromVec(hwy::SizeTag<2> /*tag*/, const Vec512<T> v) {
   return Mask512<T>{_mm512_movepi16_mask(v.raw)};
 }
 template <typename T>
-HWY_API Mask512<T> MaskFromVec(hwy::SizeTag<4> /*tag*/, const Vec512<T> v) {
+HWY_INLINE Mask512<T> MaskFromVec(hwy::SizeTag<4> /*tag*/, const Vec512<T> v) {
   return Mask512<T>{_mm512_movepi32_mask(v.raw)};
 }
 template <typename T>
-HWY_API Mask512<T> MaskFromVec(hwy::SizeTag<8> /*tag*/, const Vec512<T> v) {
+HWY_INLINE Mask512<T> MaskFromVec(hwy::SizeTag<8> /*tag*/, const Vec512<T> v) {
   return Mask512<T>{_mm512_movepi64_mask(v.raw)};
 }
 
@@ -1455,7 +1561,7 @@ HWY_API Vec512<T> VecFromMask(Full512<T> /* tag */, const Mask512<T> v) {
 namespace detail {
 
 template <typename T>
-HWY_API Mask512<T> Not(hwy::SizeTag<1> /*tag*/, const Mask512<T> m) {
+HWY_INLINE Mask512<T> Not(hwy::SizeTag<1> /*tag*/, const Mask512<T> m) {
 #if HWY_COMPILER_HAS_MASK_INTRINSICS
   return Mask512<T>{_knot_mask64(m.raw)};
 #else
@@ -1463,7 +1569,7 @@ HWY_API Mask512<T> Not(hwy::SizeTag<1> /*tag*/, const Mask512<T> m) {
 #endif
 }
 template <typename T>
-HWY_API Mask512<T> Not(hwy::SizeTag<2> /*tag*/, const Mask512<T> m) {
+HWY_INLINE Mask512<T> Not(hwy::SizeTag<2> /*tag*/, const Mask512<T> m) {
 #if HWY_COMPILER_HAS_MASK_INTRINSICS
   return Mask512<T>{_knot_mask32(m.raw)};
 #else
@@ -1471,7 +1577,7 @@ HWY_API Mask512<T> Not(hwy::SizeTag<2> /*tag*/, const Mask512<T> m) {
 #endif
 }
 template <typename T>
-HWY_API Mask512<T> Not(hwy::SizeTag<4> /*tag*/, const Mask512<T> m) {
+HWY_INLINE Mask512<T> Not(hwy::SizeTag<4> /*tag*/, const Mask512<T> m) {
 #if HWY_COMPILER_HAS_MASK_INTRINSICS
   return Mask512<T>{_knot_mask16(m.raw)};
 #else
@@ -1479,7 +1585,7 @@ HWY_API Mask512<T> Not(hwy::SizeTag<4> /*tag*/, const Mask512<T> m) {
 #endif
 }
 template <typename T>
-HWY_API Mask512<T> Not(hwy::SizeTag<8> /*tag*/, const Mask512<T> m) {
+HWY_INLINE Mask512<T> Not(hwy::SizeTag<8> /*tag*/, const Mask512<T> m) {
 #if HWY_COMPILER_HAS_MASK_INTRINSICS
   return Mask512<T>{_knot_mask8(m.raw)};
 #else
@@ -1488,8 +1594,8 @@ HWY_API Mask512<T> Not(hwy::SizeTag<8> /*tag*/, const Mask512<T> m) {
 }
 
 template <typename T>
-HWY_API Mask512<T> And(hwy::SizeTag<1> /*tag*/, const Mask512<T> a,
-                       const Mask512<T> b) {
+HWY_INLINE Mask512<T> And(hwy::SizeTag<1> /*tag*/, const Mask512<T> a,
+                          const Mask512<T> b) {
 #if HWY_COMPILER_HAS_MASK_INTRINSICS
   return Mask512<T>{_kand_mask64(a.raw, b.raw)};
 #else
@@ -1497,8 +1603,8 @@ HWY_API Mask512<T> And(hwy::SizeTag<1> /*tag*/, const Mask512<T> a,
 #endif
 }
 template <typename T>
-HWY_API Mask512<T> And(hwy::SizeTag<2> /*tag*/, const Mask512<T> a,
-                       const Mask512<T> b) {
+HWY_INLINE Mask512<T> And(hwy::SizeTag<2> /*tag*/, const Mask512<T> a,
+                          const Mask512<T> b) {
 #if HWY_COMPILER_HAS_MASK_INTRINSICS
   return Mask512<T>{_kand_mask32(a.raw, b.raw)};
 #else
@@ -1506,8 +1612,8 @@ HWY_API Mask512<T> And(hwy::SizeTag<2> /*tag*/, const Mask512<T> a,
 #endif
 }
 template <typename T>
-HWY_API Mask512<T> And(hwy::SizeTag<4> /*tag*/, const Mask512<T> a,
-                       const Mask512<T> b) {
+HWY_INLINE Mask512<T> And(hwy::SizeTag<4> /*tag*/, const Mask512<T> a,
+                          const Mask512<T> b) {
 #if HWY_COMPILER_HAS_MASK_INTRINSICS
   return Mask512<T>{_kand_mask16(a.raw, b.raw)};
 #else
@@ -1515,8 +1621,8 @@ HWY_API Mask512<T> And(hwy::SizeTag<4> /*tag*/, const Mask512<T> a,
 #endif
 }
 template <typename T>
-HWY_API Mask512<T> And(hwy::SizeTag<8> /*tag*/, const Mask512<T> a,
-                       const Mask512<T> b) {
+HWY_INLINE Mask512<T> And(hwy::SizeTag<8> /*tag*/, const Mask512<T> a,
+                          const Mask512<T> b) {
 #if HWY_COMPILER_HAS_MASK_INTRINSICS
   return Mask512<T>{_kand_mask8(a.raw, b.raw)};
 #else
@@ -1525,8 +1631,8 @@ HWY_API Mask512<T> And(hwy::SizeTag<8> /*tag*/, const Mask512<T> a,
 }
 
 template <typename T>
-HWY_API Mask512<T> AndNot(hwy::SizeTag<1> /*tag*/, const Mask512<T> a,
-                          const Mask512<T> b) {
+HWY_INLINE Mask512<T> AndNot(hwy::SizeTag<1> /*tag*/, const Mask512<T> a,
+                             const Mask512<T> b) {
 #if HWY_COMPILER_HAS_MASK_INTRINSICS
   return Mask512<T>{_kandn_mask64(a.raw, b.raw)};
 #else
@@ -1534,8 +1640,8 @@ HWY_API Mask512<T> AndNot(hwy::SizeTag<1> /*tag*/, const Mask512<T> a,
 #endif
 }
 template <typename T>
-HWY_API Mask512<T> AndNot(hwy::SizeTag<2> /*tag*/, const Mask512<T> a,
-                          const Mask512<T> b) {
+HWY_INLINE Mask512<T> AndNot(hwy::SizeTag<2> /*tag*/, const Mask512<T> a,
+                             const Mask512<T> b) {
 #if HWY_COMPILER_HAS_MASK_INTRINSICS
   return Mask512<T>{_kandn_mask32(a.raw, b.raw)};
 #else
@@ -1543,8 +1649,8 @@ HWY_API Mask512<T> AndNot(hwy::SizeTag<2> /*tag*/, const Mask512<T> a,
 #endif
 }
 template <typename T>
-HWY_API Mask512<T> AndNot(hwy::SizeTag<4> /*tag*/, const Mask512<T> a,
-                          const Mask512<T> b) {
+HWY_INLINE Mask512<T> AndNot(hwy::SizeTag<4> /*tag*/, const Mask512<T> a,
+                             const Mask512<T> b) {
 #if HWY_COMPILER_HAS_MASK_INTRINSICS
   return Mask512<T>{_kandn_mask16(a.raw, b.raw)};
 #else
@@ -1552,8 +1658,8 @@ HWY_API Mask512<T> AndNot(hwy::SizeTag<4> /*tag*/, const Mask512<T> a,
 #endif
 }
 template <typename T>
-HWY_API Mask512<T> AndNot(hwy::SizeTag<8> /*tag*/, const Mask512<T> a,
-                          const Mask512<T> b) {
+HWY_INLINE Mask512<T> AndNot(hwy::SizeTag<8> /*tag*/, const Mask512<T> a,
+                             const Mask512<T> b) {
 #if HWY_COMPILER_HAS_MASK_INTRINSICS
   return Mask512<T>{_kandn_mask8(a.raw, b.raw)};
 #else
@@ -1562,8 +1668,8 @@ HWY_API Mask512<T> AndNot(hwy::SizeTag<8> /*tag*/, const Mask512<T> a,
 }
 
 template <typename T>
-HWY_API Mask512<T> Or(hwy::SizeTag<1> /*tag*/, const Mask512<T> a,
-                      const Mask512<T> b) {
+HWY_INLINE Mask512<T> Or(hwy::SizeTag<1> /*tag*/, const Mask512<T> a,
+                         const Mask512<T> b) {
 #if HWY_COMPILER_HAS_MASK_INTRINSICS
   return Mask512<T>{_kor_mask64(a.raw, b.raw)};
 #else
@@ -1571,8 +1677,8 @@ HWY_API Mask512<T> Or(hwy::SizeTag<1> /*tag*/, const Mask512<T> a,
 #endif
 }
 template <typename T>
-HWY_API Mask512<T> Or(hwy::SizeTag<2> /*tag*/, const Mask512<T> a,
-                      const Mask512<T> b) {
+HWY_INLINE Mask512<T> Or(hwy::SizeTag<2> /*tag*/, const Mask512<T> a,
+                         const Mask512<T> b) {
 #if HWY_COMPILER_HAS_MASK_INTRINSICS
   return Mask512<T>{_kor_mask32(a.raw, b.raw)};
 #else
@@ -1580,8 +1686,8 @@ HWY_API Mask512<T> Or(hwy::SizeTag<2> /*tag*/, const Mask512<T> a,
 #endif
 }
 template <typename T>
-HWY_API Mask512<T> Or(hwy::SizeTag<4> /*tag*/, const Mask512<T> a,
-                      const Mask512<T> b) {
+HWY_INLINE Mask512<T> Or(hwy::SizeTag<4> /*tag*/, const Mask512<T> a,
+                         const Mask512<T> b) {
 #if HWY_COMPILER_HAS_MASK_INTRINSICS
   return Mask512<T>{_kor_mask16(a.raw, b.raw)};
 #else
@@ -1589,8 +1695,8 @@ HWY_API Mask512<T> Or(hwy::SizeTag<4> /*tag*/, const Mask512<T> a,
 #endif
 }
 template <typename T>
-HWY_API Mask512<T> Or(hwy::SizeTag<8> /*tag*/, const Mask512<T> a,
-                      const Mask512<T> b) {
+HWY_INLINE Mask512<T> Or(hwy::SizeTag<8> /*tag*/, const Mask512<T> a,
+                         const Mask512<T> b) {
 #if HWY_COMPILER_HAS_MASK_INTRINSICS
   return Mask512<T>{_kor_mask8(a.raw, b.raw)};
 #else
@@ -1599,8 +1705,8 @@ HWY_API Mask512<T> Or(hwy::SizeTag<8> /*tag*/, const Mask512<T> a,
 }
 
 template <typename T>
-HWY_API Mask512<T> Xor(hwy::SizeTag<1> /*tag*/, const Mask512<T> a,
-                       const Mask512<T> b) {
+HWY_INLINE Mask512<T> Xor(hwy::SizeTag<1> /*tag*/, const Mask512<T> a,
+                          const Mask512<T> b) {
 #if HWY_COMPILER_HAS_MASK_INTRINSICS
   return Mask512<T>{_kxor_mask64(a.raw, b.raw)};
 #else
@@ -1608,8 +1714,8 @@ HWY_API Mask512<T> Xor(hwy::SizeTag<1> /*tag*/, const Mask512<T> a,
 #endif
 }
 template <typename T>
-HWY_API Mask512<T> Xor(hwy::SizeTag<2> /*tag*/, const Mask512<T> a,
-                       const Mask512<T> b) {
+HWY_INLINE Mask512<T> Xor(hwy::SizeTag<2> /*tag*/, const Mask512<T> a,
+                          const Mask512<T> b) {
 #if HWY_COMPILER_HAS_MASK_INTRINSICS
   return Mask512<T>{_kxor_mask32(a.raw, b.raw)};
 #else
@@ -1617,8 +1723,8 @@ HWY_API Mask512<T> Xor(hwy::SizeTag<2> /*tag*/, const Mask512<T> a,
 #endif
 }
 template <typename T>
-HWY_API Mask512<T> Xor(hwy::SizeTag<4> /*tag*/, const Mask512<T> a,
-                       const Mask512<T> b) {
+HWY_INLINE Mask512<T> Xor(hwy::SizeTag<4> /*tag*/, const Mask512<T> a,
+                          const Mask512<T> b) {
 #if HWY_COMPILER_HAS_MASK_INTRINSICS
   return Mask512<T>{_kxor_mask16(a.raw, b.raw)};
 #else
@@ -1626,8 +1732,8 @@ HWY_API Mask512<T> Xor(hwy::SizeTag<4> /*tag*/, const Mask512<T> a,
 #endif
 }
 template <typename T>
-HWY_API Mask512<T> Xor(hwy::SizeTag<8> /*tag*/, const Mask512<T> a,
-                       const Mask512<T> b) {
+HWY_INLINE Mask512<T> Xor(hwy::SizeTag<8> /*tag*/, const Mask512<T> a,
+                          const Mask512<T> b) {
 #if HWY_COMPILER_HAS_MASK_INTRINSICS
   return Mask512<T>{_kxor_mask8(a.raw, b.raw)};
 #else
@@ -1807,28 +1913,28 @@ HWY_DIAGNOSTICS_OFF(disable : 4245 4365, ignored "-Wsign-conversion")
 namespace detail {
 
 template <typename T>
-HWY_API void ScatterOffset(hwy::SizeTag<4> /* tag */, Vec512<T> v,
-                           Full512<T> /* tag */, T* HWY_RESTRICT base,
-                           const Vec512<int32_t> offset) {
+HWY_INLINE void ScatterOffset(hwy::SizeTag<4> /* tag */, Vec512<T> v,
+                              Full512<T> /* tag */, T* HWY_RESTRICT base,
+                              const Vec512<int32_t> offset) {
   _mm512_i32scatter_epi32(base, offset.raw, v.raw, 1);
 }
 template <typename T>
-HWY_API void ScatterIndex(hwy::SizeTag<4> /* tag */, Vec512<T> v,
-                          Full512<T> /* tag */, T* HWY_RESTRICT base,
-                          const Vec512<int32_t> index) {
+HWY_INLINE void ScatterIndex(hwy::SizeTag<4> /* tag */, Vec512<T> v,
+                             Full512<T> /* tag */, T* HWY_RESTRICT base,
+                             const Vec512<int32_t> index) {
   _mm512_i32scatter_epi32(base, index.raw, v.raw, 4);
 }
 
 template <typename T>
-HWY_API void ScatterOffset(hwy::SizeTag<8> /* tag */, Vec512<T> v,
-                           Full512<T> /* tag */, T* HWY_RESTRICT base,
-                           const Vec512<int64_t> offset) {
+HWY_INLINE void ScatterOffset(hwy::SizeTag<8> /* tag */, Vec512<T> v,
+                              Full512<T> /* tag */, T* HWY_RESTRICT base,
+                              const Vec512<int64_t> offset) {
   _mm512_i64scatter_epi64(base, offset.raw, v.raw, 1);
 }
 template <typename T>
-HWY_API void ScatterIndex(hwy::SizeTag<8> /* tag */, Vec512<T> v,
-                          Full512<T> /* tag */, T* HWY_RESTRICT base,
-                          const Vec512<int64_t> index) {
+HWY_INLINE void ScatterIndex(hwy::SizeTag<8> /* tag */, Vec512<T> v,
+                             Full512<T> /* tag */, T* HWY_RESTRICT base,
+                             const Vec512<int64_t> index) {
   _mm512_i64scatter_epi64(base, index.raw, v.raw, 8);
 }
 
@@ -1847,31 +1953,25 @@ HWY_API void ScatterIndex(Vec512<T> v, Full512<T> d, T* HWY_RESTRICT base,
   return detail::ScatterIndex(hwy::SizeTag<sizeof(T)>(), v, d, base, index);
 }
 
-template <>
-HWY_INLINE void ScatterOffset<float>(Vec512<float> v, Full512<float> /* tag */,
-                                     float* HWY_RESTRICT base,
-                                     const Vec512<int32_t> offset) {
+HWY_API void ScatterOffset(Vec512<float> v, Full512<float> /* tag */,
+                           float* HWY_RESTRICT base,
+                           const Vec512<int32_t> offset) {
   _mm512_i32scatter_ps(base, offset.raw, v.raw, 1);
 }
-template <>
-HWY_INLINE void ScatterIndex<float>(Vec512<float> v, Full512<float> /* tag */,
-                                    float* HWY_RESTRICT base,
-                                    const Vec512<int32_t> index) {
+HWY_API void ScatterIndex(Vec512<float> v, Full512<float> /* tag */,
+                          float* HWY_RESTRICT base,
+                          const Vec512<int32_t> index) {
   _mm512_i32scatter_ps(base, index.raw, v.raw, 4);
 }
 
-template <>
-HWY_INLINE void ScatterOffset<double>(Vec512<double> v,
-                                      Full512<double> /* tag */,
-                                      double* HWY_RESTRICT base,
-                                      const Vec512<int64_t> offset) {
+HWY_API void ScatterOffset(Vec512<double> v, Full512<double> /* tag */,
+                           double* HWY_RESTRICT base,
+                           const Vec512<int64_t> offset) {
   _mm512_i64scatter_pd(base, offset.raw, v.raw, 1);
 }
-template <>
-HWY_INLINE void ScatterIndex<double>(Vec512<double> v,
-                                     Full512<double> /* tag */,
-                                     double* HWY_RESTRICT base,
-                                     const Vec512<int64_t> index) {
+HWY_API void ScatterIndex(Vec512<double> v, Full512<double> /* tag */,
+                          double* HWY_RESTRICT base,
+                          const Vec512<int64_t> index) {
   _mm512_i64scatter_pd(base, index.raw, v.raw, 8);
 }
 
@@ -1880,28 +1980,32 @@ HWY_INLINE void ScatterIndex<double>(Vec512<double> v,
 namespace detail {
 
 template <typename T>
-HWY_API Vec512<T> GatherOffset(hwy::SizeTag<4> /* tag */, Full512<T> /* tag */,
-                               const T* HWY_RESTRICT base,
-                               const Vec512<int32_t> offset) {
+HWY_INLINE Vec512<T> GatherOffset(hwy::SizeTag<4> /* tag */,
+                                  Full512<T> /* tag */,
+                                  const T* HWY_RESTRICT base,
+                                  const Vec512<int32_t> offset) {
   return Vec512<T>{_mm512_i32gather_epi32(offset.raw, base, 1)};
 }
 template <typename T>
-HWY_API Vec512<T> GatherIndex(hwy::SizeTag<4> /* tag */, Full512<T> /* tag */,
-                              const T* HWY_RESTRICT base,
-                              const Vec512<int32_t> index) {
+HWY_INLINE Vec512<T> GatherIndex(hwy::SizeTag<4> /* tag */,
+                                 Full512<T> /* tag */,
+                                 const T* HWY_RESTRICT base,
+                                 const Vec512<int32_t> index) {
   return Vec512<T>{_mm512_i32gather_epi32(index.raw, base, 4)};
 }
 
 template <typename T>
-HWY_API Vec512<T> GatherOffset(hwy::SizeTag<8> /* tag */, Full512<T> /* tag */,
-                               const T* HWY_RESTRICT base,
-                               const Vec512<int64_t> offset) {
+HWY_INLINE Vec512<T> GatherOffset(hwy::SizeTag<8> /* tag */,
+                                  Full512<T> /* tag */,
+                                  const T* HWY_RESTRICT base,
+                                  const Vec512<int64_t> offset) {
   return Vec512<T>{_mm512_i64gather_epi64(offset.raw, base, 1)};
 }
 template <typename T>
-HWY_API Vec512<T> GatherIndex(hwy::SizeTag<8> /* tag */, Full512<T> /* tag */,
-                              const T* HWY_RESTRICT base,
-                              const Vec512<int64_t> index) {
+HWY_INLINE Vec512<T> GatherIndex(hwy::SizeTag<8> /* tag */,
+                                 Full512<T> /* tag */,
+                                 const T* HWY_RESTRICT base,
+                                 const Vec512<int64_t> index) {
   return Vec512<T>{_mm512_i64gather_epi64(index.raw, base, 8)};
 }
 
@@ -1920,29 +2024,25 @@ HWY_API Vec512<T> GatherIndex(Full512<T> d, const T* HWY_RESTRICT base,
   return detail::GatherIndex(hwy::SizeTag<sizeof(T)>(), d, base, index);
 }
 
-template <>
-HWY_INLINE Vec512<float> GatherOffset<float>(Full512<float> /* tag */,
-                                             const float* HWY_RESTRICT base,
-                                             const Vec512<int32_t> offset) {
+HWY_API Vec512<float> GatherOffset(Full512<float> /* tag */,
+                                   const float* HWY_RESTRICT base,
+                                   const Vec512<int32_t> offset) {
   return Vec512<float>{_mm512_i32gather_ps(offset.raw, base, 1)};
 }
-template <>
-HWY_INLINE Vec512<float> GatherIndex<float>(Full512<float> /* tag */,
-                                            const float* HWY_RESTRICT base,
-                                            const Vec512<int32_t> index) {
+HWY_API Vec512<float> GatherIndex(Full512<float> /* tag */,
+                                  const float* HWY_RESTRICT base,
+                                  const Vec512<int32_t> index) {
   return Vec512<float>{_mm512_i32gather_ps(index.raw, base, 4)};
 }
 
-template <>
-HWY_INLINE Vec512<double> GatherOffset<double>(Full512<double> /* tag */,
-                                               const double* HWY_RESTRICT base,
-                                               const Vec512<int64_t> offset) {
+HWY_API Vec512<double> GatherOffset(Full512<double> /* tag */,
+                                    const double* HWY_RESTRICT base,
+                                    const Vec512<int64_t> offset) {
   return Vec512<double>{_mm512_i64gather_pd(offset.raw, base, 1)};
 }
-template <>
-HWY_INLINE Vec512<double> GatherIndex<double>(Full512<double> /* tag */,
-                                              const double* HWY_RESTRICT base,
-                                              const Vec512<int64_t> index) {
+HWY_API Vec512<double> GatherIndex(Full512<double> /* tag */,
+                                   const double* HWY_RESTRICT base,
+                                   const Vec512<int64_t> index) {
   return Vec512<double>{_mm512_i64gather_pd(index.raw, base, 8)};
 }
 
@@ -1950,39 +2050,43 @@ HWY_DIAGNOSTICS(pop)
 
 // ================================================== SWIZZLE
 
-template <typename T>
-HWY_API T GetLane(const Vec512<T> v) {
-  return GetLane(LowerHalf(v));
-}
-
-// ------------------------------ Extract half
+// ------------------------------ LowerHalf
 
 template <typename T>
-HWY_API Vec256<T> LowerHalf(Vec512<T> v) {
+HWY_API Vec256<T> LowerHalf(Full256<T> /* tag */, Vec512<T> v) {
   return Vec256<T>{_mm512_castsi512_si256(v.raw)};
 }
-template <>
-HWY_INLINE Vec256<float> LowerHalf(Vec512<float> v) {
+HWY_API Vec256<float> LowerHalf(Full256<float> /* tag */, Vec512<float> v) {
   return Vec256<float>{_mm512_castps512_ps256(v.raw)};
 }
-template <>
-HWY_INLINE Vec256<double> LowerHalf(Vec512<double> v) {
+HWY_API Vec256<double> LowerHalf(Full256<double> /* tag */, Vec512<double> v) {
   return Vec256<double>{_mm512_castpd512_pd256(v.raw)};
 }
 
 template <typename T>
-HWY_API Vec256<T> UpperHalf(Vec512<T> v) {
+HWY_API Vec256<T> LowerHalf(Vec512<T> v) {
+  return LowerHalf(Full256<T>(), v);
+}
+
+// ------------------------------ UpperHalf
+
+template <typename T>
+HWY_API Vec256<T> UpperHalf(Full256<T> /* tag */, Vec512<T> v) {
   return Vec256<T>{_mm512_extracti32x8_epi32(v.raw, 1)};
 }
-template <>
-HWY_INLINE Vec256<float> UpperHalf(Vec512<float> v) {
+HWY_API Vec256<float> UpperHalf(Full256<float> /* tag */, Vec512<float> v) {
   return Vec256<float>{_mm512_extractf32x8_ps(v.raw, 1)};
 }
-template <>
-HWY_INLINE Vec256<double> UpperHalf(Vec512<double> v) {
+HWY_API Vec256<double> UpperHalf(Full256<double> /* tag */, Vec512<double> v) {
   return Vec256<double>{_mm512_extractf64x4_pd(v.raw, 1)};
 }
 
+// ------------------------------ GetLane (LowerHalf)
+template <typename T>
+HWY_API T GetLane(const Vec512<T> v) {
+  return GetLane(LowerHalf(v));
+}
+
 // ------------------------------ ZeroExtendVector
 
 // Unfortunately the initial _mm512_castsi256_si512 intrinsic leaves the upper
@@ -1997,23 +2101,23 @@ HWY_INLINE Vec256<double> UpperHalf(Vec512<double> v) {
 // https://gcc.godbolt.org/z/1MKGaP.
 
 template <typename T>
-HWY_API Vec512<T> ZeroExtendVector(Vec256<T> lo) {
+HWY_API Vec512<T> ZeroExtendVector(Full512<T> /* tag */, Vec256<T> lo) {
 #if !HWY_COMPILER_CLANG && HWY_COMPILER_GCC && (HWY_COMPILER_GCC < 1000)
   return Vec512<T>{_mm512_inserti32x8(_mm512_setzero_si512(), lo.raw, 0)};
 #else
   return Vec512<T>{_mm512_zextsi256_si512(lo.raw)};
 #endif
 }
-template <>
-HWY_INLINE Vec512<float> ZeroExtendVector(Vec256<float> lo) {
+HWY_API Vec512<float> ZeroExtendVector(Full512<float> /* tag */,
+                                       Vec256<float> lo) {
 #if !HWY_COMPILER_CLANG && HWY_COMPILER_GCC && (HWY_COMPILER_GCC < 1000)
   return Vec512<float>{_mm512_insertf32x8(_mm512_setzero_ps(), lo.raw, 0)};
 #else
   return Vec512<float>{_mm512_zextps256_ps512(lo.raw)};
 #endif
 }
-template <>
-HWY_INLINE Vec512<double> ZeroExtendVector(Vec256<double> lo) {
+HWY_API Vec512<double> ZeroExtendVector(Full512<double> /* tag */,
+                                        Vec256<double> lo) {
 #if !HWY_COMPILER_CLANG && HWY_COMPILER_GCC && (HWY_COMPILER_GCC < 1000)
   return Vec512<double>{_mm512_insertf64x4(_mm512_setzero_pd(), lo.raw, 0)};
 #else
@@ -2024,61 +2128,68 @@ HWY_INLINE Vec512<double> ZeroExtendVector(Vec256<double> lo) {
 // ------------------------------ Combine
 
 template <typename T>
-HWY_API Vec512<T> Combine(Vec256<T> hi, Vec256<T> lo) {
-  const auto lo512 = ZeroExtendVector(lo);
+HWY_API Vec512<T> Combine(Full512<T> d, Vec256<T> hi, Vec256<T> lo) {
+  const auto lo512 = ZeroExtendVector(d, lo);
   return Vec512<T>{_mm512_inserti32x8(lo512.raw, hi.raw, 1)};
 }
-template <>
-HWY_INLINE Vec512<float> Combine(Vec256<float> hi, Vec256<float> lo) {
-  const auto lo512 = ZeroExtendVector(lo);
+HWY_API Vec512<float> Combine(Full512<float> d, Vec256<float> hi,
+                              Vec256<float> lo) {
+  const auto lo512 = ZeroExtendVector(d, lo);
   return Vec512<float>{_mm512_insertf32x8(lo512.raw, hi.raw, 1)};
 }
-template <>
-HWY_INLINE Vec512<double> Combine(Vec256<double> hi, Vec256<double> lo) {
-  const auto lo512 = ZeroExtendVector(lo);
+HWY_API Vec512<double> Combine(Full512<double> d, Vec256<double> hi,
+                               Vec256<double> lo) {
+  const auto lo512 = ZeroExtendVector(d, lo);
   return Vec512<double>{_mm512_insertf64x4(lo512.raw, hi.raw, 1)};
 }
 
-// ------------------------------ Shift vector by constant #bytes
+// ------------------------------ ShiftLeftBytes
 
-// 0x01..0F, kBytes = 1 => 0x02..0F00
 template <int kBytes, typename T>
-HWY_API Vec512<T> ShiftLeftBytes(const Vec512<T> v) {
+HWY_API Vec512<T> ShiftLeftBytes(Full512<T> /* tag */, const Vec512<T> v) {
   static_assert(0 <= kBytes && kBytes <= 16, "Invalid kBytes");
   return Vec512<T>{_mm512_bslli_epi128(v.raw, kBytes)};
 }
 
+template <int kBytes, typename T>
+HWY_API Vec512<T> ShiftLeftBytes(const Vec512<T> v) {
+  return ShiftLeftBytes<kBytes>(Full512<T>(), v);
+}
+
+// ------------------------------ ShiftLeftLanes
+
 template <int kLanes, typename T>
-HWY_API Vec512<T> ShiftLeftLanes(const Vec512<T> v) {
-  const Full512<uint8_t> d8;
-  const Full512<T> d;
+HWY_API Vec512<T> ShiftLeftLanes(Full512<T> d, const Vec512<T> v) {
+  const Repartition<uint8_t, decltype(d)> d8;
   return BitCast(d, ShiftLeftBytes<kLanes * sizeof(T)>(BitCast(d8, v)));
 }
 
-// 0x01..0F, kBytes = 1 => 0x0001..0E
+template <int kLanes, typename T>
+HWY_API Vec512<T> ShiftLeftLanes(const Vec512<T> v) {
+  return ShiftLeftLanes<kLanes>(Full512<T>(), v);
+}
+
+// ------------------------------ ShiftRightBytes
 template <int kBytes, typename T>
-HWY_API Vec512<T> ShiftRightBytes(const Vec512<T> v) {
+HWY_API Vec512<T> ShiftRightBytes(Full512<T> /* tag */, const Vec512<T> v) {
   static_assert(0 <= kBytes && kBytes <= 16, "Invalid kBytes");
   return Vec512<T>{_mm512_bsrli_epi128(v.raw, kBytes)};
 }
 
+// ------------------------------ ShiftRightLanes
 template <int kLanes, typename T>
-HWY_API Vec512<T> ShiftRightLanes(const Vec512<T> v) {
-  const Full512<uint8_t> d8;
-  const Full512<T> d;
+HWY_API Vec512<T> ShiftRightLanes(Full512<T> d, const Vec512<T> v) {
+  const Repartition<uint8_t, decltype(d)> d8;
   return BitCast(d, ShiftRightBytes<kLanes * sizeof(T)>(BitCast(d8, v)));
 }
 
-// ------------------------------ Extract from 2x 128-bit at constant offset
+// ------------------------------ CombineShiftRightBytes
 
-// Extracts 128 bits from <hi, lo> by skipping the least-significant kBytes.
-template <int kBytes, typename T>
-HWY_API Vec512<T> CombineShiftRightBytes(const Vec512<T> hi,
-                                         const Vec512<T> lo) {
-  const Full512<uint8_t> d8;
-  const Vec512<uint8_t> extracted_bytes{
-      _mm512_alignr_epi8(BitCast(d8, hi).raw, BitCast(d8, lo).raw, kBytes)};
-  return BitCast(Full512<T>(), extracted_bytes);
+template <int kBytes, typename T, class V = Vec512<T>>
+HWY_API V CombineShiftRightBytes(Full512<T> d, V hi, V lo) {
+  const Repartition<uint8_t, decltype(d)> d8;
+  return BitCast(d, Vec512<uint8_t>{_mm512_alignr_epi8(
+                        BitCast(d8, hi).raw, BitCast(d8, lo).raw, kBytes)});
 }
 
 // ------------------------------ Broadcast/splat any lane
@@ -2254,7 +2365,7 @@ HWY_API Vec512<float> TableLookupLanes(const Vec512<float> v,
   return Vec512<float>{_mm512_permutexvar_ps(idx.raw, v.raw)};
 }
 
-// ------------------------------ Interleave lanes
+// ------------------------------ InterleaveLower
 
 // Interleaves lanes from halves of the 128-bit blocks of "a" (which provides
 // the least-significant lane) and "b". To concatenate two half-width integers
@@ -2303,6 +2414,17 @@ HWY_API Vec512<double> InterleaveLower(const Vec512<double> a,
   return Vec512<double>{_mm512_unpacklo_pd(a.raw, b.raw)};
 }
 
+// Additional overload for the optional Simd<> tag.
+template <typename T, class V = Vec512<T>>
+HWY_API V InterleaveLower(Full512<T> /* tag */, V a, V b) {
+  return InterleaveLower(a, b);
+}
+
+// ------------------------------ InterleaveUpper
+
+// All functions inside detail lack the required D parameter.
+namespace detail {
+
 HWY_API Vec512<uint8_t> InterleaveUpper(const Vec512<uint8_t> a,
                                         const Vec512<uint8_t> b) {
   return Vec512<uint8_t>{_mm512_unpackhi_epi8(a.raw, b.raw)};
@@ -2346,130 +2468,102 @@ HWY_API Vec512<double> InterleaveUpper(const Vec512<double> a,
   return Vec512<double>{_mm512_unpackhi_pd(a.raw, b.raw)};
 }
 
-// ------------------------------ Zip lanes
-
-// Same as interleave_*, except that the return lanes are double-width integers;
-// this is necessary because the single-lane scalar cannot return two values.
+}  // namespace detail
 
-HWY_API Vec512<uint16_t> ZipLower(const Vec512<uint8_t> a,
-                                  const Vec512<uint8_t> b) {
-  return Vec512<uint16_t>{_mm512_unpacklo_epi8(a.raw, b.raw)};
-}
-HWY_API Vec512<uint32_t> ZipLower(const Vec512<uint16_t> a,
-                                  const Vec512<uint16_t> b) {
-  return Vec512<uint32_t>{_mm512_unpacklo_epi16(a.raw, b.raw)};
-}
-HWY_API Vec512<uint64_t> ZipLower(const Vec512<uint32_t> a,
-                                  const Vec512<uint32_t> b) {
-  return Vec512<uint64_t>{_mm512_unpacklo_epi32(a.raw, b.raw)};
+template <typename T, class V = Vec512<T>>
+HWY_API V InterleaveUpper(Full512<T> /* tag */, V a, V b) {
+  return detail::InterleaveUpper(a, b);
 }
 
-HWY_API Vec512<int16_t> ZipLower(const Vec512<int8_t> a,
-                                 const Vec512<int8_t> b) {
-  return Vec512<int16_t>{_mm512_unpacklo_epi8(a.raw, b.raw)};
-}
-HWY_API Vec512<int32_t> ZipLower(const Vec512<int16_t> a,
-                                 const Vec512<int16_t> b) {
-  return Vec512<int32_t>{_mm512_unpacklo_epi16(a.raw, b.raw)};
-}
-HWY_API Vec512<int64_t> ZipLower(const Vec512<int32_t> a,
-                                 const Vec512<int32_t> b) {
-  return Vec512<int64_t>{_mm512_unpacklo_epi32(a.raw, b.raw)};
-}
+// ------------------------------ ZipLower/ZipUpper (InterleaveLower)
 
-HWY_API Vec512<uint16_t> ZipUpper(const Vec512<uint8_t> a,
-                                  const Vec512<uint8_t> b) {
-  return Vec512<uint16_t>{_mm512_unpackhi_epi8(a.raw, b.raw)};
-}
-HWY_API Vec512<uint32_t> ZipUpper(const Vec512<uint16_t> a,
-                                  const Vec512<uint16_t> b) {
-  return Vec512<uint32_t>{_mm512_unpackhi_epi16(a.raw, b.raw)};
+// Same as Interleave*, except that the return lanes are double-width integers;
+// this is necessary because the single-lane scalar cannot return two values.
+template <typename T, typename TW = MakeWide<T>>
+HWY_API Vec512<TW> ZipLower(Vec512<T> a, Vec512<T> b) {
+  return BitCast(Full512<TW>(), InterleaveLower(a, b));
 }
-HWY_API Vec512<uint64_t> ZipUpper(const Vec512<uint32_t> a,
-                                  const Vec512<uint32_t> b) {
-  return Vec512<uint64_t>{_mm512_unpackhi_epi32(a.raw, b.raw)};
+template <typename T, typename TW = MakeWide<T>>
+HWY_API Vec512<TW> ZipLower(Full512<TW> d, Vec512<T> a, Vec512<T> b) {
+  return BitCast(Full512<TW>(), InterleaveLower(d, a, b));
 }
 
-HWY_API Vec512<int16_t> ZipUpper(const Vec512<int8_t> a,
-                                 const Vec512<int8_t> b) {
-  return Vec512<int16_t>{_mm512_unpackhi_epi8(a.raw, b.raw)};
-}
-HWY_API Vec512<int32_t> ZipUpper(const Vec512<int16_t> a,
-                                 const Vec512<int16_t> b) {
-  return Vec512<int32_t>{_mm512_unpackhi_epi16(a.raw, b.raw)};
-}
-HWY_API Vec512<int64_t> ZipUpper(const Vec512<int32_t> a,
-                                 const Vec512<int32_t> b) {
-  return Vec512<int64_t>{_mm512_unpackhi_epi32(a.raw, b.raw)};
+template <typename T, typename TW = MakeWide<T>>
+HWY_API Vec512<TW> ZipUpper(Full512<TW> d, Vec512<T> a, Vec512<T> b) {
+  return BitCast(Full512<TW>(), InterleaveUpper(d, a, b));
 }
 
 // ------------------------------ Concat* halves
 
 // hiH,hiL loH,loL |-> hiL,loL (= lower halves)
 template <typename T>
-HWY_API Vec512<T> ConcatLowerLower(const Vec512<T> hi, const Vec512<T> lo) {
+HWY_API Vec512<T> ConcatLowerLower(Full512<T> /* tag */, const Vec512<T> hi,
+                                   const Vec512<T> lo) {
   return Vec512<T>{_mm512_shuffle_i32x4(lo.raw, hi.raw, _MM_PERM_BABA)};
 }
-template <>
-HWY_INLINE Vec512<float> ConcatLowerLower(const Vec512<float> hi,
-                                          const Vec512<float> lo) {
+HWY_API Vec512<float> ConcatLowerLower(Full512<float> /* tag */,
+                                       const Vec512<float> hi,
+                                       const Vec512<float> lo) {
   return Vec512<float>{_mm512_shuffle_f32x4(lo.raw, hi.raw, _MM_PERM_BABA)};
 }
-template <>
-HWY_INLINE Vec512<double> ConcatLowerLower(const Vec512<double> hi,
-                                           const Vec512<double> lo) {
+HWY_API Vec512<double> ConcatLowerLower(Full512<double> /* tag */,
+                                        const Vec512<double> hi,
+                                        const Vec512<double> lo) {
   return Vec512<double>{_mm512_shuffle_f64x2(lo.raw, hi.raw, _MM_PERM_BABA)};
 }
 
 // hiH,hiL loH,loL |-> hiH,loH (= upper halves)
 template <typename T>
-HWY_API Vec512<T> ConcatUpperUpper(const Vec512<T> hi, const Vec512<T> lo) {
+HWY_API Vec512<T> ConcatUpperUpper(Full512<T> /* tag */, const Vec512<T> hi,
+                                   const Vec512<T> lo) {
   return Vec512<T>{_mm512_shuffle_i32x4(lo.raw, hi.raw, _MM_PERM_DCDC)};
 }
-template <>
-HWY_INLINE Vec512<float> ConcatUpperUpper(const Vec512<float> hi,
-                                          const Vec512<float> lo) {
+HWY_API Vec512<float> ConcatUpperUpper(Full512<float> /* tag */,
+                                       const Vec512<float> hi,
+                                       const Vec512<float> lo) {
   return Vec512<float>{_mm512_shuffle_f32x4(lo.raw, hi.raw, _MM_PERM_DCDC)};
 }
-template <>
-HWY_INLINE Vec512<double> ConcatUpperUpper(const Vec512<double> hi,
-                                           const Vec512<double> lo) {
+HWY_API Vec512<double> ConcatUpperUpper(Full512<double> /* tag */,
+                                        const Vec512<double> hi,
+                                        const Vec512<double> lo) {
   return Vec512<double>{_mm512_shuffle_f64x2(lo.raw, hi.raw, _MM_PERM_DCDC)};
 }
 
 // hiH,hiL loH,loL |-> hiL,loH (= inner halves / swap blocks)
 template <typename T>
-HWY_API Vec512<T> ConcatLowerUpper(const Vec512<T> hi, const Vec512<T> lo) {
+HWY_API Vec512<T> ConcatLowerUpper(Full512<T> /* tag */, const Vec512<T> hi,
+                                   const Vec512<T> lo) {
   return Vec512<T>{_mm512_shuffle_i32x4(lo.raw, hi.raw, 0x4E)};
 }
-template <>
-HWY_INLINE Vec512<float> ConcatLowerUpper(const Vec512<float> hi,
-                                          const Vec512<float> lo) {
+HWY_API Vec512<float> ConcatLowerUpper(Full512<float> /* tag */,
+                                       const Vec512<float> hi,
+                                       const Vec512<float> lo) {
   return Vec512<float>{_mm512_shuffle_f32x4(lo.raw, hi.raw, 0x4E)};
 }
-template <>
-HWY_INLINE Vec512<double> ConcatLowerUpper(const Vec512<double> hi,
-                                           const Vec512<double> lo) {
+HWY_API Vec512<double> ConcatLowerUpper(Full512<double> /* tag */,
+                                        const Vec512<double> hi,
+                                        const Vec512<double> lo) {
   return Vec512<double>{_mm512_shuffle_f64x2(lo.raw, hi.raw, 0x4E)};
 }
 
 // hiH,hiL loH,loL |-> hiH,loL (= outer halves)
 template <typename T>
-HWY_API Vec512<T> ConcatUpperLower(const Vec512<T> hi, const Vec512<T> lo) {
+HWY_API Vec512<T> ConcatUpperLower(Full512<T> /* tag */, const Vec512<T> hi,
+                                   const Vec512<T> lo) {
   // There are no imm8 blend in AVX512. Use blend16 because 32-bit masks
   // are efficiently loaded from 32-bit regs.
   const __mmask32 mask = /*_cvtu32_mask32 */ (0x0000FFFF);
   return Vec512<T>{_mm512_mask_blend_epi16(mask, hi.raw, lo.raw)};
 }
-template <>
-HWY_INLINE Vec512<float> ConcatUpperLower(const Vec512<float> hi,
-                                          const Vec512<float> lo) {
+HWY_API Vec512<float> ConcatUpperLower(Full512<float> /* tag */,
+                                       const Vec512<float> hi,
+                                       const Vec512<float> lo) {
   const __mmask16 mask = /*_cvtu32_mask16 */ (0x00FF);
   return Vec512<float>{_mm512_mask_blend_ps(mask, hi.raw, lo.raw)};
 }
-template <>
-HWY_INLINE Vec512<double> ConcatUpperLower(const Vec512<double> hi,
-                                           const Vec512<double> lo) {
+HWY_API Vec512<double> ConcatUpperLower(Full512<double> /* tag */,
+                                        const Vec512<double> hi,
+                                        const Vec512<double> lo) {
   const __mmask8 mask = /*_cvtu32_mask8 */ (0x0F);
   return Vec512<double>{_mm512_mask_blend_pd(mask, hi.raw, lo.raw)};
 }
@@ -2483,16 +2577,54 @@ HWY_API Vec512<T> OddEven(const Vec512<T> a, const Vec512<T> b) {
   return IfThenElse(Mask512<T>{0x5555555555555555ull >> shift}, b, a);
 }
 
-// ------------------------------ Shuffle bytes with variable indices
+// ------------------------------ TableLookupBytes (ZeroExtendVector)
 
-// Returns vector of bytes[from[i]]. "from" is also interpreted as bytes, i.e.
-// lane indices in [0, 16).
+// Both full
 template <typename T>
-HWY_API Vec512<T> TableLookupBytes(const Vec512<T> bytes,
-                                   const Vec512<T> from) {
+HWY_API Vec512<T> TableLookupBytes(Vec512<T> bytes, Vec512<T> from) {
   return Vec512<T>{_mm512_shuffle_epi8(bytes.raw, from.raw)};
 }
 
+// Partial index vector
+template <typename T, typename TI, size_t NI>
+HWY_API Vec128<TI, NI> TableLookupBytes(Vec512<T> bytes, Vec128<TI, NI> from) {
+  const Full512<TI> d512;
+  const Half<decltype(d512)> d256;
+  const Half<decltype(d256)> d128;
+  // First expand to full 128, then 256, then 512.
+  const Vec128<TI> from_full{from.raw};
+  const auto from_512 =
+      ZeroExtendVector(d512, ZeroExtendVector(d256, from_full));
+  const auto tbl_full = TableLookupBytes(bytes, from_512);
+  // Shrink to 256, then 128, then partial.
+  return Vec128<TI, NI>{LowerHalf(d128, LowerHalf(d256, tbl_full)).raw};
+}
+template <typename T, typename TI>
+HWY_API Vec256<TI> TableLookupBytes(Vec512<T> bytes, Vec256<TI> from) {
+  const auto from_512 = ZeroExtendVector(Full512<TI>(), from);
+  return LowerHalf(Full256<TI>(), TableLookupBytes(bytes, from_512));
+}
+
+// Partial table vector
+template <typename T, size_t N, typename TI>
+HWY_API Vec512<TI> TableLookupBytes(Vec128<T, N> bytes, Vec512<TI> from) {
+  const Full512<TI> d512;
+  const Half<decltype(d512)> d256;
+  const Half<decltype(d256)> d128;
+  // First expand to full 128, then 256, then 512.
+  const Vec128<T> bytes_full{bytes.raw};
+  const auto bytes_512 =
+      ZeroExtendVector(d512, ZeroExtendVector(d256, bytes_full));
+  return TableLookupBytes(bytes_512, from);
+}
+template <typename T, typename TI>
+HWY_API Vec512<TI> TableLookupBytes(Vec256<T> bytes, Vec512<TI> from) {
+  const auto bytes_512 = ZeroExtendVector(Full512<T>(), bytes);
+  return TableLookupBytes(bytes_512, from);
+}
+
+// Partial both are handled by x86_128/256.
+
 // ================================================== CONVERT
 
 // ------------------------------ Promotions (part w/ narrow lanes -> full)
@@ -2696,6 +2828,74 @@ HWY_API Vec512<int32_t> NearestInt(const Vec512<float> v) {
   return detail::FixConversionOverflow(di, v, _mm512_cvtps_epi32(v.raw));
 }
 
+// ================================================== CRYPTO
+
+#if !defined(HWY_DISABLE_PCLMUL_AES)
+
+// Per-target flag to prevent generic_ops-inl.h from defining AESRound.
+#ifdef HWY_NATIVE_AES
+#undef HWY_NATIVE_AES
+#else
+#define HWY_NATIVE_AES
+#endif
+
+HWY_API Vec512<uint8_t> AESRound(Vec512<uint8_t> state,
+                                 Vec512<uint8_t> round_key) {
+#if HWY_TARGET == HWY_AVX3_DL
+  return Vec512<uint8_t>{_mm512_aesenc_epi128(state.raw, round_key.raw)};
+#else
+  alignas(64) uint8_t a[64];
+  alignas(64) uint8_t b[64];
+  const Full512<uint8_t> d;
+  const Full128<uint8_t> d128;
+  Store(state, d, a);
+  Store(round_key, d, b);
+  for (size_t i = 0; i < 64; i += 16) {
+    const auto enc = AESRound(Load(d128, a + i), Load(d128, b + i));
+    Store(enc, d128, a + i);
+  }
+  return Load(d, a);
+#endif
+}
+
+HWY_API Vec512<uint64_t> CLMulLower(Vec512<uint64_t> va, Vec512<uint64_t> vb) {
+#if HWY_TARGET == HWY_AVX3_DL
+  return Vec512<uint64_t>{_mm512_clmulepi64_epi128(va.raw, vb.raw, 0x00)};
+#else
+  alignas(64) uint64_t a[8];
+  alignas(64) uint64_t b[8];
+  const Full512<uint64_t> d;
+  const Full128<uint64_t> d128;
+  Store(va, d, a);
+  Store(vb, d, b);
+  for (size_t i = 0; i < 8; i += 2) {
+    const auto mul = CLMulLower(Load(d128, a + i), Load(d128, b + i));
+    Store(mul, d128, a + i);
+  }
+  return Load(d, a);
+#endif
+}
+
+HWY_API Vec512<uint64_t> CLMulUpper(Vec512<uint64_t> va, Vec512<uint64_t> vb) {
+#if HWY_TARGET == HWY_AVX3_DL
+  return Vec512<uint64_t>{_mm512_clmulepi64_epi128(va.raw, vb.raw, 0x11)};
+#else
+  alignas(64) uint64_t a[8];
+  alignas(64) uint64_t b[8];
+  const Full512<uint64_t> d;
+  const Full128<uint64_t> d128;
+  Store(va, d, a);
+  Store(vb, d, b);
+  for (size_t i = 0; i < 8; i += 2) {
+    const auto mul = CLMulUpper(Load(d128, a + i), Load(d128, b + i));
+    Store(mul, d128, a + i);
+  }
+  return Load(d, a);
+#endif
+}
+
+#endif  // HWY_DISABLE_PCLMUL_AES
+
 // ================================================== MISC
 
 // Returns a vector with lane i=[0, N) set to "first" + i.
@@ -2715,7 +2915,7 @@ Vec512<T> Iota(const Full512<T> d, const T2 first) {
 namespace detail {
 
 template <typename T>
-HWY_API bool AllFalse(hwy::SizeTag<1> /*tag*/, const Mask512<T> v) {
+HWY_INLINE bool AllFalse(hwy::SizeTag<1> /*tag*/, const Mask512<T> v) {
 #if HWY_COMPILER_HAS_MASK_INTRINSICS
   return _kortestz_mask64_u8(v.raw, v.raw);
 #else
@@ -2723,7 +2923,7 @@ HWY_API bool AllFalse(hwy::SizeTag<1> /*tag*/, const Mask512<T> v) {
 #endif
 }
 template <typename T>
-HWY_API bool AllFalse(hwy::SizeTag<2> /*tag*/, const Mask512<T> v) {
+HWY_INLINE bool AllFalse(hwy::SizeTag<2> /*tag*/, const Mask512<T> v) {
 #if HWY_COMPILER_HAS_MASK_INTRINSICS
   return _kortestz_mask32_u8(v.raw, v.raw);
 #else
@@ -2731,7 +2931,7 @@ HWY_API bool AllFalse(hwy::SizeTag<2> /*tag*/, const Mask512<T> v) {
 #endif
 }
 template <typename T>
-HWY_API bool AllFalse(hwy::SizeTag<4> /*tag*/, const Mask512<T> v) {
+HWY_INLINE bool AllFalse(hwy::SizeTag<4> /*tag*/, const Mask512<T> v) {
 #if HWY_COMPILER_HAS_MASK_INTRINSICS
   return _kortestz_mask16_u8(v.raw, v.raw);
 #else
@@ -2739,7 +2939,7 @@ HWY_API bool AllFalse(hwy::SizeTag<4> /*tag*/, const Mask512<T> v) {
 #endif
 }
 template <typename T>
-HWY_API bool AllFalse(hwy::SizeTag<8> /*tag*/, const Mask512<T> v) {
+HWY_INLINE bool AllFalse(hwy::SizeTag<8> /*tag*/, const Mask512<T> v) {
 #if HWY_COMPILER_HAS_MASK_INTRINSICS
   return _kortestz_mask8_u8(v.raw, v.raw);
 #else
@@ -2750,14 +2950,14 @@ HWY_API bool AllFalse(hwy::SizeTag<8> /*tag*/, const Mask512<T> v) {
 }  // namespace detail
 
 template <typename T>
-HWY_API bool AllFalse(const Mask512<T> v) {
+HWY_API bool AllFalse(const Full512<T> /* tag */, const Mask512<T> v) {
   return detail::AllFalse(hwy::SizeTag<sizeof(T)>(), v);
 }
 
 namespace detail {
 
 template <typename T>
-HWY_API bool AllTrue(hwy::SizeTag<1> /*tag*/, const Mask512<T> v) {
+HWY_INLINE bool AllTrue(hwy::SizeTag<1> /*tag*/, const Mask512<T> v) {
 #if HWY_COMPILER_HAS_MASK_INTRINSICS
   return _kortestc_mask64_u8(v.raw, v.raw);
 #else
@@ -2765,7 +2965,7 @@ HWY_API bool AllTrue(hwy::SizeTag<1> /*tag*/, const Mask512<T> v) {
 #endif
 }
 template <typename T>
-HWY_API bool AllTrue(hwy::SizeTag<2> /*tag*/, const Mask512<T> v) {
+HWY_INLINE bool AllTrue(hwy::SizeTag<2> /*tag*/, const Mask512<T> v) {
 #if HWY_COMPILER_HAS_MASK_INTRINSICS
   return _kortestc_mask32_u8(v.raw, v.raw);
 #else
@@ -2773,7 +2973,7 @@ HWY_API bool AllTrue(hwy::SizeTag<2> /*tag*/, const Mask512<T> v) {
 #endif
 }
 template <typename T>
-HWY_API bool AllTrue(hwy::SizeTag<4> /*tag*/, const Mask512<T> v) {
+HWY_INLINE bool AllTrue(hwy::SizeTag<4> /*tag*/, const Mask512<T> v) {
 #if HWY_COMPILER_HAS_MASK_INTRINSICS
   return _kortestc_mask16_u8(v.raw, v.raw);
 #else
@@ -2781,7 +2981,7 @@ HWY_API bool AllTrue(hwy::SizeTag<4> /*tag*/, const Mask512<T> v) {
 #endif
 }
 template <typename T>
-HWY_API bool AllTrue(hwy::SizeTag<8> /*tag*/, const Mask512<T> v) {
+HWY_INLINE bool AllTrue(hwy::SizeTag<8> /*tag*/, const Mask512<T> v) {
 #if HWY_COMPILER_HAS_MASK_INTRINSICS
   return _kortestc_mask8_u8(v.raw, v.raw);
 #else
@@ -2792,22 +2992,35 @@ HWY_API bool AllTrue(hwy::SizeTag<8> /*tag*/, const Mask512<T> v) {
 }  // namespace detail
 
 template <typename T>
-HWY_API bool AllTrue(const Mask512<T> v) {
+HWY_API bool AllTrue(const Full512<T> /* tag */, const Mask512<T> v) {
   return detail::AllTrue(hwy::SizeTag<sizeof(T)>(), v);
 }
 
 template <typename T>
-HWY_INLINE size_t StoreMaskBits(const Mask512<T> mask, uint8_t* p) {
+HWY_API size_t StoreMaskBits(const Full512<T> /* tag */, const Mask512<T> mask,
+                             uint8_t* p) {
   const size_t kNumBytes = 8 / sizeof(T);
   CopyBytes<kNumBytes>(&mask.raw, p);
   return kNumBytes;
 }
 
 template <typename T>
-HWY_API size_t CountTrue(const Mask512<T> mask) {
+HWY_API size_t CountTrue(const Full512<T> /* tag */, const Mask512<T> mask) {
   return PopCount(mask.raw);
 }
 
+template <typename T, HWY_IF_NOT_LANE_SIZE(T, 1)>
+HWY_API intptr_t FindFirstTrue(const Full512<T> /* tag */,
+                               const Mask512<T> mask) {
+  return mask.raw ? Num0BitsBelowLS1Bit_Nonzero32(mask.raw) : -1;
+}
+
+template <typename T, HWY_IF_LANE_SIZE(T, 1)>
+HWY_API intptr_t FindFirstTrue(const Full512<T> /* tag */,
+                               const Mask512<T> mask) {
+  return mask.raw ? Num0BitsBelowLS1Bit_Nonzero64(mask.raw) : -1;
+}
+
 // ------------------------------ Compress
 
 HWY_API Vec512<uint32_t> Compress(Vec512<uint32_t> v,
@@ -2841,8 +3054,8 @@ namespace detail {
 // Ignore IDE redefinition error for these two functions: if this header is
 // included, then the functions weren't actually defined in x86_256-inl.h.
 template <typename T>
-HWY_API Vec256<T> Compress(hwy::SizeTag<2> /*tag*/, Vec256<T> v,
-                           const uint64_t mask_bits) {
+HWY_INLINE Vec256<T> Compress(hwy::SizeTag<2> /*tag*/, Vec256<T> v,
+                              const uint64_t mask_bits) {
   using D = Full256<T>;
   const Rebind<uint16_t, D> du;
   const Rebind<int32_t, D> dw;       // 512-bit, not 256!
@@ -2867,7 +3080,7 @@ HWY_API Vec512<T> Compress(Vec512<T> v, const Mask512<T> mask) {
   const Repartition<int32_t, D> dw;
   const auto vu16 = BitCast(du, v);  // (required for float16_t inputs)
   const auto promoted0 = PromoteTo(dw, LowerHalf(vu16));
-  const auto promoted1 = PromoteTo(dw, UpperHalf(vu16));
+  const auto promoted1 = PromoteTo(dw, UpperHalf(Half<decltype(du)>(), vu16));
 
   const Mask512<int32_t> mask0{static_cast<__mmask16>(mask.raw & 0xFFFF)};
   const Mask512<int32_t> mask1{static_cast<__mmask16>(mask.raw >> 16)};
@@ -2879,7 +3092,7 @@ HWY_API Vec512<T> Compress(Vec512<T> v, const Mask512<T> mask) {
   const auto demoted1 = ZeroExtendVector(DemoteTo(dh, compressed1));
 
   // Concatenate into single vector by shifting upper with writemask.
-  const size_t num0 = CountTrue(mask0);
+  const size_t num0 = CountTrue(dw, mask0);
   const __mmask32 m_upper = ~((1u << num0) - 1);
   alignas(64) uint16_t iota[64] = {
       0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,
@@ -2908,47 +3121,45 @@ HWY_API size_t CompressStore(Vec512<T> v, const Mask512<T> mask, Full512<T> d,
   // using StoreU to concatenate the results would cause page faults if
   // `aligned` is the last valid vector. Instead rely on in-register splicing.
   Store(Compress(v, mask), d, aligned);
-  return CountTrue(mask);
+  return CountTrue(d, mask);
 }
 
 HWY_API size_t CompressStore(Vec512<uint32_t> v, const Mask512<uint32_t> mask,
-                             Full512<uint32_t> /* tag */,
+                             Full512<uint32_t> d,
                              uint32_t* HWY_RESTRICT aligned) {
   _mm512_mask_compressstoreu_epi32(aligned, mask.raw, v.raw);
-  return CountTrue(mask);
+  return CountTrue(d, mask);
 }
 HWY_API size_t CompressStore(Vec512<int32_t> v, const Mask512<int32_t> mask,
-                             Full512<int32_t> /* tag */,
+                             Full512<int32_t> d,
                              int32_t* HWY_RESTRICT aligned) {
   _mm512_mask_compressstoreu_epi32(aligned, mask.raw, v.raw);
-  return CountTrue(mask);
+  return CountTrue(d, mask);
 }
 
 HWY_API size_t CompressStore(Vec512<uint64_t> v, const Mask512<uint64_t> mask,
-                             Full512<uint64_t> /* tag */,
+                             Full512<uint64_t> d,
                              uint64_t* HWY_RESTRICT aligned) {
   _mm512_mask_compressstoreu_epi64(aligned, mask.raw, v.raw);
-  return CountTrue(mask);
+  return CountTrue(d, mask);
 }
 HWY_API size_t CompressStore(Vec512<int64_t> v, const Mask512<int64_t> mask,
-                             Full512<int64_t> /* tag */,
+                             Full512<int64_t> d,
                              int64_t* HWY_RESTRICT aligned) {
   _mm512_mask_compressstoreu_epi64(aligned, mask.raw, v.raw);
-  return CountTrue(mask);
+  return CountTrue(d, mask);
 }
 
 HWY_API size_t CompressStore(Vec512<float> v, const Mask512<float> mask,
-                             Full512<float> /* tag */,
-                             float* HWY_RESTRICT aligned) {
+                             Full512<float> d, float* HWY_RESTRICT aligned) {
   _mm512_mask_compressstoreu_ps(aligned, mask.raw, v.raw);
-  return CountTrue(mask);
+  return CountTrue(d, mask);
 }
 
 HWY_API size_t CompressStore(Vec512<double> v, const Mask512<double> mask,
-                             Full512<double> /* tag */,
-                             double* HWY_RESTRICT aligned) {
+                             Full512<double> d, double* HWY_RESTRICT aligned) {
   _mm512_mask_compressstoreu_pd(aligned, mask.raw, v.raw);
-  return CountTrue(mask);
+  return CountTrue(d, mask);
 }
 
 // ------------------------------ StoreInterleaved3 (CombineShiftRightBytes,
@@ -2970,7 +3181,7 @@ HWY_API void StoreInterleaved3(const Vec512<uint8_t> a, const Vec512<uint8_t> b,
       0x80, 2, 0x80, 0x80, 3, 0x80, 0x80, 4, 0x80, 0x80};
   const auto shuf_r0 = LoadDup128(d, tbl_r0);
   const auto shuf_g0 = LoadDup128(d, tbl_g0);  // cannot reuse r0 due to 5
-  const auto shuf_b0 = CombineShiftRightBytes<15>(shuf_g0, shuf_g0);
+  const auto shuf_b0 = CombineShiftRightBytes<15>(d, shuf_g0, shuf_g0);
   const auto r0 = TableLookupBytes(a, shuf_r0);  // 5..4..3..2..1..0
   const auto g0 = TableLookupBytes(b, shuf_g0);  // ..4..3..2..1..0.
   const auto b0 = TableLookupBytes(c, shuf_b0);  // .4..3..2..1..0..
@@ -3015,17 +3226,19 @@ HWY_API void StoreInterleaved3(const Vec512<uint8_t> a, const Vec512<uint8_t> b,
 HWY_API void StoreInterleaved4(const Vec512<uint8_t> v0,
                                const Vec512<uint8_t> v1,
                                const Vec512<uint8_t> v2,
-                               const Vec512<uint8_t> v3, Full512<uint8_t> d,
+                               const Vec512<uint8_t> v3, Full512<uint8_t> d8,
                                uint8_t* HWY_RESTRICT unaligned) {
+  const RepartitionToWide<decltype(d8)> d16;
+  const RepartitionToWide<decltype(d16)> d32;
   // let a,b,c,d denote v0..3.
-  const auto ba0 = ZipLower(v0, v1);  // b7 a7 .. b0 a0
-  const auto dc0 = ZipLower(v2, v3);  // d7 c7 .. d0 c0
-  const auto ba8 = ZipUpper(v0, v1);
-  const auto dc8 = ZipUpper(v2, v3);
-  const auto i = ZipLower(ba0, dc0).raw;  // 4x128bit: d..a3 d..a0
-  const auto j = ZipUpper(ba0, dc0).raw;  // 4x128bit: d..a7 d..a4
-  const auto k = ZipLower(ba8, dc8).raw;  // 4x128bit: d..aB d..a8
-  const auto l = ZipUpper(ba8, dc8).raw;  // 4x128bit: d..aF d..aC
+  const auto ba0 = ZipLower(d16, v0, v1);  // b7 a7 .. b0 a0
+  const auto dc0 = ZipLower(d16, v2, v3);  // d7 c7 .. d0 c0
+  const auto ba8 = ZipUpper(d16, v0, v1);
+  const auto dc8 = ZipUpper(d16, v2, v3);
+  const auto i = ZipLower(d32, ba0, dc0).raw;  // 4x128bit: d..a3 d..a0
+  const auto j = ZipUpper(d32, ba0, dc0).raw;  // 4x128bit: d..a7 d..a4
+  const auto k = ZipLower(d32, ba8, dc8).raw;  // 4x128bit: d..aB d..a8
+  const auto l = ZipUpper(d32, ba8, dc8).raw;  // 4x128bit: d..aF d..aC
   // 128-bit blocks were independent until now; transpose 4x4.
   const auto j1_j0_i1_i0 = _mm512_shuffle_i64x2(i, j, _MM_SHUFFLE(1, 0, 1, 0));
   const auto l1_l0_k1_k0 = _mm512_shuffle_i64x2(k, l, _MM_SHUFFLE(1, 0, 1, 0));
@@ -3037,74 +3250,227 @@ HWY_API void StoreInterleaved4(const Vec512<uint8_t> v0,
   const auto l1_k1_j1_i1 = _mm512_shuffle_i64x2(j1_j0_i1_i0, l1_l0_k1_k0, k31);
   const auto l2_k2_j2_i2 = _mm512_shuffle_i64x2(j3_j2_i3_i2, l3_l2_k3_k2, k20);
   const auto l3_k3_j3_i3 = _mm512_shuffle_i64x2(j3_j2_i3_i2, l3_l2_k3_k2, k31);
-  StoreU(Vec512<uint8_t>{l0_k0_j0_i0}, d, unaligned + 0 * 64);
-  StoreU(Vec512<uint8_t>{l1_k1_j1_i1}, d, unaligned + 1 * 64);
-  StoreU(Vec512<uint8_t>{l2_k2_j2_i2}, d, unaligned + 2 * 64);
-  StoreU(Vec512<uint8_t>{l3_k3_j3_i3}, d, unaligned + 3 * 64);
+  StoreU(Vec512<uint8_t>{l0_k0_j0_i0}, d8, unaligned + 0 * 64);
+  StoreU(Vec512<uint8_t>{l1_k1_j1_i1}, d8, unaligned + 1 * 64);
+  StoreU(Vec512<uint8_t>{l2_k2_j2_i2}, d8, unaligned + 2 * 64);
+  StoreU(Vec512<uint8_t>{l3_k3_j3_i3}, d8, unaligned + 3 * 64);
+}
+
+// ------------------------------ MulEven/Odd (Shuffle2301, InterleaveLower)
+
+HWY_INLINE Vec512<uint64_t> MulEven(const Vec512<uint64_t> a,
+                                    const Vec512<uint64_t> b) {
+  const DFromV<decltype(a)> du64;
+  const RepartitionToNarrow<decltype(du64)> du32;
+  const auto maskL = Set(du64, 0xFFFFFFFFULL);
+  const auto a32 = BitCast(du32, a);
+  const auto b32 = BitCast(du32, b);
+  // Inputs for MulEven: we only need the lower 32 bits
+  const auto aH = Shuffle2301(a32);
+  const auto bH = Shuffle2301(b32);
+
+  // Knuth double-word multiplication. We use 32x32 = 64 MulEven and only need
+  // the even (lower 64 bits of every 128-bit block) results. See
+  // https://github.com/hcs0/Hackers-Delight/blob/master/muldwu.c.tat
+  const auto aLbL = MulEven(a32, b32);
+  const auto w3 = aLbL & maskL;
+
+  const auto t2 = MulEven(aH, b32) + ShiftRight<32>(aLbL);
+  const auto w2 = t2 & maskL;
+  const auto w1 = ShiftRight<32>(t2);
+
+  const auto t = MulEven(a32, bH) + w2;
+  const auto k = ShiftRight<32>(t);
+
+  const auto mulH = MulEven(aH, bH) + w1 + k;
+  const auto mulL = ShiftLeft<32>(t) + w3;
+  return InterleaveLower(mulL, mulH);
+}
+
+HWY_INLINE Vec512<uint64_t> MulOdd(const Vec512<uint64_t> a,
+                                   const Vec512<uint64_t> b) {
+  const DFromV<decltype(a)> du64;
+  const RepartitionToNarrow<decltype(du64)> du32;
+  const auto maskL = Set(du64, 0xFFFFFFFFULL);
+  const auto a32 = BitCast(du32, a);
+  const auto b32 = BitCast(du32, b);
+  // Inputs for MulEven: we only need bits [95:64] (= upper half of input)
+  const auto aH = Shuffle2301(a32);
+  const auto bH = Shuffle2301(b32);
+
+  // Same as above, but we're using the odd results (upper 64 bits per block).
+  const auto aLbL = MulEven(a32, b32);
+  const auto w3 = aLbL & maskL;
+
+  const auto t2 = MulEven(aH, b32) + ShiftRight<32>(aLbL);
+  const auto w2 = t2 & maskL;
+  const auto w1 = ShiftRight<32>(t2);
+
+  const auto t = MulEven(a32, bH) + w2;
+  const auto k = ShiftRight<32>(t);
+
+  const auto mulH = MulEven(aH, bH) + w1 + k;
+  const auto mulL = ShiftLeft<32>(t) + w3;
+  return InterleaveUpper(du64, mulL, mulH);
 }
 
 // ------------------------------ Reductions
 
 // Returns the sum in each lane.
-HWY_API Vec512<int32_t> SumOfLanes(const Vec512<int32_t> v) {
-  return Set(Full512<int32_t>(), _mm512_reduce_add_epi32(v.raw));
+HWY_API Vec512<int32_t> SumOfLanes(Full512<int32_t> d, Vec512<int32_t> v) {
+  return Set(d, _mm512_reduce_add_epi32(v.raw));
 }
-HWY_API Vec512<int64_t> SumOfLanes(const Vec512<int64_t> v) {
-  return Set(Full512<int64_t>(), _mm512_reduce_add_epi64(v.raw));
+HWY_API Vec512<int64_t> SumOfLanes(Full512<int64_t> d, Vec512<int64_t> v) {
+  return Set(d, _mm512_reduce_add_epi64(v.raw));
 }
-HWY_API Vec512<uint32_t> SumOfLanes(const Vec512<uint32_t> v) {
-  return BitCast(Full512<uint32_t>(),
-                 SumOfLanes(BitCast(Full512<int32_t>(), v)));
+HWY_API Vec512<uint32_t> SumOfLanes(Full512<uint32_t> d, Vec512<uint32_t> v) {
+  return Set(d, _mm512_reduce_add_epi32(v.raw));
 }
-HWY_API Vec512<uint64_t> SumOfLanes(const Vec512<uint64_t> v) {
-  return BitCast(Full512<uint64_t>(),
-                 SumOfLanes(BitCast(Full512<int64_t>(), v)));
+HWY_API Vec512<uint64_t> SumOfLanes(Full512<uint64_t> d, Vec512<uint64_t> v) {
+  return Set(d, _mm512_reduce_add_epi64(v.raw));
 }
-HWY_API Vec512<float> SumOfLanes(const Vec512<float> v) {
-  return Set(Full512<float>(), _mm512_reduce_add_ps(v.raw));
+HWY_API Vec512<float> SumOfLanes(Full512<float> d, Vec512<float> v) {
+  return Set(d, _mm512_reduce_add_ps(v.raw));
 }
-HWY_API Vec512<double> SumOfLanes(const Vec512<double> v) {
-  return Set(Full512<double>(), _mm512_reduce_add_pd(v.raw));
+HWY_API Vec512<double> SumOfLanes(Full512<double> d, Vec512<double> v) {
+  return Set(d, _mm512_reduce_add_pd(v.raw));
 }
 
 // Returns the minimum in each lane.
-HWY_API Vec512<int32_t> MinOfLanes(const Vec512<int32_t> v) {
-  return Set(Full512<int32_t>(), _mm512_reduce_min_epi32(v.raw));
+HWY_API Vec512<int32_t> MinOfLanes(Full512<int32_t> d, Vec512<int32_t> v) {
+  return Set(d, _mm512_reduce_min_epi32(v.raw));
 }
-HWY_API Vec512<int64_t> MinOfLanes(const Vec512<int64_t> v) {
-  return Set(Full512<int64_t>(), _mm512_reduce_min_epi64(v.raw));
+HWY_API Vec512<int64_t> MinOfLanes(Full512<int64_t> d, Vec512<int64_t> v) {
+  return Set(d, _mm512_reduce_min_epi64(v.raw));
 }
-HWY_API Vec512<uint32_t> MinOfLanes(const Vec512<uint32_t> v) {
-  return Set(Full512<uint32_t>(), _mm512_reduce_min_epu32(v.raw));
+HWY_API Vec512<uint32_t> MinOfLanes(Full512<uint32_t> d, Vec512<uint32_t> v) {
+  return Set(d, _mm512_reduce_min_epu32(v.raw));
 }
-HWY_API Vec512<uint64_t> MinOfLanes(const Vec512<uint64_t> v) {
-  return Set(Full512<uint64_t>(), _mm512_reduce_min_epu64(v.raw));
+HWY_API Vec512<uint64_t> MinOfLanes(Full512<uint64_t> d, Vec512<uint64_t> v) {
+  return Set(d, _mm512_reduce_min_epu64(v.raw));
 }
-HWY_API Vec512<float> MinOfLanes(const Vec512<float> v) {
-  return Set(Full512<float>(), _mm512_reduce_min_ps(v.raw));
+HWY_API Vec512<float> MinOfLanes(Full512<float> d, Vec512<float> v) {
+  return Set(d, _mm512_reduce_min_ps(v.raw));
 }
-HWY_API Vec512<double> MinOfLanes(const Vec512<double> v) {
-  return Set(Full512<double>(), _mm512_reduce_min_pd(v.raw));
+HWY_API Vec512<double> MinOfLanes(Full512<double> d, Vec512<double> v) {
+  return Set(d, _mm512_reduce_min_pd(v.raw));
 }
 
 // Returns the maximum in each lane.
-HWY_API Vec512<int32_t> MaxOfLanes(const Vec512<int32_t> v) {
-  return Set(Full512<int32_t>(), _mm512_reduce_max_epi32(v.raw));
+HWY_API Vec512<int32_t> MaxOfLanes(Full512<int32_t> d, Vec512<int32_t> v) {
+  return Set(d, _mm512_reduce_max_epi32(v.raw));
+}
+HWY_API Vec512<int64_t> MaxOfLanes(Full512<int64_t> d, Vec512<int64_t> v) {
+  return Set(d, _mm512_reduce_max_epi64(v.raw));
+}
+HWY_API Vec512<uint32_t> MaxOfLanes(Full512<uint32_t> d, Vec512<uint32_t> v) {
+  return Set(d, _mm512_reduce_max_epu32(v.raw));
+}
+HWY_API Vec512<uint64_t> MaxOfLanes(Full512<uint64_t> d, Vec512<uint64_t> v) {
+  return Set(d, _mm512_reduce_max_epu64(v.raw));
+}
+HWY_API Vec512<float> MaxOfLanes(Full512<float> d, Vec512<float> v) {
+  return Set(d, _mm512_reduce_max_ps(v.raw));
+}
+HWY_API Vec512<double> MaxOfLanes(Full512<double> d, Vec512<double> v) {
+  return Set(d, _mm512_reduce_max_pd(v.raw));
+}
+
+// ================================================== DEPRECATED
+
+template <typename T>
+HWY_API size_t StoreMaskBits(const Mask512<T> mask, uint8_t* p) {
+  return StoreMaskBits(Full512<T>(), mask, p);
+}
+
+template <typename T>
+HWY_API bool AllTrue(const Mask512<T> mask) {
+  return AllTrue(Full512<T>(), mask);
+}
+
+template <typename T>
+HWY_API bool AllFalse(const Mask512<T> mask) {
+  return AllFalse(Full512<T>(), mask);
+}
+
+template <typename T>
+HWY_API size_t CountTrue(const Mask512<T> mask) {
+  return CountTrue(Full512<T>(), mask);
+}
+
+template <typename T>
+HWY_API Vec512<T> SumOfLanes(Vec512<T> v) {
+  return SumOfLanes(Full512<T>(), v);
+}
+
+template <typename T>
+HWY_API Vec512<T> MinOfLanes(Vec512<T> v) {
+  return MinOfLanes(Full512<T>(), v);
+}
+
+template <typename T>
+HWY_API Vec512<T> MaxOfLanes(Vec512<T> v) {
+  return MaxOfLanes(Full512<T>(), v);
+}
+
+template <typename T>
+HWY_API Vec256<T> UpperHalf(Vec512<T> v) {
+  return UpperHalf(Full256<T>(), v);
+}
+
+template <int kBytes, typename T>
+HWY_API Vec512<T> ShiftRightBytes(const Vec512<T> v) {
+  return ShiftRightBytes<kBytes>(Full512<T>(), v);
+}
+
+template <int kLanes, typename T>
+HWY_API Vec512<T> ShiftRightLanes(const Vec512<T> v) {
+  return ShiftRightBytes<kLanes>(Full512<T>(), v);
+}
+
+template <size_t kBytes, typename T>
+HWY_API Vec512<T> CombineShiftRightBytes(Vec512<T> hi, Vec512<T> lo) {
+  return CombineShiftRightBytes<kBytes>(Full512<T>(), hi, lo);
+}
+
+template <typename T>
+HWY_API Vec512<T> InterleaveUpper(Vec512<T> a, Vec512<T> b) {
+  return InterleaveUpper(Full512<T>(), a, b);
+}
+
+template <typename T>
+HWY_API Vec512<MakeWide<T>> ZipUpper(Vec512<T> a, Vec512<T> b) {
+  return InterleaveUpper(Full512<MakeWide<T>>(), a, b);
 }
-HWY_API Vec512<int64_t> MaxOfLanes(const Vec512<int64_t> v) {
-  return Set(Full512<int64_t>(), _mm512_reduce_max_epi64(v.raw));
+
+template <typename T>
+HWY_API Vec512<T> Combine(Vec256<T> hi, Vec256<T> lo) {
+  return Combine(Full512<T>(), hi, lo);
+}
+
+template <typename T>
+HWY_API Vec512<T> ZeroExtendVector(Vec256<T> lo) {
+  return ZeroExtendVector(Full512<T>(), lo);
 }
-HWY_API Vec512<uint32_t> MaxOfLanes(const Vec512<uint32_t> v) {
-  return Set(Full512<uint32_t>(), _mm512_reduce_max_epu32(v.raw));
+
+template <typename T>
+HWY_API Vec512<T> ConcatLowerLower(Vec512<T> hi, Vec512<T> lo) {
+  return ConcatLowerLower(Full512<T>(), hi, lo);
 }
-HWY_API Vec512<uint64_t> MaxOfLanes(const Vec512<uint64_t> v) {
-  return Set(Full512<uint64_t>(), _mm512_reduce_max_epu64(v.raw));
+
+template <typename T>
+HWY_API Vec512<T> ConcatLowerUpper(Vec512<T> hi, Vec512<T> lo) {
+  return ConcatLowerUpper(Full512<T>(), hi, lo);
 }
-HWY_API Vec512<float> MaxOfLanes(const Vec512<float> v) {
-  return Set(Full512<float>(), _mm512_reduce_max_ps(v.raw));
+
+template <typename T>
+HWY_API Vec512<T> ConcatUpperLower(Vec512<T> hi, Vec512<T> lo) {
+  return ConcatUpperLower(Full512<T>(), hi, lo);
 }
-HWY_API Vec512<double> MaxOfLanes(const Vec512<double> v) {
-  return Set(Full512<double>(), _mm512_reduce_max_pd(v.raw));
+
+template <typename T>
+HWY_API Vec512<T> ConcatUpperUpper(Vec512<T> hi, Vec512<T> lo) {
+  return ConcatUpperUpper(Full512<T>(), hi, lo);
 }
 
 // NOLINTNEXTLINE(google-readability-namespace-comments)
diff --git a/third_party/highway/hwy/targets.cc b/third_party/highway/hwy/targets.cc
index f910ccd07c552..c0b1c1ac0d562 100644
--- a/third_party/highway/hwy/targets.cc
+++ b/third_party/highway/hwy/targets.cc
@@ -100,9 +100,12 @@ constexpr uint32_t kSSE = 1 << 0;
 constexpr uint32_t kSSE2 = 1 << 1;
 constexpr uint32_t kSSE3 = 1 << 2;
 constexpr uint32_t kSSSE3 = 1 << 3;
+constexpr uint32_t kGroupSSSE3 = kSSE | kSSE2 | kSSE3 | kSSSE3;
+
 constexpr uint32_t kSSE41 = 1 << 4;
 constexpr uint32_t kSSE42 = 1 << 5;
-constexpr uint32_t kGroupSSE4 = kSSE | kSSE2 | kSSE3 | kSSSE3 | kSSE41 | kSSE42;
+constexpr uint32_t kCLMUL = 1 << 6;
+constexpr uint32_t kGroupSSE4 = kSSE41 | kSSE42 | kCLMUL | kGroupSSSE3;
 
 constexpr uint32_t kAVX = 1u << 6;
 constexpr uint32_t kAVX2 = 1u << 7;
@@ -116,16 +119,26 @@ constexpr uint32_t kBMI2 = 1u << 11;
 // [https://www.virtualbox.org/ticket/15471]. Thus we provide the option of
 // avoiding using and requiring these so AVX2 can still be used.
 #ifdef HWY_DISABLE_BMI2_FMA
-constexpr uint32_t kGroupAVX2 = kAVX | kAVX2 | kLZCNT;
+constexpr uint32_t kGroupAVX2 = kAVX | kAVX2 | kLZCNT | kGroupSSE4;
 #else
-constexpr uint32_t kGroupAVX2 = kAVX | kAVX2 | kFMA | kLZCNT | kBMI | kBMI2;
+constexpr uint32_t kGroupAVX2 =
+    kAVX | kAVX2 | kFMA | kLZCNT | kBMI | kBMI2 | kGroupSSE4;
 #endif
 
 constexpr uint32_t kAVX512F = 1u << 12;
 constexpr uint32_t kAVX512VL = 1u << 13;
 constexpr uint32_t kAVX512DQ = 1u << 14;
 constexpr uint32_t kAVX512BW = 1u << 15;
-constexpr uint32_t kGroupAVX3 = kAVX512F | kAVX512VL | kAVX512DQ | kAVX512BW;
+constexpr uint32_t kGroupAVX3 =
+    kAVX512F | kAVX512VL | kAVX512DQ | kAVX512BW | kGroupAVX2;
+
+constexpr uint32_t kVNNI = 1u << 16;
+constexpr uint32_t kVPCLMULQDQ = 1u << 17;
+constexpr uint32_t kVAES = 1u << 18;
+constexpr uint32_t kPOPCNTDQ = 1u << 19;
+constexpr uint32_t kBITALG = 1u << 20;
+constexpr uint32_t kGroupAVX3_DL =
+    kVNNI | kVPCLMULQDQ | kVAES | kPOPCNTDQ | kBITALG | kGroupAVX3;
 #endif  // HWY_ARCH_X86
 
 }  // namespace
@@ -193,69 +206,86 @@ uint32_t SupportedTargets() {
   bits = HWY_SCALAR;
 
 #if HWY_ARCH_X86
-  uint32_t flags = 0;
-  uint32_t abcd[4];
-
-  Cpuid(0, 0, abcd);
-  const uint32_t max_level = abcd[0];
-
-  // Standard feature flags
-  Cpuid(1, 0, abcd);
-  flags |= IsBitSet(abcd[3], 25) ? kSSE : 0;
-  flags |= IsBitSet(abcd[3], 26) ? kSSE2 : 0;
-  flags |= IsBitSet(abcd[2], 0) ? kSSE3 : 0;
-  flags |= IsBitSet(abcd[2], 9) ? kSSSE3 : 0;
-  flags |= IsBitSet(abcd[2], 19) ? kSSE41 : 0;
-  flags |= IsBitSet(abcd[2], 20) ? kSSE42 : 0;
-  flags |= IsBitSet(abcd[2], 12) ? kFMA : 0;
-  flags |= IsBitSet(abcd[2], 28) ? kAVX : 0;
-  const bool has_osxsave = IsBitSet(abcd[2], 27);
-
-  // Extended feature flags
-  Cpuid(0x80000001U, 0, abcd);
-  flags |= IsBitSet(abcd[2], 5) ? kLZCNT : 0;
-
-  // Extended features
-  if (max_level >= 7) {
-    Cpuid(7, 0, abcd);
-    flags |= IsBitSet(abcd[1], 3) ? kBMI : 0;
-    flags |= IsBitSet(abcd[1], 5) ? kAVX2 : 0;
-    flags |= IsBitSet(abcd[1], 8) ? kBMI2 : 0;
-
-    flags |= IsBitSet(abcd[1], 16) ? kAVX512F : 0;
-    flags |= IsBitSet(abcd[1], 17) ? kAVX512DQ : 0;
-    flags |= IsBitSet(abcd[1], 30) ? kAVX512BW : 0;
-    flags |= IsBitSet(abcd[1], 31) ? kAVX512VL : 0;
+  bool has_osxsave = false;
+  {  // ensures we do not accidentally use flags outside this block
+    uint32_t flags = 0;
+    uint32_t abcd[4];
+
+    Cpuid(0, 0, abcd);
+    const uint32_t max_level = abcd[0];
+
+    // Standard feature flags
+    Cpuid(1, 0, abcd);
+    flags |= IsBitSet(abcd[3], 25) ? kSSE : 0;
+    flags |= IsBitSet(abcd[3], 26) ? kSSE2 : 0;
+    flags |= IsBitSet(abcd[2], 0) ? kSSE3 : 0;
+    flags |= IsBitSet(abcd[2], 1) ? kCLMUL : 0;
+    flags |= IsBitSet(abcd[2], 9) ? kSSSE3 : 0;
+    flags |= IsBitSet(abcd[2], 19) ? kSSE41 : 0;
+    flags |= IsBitSet(abcd[2], 20) ? kSSE42 : 0;
+    flags |= IsBitSet(abcd[2], 12) ? kFMA : 0;
+    flags |= IsBitSet(abcd[2], 28) ? kAVX : 0;
+    has_osxsave = IsBitSet(abcd[2], 27);
+
+    // Extended feature flags
+    Cpuid(0x80000001U, 0, abcd);
+    flags |= IsBitSet(abcd[2], 5) ? kLZCNT : 0;
+
+    // Extended features
+    if (max_level >= 7) {
+      Cpuid(7, 0, abcd);
+      flags |= IsBitSet(abcd[1], 3) ? kBMI : 0;
+      flags |= IsBitSet(abcd[1], 5) ? kAVX2 : 0;
+      flags |= IsBitSet(abcd[1], 8) ? kBMI2 : 0;
+
+      flags |= IsBitSet(abcd[1], 16) ? kAVX512F : 0;
+      flags |= IsBitSet(abcd[1], 17) ? kAVX512DQ : 0;
+      flags |= IsBitSet(abcd[1], 30) ? kAVX512BW : 0;
+      flags |= IsBitSet(abcd[1], 31) ? kAVX512VL : 0;
+
+      flags |= IsBitSet(abcd[2], 9) ? kVAES : 0;
+      flags |= IsBitSet(abcd[2], 10) ? kVPCLMULQDQ : 0;
+      flags |= IsBitSet(abcd[2], 11) ? kVNNI : 0;
+      flags |= IsBitSet(abcd[2], 12) ? kBITALG : 0;
+      flags |= IsBitSet(abcd[2], 14) ? kPOPCNTDQ : 0;
+    }
+
+    // Set target bit(s) if all their group's flags are all set.
+    if ((flags & kGroupAVX3_DL) == kGroupAVX3_DL) {
+      bits |= HWY_AVX3_DL;
+    }
+    if ((flags & kGroupAVX3) == kGroupAVX3) {
+      bits |= HWY_AVX3;
+    }
+    if ((flags & kGroupAVX2) == kGroupAVX2) {
+      bits |= HWY_AVX2;
+    }
+    if ((flags & kGroupSSE4) == kGroupSSE4) {
+      bits |= HWY_SSE4;
+    }
+    if ((flags & kGroupSSSE3) == kGroupSSSE3) {
+      bits |= HWY_SSSE3;
+    }
   }
 
-  // Verify OS support for XSAVE, without which XMM/YMM registers are not
-  // preserved across context switches and are not safe to use.
+  // Clear bits if the OS does not support XSAVE - otherwise, registers
+  // are not preserved across context switches.
   if (has_osxsave) {
     const uint32_t xcr0 = ReadXCR0();
     // XMM
     if (!IsBitSet(xcr0, 1)) {
-      flags = 0;
+      bits &= ~(HWY_SSSE3 | HWY_SSE4 | HWY_AVX2 | HWY_AVX3 | HWY_AVX3_DL);
     }
     // YMM
     if (!IsBitSet(xcr0, 2)) {
-      flags &= ~kGroupAVX2;
+      bits &= ~(HWY_AVX2 | HWY_AVX3 | HWY_AVX3_DL);
     }
     // ZMM + opmask
     if ((xcr0 & 0x70) != 0x70) {
-      flags &= ~kGroupAVX3;
+      bits &= ~(HWY_AVX3 | HWY_AVX3_DL);
     }
   }
 
-  // Set target bit(s) if all their group's flags are all set.
-  if ((flags & kGroupAVX3) == kGroupAVX3) {
-    bits |= HWY_AVX3;
-  }
-  if ((flags & kGroupAVX2) == kGroupAVX2) {
-    bits |= HWY_AVX2;
-  }
-  if ((flags & kGroupSSE4) == kGroupSSE4) {
-    bits |= HWY_SSE4;
-  }
 #else
   // TODO(janwas): detect for other platforms
   bits = HWY_ENABLED_BASELINE;
diff --git a/third_party/highway/hwy/targets.h b/third_party/highway/hwy/targets.h
index 5f0195317d834..b9e8984c2c853 100644
--- a/third_party/highway/hwy/targets.h
+++ b/third_party/highway/hwy/targets.h
@@ -21,282 +21,23 @@
 // generate and call.
 
 #include "hwy/base.h"
-
-//------------------------------------------------------------------------------
-// Optional configuration
-
-// See ../quick_reference.md for documentation of these macros.
-
-// Uncomment to override the default baseline determined from predefined macros:
-// #define HWY_BASELINE_TARGETS (HWY_SSE4 | HWY_SCALAR)
-
-// Uncomment to override the default blocklist:
-// #define HWY_BROKEN_TARGETS HWY_AVX3
-
-// Uncomment to definitely avoid generating those target(s):
-// #define HWY_DISABLED_TARGETS HWY_SSE4
-
-// Uncomment to avoid emitting BMI/BMI2/FMA instructions (allows generating
-// AVX2 target for VMs which support AVX2 but not the other instruction sets)
-// #define HWY_DISABLE_BMI2_FMA
-
-//------------------------------------------------------------------------------
-// Targets
-
-// Unique bit value for each target. A lower value is "better" (e.g. more lanes)
-// than a higher value within the same group/platform - see HWY_STATIC_TARGET.
-//
-// All values are unconditionally defined so we can test HWY_TARGETS without
-// first checking the HWY_ARCH_*.
-//
-// The C99 preprocessor evaluates #if expressions using intmax_t types, so we
-// can use 32-bit literals.
-
-// 1,2,4: reserved
-#define HWY_AVX3 8
-#define HWY_AVX2 16
-// 32: reserved for AVX
-#define HWY_SSE4 64
-// 0x80, 0x100, 0x200: reserved for SSSE3, SSE3, SSE2
-
-// The highest bit in the HWY_TARGETS mask that a x86 target can have. Used for
-// dynamic dispatch. All x86 target bits must be lower or equal to
-// (1 << HWY_HIGHEST_TARGET_BIT_X86) and they can only use
-// HWY_MAX_DYNAMIC_TARGETS in total.
-#define HWY_HIGHEST_TARGET_BIT_X86 9
-
-#define HWY_SVE2 0x400
-#define HWY_SVE 0x800
-// 0x1000 reserved for Helium
-#define HWY_NEON 0x2000
-
-#define HWY_HIGHEST_TARGET_BIT_ARM 13
-
-// 0x4000, 0x8000 reserved
-#define HWY_PPC8 0x10000  // v2.07 or 3
-// 0x20000, 0x40000 reserved for prior VSX/AltiVec
-
-#define HWY_HIGHEST_TARGET_BIT_PPC 18
-
-// 0x80000 reserved
-#define HWY_WASM 0x100000
-
-#define HWY_HIGHEST_TARGET_BIT_WASM 20
-
-// 0x200000, 0x400000, 0x800000 reserved
-
-#define HWY_RVV 0x1000000
-
-#define HWY_HIGHEST_TARGET_BIT_RVV 24
-
-// 0x2000000, 0x4000000, 0x8000000, 0x10000000 reserved
-
-#define HWY_SCALAR 0x20000000
-
-#define HWY_HIGHEST_TARGET_BIT_SCALAR 29
-
-// Cannot use higher values, otherwise HWY_TARGETS computation might overflow.
-
-//------------------------------------------------------------------------------
-// Set default blocklists
-
-// Disabled means excluded from enabled at user's request. A separate config
-// macro allows disabling without deactivating the blocklist below.
-#ifndef HWY_DISABLED_TARGETS
-#define HWY_DISABLED_TARGETS 0
-#endif
-
-// Broken means excluded from enabled due to known compiler issues. Allow the
-// user to override this blocklist without any guarantee of success.
-#ifndef HWY_BROKEN_TARGETS
-
-// x86 clang-6: we saw multiple AVX2/3 compile errors and in one case invalid
-// SSE4 codegen (possibly only for msan), so disable all those targets.
-#if HWY_ARCH_X86 && (HWY_COMPILER_CLANG != 0 && HWY_COMPILER_CLANG < 700)
-#define HWY_BROKEN_TARGETS (HWY_SSE4 | HWY_AVX2 | HWY_AVX3)
-// This entails a major speed reduction, so warn unless the user explicitly
-// opts in to scalar-only.
-#if !defined(HWY_COMPILE_ONLY_SCALAR)
-#pragma message("x86 Clang <= 6: define HWY_COMPILE_ONLY_SCALAR or upgrade.")
-#endif
-
-// 32-bit may fail to compile AVX2/3.
-#elif HWY_ARCH_X86_32
-#define HWY_BROKEN_TARGETS (HWY_AVX2 | HWY_AVX3)
-
-// MSVC AVX3 support is buggy: https://github.com/Mysticial/Flops/issues/16
-#elif HWY_COMPILER_MSVC != 0
-#define HWY_BROKEN_TARGETS (HWY_AVX3)
-
-// armv7be has not been tested and is not yet supported.
-#elif HWY_ARCH_ARM_V7 && (defined(__ARM_BIG_ENDIAN) || defined(__BIG_ENDIAN))
-#define HWY_BROKEN_TARGETS (HWY_NEON)
-
-#else
-#define HWY_BROKEN_TARGETS 0
-#endif
-
-#endif  // HWY_BROKEN_TARGETS
-
-// Enabled means not disabled nor blocklisted.
-#define HWY_ENABLED(targets) \
-  ((targets) & ~((HWY_DISABLED_TARGETS) | (HWY_BROKEN_TARGETS)))
-
-//------------------------------------------------------------------------------
-// Detect baseline targets using predefined macros
-
-// Baseline means the targets for which the compiler is allowed to generate
-// instructions, implying the target CPU would have to support them. Do not use
-// this directly because it does not take the blocklist into account. Allow the
-// user to override this without any guarantee of success.
-#ifndef HWY_BASELINE_TARGETS
-
-// Also check HWY_ARCH to ensure that simulating unknown platforms ends up with
-// HWY_TARGET == HWY_SCALAR.
-
-#if HWY_ARCH_WASM && defined(__wasm_simd128__)
-#define HWY_BASELINE_WASM HWY_WASM
-#else
-#define HWY_BASELINE_WASM 0
-#endif
-
-// Avoid choosing the PPC target until we have an implementation.
-#if HWY_ARCH_PPC && defined(__VSX__) && 0
-#define HWY_BASELINE_PPC8 HWY_PPC8
-#else
-#define HWY_BASELINE_PPC8 0
-#endif
-
-// Avoid choosing the SVE[2] targets the implementation is ready.
-#if HWY_ARCH_ARM && defined(__ARM_FEATURE_SVE2) && 0
-#define HWY_BASELINE_SVE2 HWY_SVE2
-#else
-#define HWY_BASELINE_SVE2 0
-#endif
-
-#if HWY_ARCH_ARM && defined(__ARM_FEATURE_SVE) && 0
-#define HWY_BASELINE_SVE HWY_SVE
-#else
-#define HWY_BASELINE_SVE 0
-#endif
-
-// GCC 4.5.4 only defines __ARM_NEON__; 5.4 defines both.
-#if HWY_ARCH_ARM && (defined(__ARM_NEON__) || defined(__ARM_NEON))
-#define HWY_BASELINE_NEON HWY_NEON
-#else
-#define HWY_BASELINE_NEON 0
-#endif
-
-// MSVC does not set SSE4_1, but it does set AVX; checking for the latter means
-// we at least get SSE4 on machines supporting AVX but not AVX2.
-// https://stackoverflow.com/questions/18563978/
-#if HWY_ARCH_X86 && \
-    (defined(__SSE4_1__) || (HWY_COMPILER_MSVC != 0 && defined(__AVX__)))
-#define HWY_BASELINE_SSE4 HWY_SSE4
-#else
-#define HWY_BASELINE_SSE4 0
-#endif
-
-#if HWY_ARCH_X86 && defined(__AVX2__)
-#define HWY_BASELINE_AVX2 HWY_AVX2
-#else
-#define HWY_BASELINE_AVX2 0
-#endif
-
-#if HWY_ARCH_X86 && defined(__AVX512F__)
-#define HWY_BASELINE_AVX3 HWY_AVX3
-#else
-#define HWY_BASELINE_AVX3 0
-#endif
-
-#if HWY_ARCH_RVV && defined(__riscv_vector)
-#define HWY_BASELINE_RVV HWY_RVV
-#else
-#define HWY_BASELINE_RVV 0
-#endif
-
-#define HWY_BASELINE_TARGETS                                                \
-  (HWY_SCALAR | HWY_BASELINE_WASM | HWY_BASELINE_PPC8 | HWY_BASELINE_SVE2 | \
-   HWY_BASELINE_SVE | HWY_BASELINE_NEON | HWY_BASELINE_SSE4 |               \
-   HWY_BASELINE_AVX2 | HWY_BASELINE_AVX3 | HWY_BASELINE_RVV)
-
-#endif  // HWY_BASELINE_TARGETS
-
-//------------------------------------------------------------------------------
-// Choose target for static dispatch
-
-#define HWY_ENABLED_BASELINE HWY_ENABLED(HWY_BASELINE_TARGETS)
-#if HWY_ENABLED_BASELINE == 0
-#error "At least one baseline target must be defined and enabled"
-#endif
-
-// Best baseline, used for static dispatch. This is the least-significant 1-bit
-// within HWY_ENABLED_BASELINE and lower bit values imply "better".
-#define HWY_STATIC_TARGET (HWY_ENABLED_BASELINE & -HWY_ENABLED_BASELINE)
-
-// Start by assuming static dispatch. If we later use dynamic dispatch, this
-// will be defined to other targets during the multiple-inclusion, and finally
-// return to the initial value. Defining this outside begin/end_target ensures
-// inl headers successfully compile by themselves (required by Bazel).
-#define HWY_TARGET HWY_STATIC_TARGET
-
-//------------------------------------------------------------------------------
-// Choose targets for dynamic dispatch according to one of four policies
-
-#if (defined(HWY_COMPILE_ONLY_SCALAR) + defined(HWY_COMPILE_ONLY_STATIC) + \
-     defined(HWY_COMPILE_ALL_ATTAINABLE)) > 1
-#error "Invalid config: can only define a single policy for targets"
-#endif
-
-// Attainable means enabled and the compiler allows intrinsics (even when not
-// allowed to autovectorize). Used in 3 and 4.
-#if HWY_ARCH_X86
-#define HWY_ATTAINABLE_TARGETS \
-  HWY_ENABLED(HWY_SCALAR | HWY_SSE4 | HWY_AVX2 | HWY_AVX3)
-#else
-#define HWY_ATTAINABLE_TARGETS HWY_ENABLED_BASELINE
-#endif
-
-// 1) For older compilers: disable all SIMD (could also set HWY_DISABLED_TARGETS
-// to ~HWY_SCALAR, but this is more explicit).
-#if defined(HWY_COMPILE_ONLY_SCALAR)
-#undef HWY_STATIC_TARGET
-#define HWY_STATIC_TARGET HWY_SCALAR  // override baseline
-#define HWY_TARGETS HWY_SCALAR
-
-// 2) For forcing static dispatch without code changes (removing HWY_EXPORT)
-#elif defined(HWY_COMPILE_ONLY_STATIC)
-#define HWY_TARGETS HWY_STATIC_TARGET
-
-// 3) For tests: include all attainable targets (in particular: scalar)
-#elif defined(HWY_COMPILE_ALL_ATTAINABLE)
-#define HWY_TARGETS HWY_ATTAINABLE_TARGETS
-
-// 4) Default: attainable WITHOUT non-best baseline. This reduces code size by
-// excluding superseded targets, in particular scalar.
-#else
-
-#define HWY_TARGETS (HWY_ATTAINABLE_TARGETS & (2 * HWY_STATIC_TARGET - 1))
-
-#endif  // target policy
-
-// HWY_ONCE and the multiple-inclusion mechanism rely on HWY_STATIC_TARGET being
-// one of the dynamic targets. This also implies HWY_TARGETS != 0 and
-// (HWY_TARGETS & HWY_ENABLED_BASELINE) != 0.
-#if (HWY_TARGETS & HWY_STATIC_TARGET) == 0
-#error "Logic error: best baseline should be included in dynamic targets"
-#endif
-
-//------------------------------------------------------------------------------
+#include "hwy/detect_targets.h"
 
 namespace hwy {
 
 // Returns (cached) bitfield of enabled targets that are supported on this CPU.
-// Implemented in supported_targets.cc; unconditionally compiled to support the
-// use case of binary-only distributions. The HWY_SUPPORTED_TARGETS wrapper may
-// allow eliding calls to this function.
+// Implemented in targets.cc; unconditionally compiled to support the use case
+// of binary-only distributions. The HWY_SUPPORTED_TARGETS wrapper may allow
+// eliding calls to this function.
 uint32_t SupportedTargets();
 
+// Evaluates to a function call, or literal if there is a single target.
+#if (HWY_TARGETS & (HWY_TARGETS - 1)) == 0
+#define HWY_SUPPORTED_TARGETS HWY_TARGETS
+#else
+#define HWY_SUPPORTED_TARGETS hwy::SupportedTargets()
+#endif
+
 // Disable from runtime dispatch the mask of compiled in targets. Targets that
 // were not enabled at compile time are ignored. This function is useful to
 // disable a target supported by the CPU that is known to have bugs or when a
@@ -305,17 +46,9 @@ uint32_t SupportedTargets();
 // returns at least the baseline target.
 void DisableTargets(uint32_t disabled_targets);
 
-// Single target: reduce code size by eliding the call and conditional branches
-// inside Choose*() functions.
-#if (HWY_TARGETS & (HWY_TARGETS - 1)) == 0
-#define HWY_SUPPORTED_TARGETS HWY_TARGETS
-#else
-#define HWY_SUPPORTED_TARGETS hwy::SupportedTargets()
-#endif
-
 // Set the mock mask of CPU supported targets instead of the actual CPU
 // supported targets computed in SupportedTargets(). The return value of
-// SupportedTargets() will still be affected by the DisabledTargets() mask
+// SupportedTargets() will still be affected by the DisableTargets() mask
 // regardless of this mock, to prevent accidentally adding targets that are
 // known to be buggy in the current CPU. Call with a mask of 0 to disable the
 // mock and use the actual CPU supported targets instead.
@@ -341,12 +74,16 @@ HWY_INLINE std::vector<uint32_t> SupportedAndGeneratedTargets() {
 static inline HWY_MAYBE_UNUSED const char* TargetName(uint32_t target) {
   switch (target) {
 #if HWY_ARCH_X86
+    case HWY_SSSE3:
+      return "SSSE3";
     case HWY_SSE4:
       return "SSE4";
     case HWY_AVX2:
       return "AVX2";
     case HWY_AVX3:
       return "AVX3";
+    case HWY_AVX3_DL:
+      return "AVX3_DL";
 #endif
 
 #if HWY_ARCH_ARM
@@ -424,17 +161,17 @@ static inline HWY_MAYBE_UNUSED const char* TargetName(uint32_t target) {
 // HWY_MAX_DYNAMIC_TARGETS) bit. This list must contain exactly
 // HWY_MAX_DYNAMIC_TARGETS elements and does not include SCALAR. The first entry
 // corresponds to the best target. Don't include a "," at the end of the list.
-#define HWY_CHOOSE_TARGET_LIST(func_name)        \
-  nullptr,                        /* reserved */ \
-      nullptr,                    /* reserved */ \
-      nullptr,                    /* reserved */ \
-      HWY_CHOOSE_AVX3(func_name), /* AVX3 */     \
-      HWY_CHOOSE_AVX2(func_name), /* AVX2 */     \
-      nullptr,                    /* AVX */      \
-      HWY_CHOOSE_SSE4(func_name), /* SSE4 */     \
-      nullptr,                    /* SSSE3 */    \
-      nullptr,                    /* SSE3 */     \
-      nullptr                     /* SSE2 */
+#define HWY_CHOOSE_TARGET_LIST(func_name)           \
+  nullptr,                           /* reserved */ \
+      nullptr,                       /* reserved */ \
+      HWY_CHOOSE_AVX3_DL(func_name), /* AVX3_DL */  \
+      HWY_CHOOSE_AVX3(func_name),    /* AVX3 */     \
+      HWY_CHOOSE_AVX2(func_name),    /* AVX2 */     \
+      nullptr,                       /* AVX */      \
+      HWY_CHOOSE_SSE4(func_name),    /* SSE4 */     \
+      HWY_CHOOSE_SSSE3(func_name),   /* SSSE3 */    \
+      nullptr,                       /* SSE3 */     \
+      nullptr                        /* SSE2 */
 
 #elif HWY_ARCH_ARM
 // See HWY_ARCH_X86 above for details.
diff --git a/third_party/highway/hwy/targets_test.cc b/third_party/highway/hwy/targets_test.cc
index 4cb9291d15f9f..a593fcedef8a4 100644
--- a/third_party/highway/hwy/targets_test.cc
+++ b/third_party/highway/hwy/targets_test.cc
@@ -23,10 +23,14 @@ namespace fake {
     uint32_t FakeFunction(int) { return HWY_##TGT; } \
   }
 
+DECLARE_FUNCTION(AVX3_DL)
 DECLARE_FUNCTION(AVX3)
 DECLARE_FUNCTION(AVX2)
 DECLARE_FUNCTION(SSE4)
+DECLARE_FUNCTION(SSSE3)
 DECLARE_FUNCTION(NEON)
+DECLARE_FUNCTION(SVE)
+DECLARE_FUNCTION(SVE2)
 DECLARE_FUNCTION(PPC8)
 DECLARE_FUNCTION(WASM)
 DECLARE_FUNCTION(RVV)
@@ -49,10 +53,14 @@ void CheckFakeFunction() {
     /* Second call uses the cached value from the previous call. */         \
     EXPECT_EQ(uint32_t(HWY_##TGT), HWY_DYNAMIC_DISPATCH(FakeFunction)(42)); \
   }
+  CHECK_ARRAY_ENTRY(AVX3_DL)
   CHECK_ARRAY_ENTRY(AVX3)
   CHECK_ARRAY_ENTRY(AVX2)
   CHECK_ARRAY_ENTRY(SSE4)
+  CHECK_ARRAY_ENTRY(SSSE3)
   CHECK_ARRAY_ENTRY(NEON)
+  CHECK_ARRAY_ENTRY(SVE)
+  CHECK_ARRAY_ENTRY(SVE2)
   CHECK_ARRAY_ENTRY(PPC8)
   CHECK_ARRAY_ENTRY(WASM)
   CHECK_ARRAY_ENTRY(RVV)
@@ -100,3 +108,9 @@ TEST_F(HwyTargetsTest, DisabledTargetsTest) {
 }
 
 }  // namespace hwy
+
+// Ought not to be necessary, but without this, no tests run on RVV.
+int main(int argc, char **argv) {
+  ::testing::InitGoogleTest(&argc, argv);
+  return RUN_ALL_TESTS();
+}
diff --git a/third_party/highway/hwy/tests/arithmetic_test.cc b/third_party/highway/hwy/tests/arithmetic_test.cc
index 07086356e6d45..7e2ac26e7e766 100644
--- a/third_party/highway/hwy/tests/arithmetic_test.cc
+++ b/third_party/highway/hwy/tests/arithmetic_test.cc
@@ -40,7 +40,7 @@ struct TestPlusMinus {
     for (size_t i = 0; i < N; ++i) {
       lanes[i] = static_cast<T>((2 + i) + (3 + i));
     }
-    HWY_ASSERT_VEC_EQ(d, lanes.get(), v2 + v3);
+    HWY_ASSERT_VEC_EQ(d, lanes.get(), Add(v2, v3));
     HWY_ASSERT_VEC_EQ(d, Set(d, 2), Sub(v4, v2));
 
     for (size_t i = 0; i < N; ++i) {
@@ -376,7 +376,7 @@ class TestSignedRightShifts {
 
     // First test positive values, negative are checked below.
     const auto v0 = Zero(d);
-    const auto values = Iota(d, 0) & Set(d, kMax);
+    const auto values = And(Iota(d, 0), Set(d, kMax));
 
     // Shift by 0
     HWY_ASSERT_VEC_EQ(d, values, ShiftRight<0>(values));
@@ -616,7 +616,7 @@ struct TestUnsignedMul {
     for (size_t i = 0; i < N; ++i) {
       expected[i] = static_cast<T>((1 + i) * (1 + i));
     }
-    HWY_ASSERT_VEC_EQ(d, expected.get(), vi * vi);
+    HWY_ASSERT_VEC_EQ(d, expected.get(), Mul(vi, vi));
 
     for (size_t i = 0; i < N; ++i) {
       expected[i] = static_cast<T>((1 + i) * (3 + i));
@@ -747,10 +747,52 @@ struct TestMulEven {
   }
 };
 
+struct TestMulEvenOdd64 {
+  template <typename T, class D>
+  HWY_NOINLINE void operator()(T /*unused*/, D d) {
+#if HWY_TARGET != HWY_SCALAR
+    const auto v0 = Zero(d);
+    HWY_ASSERT_VEC_EQ(d, Zero(d), MulEven(v0, v0));
+    HWY_ASSERT_VEC_EQ(d, Zero(d), MulOdd(v0, v0));
+
+    const size_t N = Lanes(d);
+    if (N == 1) return;
+
+    auto in1 = AllocateAligned<T>(N);
+    auto in2 = AllocateAligned<T>(N);
+    auto expected_even = AllocateAligned<T>(N);
+    auto expected_odd = AllocateAligned<T>(N);
+
+    // Random inputs in each lane
+    RandomState rng;
+    for (size_t rep = 0; rep < 1000; ++rep) {
+      for (size_t i = 0; i < N; ++i) {
+        in1[i] = Random64(&rng);
+        in2[i] = Random64(&rng);
+      }
+
+      for (size_t i = 0; i < N; i += 2) {
+        expected_even[i] = Mul128(in1[i], in2[i], &expected_even[i + 1]);
+        expected_odd[i] = Mul128(in1[i + 1], in2[i + 1], &expected_odd[i + 1]);
+      }
+
+      const auto a = Load(d, in1.get());
+      const auto b = Load(d, in2.get());
+      HWY_ASSERT_VEC_EQ(d, expected_even.get(), MulEven(a, b));
+      HWY_ASSERT_VEC_EQ(d, expected_odd.get(), MulOdd(a, b));
+    }
+#else
+    (void)d;
+#endif  // HWY_TARGET != HWY_SCALAR
+  }
+};
+
 HWY_NOINLINE void TestAllMulEven() {
   ForPartialVectors<TestMulEven> test;
   test(int32_t());
   test(uint32_t());
+
+  ForGE128Vectors<TestMulEvenOdd64>()(uint64_t());
 }
 
 struct TestMulAdd {
@@ -853,7 +895,7 @@ struct TestApproximateReciprocal {
 
     double max_l1 = 0.0;
     for (size_t i = 0; i < N; ++i) {
-      max_l1 = std::max<double>(max_l1, std::abs((1.0 / input[i]) - actual[i]));
+      max_l1 = HWY_MAX(max_l1, std::abs((1.0 / input[i]) - actual[i]));
     }
     const double max_rel = max_l1 / std::abs(1.0 / input[N - 1]);
     printf("max err %f\n", max_rel);
@@ -870,7 +912,7 @@ struct TestSquareRoot {
   template <typename T, class D>
   HWY_NOINLINE void operator()(T /*unused*/, D d) {
     const auto vi = Iota(d, 0);
-    HWY_ASSERT_VEC_EQ(d, vi, Sqrt(vi * vi));
+    HWY_ASSERT_VEC_EQ(d, vi, Sqrt(Mul(vi, vi)));
   }
 };
 
@@ -901,36 +943,53 @@ template <typename T, class D>
 AlignedFreeUniquePtr<T[]> RoundTestCases(T /*unused*/, D d, size_t& padded) {
   const T eps = std::numeric_limits<T>::epsilon();
   const T test_cases[] = {
-      // +/- 1
-      T(1), T(-1),
-      // +/- 0
-      T(0), T(-0),
-      // near 0
-      T(0.4), T(-0.4),
-      // +/- integer
-      T(4), T(-32),
-      // positive near limit
-      MantissaEnd<T>() - T(1.5), MantissaEnd<T>() + T(1.5),
-      // negative near limit
-      -MantissaEnd<T>() - T(1.5), -MantissaEnd<T>() + T(1.5),
-      // +/- huge (but still fits in float)
-      T(1E34), T(-1E35),
-      // positive tiebreak
-      T(1.5), T(2.5),
-      // negative tiebreak
-      T(-1.5), T(-2.5),
-      // positive +/- delta
-      T(2.0001), T(3.9999),
-      // negative +/- delta
-      T(-999.9999), T(-998.0001),
-      // positive +/- epsilon
-      T(1) + eps, T(1) - eps,
-      // negative +/- epsilon
-      T(-1) + eps, T(-1) - eps,
-      // +/- infinity
-      std::numeric_limits<T>::infinity(), -std::numeric_limits<T>::infinity(),
-      // qNaN
-      GetLane(NaN(d))};
+    // +/- 1
+    T(1),
+    T(-1),
+    // +/- 0
+    T(0),
+    T(-0),
+    // near 0
+    T(0.4),
+    T(-0.4),
+    // +/- integer
+    T(4),
+    T(-32),
+    // positive near limit
+    MantissaEnd<T>() - T(1.5),
+    MantissaEnd<T>() + T(1.5),
+    // negative near limit
+    -MantissaEnd<T>() - T(1.5),
+    -MantissaEnd<T>() + T(1.5),
+    // positive tiebreak
+    T(1.5),
+    T(2.5),
+    // negative tiebreak
+    T(-1.5),
+    T(-2.5),
+    // positive +/- delta
+    T(2.0001),
+    T(3.9999),
+    // negative +/- delta
+    T(-999.9999),
+    T(-998.0001),
+    // positive +/- epsilon
+    T(1) + eps,
+    T(1) - eps,
+    // negative +/- epsilon
+    T(-1) + eps,
+    T(-1) - eps,
+#if !defined(HWY_EMULATE_SVE)  // these are not safe to just cast to int
+    // +/- huge (but still fits in float)
+    T(1E34),
+    T(-1E35),
+    // +/- infinity
+    std::numeric_limits<T>::infinity(),
+    -std::numeric_limits<T>::infinity(),
+    // qNaN
+    GetLane(NaN(d))
+#endif
+  };
   const size_t kNumTestCases = sizeof(test_cases) / sizeof(test_cases[0]);
   const size_t N = Lanes(d);
   padded = RoundUpTo(kNumTestCases, N);  // allow loading whole vectors
@@ -1074,30 +1133,31 @@ struct TestSumOfLanes {
       in_lanes[i] = i < kBits ? static_cast<T>(1ull << i) : 0;
       sum += static_cast<double>(in_lanes[i]);
     }
-    HWY_ASSERT_VEC_EQ(d, Set(d, T(sum)), SumOfLanes(Load(d, in_lanes.get())));
+    HWY_ASSERT_VEC_EQ(d, Set(d, T(sum)),
+                      SumOfLanes(d, Load(d, in_lanes.get())));
 
     // Lane i = i (iota) to include upper lanes
     sum = 0.0;
     for (size_t i = 0; i < N; ++i) {
       sum += static_cast<double>(i);
     }
-    HWY_ASSERT_VEC_EQ(d, Set(d, T(sum)), SumOfLanes(Iota(d, 0)));
+    HWY_ASSERT_VEC_EQ(d, Set(d, T(sum)), SumOfLanes(d, Iota(d, 0)));
   }
 };
 
 HWY_NOINLINE void TestAllSumOfLanes() {
-  const ForPartialVectors<TestSumOfLanes> sum;
+  const ForPartialVectors<TestSumOfLanes> test;
 
   // No u8/u16/i8/i16.
-  sum(uint32_t());
-  sum(int32_t());
+  test(uint32_t());
+  test(int32_t());
 
 #if HWY_CAP_INTEGER64
-  sum(uint64_t());
-  sum(int64_t());
+  test(uint64_t());
+  test(int64_t());
 #endif
 
-  ForFloatTypes(sum);
+  ForFloatTypes(test);
 }
 
 struct TestMinOfLanes {
@@ -1112,17 +1172,17 @@ struct TestMinOfLanes {
     constexpr size_t kBits = HWY_MIN(sizeof(T) * 8 - 1, 51);
     for (size_t i = 0; i < N; ++i) {
       in_lanes[i] = i < kBits ? static_cast<T>(1ull << i) : 2;
-      min = std::min(min, in_lanes[i]);
+      min = HWY_MIN(min, in_lanes[i]);
     }
-    HWY_ASSERT_VEC_EQ(d, Set(d, min), MinOfLanes(Load(d, in_lanes.get())));
+    HWY_ASSERT_VEC_EQ(d, Set(d, min), MinOfLanes(d, Load(d, in_lanes.get())));
 
     // Lane i = N - i to include upper lanes
     min = HighestValue<T>();
     for (size_t i = 0; i < N; ++i) {
       in_lanes[i] = static_cast<T>(N - i);  // no 8-bit T so no wraparound
-      min = std::min(min, in_lanes[i]);
+      min = HWY_MIN(min, in_lanes[i]);
     }
-    HWY_ASSERT_VEC_EQ(d, Set(d, min), MinOfLanes(Load(d, in_lanes.get())));
+    HWY_ASSERT_VEC_EQ(d, Set(d, min), MinOfLanes(d, Load(d, in_lanes.get())));
   }
 };
 
@@ -1137,17 +1197,17 @@ struct TestMaxOfLanes {
     constexpr size_t kBits = HWY_MIN(sizeof(T) * 8 - 1, 51);
     for (size_t i = 0; i < N; ++i) {
       in_lanes[i] = i < kBits ? static_cast<T>(1ull << i) : 0;
-      max = std::max(max, in_lanes[i]);
+      max = HWY_MAX(max, in_lanes[i]);
     }
-    HWY_ASSERT_VEC_EQ(d, Set(d, max), MaxOfLanes(Load(d, in_lanes.get())));
+    HWY_ASSERT_VEC_EQ(d, Set(d, max), MaxOfLanes(d, Load(d, in_lanes.get())));
 
     // Lane i = i to include upper lanes
     max = LowestValue<T>();
     for (size_t i = 0; i < N; ++i) {
       in_lanes[i] = static_cast<T>(i);  // no 8-bit T so no wraparound
-      max = std::max(max, in_lanes[i]);
+      max = HWY_MAX(max, in_lanes[i]);
     }
-    HWY_ASSERT_VEC_EQ(d, Set(d, max), MaxOfLanes(Load(d, in_lanes.get())));
+    HWY_ASSERT_VEC_EQ(d, Set(d, max), MaxOfLanes(d, Load(d, in_lanes.get())));
   }
 };
 
@@ -1219,6 +1279,7 @@ HWY_NOINLINE void TestAllNeg() {
 HWY_AFTER_NAMESPACE();
 
 #if HWY_ONCE
+
 namespace hwy {
 HWY_BEFORE_TEST(HwyArithmeticTest);
 HWY_EXPORT_AND_TEST_P(HwyArithmeticTest, TestAllPlusMinus);
@@ -1246,4 +1307,11 @@ HWY_EXPORT_AND_TEST_P(HwyArithmeticTest, TestAllFloor);
 HWY_EXPORT_AND_TEST_P(HwyArithmeticTest, TestAllAbsDiff);
 HWY_EXPORT_AND_TEST_P(HwyArithmeticTest, TestAllNeg);
 }  // namespace hwy
+
+// Ought not to be necessary, but without this, no tests run on RVV.
+int main(int argc, char** argv) {
+  ::testing::InitGoogleTest(&argc, argv);
+  return RUN_ALL_TESTS();
+}
+
 #endif
diff --git a/third_party/highway/hwy/tests/blockwise_test.cc b/third_party/highway/hwy/tests/blockwise_test.cc
new file mode 100644
index 0000000000000..234e606366952
--- /dev/null
+++ b/third_party/highway/hwy/tests/blockwise_test.cc
@@ -0,0 +1,655 @@
+// Copyright 2019 Google LLC
+//
+// Licensed under the Apache License, Version 2.0 (the "License");
+// you may not use this file except in compliance with the License.
+// You may obtain a copy of the License at
+//
+//      http://www.apache.org/licenses/LICENSE-2.0
+//
+// Unless required by applicable law or agreed to in writing, software
+// distributed under the License is distributed on an "AS IS" BASIS,
+// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+// See the License for the specific language governing permissions and
+// limitations under the License.
+
+#include <stddef.h>
+#include <stdint.h>
+#include <string.h>
+
+#undef HWY_TARGET_INCLUDE
+#define HWY_TARGET_INCLUDE "tests/blockwise_test.cc"
+#include "hwy/foreach_target.h"
+#include "hwy/highway.h"
+#include "hwy/tests/test_util-inl.h"
+
+HWY_BEFORE_NAMESPACE();
+namespace hwy {
+namespace HWY_NAMESPACE {
+
+struct TestShiftBytes {
+  template <class T, class D>
+  HWY_NOINLINE void operator()(T /*unused*/, D d) {
+    // Scalar does not define Shift*Bytes.
+#if HWY_TARGET != HWY_SCALAR || HWY_IDE
+    const Repartition<uint8_t, D> du8;
+    const size_t N8 = Lanes(du8);
+
+    // Zero remains zero
+    const auto v0 = Zero(d);
+    HWY_ASSERT_VEC_EQ(d, v0, ShiftLeftBytes<1>(v0));
+    HWY_ASSERT_VEC_EQ(d, v0, ShiftLeftBytes<1>(d, v0));
+    HWY_ASSERT_VEC_EQ(d, v0, ShiftRightBytes<1>(d, v0));
+
+    // Zero after shifting out the high/low byte
+    auto bytes = AllocateAligned<uint8_t>(N8);
+    std::fill(bytes.get(), bytes.get() + N8, 0);
+    bytes[N8 - 1] = 0x7F;
+    const auto vhi = BitCast(d, Load(du8, bytes.get()));
+    bytes[N8 - 1] = 0;
+    bytes[0] = 0x7F;
+    const auto vlo = BitCast(d, Load(du8, bytes.get()));
+    HWY_ASSERT_VEC_EQ(d, v0, ShiftLeftBytes<1>(vhi));
+    HWY_ASSERT_VEC_EQ(d, v0, ShiftLeftBytes<1>(d, vhi));
+    HWY_ASSERT_VEC_EQ(d, v0, ShiftRightBytes<1>(d, vlo));
+
+    // Check expected result with Iota
+    const size_t N = Lanes(d);
+    auto in = AllocateAligned<T>(N);
+    const uint8_t* in_bytes = reinterpret_cast<const uint8_t*>(in.get());
+    const auto v = BitCast(d, Iota(du8, 1));
+    Store(v, d, in.get());
+
+    auto expected = AllocateAligned<T>(N);
+    uint8_t* expected_bytes = reinterpret_cast<uint8_t*>(expected.get());
+
+    const size_t kBlockSize = HWY_MIN(N8, 16);
+    for (size_t block = 0; block < N8; block += kBlockSize) {
+      expected_bytes[block] = 0;
+      memcpy(expected_bytes + block + 1, in_bytes + block, kBlockSize - 1);
+    }
+    HWY_ASSERT_VEC_EQ(d, expected.get(), ShiftLeftBytes<1>(v));
+    HWY_ASSERT_VEC_EQ(d, expected.get(), ShiftLeftBytes<1>(d, v));
+
+    for (size_t block = 0; block < N8; block += kBlockSize) {
+      memcpy(expected_bytes + block, in_bytes + block + 1, kBlockSize - 1);
+      expected_bytes[block + kBlockSize - 1] = 0;
+    }
+    HWY_ASSERT_VEC_EQ(d, expected.get(), ShiftRightBytes<1>(d, v));
+#else
+    (void)d;
+#endif  // #if HWY_TARGET != HWY_SCALAR
+  }
+};
+
+HWY_NOINLINE void TestAllShiftBytes() {
+  ForIntegerTypes(ForPartialVectors<TestShiftBytes>());
+}
+
+struct TestShiftLanes {
+  template <class T, class D>
+  HWY_NOINLINE void operator()(T /*unused*/, D d) {
+    // Scalar does not define Shift*Lanes.
+#if HWY_TARGET != HWY_SCALAR || HWY_IDE
+    const auto v = Iota(d, T(1));
+    const size_t N = Lanes(d);
+    auto expected = AllocateAligned<T>(N);
+
+    HWY_ASSERT_VEC_EQ(d, v, ShiftLeftLanes<0>(v));
+    HWY_ASSERT_VEC_EQ(d, v, ShiftLeftLanes<0>(d, v));
+    HWY_ASSERT_VEC_EQ(d, v, ShiftRightLanes<0>(d, v));
+
+    constexpr size_t kLanesPerBlock = 16 / sizeof(T);
+
+    for (size_t i = 0; i < N; ++i) {
+      expected[i] = (i % kLanesPerBlock) == 0 ? T(0) : T(i);
+    }
+    HWY_ASSERT_VEC_EQ(d, expected.get(), ShiftLeftLanes<1>(v));
+    HWY_ASSERT_VEC_EQ(d, expected.get(), ShiftLeftLanes<1>(d, v));
+
+    for (size_t i = 0; i < N; ++i) {
+      const size_t mod = i % kLanesPerBlock;
+      expected[i] = mod == (kLanesPerBlock - 1) || i >= N - 1 ? T(0) : T(2 + i);
+    }
+    HWY_ASSERT_VEC_EQ(d, expected.get(), ShiftRightLanes<1>(d, v));
+#else
+    (void)d;
+#endif  // #if HWY_TARGET != HWY_SCALAR
+  }
+};
+
+HWY_NOINLINE void TestAllShiftLanes() {
+  ForAllTypes(ForPartialVectors<TestShiftLanes>());
+}
+
+template <typename D, int kLane>
+struct TestBroadcastR {
+  HWY_NOINLINE void operator()() const {
+    using T = typename D::T;
+    const D d;
+    const size_t N = Lanes(d);
+    if (kLane >= N) return;
+    auto in_lanes = AllocateAligned<T>(N);
+    std::fill(in_lanes.get(), in_lanes.get() + N, T(0));
+    const size_t blockN = HWY_MIN(N * sizeof(T), 16) / sizeof(T);
+    // Need to set within each 128-bit block
+    for (size_t block = 0; block < N; block += blockN) {
+      in_lanes[block + kLane] = static_cast<T>(block + 1);
+    }
+    const auto in = Load(d, in_lanes.get());
+    auto expected = AllocateAligned<T>(N);
+    for (size_t block = 0; block < N; block += blockN) {
+      for (size_t i = 0; i < blockN; ++i) {
+        expected[block + i] = T(block + 1);
+      }
+    }
+    HWY_ASSERT_VEC_EQ(d, expected.get(), Broadcast<kLane>(in));
+
+    TestBroadcastR<D, kLane - 1>()();
+  }
+};
+
+template <class D>
+struct TestBroadcastR<D, -1> {
+  void operator()() const {}
+};
+
+struct TestBroadcast {
+  template <class T, class D>
+  HWY_NOINLINE void operator()(T /*unused*/, D d) {
+    TestBroadcastR<D, HWY_MIN(MaxLanes(d), 16 / sizeof(T)) - 1>()();
+  }
+};
+
+HWY_NOINLINE void TestAllBroadcast() {
+  const ForPartialVectors<TestBroadcast> test;
+  // No u8.
+  test(uint16_t());
+  test(uint32_t());
+#if HWY_CAP_INTEGER64
+  test(uint64_t());
+#endif
+
+  // No i8.
+  test(int16_t());
+  test(int32_t());
+#if HWY_CAP_INTEGER64
+  test(int64_t());
+#endif
+
+  ForFloatTypes(test);
+}
+
+template <bool kFull>
+struct ChooseTableSize {
+  template <typename T, typename DIdx>
+  using type = DIdx;
+};
+template <>
+struct ChooseTableSize<true> {
+  template <typename T, typename DIdx>
+  using type = HWY_FULL(T);
+};
+
+template <bool kFull>
+struct TestTableLookupBytes {
+  template <class T, class D>
+  HWY_NOINLINE void operator()(T /*unused*/, D d) {
+#if HWY_TARGET != HWY_SCALAR
+    RandomState rng;
+    const typename ChooseTableSize<kFull>::template type<T, D> d_tbl;
+    const Repartition<uint8_t, decltype(d_tbl)> d_tbl8;
+    const size_t NT8 = Lanes(d_tbl8);
+
+    const Repartition<uint8_t, D> d8;
+    const size_t N = Lanes(d);
+    const size_t N8 = Lanes(d8);
+
+    // Random input bytes
+    auto in_bytes = AllocateAligned<uint8_t>(NT8);
+    for (size_t i = 0; i < NT8; ++i) {
+      in_bytes[i] = Random32(&rng) & 0xFF;
+    }
+    const auto in = BitCast(d_tbl, Load(d_tbl8, in_bytes.get()));
+
+    // Enough test data; for larger vectors, upper lanes will be zero.
+    const uint8_t index_bytes_source[64] = {
+        // Same index as source, multiple outputs from same input,
+        // unused input (9), ascending/descending and nonconsecutive neighbors.
+        0,  2,  1, 2, 15, 12, 13, 14, 6,  7,  8,  5,  4,  3,  10, 11,
+        11, 10, 3, 4, 5,  8,  7,  6,  14, 13, 12, 15, 2,  1,  2,  0,
+        4,  3,  2, 2, 5,  6,  7,  7,  15, 15, 15, 15, 15, 15, 0,  1};
+    auto index_bytes = AllocateAligned<uint8_t>(N8);
+    const size_t max_index = HWY_MIN(N8, 16) - 1;
+    for (size_t i = 0; i < N8; ++i) {
+      index_bytes[i] = (i < 64) ? index_bytes_source[i] : 0;
+      // Avoid asan error for partial vectors.
+      index_bytes[i] = static_cast<uint8_t>(HWY_MIN(index_bytes[i], max_index));
+    }
+    const auto indices = Load(d, reinterpret_cast<const T*>(index_bytes.get()));
+
+    auto expected = AllocateAligned<T>(N);
+    uint8_t* expected_bytes = reinterpret_cast<uint8_t*>(expected.get());
+
+    for (size_t block = 0; block < N8; block += 16) {
+      for (size_t i = 0; i < 16 && (block + i) < N8; ++i) {
+        const uint8_t index = index_bytes[block + i];
+        HWY_ASSERT(block + index < N8);  // indices were already capped to N8.
+        // For large vectors, the lane index may wrap around due to block.
+        expected_bytes[block + i] = in_bytes[(block & 0xFF) + index];
+      }
+    }
+    HWY_ASSERT_VEC_EQ(d, expected.get(), TableLookupBytes(in, indices));
+
+    // Individually test zeroing each byte position.
+    for (size_t i = 0; i < N8; ++i) {
+      const uint8_t prev_expected = expected_bytes[i];
+      const uint8_t prev_index = index_bytes[i];
+      expected_bytes[i] = 0;
+
+      const int idx = 0x80 + ((Random32(&rng) & 7) << 4);
+      HWY_ASSERT(0x80 <= idx && idx < 256);
+      index_bytes[i] = static_cast<uint8_t>(idx);
+
+      const auto indices =
+          Load(d, reinterpret_cast<const T*>(index_bytes.get()));
+      HWY_ASSERT_VEC_EQ(d, expected.get(), TableLookupBytesOr0(in, indices));
+      expected_bytes[i] = prev_expected;
+      index_bytes[i] = prev_index;
+    }
+#else
+    (void)d;
+#endif
+  }
+};
+
+HWY_NOINLINE void TestAllTableLookupBytes() {
+  // Partial index, same-sized table.
+  ForIntegerTypes(ForPartialVectors<TestTableLookupBytes<false>>());
+
+// TODO(janwas): requires LMUL trunc/ext, which is not yet implemented.
+#if HWY_TARGET != HWY_RVV
+  // Partial index, full-size table.
+  ForIntegerTypes(ForPartialVectors<TestTableLookupBytes<true>>());
+#endif
+}
+
+struct TestInterleaveLower {
+  template <class T, class D>
+  HWY_NOINLINE void operator()(T /*unused*/, D d) {
+    using TU = MakeUnsigned<T>;
+    const size_t N = Lanes(d);
+    auto even_lanes = AllocateAligned<T>(N);
+    auto odd_lanes = AllocateAligned<T>(N);
+    auto expected = AllocateAligned<T>(N);
+    for (size_t i = 0; i < N; ++i) {
+      even_lanes[i] = static_cast<T>(2 * i + 0);
+      odd_lanes[i] = static_cast<T>(2 * i + 1);
+    }
+    const auto even = Load(d, even_lanes.get());
+    const auto odd = Load(d, odd_lanes.get());
+
+    const size_t blockN = HWY_MIN(16 / sizeof(T), N);
+    for (size_t i = 0; i < Lanes(d); ++i) {
+      const size_t block = i / blockN;
+      const size_t index = (i % blockN) + block * 2 * blockN;
+      expected[i] = static_cast<T>(index & LimitsMax<TU>());
+    }
+    HWY_ASSERT_VEC_EQ(d, expected.get(), InterleaveLower(even, odd));
+    HWY_ASSERT_VEC_EQ(d, expected.get(), InterleaveLower(d, even, odd));
+  }
+};
+
+struct TestInterleaveUpper {
+  template <class T, class D>
+  HWY_NOINLINE void operator()(T /*unused*/, D d) {
+    const size_t N = Lanes(d);
+    if (N == 1) return;
+    auto even_lanes = AllocateAligned<T>(N);
+    auto odd_lanes = AllocateAligned<T>(N);
+    auto expected = AllocateAligned<T>(N);
+    for (size_t i = 0; i < N; ++i) {
+      even_lanes[i] = static_cast<T>(2 * i + 0);
+      odd_lanes[i] = static_cast<T>(2 * i + 1);
+    }
+    const auto even = Load(d, even_lanes.get());
+    const auto odd = Load(d, odd_lanes.get());
+
+    const size_t blockN = HWY_MIN(16 / sizeof(T), N);
+    for (size_t i = 0; i < Lanes(d); ++i) {
+      const size_t block = i / blockN;
+      expected[i] = T((i % blockN) + block * 2 * blockN + blockN);
+    }
+    HWY_ASSERT_VEC_EQ(d, expected.get(), InterleaveUpper(d, even, odd));
+  }
+};
+
+HWY_NOINLINE void TestAllInterleave() {
+  // Not DemoteVectors because this cannot be supported by HWY_SCALAR.
+  ForAllTypes(ForShrinkableVectors<TestInterleaveLower>());
+  ForAllTypes(ForShrinkableVectors<TestInterleaveUpper>());
+}
+
+struct TestZipLower {
+  template <class T, class D>
+  HWY_NOINLINE void operator()(T /*unused*/, D d) {
+    using WideT = MakeWide<T>;
+    static_assert(sizeof(T) * 2 == sizeof(WideT), "Must be double-width");
+    static_assert(IsSigned<T>() == IsSigned<WideT>(), "Must have same sign");
+    const size_t N = Lanes(d);
+    auto even_lanes = AllocateAligned<T>(N);
+    auto odd_lanes = AllocateAligned<T>(N);
+    for (size_t i = 0; i < N; ++i) {
+      even_lanes[i] = static_cast<T>(2 * i + 0);
+      odd_lanes[i] = static_cast<T>(2 * i + 1);
+    }
+    const auto even = Load(d, even_lanes.get());
+    const auto odd = Load(d, odd_lanes.get());
+
+    const Repartition<WideT, D> dw;
+    const size_t NW = Lanes(dw);
+    auto expected = AllocateAligned<WideT>(NW);
+    const WideT blockN = static_cast<WideT>(HWY_MIN(16 / sizeof(WideT), NW));
+
+    for (size_t i = 0; i < NW; ++i) {
+      const size_t block = i / blockN;
+      // Value of least-significant lane in lo-vector.
+      const WideT lo =
+          static_cast<WideT>(2 * (i % blockN) + 4 * block * blockN);
+      const WideT kBits = static_cast<WideT>(sizeof(T) * 8);
+      expected[i] =
+          static_cast<WideT>((static_cast<WideT>(lo + 1) << kBits) + lo);
+    }
+    HWY_ASSERT_VEC_EQ(dw, expected.get(), ZipLower(even, odd));
+    HWY_ASSERT_VEC_EQ(dw, expected.get(), ZipLower(dw, even, odd));
+  }
+};
+
+struct TestZipUpper {
+  template <class T, class D>
+  HWY_NOINLINE void operator()(T /*unused*/, D d) {
+    using WideT = MakeWide<T>;
+    static_assert(sizeof(T) * 2 == sizeof(WideT), "Must be double-width");
+    static_assert(IsSigned<T>() == IsSigned<WideT>(), "Must have same sign");
+    const size_t N = Lanes(d);
+    if (N < 16 / sizeof(T)) return;
+    auto even_lanes = AllocateAligned<T>(N);
+    auto odd_lanes = AllocateAligned<T>(N);
+    for (size_t i = 0; i < Lanes(d); ++i) {
+      even_lanes[i] = static_cast<T>(2 * i + 0);
+      odd_lanes[i] = static_cast<T>(2 * i + 1);
+    }
+    const auto even = Load(d, even_lanes.get());
+    const auto odd = Load(d, odd_lanes.get());
+
+        const Repartition<WideT, D> dw;
+    const size_t NW = Lanes(dw);
+    auto expected = AllocateAligned<WideT>(NW);
+    const WideT blockN = static_cast<WideT>(HWY_MIN(16 / sizeof(WideT), NW));
+
+    for (size_t i = 0; i < NW; ++i) {
+      const size_t block = i / blockN;
+      const WideT lo =
+          static_cast<WideT>(2 * (i % blockN) + 4 * block * blockN);
+      const WideT kBits = static_cast<WideT>(sizeof(T) * 8);
+      expected[i] = static_cast<WideT>(
+          (static_cast<WideT>(lo + 2 * blockN + 1) << kBits) + lo + 2 * blockN);
+    }
+    HWY_ASSERT_VEC_EQ(dw, expected.get(), ZipUpper(dw, even, odd));
+  }
+};
+
+HWY_NOINLINE void TestAllZip() {
+  const ForDemoteVectors<TestZipLower> lower_unsigned;
+  // TODO(janwas): enable after LowerHalf available
+#if HWY_TARGET != HWY_RVV
+  lower_unsigned(uint8_t());
+#endif
+  lower_unsigned(uint16_t());
+#if HWY_CAP_INTEGER64
+  lower_unsigned(uint32_t());  // generates u64
+#endif
+
+  const ForDemoteVectors<TestZipLower> lower_signed;
+#if HWY_TARGET != HWY_RVV
+  lower_signed(int8_t());
+#endif
+  lower_signed(int16_t());
+#if HWY_CAP_INTEGER64
+  lower_signed(int32_t());  // generates i64
+#endif
+
+  const ForShrinkableVectors<TestZipUpper> upper_unsigned;
+#if HWY_TARGET != HWY_RVV
+  upper_unsigned(uint8_t());
+#endif
+  upper_unsigned(uint16_t());
+#if HWY_CAP_INTEGER64
+  upper_unsigned(uint32_t());  // generates u64
+#endif
+
+  const ForShrinkableVectors<TestZipUpper> upper_signed;
+#if HWY_TARGET != HWY_RVV
+  upper_signed(int8_t());
+#endif
+  upper_signed(int16_t());
+#if HWY_CAP_INTEGER64
+  upper_signed(int32_t());  // generates i64
+#endif
+
+  // No float - concatenating f32 does not result in a f64
+}
+
+template <int kBytes>
+struct TestCombineShiftRightBytesR {
+  template <class T, class D>
+  HWY_NOINLINE void operator()(T t, D d) {
+// Scalar does not define CombineShiftRightBytes.
+#if HWY_TARGET != HWY_SCALAR || HWY_IDE
+    const size_t kBlockSize = 16;
+    static_assert(kBytes < kBlockSize, "Shift count is per block");
+    const Repartition<uint8_t, D> d8;
+    const size_t N8 = Lanes(d8);
+    if (N8 < 16) return;
+    auto hi_bytes = AllocateAligned<uint8_t>(N8);
+    auto lo_bytes = AllocateAligned<uint8_t>(N8);
+    auto expected_bytes = AllocateAligned<uint8_t>(N8);
+    uint8_t combined[2 * kBlockSize];
+
+    // Random inputs in each lane
+    RandomState rng;
+    for (size_t rep = 0; rep < 100; ++rep) {
+      for (size_t i = 0; i < N8; ++i) {
+        hi_bytes[i] = static_cast<uint8_t>(Random64(&rng) & 0xFF);
+        lo_bytes[i] = static_cast<uint8_t>(Random64(&rng) & 0xFF);
+      }
+      for (size_t i = 0; i < N8; i += kBlockSize) {
+        CopyBytes<kBlockSize>(&lo_bytes[i], combined);
+        CopyBytes<kBlockSize>(&hi_bytes[i], combined + kBlockSize);
+        CopyBytes<kBlockSize>(combined + kBytes, &expected_bytes[i]);
+      }
+
+      const auto hi = BitCast(d, Load(d8, hi_bytes.get()));
+      const auto lo = BitCast(d, Load(d8, lo_bytes.get()));
+      const auto expected = BitCast(d, Load(d8, expected_bytes.get()));
+      HWY_ASSERT_VEC_EQ(d, expected, CombineShiftRightBytes<kBytes>(d, hi, lo));
+    }
+
+    TestCombineShiftRightBytesR<kBytes - 1>()(t, d);
+#else
+    (void)t;
+    (void)d;
+#endif  // #if HWY_TARGET != HWY_SCALAR
+  }
+};
+
+template <int kLanes>
+struct TestCombineShiftRightLanesR {
+  template <class T, class D>
+  HWY_NOINLINE void operator()(T t, D d) {
+// Scalar does not define CombineShiftRightBytes (needed for *Lanes).
+#if HWY_TARGET != HWY_SCALAR || HWY_IDE
+    const Repartition<uint8_t, D> d8;
+    const size_t N8 = Lanes(d8);
+    if (N8 < 16) return;
+
+    auto hi_bytes = AllocateAligned<uint8_t>(N8);
+    auto lo_bytes = AllocateAligned<uint8_t>(N8);
+    auto expected_bytes = AllocateAligned<uint8_t>(N8);
+    const size_t kBlockSize = 16;
+    uint8_t combined[2 * kBlockSize];
+
+    // Random inputs in each lane
+    RandomState rng;
+    for (size_t rep = 0; rep < 100; ++rep) {
+      for (size_t i = 0; i < N8; ++i) {
+        hi_bytes[i] = static_cast<uint8_t>(Random64(&rng) & 0xFF);
+        lo_bytes[i] = static_cast<uint8_t>(Random64(&rng) & 0xFF);
+      }
+      for (size_t i = 0; i < N8; i += kBlockSize) {
+        CopyBytes<kBlockSize>(&lo_bytes[i], combined);
+        CopyBytes<kBlockSize>(&hi_bytes[i], combined + kBlockSize);
+        CopyBytes<kBlockSize>(combined + kLanes * sizeof(T),
+                              &expected_bytes[i]);
+      }
+
+      const auto hi = BitCast(d, Load(d8, hi_bytes.get()));
+      const auto lo = BitCast(d, Load(d8, lo_bytes.get()));
+      const auto expected = BitCast(d, Load(d8, expected_bytes.get()));
+      HWY_ASSERT_VEC_EQ(d, expected, CombineShiftRightLanes<kLanes>(d, hi, lo));
+    }
+
+    TestCombineShiftRightLanesR<kLanes - 1>()(t, d);
+#else
+    (void)t;
+    (void)d;
+#endif  // #if HWY_TARGET != HWY_SCALAR
+  }
+};
+
+template <>
+struct TestCombineShiftRightBytesR<0> {
+  template <class T, class D>
+  void operator()(T /*unused*/, D /*unused*/) {}
+};
+
+template <>
+struct TestCombineShiftRightLanesR<0> {
+  template <class T, class D>
+  void operator()(T /*unused*/, D /*unused*/) {}
+};
+
+struct TestCombineShiftRight {
+  template <class T, class D>
+  HWY_NOINLINE void operator()(T t, D d) {
+    constexpr size_t kMaxBytes = HWY_MIN(16, MaxLanes(d) * sizeof(T));
+    TestCombineShiftRightBytesR<kMaxBytes - 1>()(t, d);
+    TestCombineShiftRightLanesR<kMaxBytes / sizeof(T) - 1>()(t, d);
+  }
+};
+
+HWY_NOINLINE void TestAllCombineShiftRight() {
+  // Need at least 2 lanes.
+  ForAllTypes(ForShrinkableVectors<TestCombineShiftRight>());
+}
+
+class TestSpecialShuffle32 {
+ public:
+  template <class T, class D>
+  HWY_NOINLINE void operator()(T /*unused*/, D d) {
+    const auto v = Iota(d, 0);
+    VerifyLanes32(d, Shuffle2301(v), 2, 3, 0, 1, __FILE__, __LINE__);
+    VerifyLanes32(d, Shuffle1032(v), 1, 0, 3, 2, __FILE__, __LINE__);
+    VerifyLanes32(d, Shuffle0321(v), 0, 3, 2, 1, __FILE__, __LINE__);
+    VerifyLanes32(d, Shuffle2103(v), 2, 1, 0, 3, __FILE__, __LINE__);
+    VerifyLanes32(d, Shuffle0123(v), 0, 1, 2, 3, __FILE__, __LINE__);
+  }
+
+ private:
+  template <class D, class V>
+  HWY_NOINLINE void VerifyLanes32(D d, V actual, const int i3, const int i2,
+                                  const int i1, const int i0,
+                                  const char* filename, const int line) {
+    using T = TFromD<D>;
+    constexpr size_t kBlockN = 16 / sizeof(T);
+    const size_t N = Lanes(d);
+    if (N < 4) return;
+    auto expected = AllocateAligned<T>(N);
+    for (size_t block = 0; block < N; block += kBlockN) {
+      expected[block + 3] = static_cast<T>(block + i3);
+      expected[block + 2] = static_cast<T>(block + i2);
+      expected[block + 1] = static_cast<T>(block + i1);
+      expected[block + 0] = static_cast<T>(block + i0);
+    }
+    AssertVecEqual(d, expected.get(), actual, filename, line);
+  }
+};
+
+class TestSpecialShuffle64 {
+ public:
+  template <class T, class D>
+  HWY_NOINLINE void operator()(T /*unused*/, D d) {
+    const auto v = Iota(d, 0);
+    VerifyLanes64(d, Shuffle01(v), 0, 1, __FILE__, __LINE__);
+  }
+
+ private:
+  template <class D, class V>
+  HWY_NOINLINE void VerifyLanes64(D d, V actual, const int i1, const int i0,
+                                  const char* filename, const int line) {
+    using T = TFromD<D>;
+    constexpr size_t kBlockN = 16 / sizeof(T);
+    const size_t N = Lanes(d);
+    if (N < 2) return;
+    auto expected = AllocateAligned<T>(N);
+    for (size_t block = 0; block < N; block += kBlockN) {
+      expected[block + 1] = static_cast<T>(block + i1);
+      expected[block + 0] = static_cast<T>(block + i0);
+    }
+    AssertVecEqual(d, expected.get(), actual, filename, line);
+  }
+};
+
+HWY_NOINLINE void TestAllSpecialShuffles() {
+  const ForGE128Vectors<TestSpecialShuffle32> test32;
+  test32(uint32_t());
+  test32(int32_t());
+  test32(float());
+
+#if HWY_CAP_INTEGER64
+  const ForGE128Vectors<TestSpecialShuffle64> test64;
+  test64(uint64_t());
+  test64(int64_t());
+#endif
+
+#if HWY_CAP_FLOAT64
+  const ForGE128Vectors<TestSpecialShuffle64> test_d;
+  test_d(double());
+#endif
+}
+
+// NOLINTNEXTLINE(google-readability-namespace-comments)
+}  // namespace HWY_NAMESPACE
+}  // namespace hwy
+HWY_AFTER_NAMESPACE();
+
+#if HWY_ONCE
+
+namespace hwy {
+HWY_BEFORE_TEST(HwyBlockwiseTest);
+HWY_EXPORT_AND_TEST_P(HwyBlockwiseTest, TestAllShiftBytes);
+HWY_EXPORT_AND_TEST_P(HwyBlockwiseTest, TestAllShiftLanes);
+HWY_EXPORT_AND_TEST_P(HwyBlockwiseTest, TestAllBroadcast);
+HWY_EXPORT_AND_TEST_P(HwyBlockwiseTest, TestAllTableLookupBytes);
+HWY_EXPORT_AND_TEST_P(HwyBlockwiseTest, TestAllInterleave);
+HWY_EXPORT_AND_TEST_P(HwyBlockwiseTest, TestAllZip);
+HWY_EXPORT_AND_TEST_P(HwyBlockwiseTest, TestAllCombineShiftRight);
+HWY_EXPORT_AND_TEST_P(HwyBlockwiseTest, TestAllSpecialShuffles);
+}  // namespace hwy
+
+// Ought not to be necessary, but without this, no tests run on RVV.
+int main(int argc, char** argv) {
+  ::testing::InitGoogleTest(&argc, argv);
+  return RUN_ALL_TESTS();
+}
+
+#endif
diff --git a/third_party/highway/hwy/tests/combine_test.cc b/third_party/highway/hwy/tests/combine_test.cc
index 4f7942f67cc58..bbeb867ce8aed 100644
--- a/third_party/highway/hwy/tests/combine_test.cc
+++ b/third_party/highway/hwy/tests/combine_test.cc
@@ -36,16 +36,21 @@ struct TestLowerHalf {
 
     const size_t N = Lanes(d);
     auto lanes = AllocateAligned<T>(N);
+    auto lanes2 = AllocateAligned<T>(N);
     std::fill(lanes.get(), lanes.get() + N, T(0));
+    std::fill(lanes2.get(), lanes2.get() + N, T(0));
     const auto v = Iota(d, 1);
-    Store(LowerHalf(v), d2, lanes.get());
+    Store(LowerHalf(d2, v), d2, lanes.get());
+    Store(LowerHalf(v), d2, lanes2.get());  // optionally without D
     size_t i = 0;
     for (; i < Lanes(d2); ++i) {
       HWY_ASSERT_EQ(T(1 + i), lanes[i]);
+      HWY_ASSERT_EQ(T(1 + i), lanes2[i]);
     }
     // Other half remains unchanged
     for (; i < N; ++i) {
       HWY_ASSERT_EQ(T(0), lanes[i]);
+      HWY_ASSERT_EQ(T(0), lanes2[i]);
     }
   }
 };
@@ -53,29 +58,35 @@ struct TestLowerHalf {
 struct TestLowerQuarter {
   template <class T, class D>
   HWY_NOINLINE void operator()(T /*unused*/, D d) {
-    const Half<Half<D>> d4;
+    const Half<D> d2;
+    const Half<decltype(d2)> d4;
 
     const size_t N = Lanes(d);
     auto lanes = AllocateAligned<T>(N);
+    auto lanes2 = AllocateAligned<T>(N);
     std::fill(lanes.get(), lanes.get() + N, T(0));
+    std::fill(lanes2.get(), lanes2.get() + N, T(0));
     const auto v = Iota(d, 1);
-    const auto lo = LowerHalf(LowerHalf(v));
+    const auto lo = LowerHalf(d4, LowerHalf(d2, v));
+    const auto lo2 = LowerHalf(LowerHalf(v));  // optionally without D
     Store(lo, d4, lanes.get());
+    Store(lo2, d4, lanes2.get());
     size_t i = 0;
     for (; i < Lanes(d4); ++i) {
       HWY_ASSERT_EQ(T(i + 1), lanes[i]);
+      HWY_ASSERT_EQ(T(i + 1), lanes2[i]);
     }
     // Upper 3/4 remain unchanged
     for (; i < N; ++i) {
       HWY_ASSERT_EQ(T(0), lanes[i]);
+      HWY_ASSERT_EQ(T(0), lanes2[i]);
     }
   }
 };
 
 HWY_NOINLINE void TestAllLowerHalf() {
-  constexpr size_t kDiv = 1;
-  ForAllTypes(ForPartialVectors<TestLowerHalf, kDiv, /*kMinLanes=*/2>());
-  ForAllTypes(ForPartialVectors<TestLowerQuarter, kDiv, /*kMinLanes=*/4>());
+  ForAllTypes(ForDemoteVectors<TestLowerHalf>());
+  ForAllTypes(ForDemoteVectors<TestLowerQuarter, 4>());
 }
 
 struct TestUpperHalf {
@@ -90,7 +101,7 @@ struct TestUpperHalf {
     auto lanes = AllocateAligned<T>(N);
     std::fill(lanes.get(), lanes.get() + N, T(0));
 
-    Store(UpperHalf(v), d2, lanes.get());
+    Store(UpperHalf(d2, v), d2, lanes.get());
     size_t i = 0;
     for (; i < Lanes(d2); ++i) {
       HWY_ASSERT_EQ(T(Lanes(d2) + 1 + i), lanes[i]);
@@ -106,22 +117,22 @@ struct TestUpperHalf {
 };
 
 HWY_NOINLINE void TestAllUpperHalf() {
-  ForAllTypes(ForGE128Vectors<TestUpperHalf>());
+  ForAllTypes(ForShrinkableVectors<TestUpperHalf>());
 }
 
 struct TestZeroExtendVector {
   template <class T, class D>
   HWY_NOINLINE void operator()(T /*unused*/, D d) {
-#if HWY_CAP_GE256
     const Twice<D> d2;
 
     const auto v = Iota(d, 1);
     const size_t N2 = Lanes(d2);
+    HWY_ASSERT(N2 != 0);
     auto lanes = AllocateAligned<T>(N2);
     Store(v, d, &lanes[0]);
     Store(v, d, &lanes[N2 / 2]);
 
-    const auto ext = ZeroExtendVector(v);
+    const auto ext = ZeroExtendVector(d2, v);
     Store(ext, d2, lanes.get());
 
     size_t i = 0;
@@ -133,9 +144,6 @@ struct TestZeroExtendVector {
     for (; i < N2; ++i) {
       HWY_ASSERT_EQ(T(0), lanes[i]);
     }
-#else
-    (void)d;
-#endif
   }
 };
 
@@ -146,21 +154,17 @@ HWY_NOINLINE void TestAllZeroExtendVector() {
 struct TestCombine {
   template <class T, class D>
   HWY_NOINLINE void operator()(T /*unused*/, D d) {
-#if HWY_CAP_GE256
     const Twice<D> d2;
     const size_t N2 = Lanes(d2);
     auto lanes = AllocateAligned<T>(N2);
 
     const auto lo = Iota(d, 1);
     const auto hi = Iota(d, N2 / 2 + 1);
-    const auto combined = Combine(hi, lo);
+    const auto combined = Combine(d2, hi, lo);
     Store(combined, d2, lanes.get());
 
     const auto expected = Iota(d2, 1);
     HWY_ASSERT_VEC_EQ(d2, expected, combined);
-#else
-    (void)d;
-#endif
   }
 };
 
@@ -168,102 +172,60 @@ HWY_NOINLINE void TestAllCombine() {
   ForAllTypes(ForExtendableVectors<TestCombine>());
 }
 
-
-template <int kBytes>
-struct TestCombineShiftRightBytesR {
-  template <class T, class D>
-  HWY_NOINLINE void operator()(T t, D d) {
-// Scalar does not define CombineShiftRightBytes.
-#if HWY_TARGET != HWY_SCALAR || HWY_IDE
-    const Repartition<uint8_t, D> d8;
-    const size_t N8 = Lanes(d8);
-    const auto lo = BitCast(d, Iota(d8, 1));
-    const auto hi = BitCast(d, Iota(d8, 1 + N8));
-
-    auto expected = AllocateAligned<T>(Lanes(d));
-    uint8_t* expected_bytes = reinterpret_cast<uint8_t*>(expected.get());
-
-    const size_t kBlockSize = 16;
-    for (size_t i = 0; i < N8; ++i) {
-      const size_t block = i / kBlockSize;
-      const size_t lane = i % kBlockSize;
-      const size_t first_lo = block * kBlockSize;
-      const size_t idx = lane + kBytes;
-      const size_t offset = (idx < kBlockSize) ? 0 : N8 - kBlockSize;
-      const bool at_end = idx >= 2 * kBlockSize;
-      expected_bytes[i] =
-          at_end ? 0 : static_cast<uint8_t>(first_lo + idx + 1 + offset);
-    }
-    HWY_ASSERT_VEC_EQ(d, expected.get(),
-                      CombineShiftRightBytes<kBytes>(hi, lo));
-
-    TestCombineShiftRightBytesR<kBytes - 1>()(t, d);
-#else
-    (void)t;
-    (void)d;
-#endif  // #if HWY_TARGET != HWY_SCALAR
-  }
-};
-
-template <int kLanes>
-struct TestCombineShiftRightLanesR {
+struct TestConcat {
   template <class T, class D>
-  HWY_NOINLINE void operator()(T t, D d) {
-// Scalar does not define CombineShiftRightBytes (needed for *Lanes).
-#if HWY_TARGET != HWY_SCALAR || HWY_IDE
-    const Repartition<uint8_t, D> d8;
-    const size_t N8 = Lanes(d8);
-    const auto lo = BitCast(d, Iota(d8, 1));
-    const auto hi = BitCast(d, Iota(d8, 1 + N8));
-
-    auto expected = AllocateAligned<T>(Lanes(d));
-
-    uint8_t* expected_bytes = reinterpret_cast<uint8_t*>(expected.get());
-
-    const size_t kBlockSize = 16;
-    for (size_t i = 0; i < N8; ++i) {
-      const size_t block = i / kBlockSize;
-      const size_t lane = i % kBlockSize;
-      const size_t first_lo = block * kBlockSize;
-      const size_t idx = lane + kLanes * sizeof(T);
-      const size_t offset = (idx < kBlockSize) ? 0 : N8 - kBlockSize;
-      const bool at_end = idx >= 2 * kBlockSize;
-      expected_bytes[i] =
-          at_end ? 0 : static_cast<uint8_t>(first_lo + idx + 1 + offset);
+  HWY_NOINLINE void operator()(T /*unused*/, D d) {
+    const size_t N = Lanes(d);
+    if (N == 1) return;
+    const size_t half_bytes = N * sizeof(T) / 2;
+
+    auto hi = AllocateAligned<T>(N);
+    auto lo = AllocateAligned<T>(N);
+    auto expected = AllocateAligned<T>(N);
+    RandomState rng;
+    for (size_t rep = 0; rep < 10; ++rep) {
+      for (size_t i = 0; i < N; ++i) {
+        hi[i] = static_cast<T>(Random64(&rng) & 0xFF);
+        lo[i] = static_cast<T>(Random64(&rng) & 0xFF);
+      }
+
+      {
+        memcpy(&expected[N / 2], &hi[N / 2], half_bytes);
+        memcpy(&expected[0], &lo[0], half_bytes);
+        const auto vhi = Load(d, hi.get());
+        const auto vlo = Load(d, lo.get());
+        HWY_ASSERT_VEC_EQ(d, expected.get(), ConcatUpperLower(d, vhi, vlo));
+      }
+
+      {
+        memcpy(&expected[N / 2], &hi[N / 2], half_bytes);
+        memcpy(&expected[0], &lo[N / 2], half_bytes);
+        const auto vhi = Load(d, hi.get());
+        const auto vlo = Load(d, lo.get());
+        HWY_ASSERT_VEC_EQ(d, expected.get(), ConcatUpperUpper(d, vhi, vlo));
+      }
+
+      {
+        memcpy(&expected[N / 2], &hi[0], half_bytes);
+        memcpy(&expected[0], &lo[N / 2], half_bytes);
+        const auto vhi = Load(d, hi.get());
+        const auto vlo = Load(d, lo.get());
+        HWY_ASSERT_VEC_EQ(d, expected.get(), ConcatLowerUpper(d, vhi, vlo));
+      }
+
+      {
+        memcpy(&expected[N / 2], &hi[0], half_bytes);
+        memcpy(&expected[0], &lo[0], half_bytes);
+        const auto vhi = Load(d, hi.get());
+        const auto vlo = Load(d, lo.get());
+        HWY_ASSERT_VEC_EQ(d, expected.get(), ConcatLowerLower(d, vhi, vlo));
+      }
     }
-    HWY_ASSERT_VEC_EQ(d, expected.get(),
-                      CombineShiftRightLanes<kLanes>(hi, lo));
-
-    TestCombineShiftRightBytesR<kLanes - 1>()(t, d);
-#else
-    (void)t;
-    (void)d;
-#endif  // #if HWY_TARGET != HWY_SCALAR
-  }
-};
-
-template <>
-struct TestCombineShiftRightBytesR<0> {
-  template <class T, class D>
-  void operator()(T /*unused*/, D /*unused*/) {}
-};
-
-template <>
-struct TestCombineShiftRightLanesR<0> {
-  template <class T, class D>
-  void operator()(T /*unused*/, D /*unused*/) {}
-};
-
-struct TestCombineShiftRight {
-  template <class T, class D>
-  HWY_NOINLINE void operator()(T t, D d) {
-    TestCombineShiftRightBytesR<15>()(t, d);
-    TestCombineShiftRightLanesR<16 / sizeof(T) - 1>()(t, d);
   }
 };
 
-HWY_NOINLINE void TestAllCombineShiftRight() {
-  ForAllTypes(ForGE128Vectors<TestCombineShiftRight>());
+HWY_NOINLINE void TestAllConcat() {
+  ForAllTypes(ForShrinkableVectors<TestConcat>());
 }
 
 // NOLINTNEXTLINE(google-readability-namespace-comments)
@@ -272,15 +234,23 @@ HWY_NOINLINE void TestAllCombineShiftRight() {
 HWY_AFTER_NAMESPACE();
 
 #if HWY_ONCE
+
 namespace hwy {
 HWY_BEFORE_TEST(HwyCombineTest);
 HWY_EXPORT_AND_TEST_P(HwyCombineTest, TestAllLowerHalf);
 HWY_EXPORT_AND_TEST_P(HwyCombineTest, TestAllUpperHalf);
 HWY_EXPORT_AND_TEST_P(HwyCombineTest, TestAllZeroExtendVector);
 HWY_EXPORT_AND_TEST_P(HwyCombineTest, TestAllCombine);
-HWY_EXPORT_AND_TEST_P(HwyCombineTest, TestAllCombineShiftRight);
+HWY_EXPORT_AND_TEST_P(HwyCombineTest, TestAllConcat);
 }  // namespace hwy
-#endif
+
+// Ought not to be necessary, but without this, no tests run on RVV.
+int main(int argc, char **argv) {
+  ::testing::InitGoogleTest(&argc, argv);
+  return RUN_ALL_TESTS();
+}
+
+#endif  // HWY_ONCE
 
 #else
 int main(int, char**) { return 0; }
diff --git a/third_party/highway/hwy/tests/compare_test.cc b/third_party/highway/hwy/tests/compare_test.cc
index 9e7803b87aca4..cbfcbc38e4924 100644
--- a/third_party/highway/hwy/tests/compare_test.cc
+++ b/third_party/highway/hwy/tests/compare_test.cc
@@ -26,25 +26,6 @@ HWY_BEFORE_NAMESPACE();
 namespace hwy {
 namespace HWY_NAMESPACE {
 
-// All types.
-struct TestMask {
-  template <typename T, class D>
-  HWY_NOINLINE void operator()(T /*unused*/, D d) {
-    const size_t N = Lanes(d);
-    auto lanes = AllocateAligned<T>(N);
-
-    std::fill(lanes.get(), lanes.get() + N, T(0));
-    const auto actual_false = MaskFromVec(Load(d, lanes.get()));
-    HWY_ASSERT_MASK_EQ(d, MaskFalse(d), actual_false);
-
-    memset(lanes.get(), 0xFF, N * sizeof(T));
-    const auto actual_true = MaskFromVec(Load(d, lanes.get()));
-    HWY_ASSERT_MASK_EQ(d, MaskTrue(d), actual_true);
-  }
-};
-
-HWY_NOINLINE void TestAllMask() { ForAllTypes(ForPartialVectors<TestMask>()); }
-
 // All types.
 struct TestEquality {
   template <typename T, class D>
@@ -57,8 +38,14 @@ struct TestEquality {
     const auto mask_true = MaskTrue(d);
 
     HWY_ASSERT_MASK_EQ(d, mask_false, Eq(v2, v3));
+    HWY_ASSERT_MASK_EQ(d, mask_false, Eq(v3, v2));
     HWY_ASSERT_MASK_EQ(d, mask_true, Eq(v2, v2));
     HWY_ASSERT_MASK_EQ(d, mask_true, Eq(v2, v2b));
+
+    HWY_ASSERT_MASK_EQ(d, mask_true, Ne(v2, v3));
+    HWY_ASSERT_MASK_EQ(d, mask_true, Ne(v3, v2));
+    HWY_ASSERT_MASK_EQ(d, mask_false, Ne(v2, v2));
+    HWY_ASSERT_MASK_EQ(d, mask_false, Ne(v2, v2b));
   }
 };
 
@@ -97,7 +84,7 @@ struct TestStrictInt {
     const T max = LimitsMax<T>();
     const auto v0 = Zero(d);
     const auto v2 = And(Iota(d, T(2)), Set(d, 127));  // 0..127
-    const auto vn = Neg(v2) - Set(d, 1);              // -1..-128
+    const auto vn = Sub(Neg(v2), Set(d, 1));          // -1..-128
 
     const auto mask_false = MaskFalse(d);
     const auto mask_true = MaskTrue(d);
@@ -131,14 +118,14 @@ struct TestStrictInt {
 };
 
 HWY_NOINLINE void TestAllStrictInt() {
-  ForSignedTypes(ForExtendableVectors<TestStrictInt>());
+  ForSignedTypes(ForPartialVectors<TestStrictInt>());
 }
 
 struct TestStrictFloat {
   template <typename T, class D>
   HWY_NOINLINE void operator()(T /*unused*/, D d) {
-    const T huge_neg = -1E35;
-    const T huge_pos = 1E36;
+    const T huge_neg = T(-1E35);
+    const T huge_pos = T(1E36);
     const auto v0 = Zero(d);
     const auto v2 = Iota(d, T(2));
     const auto vn = Neg(v2);
@@ -173,13 +160,13 @@ struct TestStrictFloat {
 };
 
 HWY_NOINLINE void TestAllStrictFloat() {
-  ForFloatTypes(ForExtendableVectors<TestStrictFloat>());
+  ForFloatTypes(ForPartialVectors<TestStrictFloat>());
 }
 
 struct TestWeakFloat {
   template <typename T, class D>
   HWY_NOINLINE void operator()(T /*unused*/, D d) {
-    const auto v2 = Iota(d, 2);
+    const auto v2 = Iota(d, T(2));
     const auto vn = Iota(d, -T(Lanes(d)));
 
     const auto mask_false = MaskFalse(d);
@@ -206,12 +193,19 @@ HWY_NOINLINE void TestAllWeakFloat() {
 HWY_AFTER_NAMESPACE();
 
 #if HWY_ONCE
+
 namespace hwy {
 HWY_BEFORE_TEST(HwyCompareTest);
-HWY_EXPORT_AND_TEST_P(HwyCompareTest, TestAllMask);
 HWY_EXPORT_AND_TEST_P(HwyCompareTest, TestAllEquality);
 HWY_EXPORT_AND_TEST_P(HwyCompareTest, TestAllStrictInt);
 HWY_EXPORT_AND_TEST_P(HwyCompareTest, TestAllStrictFloat);
 HWY_EXPORT_AND_TEST_P(HwyCompareTest, TestAllWeakFloat);
 }  // namespace hwy
+
+// Ought not to be necessary, but without this, no tests run on RVV.
+int main(int argc, char** argv) {
+  ::testing::InitGoogleTest(&argc, argv);
+  return RUN_ALL_TESTS();
+}
+
 #endif
diff --git a/third_party/highway/hwy/tests/convert_test.cc b/third_party/highway/hwy/tests/convert_test.cc
index 870955fcafbe0..6ec8680e09821 100644
--- a/third_party/highway/hwy/tests/convert_test.cc
+++ b/third_party/highway/hwy/tests/convert_test.cc
@@ -34,7 +34,10 @@ struct TestBitCast {
   template <typename T, class D>
   HWY_NOINLINE void operator()(T /*unused*/, D d) {
     const Repartition<ToT, D> dto;
-    HWY_ASSERT_EQ(Lanes(d) * sizeof(T), Lanes(dto) * sizeof(ToT));
+    const size_t N = Lanes(d);
+    const size_t Nto = Lanes(dto);
+    if (N == 0 || Nto == 0) return;
+    HWY_ASSERT_EQ(N * sizeof(T), Nto * sizeof(ToT));
     const auto vf = Iota(d, 1);
     const auto vt = BitCast(dto, vf);
     // Must return the same bits
@@ -130,8 +133,10 @@ HWY_NOINLINE void TestAllBitCast() {
 #endif  // HWY_CAP_INTEGER64
 #endif  // HWY_CAP_FLOAT64
 
+#if HWY_TARGET != HWY_SCALAR
   // For non-scalar vectors, we can cast all types to all.
-  ForAllTypes(ForGE128Vectors<TestBitCastFrom>());
+  ForAllTypes(ForGE64Vectors<TestBitCastFrom>());
+#endif
 }
 
 template <typename ToT>
@@ -160,39 +165,39 @@ struct TestPromoteTo {
 };
 
 HWY_NOINLINE void TestAllPromoteTo() {
-  const ForPartialVectors<TestPromoteTo<uint16_t>, 2> to_u16div2;
+  const ForPromoteVectors<TestPromoteTo<uint16_t>, 2> to_u16div2;
   to_u16div2(uint8_t());
 
-  const ForPartialVectors<TestPromoteTo<uint32_t>, 4> to_u32div4;
+  const ForPromoteVectors<TestPromoteTo<uint32_t>, 4> to_u32div4;
   to_u32div4(uint8_t());
 
-  const ForPartialVectors<TestPromoteTo<uint32_t>, 2> to_u32div2;
+  const ForPromoteVectors<TestPromoteTo<uint32_t>, 2> to_u32div2;
   to_u32div2(uint16_t());
 
-  const ForPartialVectors<TestPromoteTo<int16_t>, 2> to_i16div2;
+  const ForPromoteVectors<TestPromoteTo<int16_t>, 2> to_i16div2;
   to_i16div2(uint8_t());
   to_i16div2(int8_t());
 
-  const ForPartialVectors<TestPromoteTo<int32_t>, 2> to_i32div2;
+  const ForPromoteVectors<TestPromoteTo<int32_t>, 2> to_i32div2;
   to_i32div2(uint16_t());
   to_i32div2(int16_t());
 
-  const ForPartialVectors<TestPromoteTo<int32_t>, 4> to_i32div4;
+  const ForPromoteVectors<TestPromoteTo<int32_t>, 4> to_i32div4;
   to_i32div4(uint8_t());
   to_i32div4(int8_t());
 
   // Must test f16 separately because we can only load/store/convert them.
 
 #if HWY_CAP_INTEGER64
-  const ForPartialVectors<TestPromoteTo<uint64_t>, 2> to_u64div2;
+  const ForPromoteVectors<TestPromoteTo<uint64_t>, 2> to_u64div2;
   to_u64div2(uint32_t());
 
-  const ForPartialVectors<TestPromoteTo<int64_t>, 2> to_i64div2;
+  const ForPromoteVectors<TestPromoteTo<int64_t>, 2> to_i64div2;
   to_i64div2(int32_t());
 #endif
 
 #if HWY_CAP_FLOAT64
-  const ForPartialVectors<TestPromoteTo<double>, 2> to_f64div2;
+  const ForPromoteVectors<TestPromoteTo<double>, 2> to_f64div2;
   to_f64div2(int32_t());
   to_f64div2(float());
 #endif
@@ -224,14 +229,23 @@ struct TestDemoteTo {
     const T min = LimitsMin<ToT>();
     const T max = LimitsMax<ToT>();
 
+    const auto value_ok = [&](T& value) {
+      if (!IsFinite(value)) return false;
+#if HWY_EMULATE_SVE
+      // farm_sve just casts, which is undefined if the value is out of range.
+      value = HWY_MIN(HWY_MAX(min, value), max);
+#endif
+      return true;
+    };
+
     RandomState rng;
     for (size_t rep = 0; rep < 1000; ++rep) {
       for (size_t i = 0; i < N; ++i) {
         do {
           const uint64_t bits = rng();
           memcpy(&from[i], &bits, sizeof(T));
-        } while (!IsFinite(from[i]));
-        expected[i] = static_cast<ToT>(std::min(std::max(min, from[i]), max));
+        } while (!value_ok(from[i]));
+        expected[i] = static_cast<ToT>(HWY_MIN(HWY_MAX(min, from[i]), max));
       }
 
       HWY_ASSERT_VEC_EQ(to_d, expected.get(),
@@ -241,22 +255,22 @@ struct TestDemoteTo {
 };
 
 HWY_NOINLINE void TestAllDemoteToInt() {
-  ForDemoteVectors<TestDemoteTo<uint8_t>, 2>()(int16_t());
+  ForDemoteVectors<TestDemoteTo<uint8_t>>()(int16_t());
   ForDemoteVectors<TestDemoteTo<uint8_t>, 4>()(int32_t());
 
-  ForDemoteVectors<TestDemoteTo<int8_t>, 2>()(int16_t());
+  ForDemoteVectors<TestDemoteTo<int8_t>>()(int16_t());
   ForDemoteVectors<TestDemoteTo<int8_t>, 4>()(int32_t());
 
-  const ForDemoteVectors<TestDemoteTo<uint16_t>, 2> to_u16;
+  const ForDemoteVectors<TestDemoteTo<uint16_t>> to_u16;
   to_u16(int32_t());
 
-  const ForDemoteVectors<TestDemoteTo<int16_t>, 2> to_i16;
+  const ForDemoteVectors<TestDemoteTo<int16_t>> to_i16;
   to_i16(int32_t());
 }
 
 HWY_NOINLINE void TestAllDemoteToMixed() {
 #if HWY_CAP_FLOAT64
-  const ForDemoteVectors<TestDemoteTo<int32_t>, 2> to_i32;
+  const ForDemoteVectors<TestDemoteTo<int32_t>> to_i32;
   to_i32(double());
 #endif
 }
@@ -285,7 +299,7 @@ struct TestDemoteToFloat {
         const T max_abs = HighestValue<ToT>();
         // NOTE: std:: version from C++11 cmath is not defined in RVV GCC, see
         // https://lists.freebsd.org/pipermail/freebsd-current/2014-January/048130.html
-        const T clipped = copysign(std::min(magn, max_abs), from[i]);
+        const T clipped = copysign(HWY_MIN(magn, max_abs), from[i]);
         expected[i] = static_cast<ToT>(clipped);
       }
 
@@ -338,6 +352,7 @@ AlignedFreeUniquePtr<float[]> F16TestCases(D d, size_t& padded) {
 struct TestF16 {
   template <typename TF32, class DF32>
   HWY_NOINLINE void operator()(TF32 /*t*/, DF32 d32) {
+#if HWY_CAP_FLOAT16
     size_t padded;
     auto in = F16TestCases(d32, padded);
     using TF16 = float16_t;
@@ -350,10 +365,13 @@ struct TestF16 {
       Store(DemoteTo(d16, loaded), d16, temp16.get());
       HWY_ASSERT_VEC_EQ(d32, loaded, PromoteTo(d32, Load(d16, temp16.get())));
     }
+#else
+    (void)d32;
+#endif
   }
 };
 
-HWY_NOINLINE void TestAllF16() { ForDemoteVectors<TestF16, 2>()(float()); }
+HWY_NOINLINE void TestAllF16() { ForDemoteVectors<TestF16>()(float()); }
 
 struct TestConvertU8 {
   template <typename T, class D>
@@ -376,8 +394,9 @@ struct TestIntFromFloatHuge {
   template <typename TF, class DF>
   HWY_NOINLINE void operator()(TF /*unused*/, const DF df) {
     // Still does not work, although ARMv7 manual says that float->int
-    // saturates, i.e. chooses the nearest representable value.
-#if HWY_TARGET != HWY_NEON
+    // saturates, i.e. chooses the nearest representable value. Also causes
+    // out-of-memory for MSVC, and unsafe cast in farm_sve.
+#if HWY_TARGET != HWY_NEON && !HWY_COMPILER_MSVC && !defined(HWY_EMULATE_SVE)
     using TI = MakeSigned<TF>;
     const Rebind<TI, DF> di;
 
@@ -395,33 +414,33 @@ struct TestIntFromFloatHuge {
   }
 };
 
-struct TestIntFromFloat {
+class TestIntFromFloat {
   template <typename TF, class DF>
-  HWY_NOINLINE void operator()(TF /*unused*/, const DF df) {
+  static HWY_NOINLINE void TestPowers(TF /*unused*/, const DF df) {
     using TI = MakeSigned<TF>;
     const Rebind<TI, DF> di;
-    const size_t N = Lanes(df);
-
-    // Integer positive
-    HWY_ASSERT_VEC_EQ(di, Iota(di, TI(4)), ConvertTo(di, Iota(df, TF(4.0))));
-
-    // Integer negative
-    HWY_ASSERT_VEC_EQ(di, Iota(di, -TI(N)), ConvertTo(di, Iota(df, -TF(N))));
-
-    // Above positive
-    HWY_ASSERT_VEC_EQ(di, Iota(di, TI(2)), ConvertTo(di, Iota(df, TF(2.001))));
-
-    // Below positive
-    HWY_ASSERT_VEC_EQ(di, Iota(di, TI(3)), ConvertTo(di, Iota(df, TF(3.9999))));
-
-    const TF eps = static_cast<TF>(0.0001);
-    // Above negative
-    HWY_ASSERT_VEC_EQ(di, Iota(di, -TI(N)),
-                      ConvertTo(di, Iota(df, -TF(N + 1) + eps)));
+    constexpr size_t kBits = sizeof(TF) * 8;
+
+    // Powers of two, plus offsets to set some mantissa bits.
+    const uint64_t ofs_table[3] = {0ULL, 3ULL << (kBits / 2),
+                                   1ULL << (kBits - 15)};
+    for (int sign = 0; sign < 2; ++sign) {
+      for (size_t shift = 0; shift < kBits - 1; ++shift) {
+        for (uint64_t ofs : ofs_table) {
+          const int64_t mag = (int64_t(1) << shift) + ofs;
+          const int64_t val = sign ? mag : -mag;
+          HWY_ASSERT_VEC_EQ(di, Set(di, static_cast<TI>(val)),
+                            ConvertTo(di, Set(df, static_cast<TF>(val))));
+        }
+      }
+    }
+  }
 
-    // Below negative
-    HWY_ASSERT_VEC_EQ(di, Iota(di, -TI(N + 1)),
-                      ConvertTo(di, Iota(df, -TF(N + 1) - eps)));
+  template <typename TF, class DF>
+  static HWY_NOINLINE void TestRandom(TF /*unused*/, const DF df) {
+    using TI = MakeSigned<TF>;
+    const Rebind<TI, DF> di;
+    const size_t N = Lanes(df);
 
     // TF does not have enough precision to represent TI.
     const double min = static_cast<double>(LimitsMin<TI>());
@@ -437,6 +456,10 @@ struct TestIntFromFloat {
           const uint64_t bits = rng();
           memcpy(&from[i], &bits, sizeof(TF));
         } while (!std::isfinite(from[i]));
+#if defined(HWY_EMULATE_SVE)
+        // farm_sve just casts, which is undefined if the value is out of range.
+        from[i] = HWY_MIN(HWY_MAX(min / 2, from[i]), max / 2);
+#endif
         if (from[i] >= max) {
           expected[i] = LimitsMax<TI>();
         } else if (from[i] <= min) {
@@ -450,6 +473,38 @@ struct TestIntFromFloat {
                         ConvertTo(di, Load(df, from.get())));
     }
   }
+
+ public:
+  template <typename TF, class DF>
+  HWY_NOINLINE void operator()(TF tf, const DF df) {
+    using TI = MakeSigned<TF>;
+    const Rebind<TI, DF> di;
+    const size_t N = Lanes(df);
+
+    // Integer positive
+    HWY_ASSERT_VEC_EQ(di, Iota(di, TI(4)), ConvertTo(di, Iota(df, TF(4.0))));
+
+    // Integer negative
+    HWY_ASSERT_VEC_EQ(di, Iota(di, -TI(N)), ConvertTo(di, Iota(df, -TF(N))));
+
+    // Above positive
+    HWY_ASSERT_VEC_EQ(di, Iota(di, TI(2)), ConvertTo(di, Iota(df, TF(2.001))));
+
+    // Below positive
+    HWY_ASSERT_VEC_EQ(di, Iota(di, TI(3)), ConvertTo(di, Iota(df, TF(3.9999))));
+
+    const TF eps = static_cast<TF>(0.0001);
+    // Above negative
+    HWY_ASSERT_VEC_EQ(di, Iota(di, -TI(N)),
+                      ConvertTo(di, Iota(df, -TF(N + 1) + eps)));
+
+    // Below negative
+    HWY_ASSERT_VEC_EQ(di, Iota(di, -TI(N + 1)),
+                      ConvertTo(di, Iota(df, -TF(N + 1) - eps)));
+
+    TestPowers(tf, df);
+    TestRandom(tf, df);
+  }
 };
 
 HWY_NOINLINE void TestAllIntFromFloat() {
@@ -458,10 +513,10 @@ HWY_NOINLINE void TestAllIntFromFloat() {
 }
 
 struct TestFloatFromInt {
-  template <typename TI, class DI>
-  HWY_NOINLINE void operator()(TI /*unused*/, const DI di) {
-    using TF = MakeFloat<TI>;
-    const Rebind<TF, DI> df;
+  template <typename TF, class DF>
+  HWY_NOINLINE void operator()(TF /*unused*/, const DF df) {
+    using TI = MakeSigned<TF>;
+    const RebindToSigned<DF> di;
     const size_t N = Lanes(df);
 
     // Integer positive
@@ -481,10 +536,7 @@ struct TestFloatFromInt {
 };
 
 HWY_NOINLINE void TestAllFloatFromInt() {
-  ForPartialVectors<TestFloatFromInt>()(int32_t());
-#if HWY_CAP_FLOAT64 && HWY_CAP_INTEGER64
-  ForPartialVectors<TestFloatFromInt>()(int64_t());
-#endif
+  ForFloatTypes(ForPartialVectors<TestFloatFromInt>());
 }
 
 struct TestI32F64 {
@@ -521,14 +573,6 @@ struct TestI32F64 {
                       DemoteTo(di, Iota(df, -TF(N + 1) - eps)));
     HWY_ASSERT_VEC_EQ(df, Iota(df, TF(-2.0)), PromoteTo(df, Iota(di, TI(-2))));
 
-    // Huge positive float
-    HWY_ASSERT_VEC_EQ(di, Set(di, LimitsMax<TI>()),
-                      DemoteTo(di, Set(df, TF(1E12))));
-
-    // Huge negative float
-    HWY_ASSERT_VEC_EQ(di, Set(di, LimitsMin<TI>()),
-                      DemoteTo(di, Set(df, TF(-1E12))));
-
     // Max positive int
     HWY_ASSERT_VEC_EQ(df, Set(df, TF(LimitsMax<TI>())),
                       PromoteTo(df, Set(di, LimitsMax<TI>())));
@@ -536,12 +580,23 @@ struct TestI32F64 {
     // Min negative int
     HWY_ASSERT_VEC_EQ(df, Set(df, TF(LimitsMin<TI>())),
                       PromoteTo(df, Set(di, LimitsMin<TI>())));
+
+    // farm_sve just casts, which is undefined if the value is out of range.
+#if !defined(HWY_EMULATE_SVE)
+    // Huge positive float
+    HWY_ASSERT_VEC_EQ(di, Set(di, LimitsMax<TI>()),
+                      DemoteTo(di, Set(df, TF(1E12))));
+
+    // Huge negative float
+    HWY_ASSERT_VEC_EQ(di, Set(di, LimitsMin<TI>()),
+                      DemoteTo(di, Set(df, TF(-1E12))));
+#endif
   }
 };
 
 HWY_NOINLINE void TestAllI32F64() {
 #if HWY_CAP_FLOAT64
-  ForDemoteVectors<TestI32F64, 2>()(double());
+  ForDemoteVectors<TestI32F64>()(double());
 #endif
 }
 
@@ -552,6 +607,7 @@ HWY_NOINLINE void TestAllI32F64() {
 HWY_AFTER_NAMESPACE();
 
 #if HWY_ONCE
+
 namespace hwy {
 HWY_BEFORE_TEST(HwyConvertTest);
 HWY_EXPORT_AND_TEST_P(HwyConvertTest, TestAllBitCast);
@@ -565,4 +621,11 @@ HWY_EXPORT_AND_TEST_P(HwyConvertTest, TestAllIntFromFloat);
 HWY_EXPORT_AND_TEST_P(HwyConvertTest, TestAllFloatFromInt);
 HWY_EXPORT_AND_TEST_P(HwyConvertTest, TestAllI32F64);
 }  // namespace hwy
+
+// Ought not to be necessary, but without this, no tests run on RVV.
+int main(int argc, char** argv) {
+  ::testing::InitGoogleTest(&argc, argv);
+  return RUN_ALL_TESTS();
+}
+
 #endif
diff --git a/third_party/highway/hwy/tests/crypto_test.cc b/third_party/highway/hwy/tests/crypto_test.cc
new file mode 100644
index 0000000000000..c85d63af953eb
--- /dev/null
+++ b/third_party/highway/hwy/tests/crypto_test.cc
@@ -0,0 +1,549 @@
+// Copyright 2021 Google LLC
+//
+// Licensed under the Apache License, Version 2.0 (the "License");
+// you may not use this file except in compliance with the License.
+// You may obtain a copy of the License at
+//
+//      http://www.apache.org/licenses/LICENSE-2.0
+//
+// Unless required by applicable law or agreed to in writing, software
+// distributed under the License is distributed on an "AS IS" BASIS,
+// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+// See the License for the specific language governing permissions and
+// limitations under the License.
+
+#include <stddef.h>
+#include <stdint.h>
+#include <string.h>  // memcpy
+
+#include "hwy/aligned_allocator.h"
+
+#undef HWY_TARGET_INCLUDE
+#define HWY_TARGET_INCLUDE "tests/crypto_test.cc"
+#include "hwy/foreach_target.h"
+#include "hwy/highway.h"
+#include "hwy/tests/test_util-inl.h"
+
+HWY_BEFORE_NAMESPACE();
+namespace hwy {
+namespace HWY_NAMESPACE {
+
+#define HWY_PRINT_CLMUL_GOLDEN 0
+
+#if HWY_TARGET != HWY_SCALAR
+
+class TestAES {
+  template <typename T, class D>
+  HWY_NOINLINE void TestSBox(T /*unused*/, D d) {
+    // The generic implementation of the S-box is difficult to verify by
+    // inspection, so we add a white-box test that verifies it using enumeration
+    // (outputs for 0..255 vs. https://en.wikipedia.org/wiki/Rijndael_S-box).
+    const uint8_t sbox[256] = {
+        0x63, 0x7c, 0x77, 0x7b, 0xf2, 0x6b, 0x6f, 0xc5, 0x30, 0x01, 0x67, 0x2b,
+        0xfe, 0xd7, 0xab, 0x76, 0xca, 0x82, 0xc9, 0x7d, 0xfa, 0x59, 0x47, 0xf0,
+        0xad, 0xd4, 0xa2, 0xaf, 0x9c, 0xa4, 0x72, 0xc0, 0xb7, 0xfd, 0x93, 0x26,
+        0x36, 0x3f, 0xf7, 0xcc, 0x34, 0xa5, 0xe5, 0xf1, 0x71, 0xd8, 0x31, 0x15,
+        0x04, 0xc7, 0x23, 0xc3, 0x18, 0x96, 0x05, 0x9a, 0x07, 0x12, 0x80, 0xe2,
+        0xeb, 0x27, 0xb2, 0x75, 0x09, 0x83, 0x2c, 0x1a, 0x1b, 0x6e, 0x5a, 0xa0,
+        0x52, 0x3b, 0xd6, 0xb3, 0x29, 0xe3, 0x2f, 0x84, 0x53, 0xd1, 0x00, 0xed,
+        0x20, 0xfc, 0xb1, 0x5b, 0x6a, 0xcb, 0xbe, 0x39, 0x4a, 0x4c, 0x58, 0xcf,
+        0xd0, 0xef, 0xaa, 0xfb, 0x43, 0x4d, 0x33, 0x85, 0x45, 0xf9, 0x02, 0x7f,
+        0x50, 0x3c, 0x9f, 0xa8, 0x51, 0xa3, 0x40, 0x8f, 0x92, 0x9d, 0x38, 0xf5,
+        0xbc, 0xb6, 0xda, 0x21, 0x10, 0xff, 0xf3, 0xd2, 0xcd, 0x0c, 0x13, 0xec,
+        0x5f, 0x97, 0x44, 0x17, 0xc4, 0xa7, 0x7e, 0x3d, 0x64, 0x5d, 0x19, 0x73,
+        0x60, 0x81, 0x4f, 0xdc, 0x22, 0x2a, 0x90, 0x88, 0x46, 0xee, 0xb8, 0x14,
+        0xde, 0x5e, 0x0b, 0xdb, 0xe0, 0x32, 0x3a, 0x0a, 0x49, 0x06, 0x24, 0x5c,
+        0xc2, 0xd3, 0xac, 0x62, 0x91, 0x95, 0xe4, 0x79, 0xe7, 0xc8, 0x37, 0x6d,
+        0x8d, 0xd5, 0x4e, 0xa9, 0x6c, 0x56, 0xf4, 0xea, 0x65, 0x7a, 0xae, 0x08,
+        0xba, 0x78, 0x25, 0x2e, 0x1c, 0xa6, 0xb4, 0xc6, 0xe8, 0xdd, 0x74, 0x1f,
+        0x4b, 0xbd, 0x8b, 0x8a, 0x70, 0x3e, 0xb5, 0x66, 0x48, 0x03, 0xf6, 0x0e,
+        0x61, 0x35, 0x57, 0xb9, 0x86, 0xc1, 0x1d, 0x9e, 0xe1, 0xf8, 0x98, 0x11,
+        0x69, 0xd9, 0x8e, 0x94, 0x9b, 0x1e, 0x87, 0xe9, 0xce, 0x55, 0x28, 0xdf,
+        0x8c, 0xa1, 0x89, 0x0d, 0xbf, 0xe6, 0x42, 0x68, 0x41, 0x99, 0x2d, 0x0f,
+        0xb0, 0x54, 0xbb, 0x16};
+
+    // Ensure it's safe to load an entire vector by padding.
+    const size_t N = Lanes(d);
+    const size_t padded = RoundUpTo(256, N);
+    auto expected = AllocateAligned<T>(padded);
+    // Must wrap around to match the input (Iota).
+    for (size_t pos = 0; pos < padded;) {
+      const size_t remaining = HWY_MIN(padded - pos, size_t(256));
+      memcpy(expected.get() + pos, sbox, remaining);
+      pos += remaining;
+    }
+
+    for (size_t i = 0; i < 256; i += N) {
+      const auto in = Iota(d, i);
+      HWY_ASSERT_VEC_EQ(d, expected.get() + i, detail::SubBytes(in));
+    }
+  }
+
+ public:
+  template <typename T, class D>
+  HWY_NOINLINE void operator()(T t, D d) {
+    // Test vector (after first KeyAddition) from
+    // https://csrc.nist.gov/CSRC/media/Projects/Cryptographic-Standards-and-Guidelines/documents/examples/AES_Core128.pdf
+    alignas(16) constexpr uint8_t test_lanes[16] = {
+        0x40, 0xBF, 0xAB, 0xF4, 0x06, 0xEE, 0x4D, 0x30,
+        0x42, 0xCA, 0x6B, 0x99, 0x7A, 0x5C, 0x58, 0x16};
+    const auto test = LoadDup128(d, test_lanes);
+
+    // = MixColumn result
+    alignas(16) constexpr uint8_t expected0_lanes[16] = {
+        0x52, 0x9F, 0x16, 0xC2, 0x97, 0x86, 0x15, 0xCA,
+        0xE0, 0x1A, 0xAE, 0x54, 0xBA, 0x1A, 0x26, 0x59};
+    const auto expected0 = LoadDup128(d, expected0_lanes);
+
+    // = KeyAddition result
+    alignas(16) constexpr uint8_t expected_lanes[16] = {
+        0xF2, 0x65, 0xE8, 0xD5, 0x1F, 0xD2, 0x39, 0x7B,
+        0xC3, 0xB9, 0x97, 0x6D, 0x90, 0x76, 0x50, 0x5C};
+    const auto expected = LoadDup128(d, expected_lanes);
+
+    alignas(16) uint8_t key_lanes[16];
+    for (size_t i = 0; i < 16; ++i) {
+      key_lanes[i] = expected0_lanes[i] ^ expected_lanes[i];
+    }
+    const auto round_key = LoadDup128(d, key_lanes);
+
+    HWY_ASSERT_VEC_EQ(d, expected0, AESRound(test, Zero(d)));
+    HWY_ASSERT_VEC_EQ(d, expected, AESRound(test, round_key));
+
+    TestSBox(t, d);
+  }
+};
+HWY_NOINLINE void TestAllAES() { ForGE128Vectors<TestAES>()(uint8_t()); }
+
+#else
+HWY_NOINLINE void TestAllAES() {}
+#endif  // HWY_TARGET != HWY_SCALAR
+
+struct TestCLMul {
+  template <typename T, class D>
+  HWY_NOINLINE void operator()(T /*unused*/, D d) {
+    // needs 64 bit lanes and 128-bit result
+#if HWY_TARGET != HWY_SCALAR && HWY_CAP_INTEGER64
+    const size_t N = Lanes(d);
+    if (N == 1) return;
+
+    auto in1 = AllocateAligned<T>(N);
+    auto in2 = AllocateAligned<T>(N);
+
+    constexpr size_t kCLMulNum = 512;
+    // Depends on rng!
+    static constexpr uint64_t kCLMulLower[kCLMulNum] = {
+        0x24511d4ce34d6350ULL, 0x4ca582edde1236bbULL, 0x537e58f72dac25a8ULL,
+        0x4e942d5e130b9225ULL, 0x75a906c519257a68ULL, 0x1df9f85126d96c5eULL,
+        0x464e7c13f4ad286aULL, 0x138535ee35dabc40ULL, 0xb2f7477b892664ecULL,
+        0x01557b077167c25dULL, 0xf32682490ee49624ULL, 0x0025bac603b9e140ULL,
+        0xcaa86aca3e3daf40ULL, 0x1fbcfe4af73eb6c4ULL, 0x8ee8064dd0aae5dcULL,
+        0x1248cb547858c213ULL, 0x37a55ee5b10fb34cULL, 0x6eb5c97b958f86e2ULL,
+        0x4b1ab3eb655ea7cdULL, 0x1d66645a85627520ULL, 0xf8728e96daa36748ULL,
+        0x38621043e6ff5e3bULL, 0xd1d28b5da5ffefb4ULL, 0x0a5cd65931546df7ULL,
+        0x2a0639be3d844150ULL, 0x0e2d0f18c8d6f045ULL, 0xfacc770b963326c1ULL,
+        0x19611b31ca2ef141ULL, 0xabea29510dd87518ULL, 0x18a7dc4b205f2768ULL,
+        0x9d3975ea5612dc86ULL, 0x06319c139e374773ULL, 0x6641710400b4c390ULL,
+        0x356c29b6001c3670ULL, 0xe9e04d851e040a00ULL, 0x21febe561222d79aULL,
+        0xc071eaae6e148090ULL, 0x0eed351a0af94f5bULL, 0x04324eedb3c03688ULL,
+        0x39e89b136e0d6ccdULL, 0x07d0fd2777a31600ULL, 0x44b8573827209822ULL,
+        0x6d690229ea177d78ULL, 0x1b9749d960ba9f18ULL, 0x190945271c0fbb94ULL,
+        0x189aea0e07d2c88eULL, 0xf18eab6b65a6beb2ULL, 0x57744b21c13d0d84ULL,
+        0xf63050a613e95c2eULL, 0x12cd20d25f97102fULL, 0x5a5df0678dbcba60ULL,
+        0x0b08fb80948bfafcULL, 0x44cf1cbe7c6fc3c8ULL, 0x166a470ef25da288ULL,
+        0x2c498a609204e48cULL, 0x261b0a22585697ecULL, 0x737750574af7dde4ULL,
+        0x4079959c60b01e0cULL, 0x06ed8aac13f782d6ULL, 0x019d454ba9b5ef20ULL,
+        0xea1edbf96d49e858ULL, 0x17c2f3ebde9ac469ULL, 0x5cf72706e3d6f5e4ULL,
+        0x16e856aa3c841516ULL, 0x256f7e3cef83368eULL, 0x47e17c8eb2774e77ULL,
+        0x9b48ac150a804821ULL, 0x584523f61ccfdf22ULL, 0xedcb6a2a75d9e7f2ULL,
+        0x1fe3d1838e537aa7ULL, 0x778872e9f64549caULL, 0x2f1cea6f0d3faf92ULL,
+        0x0e8c4b6a9343f326ULL, 0x01902d1ba3048954ULL, 0xc5c1fd5269e91dc0ULL,
+        0x0ef8a4707817eb9cULL, 0x1f696f09a5354ca4ULL, 0x369cd9de808b818cULL,
+        0xf6917d1dd43fd784ULL, 0x7f4b76bf40dc166fULL, 0x4ce67698724ace12ULL,
+        0x02c3bf60e6e9cd92ULL, 0xb8229e45b21458e8ULL, 0x415efd41e91adf49ULL,
+        0x5edfcd516bb921cdULL, 0x5ff2c29429fd187eULL, 0x0af666b17103b3e0ULL,
+        0x1f5e4ff8f54c9a5bULL, 0x429253d8a5544ba6ULL, 0x19de2fdf9f4d9dcaULL,
+        0x29bf3d37ddc19a40ULL, 0x04d4513a879552baULL, 0x5cc7476cf71ee155ULL,
+        0x40011f8c238784a5ULL, 0x1a3ae50b0fd2ee2bULL, 0x7db22f432ba462baULL,
+        0x417290b0bee2284aULL, 0x055a6bd5bb853db2ULL, 0xaa667daeed8c2a34ULL,
+        0x0d6b316bda7f3577ULL, 0x72d35598468e3d5dULL, 0x375b594804bfd33aULL,
+        0x16ed3a319b540ae8ULL, 0x093bace4b4695afdULL, 0xc7118754ec2737ceULL,
+        0x0fff361f0505c81aULL, 0x996e9e7291321af0ULL, 0x496b1d9b0b89ba8cULL,
+        0x65a98b2e9181da9cULL, 0x70759c8dd45575dfULL, 0x3446fe727f5e2cbbULL,
+        0x1121ae609d195e74ULL, 0x5ff5d68ce8a21018ULL, 0x0e27eca3825b60d6ULL,
+        0x82f628bceca3d1daULL, 0x2756a0914e344047ULL, 0xa460406c1c708d50ULL,
+        0x63ce32a0c083e491ULL, 0xc883e5a685c480e0ULL, 0x602c951891e600f9ULL,
+        0x02ecb2e3911ca5f8ULL, 0x0d8675f4bb70781aULL, 0x43545cc3c78ea496ULL,
+        0x04164b01d6b011c2ULL, 0x3acbb323dcab2c9bULL, 0x31c5ba4e22793082ULL,
+        0x5a6484af5f7c2d10ULL, 0x1a929b16194e8078ULL, 0x7a6a75d03b313924ULL,
+        0x0553c73a35b1d525ULL, 0xf18628c51142be34ULL, 0x1b51cf80d7efd8f5ULL,
+        0x52e0ca4df63ee258ULL, 0x0e977099160650c9ULL, 0x6be1524e92024f70ULL,
+        0x0ee2152625438b9dULL, 0xfa32af436f6d8eb4ULL, 0x5ecf49c2154287e5ULL,
+        0x6b72f4ae3590569dULL, 0x086c5ee6e87bfb68ULL, 0x737a4f0dc04b6187ULL,
+        0x08c3439280edea41ULL, 0x9547944f01636c5cULL, 0x6acfbfc2571cd71fULL,
+        0x85d7842972449637ULL, 0x252ea5e5a7fad86aULL, 0x4e41468f99ba1632ULL,
+        0x095e0c3ae63b25a2ULL, 0xb005ce88fd1c9425ULL, 0x748e668abbe09f03ULL,
+        0xb2cfdf466b187d18ULL, 0x60b11e633d8fe845ULL, 0x07144c4d246db604ULL,
+        0x139bcaac55e96125ULL, 0x118679b5a6176327ULL, 0x1cebe90fa4d9f83fULL,
+        0x22244f52f0d312acULL, 0x669d4e17c9bfb713ULL, 0x96390e0b834bb0d0ULL,
+        0x01f7f0e82ba08071ULL, 0x2dffeee31ca6d284ULL, 0x1f4738745ef039feULL,
+        0x4ce0dd2b603b6420ULL, 0x0035fc905910a4d5ULL, 0x07df2b533df6fb04ULL,
+        0x1cee2735c9b910ddULL, 0x2bc4af565f7809eaULL, 0x2f876c1f5cb1076cULL,
+        0x33e079524099d056ULL, 0x169e0405d2f9efbaULL, 0x018643ab548a358cULL,
+        0x1bb6fc4331cffe92ULL, 0x05111d3a04e92faaULL, 0x23c27ecf0d638b73ULL,
+        0x1b79071dc1685d68ULL, 0x0662d20aba8e1e0cULL, 0xe7f6440277144c6fULL,
+        0x4ca38b64c22196c0ULL, 0x43c05f6d1936fbeeULL, 0x0654199d4d1faf0fULL,
+        0xf2014054e71c2d04ULL, 0x0a103e47e96b4c84ULL, 0x7986e691dd35b040ULL,
+        0x4e1ebb53c306a341ULL, 0x2775bb3d75d65ba6ULL, 0x0562ab0adeff0f15ULL,
+        0x3c2746ad5eba3eacULL, 0x1facdb5765680c60ULL, 0xb802a60027d81d00ULL,
+        0x1191d0f6366ae3a9ULL, 0x81a97b5ae0ea5d14ULL, 0x06bee05b6178a770ULL,
+        0xc7baeb2fe1d6aeb3ULL, 0x594cb5b867d04fdfULL, 0xf515a80138a4e350ULL,
+        0x646417ad8073cf38ULL, 0x4a229a43373fb8d4ULL, 0x10fa6eafff1ca453ULL,
+        0x9f060700895cc731ULL, 0x00521133d11d11f4ULL, 0xb940a2bb912a7a5cULL,
+        0x3fab180670ad2a3cULL, 0x45a5f0e5b6fdb95dULL, 0x27c1baad6f946b15ULL,
+        0x336c6bdbe527cf58ULL, 0x3b83aa602a5baea3ULL, 0xdf749153f9bcc376ULL,
+        0x1a05513a6c0b4a90ULL, 0xb81e0b570a075c47ULL, 0x471fabb40bdc27ceULL,
+        0x9dec9472f6853f60ULL, 0x361f71b88114193bULL, 0x3b550a8c4feeff00ULL,
+        0x0f6cde5a68bc9bc0ULL, 0x3f50121a925703e0ULL, 0x6967ff66d6d343a9ULL,
+        0xff6b5bd2ce7bc3ccULL, 0x05474cea08bf6cd8ULL, 0xf76eabbfaf108eb0ULL,
+        0x067529be4fc6d981ULL, 0x4d766b137cf8a988ULL, 0x2f09c7395c5cfbbdULL,
+        0x388793712da06228ULL, 0x02c9ff342c8f339aULL, 0x152c734139a860a3ULL,
+        0x35776eb2b270c04dULL, 0x0f8d8b41f11c4608ULL, 0x0c2071665be6b288ULL,
+        0xc034e212b3f71d88ULL, 0x071d961ef3276f99ULL, 0xf98598ee75b60773ULL,
+        0x062062c58c6724e4ULL, 0xd156438e2125572cULL, 0x38552d59a7f0f7c8ULL,
+        0x1a402178206e413cULL, 0x1f1f996c68293b26ULL, 0x8bce3cafe1730f7eULL,
+        0x2d0480a0828f6bf5ULL, 0x6c99cffa171f92f6ULL, 0x0087f842bb0ac681ULL,
+        0x11d7ed06e1e7fd3eULL, 0x07cb1186f2385dc6ULL, 0x5d7763ebff1e170fULL,
+        0x2dacc870231ac292ULL, 0x8486317a9ffb390cULL, 0x1c3a6dd20c959ac6ULL,
+        0x90dc96e3992e06b8ULL, 0x70d60bfa33e72b67ULL, 0x70c9bddd0985ee63ULL,
+        0x012c9767b3673093ULL, 0xfcd3bc5580f6a88aULL, 0x0ac80017ef6308c3ULL,
+        0xdb67d709ef4bba09ULL, 0x4c63e324f0e247ccULL, 0xa15481d3fe219d60ULL,
+        0x094c4279cdccb501ULL, 0x965a28c72575cb82ULL, 0x022869db25e391ebULL,
+        0x37f528c146023910ULL, 0x0c1290636917deceULL, 0x9aee25e96251ca9cULL,
+        0x728ac5ba853b69c2ULL, 0x9f272c93c4be20c8ULL, 0x06c1aa6319d28124ULL,
+        0x4324496b1ca8a4f7ULL, 0x0096ecfe7dfc0189ULL, 0x9e06131b19ae0020ULL,
+        0x15278b15902f4597ULL, 0x2a9fece8c13842d8ULL, 0x1d4e6781f0e1355eULL,
+        0x6855b712d3dbf7c0ULL, 0x06a07fad99be6f46ULL, 0x3ed9d7957e4d1d7cULL,
+        0x0c326f7cbc248bb2ULL, 0xe6363ad2c537cf51ULL, 0x0e12eb1c40723f13ULL,
+        0xf5c6ac850afba803ULL, 0x0322a79d615fa9f0ULL, 0x6116696ed97bd5f8ULL,
+        0x0d438080fbbdc9f1ULL, 0x2e4dc42c38f1e243ULL, 0x64948e9104f3a5bfULL,
+        0x9fd622371bdb5f00ULL, 0x0f12bf082b2a1b6eULL, 0x4b1f8d867d78031cULL,
+        0x134392ea9f5ef832ULL, 0xf3d70472321bc23eULL, 0x05fcbe5e9eea268eULL,
+        0x136dede7175a22cfULL, 0x1308f8baac2cbcccULL, 0xd691026f0915eb64ULL,
+        0x0e49a668345c3a38ULL, 0x24ddbbe8bc96f331ULL, 0x4d2ec9479b640578ULL,
+        0x450f0697327b359cULL, 0x32b45360f4488ee0ULL, 0x4f6d9ecec46a105aULL,
+        0x5500c63401ae8e80ULL, 0x47dea495cf6f98baULL, 0x13dc9a2dfca80babULL,
+        0xe6f8a93f7b24ca92ULL, 0x073f57a6d900a87fULL, 0x9ddb935fd3aa695aULL,
+        0x101e98d24b39e8aaULL, 0x6b8d0eb95a507ddcULL, 0x45a908b3903d209bULL,
+        0x6c96a3e119e617d4ULL, 0x2442787543d3be48ULL, 0xd3bc055c7544b364ULL,
+        0x7693bb042ca8653eULL, 0xb95e3a4ea5d0101eULL, 0x116f0d459bb94a73ULL,
+        0x841244b72cdc5e90ULL, 0x1271acced6cb34d3ULL, 0x07d289106524d638ULL,
+        0x537c9cf49c01b5bbULL, 0x8a8e16706bb7a5daULL, 0x12e50a9c499dc3a9ULL,
+        0x1cade520db2ba830ULL, 0x1add52f000d7db70ULL, 0x12cf15db2ce78e30ULL,
+        0x0657eaf606bfc866ULL, 0x4026816d3b05b1d0ULL, 0x1ba0ebdf90128e4aULL,
+        0xdfd649375996dd6eULL, 0x0f416e906c23d9aeULL, 0x384273cad0582a24ULL,
+        0x2ff27b0378a46189ULL, 0xc4ecd18a2d7a7616ULL, 0x35cef0b5cd51d640ULL,
+        0x7d582363643f48b7ULL, 0x0984ad746ad0ab7cULL, 0x2990a999835f9688ULL,
+        0x2d4df66a97b19e05ULL, 0x592c79720af99aa2ULL, 0x052863c230602cd3ULL,
+        0x5f5e2b15edcf2840ULL, 0x01dff1b694b978b0ULL, 0x14345a48b622025eULL,
+        0x028fab3b6407f715ULL, 0x3455d188e6feca50ULL, 0x1d0d40288fb1b5fdULL,
+        0x4685c5c2b6a1e5aeULL, 0x3a2077b1e5fe5adeULL, 0x1bc55d611445a0d8ULL,
+        0x05480ae95f3f83feULL, 0xbbb59cfcf7e17fb6ULL, 0x13f7f10970bbb990ULL,
+        0x6d00ac169425a352ULL, 0x7da0db397ef2d5d3ULL, 0x5b512a247f8d2479ULL,
+        0x637eaa6a977c3c32ULL, 0x3720f0ae37cba89cULL, 0x443df6e6aa7f525bULL,
+        0x28664c287dcef321ULL, 0x03c267c00cf35e49ULL, 0x690185572d4021deULL,
+        0x2707ff2596e321c2ULL, 0xd865f5af7722c380ULL, 0x1ea285658e33aafbULL,
+        0xc257c5e88755bef4ULL, 0x066f67275cfcc31eULL, 0xb09931945cc0fed0ULL,
+        0x58c1dc38d6e3a03fULL, 0xf99489678fc94ee8ULL, 0x75045bb99be5758aULL,
+        0x6c163bc34b40feefULL, 0x0420063ce7bdd3b4ULL, 0xf86ef10582bf2e28ULL,
+        0x162c3449ca14858cULL, 0x94106aa61dfe3280ULL, 0x4073ae7a4e7e4941ULL,
+        0x32b13fd179c250b4ULL, 0x0178fbb216a7e744ULL, 0xf840ae2f1cf92669ULL,
+        0x18fc709acc80243dULL, 0x20ac2ebd69f4d558ULL, 0x6e580ad9c73ad46aULL,
+        0x76d2b535b541c19dULL, 0x6c7a3fb9dd0ce0afULL, 0xc3481689b9754f28ULL,
+        0x156e813b6557abdbULL, 0x6ee372e31276eb10ULL, 0x19cf37c038c8d381ULL,
+        0x00d4d906c9ae3072ULL, 0x09f03cbb6dfbfd40ULL, 0x461ba31c4125f3cfULL,
+        0x25b29fc63ad9f05bULL, 0x6808c95c2dddede9ULL, 0x0564224337066d9bULL,
+        0xc87eb5f4a4d966f2ULL, 0x66fc66e1701f5847ULL, 0xc553a3559f74da28ULL,
+        0x1dfd841be574df43ULL, 0x3ee2f100c3ebc082ULL, 0x1a2c4f9517b56e89ULL,
+        0x502f65c4b535c8ffULL, 0x1da5663ab6f96ec0ULL, 0xba1f80b73988152cULL,
+        0x364ff12182ac8dc1ULL, 0xe3457a3c4871db31ULL, 0x6ae9cadf92fd7e84ULL,
+        0x9621ba3d6ca15186ULL, 0x00ff5af878c144ceULL, 0x918464dc130101a4ULL,
+        0x036511e6b187efa6ULL, 0x06667d66550ff260ULL, 0x7fd18913f9b51bc1ULL,
+        0x3740e6b27af77aa8ULL, 0x1f546c2fd358ff8aULL, 0x42f1424e3115c891ULL,
+        0x03767db4e3a1bb33ULL, 0xa171a1c564345060ULL, 0x0afcf632fd7b1324ULL,
+        0xb59508d933ffb7d0ULL, 0x57d766c42071be83ULL, 0x659f0447546114a2ULL,
+        0x4070364481c460aeULL, 0xa2b9752280644d52ULL, 0x04ab884bea5771bdULL,
+        0x87cd135602a232b4ULL, 0x15e54cd9a8155313ULL, 0x1e8005efaa3e1047ULL,
+        0x696b93f4ab15d39fULL, 0x0855a8e540de863aULL, 0x0bb11799e79f9426ULL,
+        0xeffa61e5c1b579baULL, 0x1e060a1d11808219ULL, 0x10e219205667c599ULL,
+        0x2f7b206091c49498ULL, 0xb48854c820064860ULL, 0x21c4aaa3bfbe4a38ULL,
+        0x8f4a032a3fa67e9cULL, 0x3146b3823401e2acULL, 0x3afee26f19d88400ULL,
+        0x167087c485791d38ULL, 0xb67a1ed945b0fb4bULL, 0x02436eb17e27f1c0ULL,
+        0xe05afce2ce2d2790ULL, 0x49c536fc6224cfebULL, 0x178865b3b862b856ULL,
+        0x1ce530de26acde5bULL, 0x87312c0b30a06f38ULL, 0x03e653b578558d76ULL,
+        0x4d3663c21d8b3accULL, 0x038003c23626914aULL, 0xd9d5a2c052a09451ULL,
+        0x39b5acfe08a49384ULL, 0x40f349956d5800e4ULL, 0x0968b6950b1bd8feULL,
+        0xd60b2ca030f3779cULL, 0x7c8bc11a23ce18edULL, 0xcc23374e27630bc2ULL,
+        0x2e38fc2a8bb33210ULL, 0xe421357814ee5c44ULL, 0x315fb65ea71ec671ULL,
+        0xfb1b0223f70ed290ULL, 0x30556c9f983eaf07ULL, 0x8dd438c3d0cd625aULL,
+        0x05a8fd0c7ffde71bULL, 0x764d1313b5aeec7aULL, 0x2036af5de9622f47ULL,
+        0x508a5bfadda292feULL, 0x3f77f04ba2830e90ULL, 0x9047cd9c66ca66d2ULL,
+        0x1168b5318a54eb21ULL, 0xc93462d221da2e15ULL, 0x4c2c7cc54abc066eULL,
+        0x767a56fec478240eULL, 0x095de72546595bd3ULL, 0xc9da535865158558ULL,
+        0x1baccf36f33e73fbULL, 0xf3d7dbe64df77f18ULL, 0x1f8ebbb7be4850b8ULL,
+        0x043c5ed77bce25a1ULL, 0x07d401041b2a178aULL, 0x9181ebb8bd8d5618ULL,
+        0x078b935dc3e4034aULL, 0x7b59c08954214300ULL, 0x03570dc2a4f84421ULL,
+        0xdd8715b82f6b4078ULL, 0x2bb49c8bb544163bULL, 0xc9eb125564d59686ULL,
+        0x5fdc7a38f80b810aULL, 0x3a4a6d8fff686544ULL, 0x28360e2418627d3aULL,
+        0x60874244c95ed992ULL, 0x2115cc1dd9c34ed3ULL, 0xfaa3ef61f55e9efcULL,
+        0x27ac9b1ef1adc7e6ULL, 0x95ea00478fec3f54ULL, 0x5aea808b2d99ab43ULL,
+        0xc8f79e51fe43a580ULL, 0x5dbccd714236ce25ULL, 0x783fa76ed0753458ULL,
+        0x48cb290f19d84655ULL, 0xc86a832f7696099aULL, 0x52f30c6fec0e71d3ULL,
+        0x77d4e91e8cdeb886ULL, 0x7169a703c6a79ccdULL, 0x98208145b9596f74ULL,
+        0x0945695c761c0796ULL, 0x0be897830d17bae0ULL, 0x033ad3924caeeeb4ULL,
+        0xedecb6cfa2d303a8ULL, 0x3f86b074818642e7ULL, 0xeefa7c878a8b03f4ULL,
+        0x093c101b80922551ULL, 0xfb3b4e6c26ac0034ULL, 0x162bf87999b94f5eULL,
+        0xeaedae76e975b17cULL, 0x1852aa090effe18eULL};
+
+    static constexpr uint64_t kCLMulUpper[kCLMulNum] = {
+        0xbb41199b1d587c69ULL, 0x514d94d55894ee29ULL, 0xebc6cd4d2efd5d16ULL,
+        0x042044ad2de477fdULL, 0xb865c8b0fcdf4b15ULL, 0x0724d7e551cc40f3ULL,
+        0xb15a16f39edb0bccULL, 0x37d64419ede7a171ULL, 0x2aa01bb80c753401ULL,
+        0x06ff3f8a95fdaf4dULL, 0x79898cc0838546deULL, 0x776acbd1b237c60aULL,
+        0x4c1753be4f4e0064ULL, 0x0ba9243601206ed3ULL, 0xd567c3b1bf3ec557ULL,
+        0x043fac7bcff61fb3ULL, 0x49356232b159fb2fULL, 0x3910c82038102d4dULL,
+        0x30592fef753eb300ULL, 0x7b2660e0c92a9e9aULL, 0x8246c9248d671ef0ULL,
+        0x5a0dcd95147af5faULL, 0x43fde953909cc0eaULL, 0x06147b972cb96e1bULL,
+        0xd84193a6b2411d80ULL, 0x00cd7711b950196fULL, 0x1088f9f4ade7fa64ULL,
+        0x05a13096ec113cfbULL, 0x958d816d53b00edcULL, 0x3846154a7cdba9cbULL,
+        0x8af516db6b27d1e6ULL, 0x1a1d462ab8a33b13ULL, 0x4040b0ac1b2c754cULL,
+        0x05127fe9af2fe1d6ULL, 0x9f96e79374321fa6ULL, 0x06ff64a4d9c326f3ULL,
+        0x28709566e158ac15ULL, 0x301701d7111ca51cULL, 0x31e0445d1b9d9544ULL,
+        0x0a95aff69bf1d03eULL, 0x7c298c8414ecb879ULL, 0x00801499b4143195ULL,
+        0x91521a00dd676a5cULL, 0x2777526a14c2f723ULL, 0xfa26aac6a6357dddULL,
+        0x1d265889b0187a4bULL, 0xcd6e70fa8ed283e4ULL, 0x18a815aa50ea92caULL,
+        0xc01e082694a263c6ULL, 0x4b40163ba53daf25ULL, 0xbc658caff6501673ULL,
+        0x3ba35359586b9652ULL, 0x74f96acc97a4936cULL, 0x3989dfdb0cf1d2cfULL,
+        0x358a01eaa50dda32ULL, 0x01109a5ed8f0802bULL, 0x55b84922e63c2958ULL,
+        0x55b14843d87551d5ULL, 0x1db8ec61b1b578d8ULL, 0x79a2d49ef8c3658fULL,
+        0xa304516816b3fbe0ULL, 0x163ecc09cc7b82f9ULL, 0xab91e8d22aabef00ULL,
+        0x0ed6b09262de8354ULL, 0xcfd47d34cf73f6f2ULL, 0x7dbd1db2390bc6c3ULL,
+        0x5ae789d3875e7b00ULL, 0x1d60fd0e70fe8fa4ULL, 0x690bc15d5ae4f6f5ULL,
+        0x121ef5565104fb44ULL, 0x6e98e89297353b54ULL, 0x42554949249d62edULL,
+        0xd6d6d16b12df78d2ULL, 0x320b33549b74975dULL, 0xd2a0618763d22e00ULL,
+        0x0808deb93cba2017ULL, 0x01bd3b2302a2cc70ULL, 0x0b7b8dd4d71c8dd6ULL,
+        0x34d60a3382a0756cULL, 0x40984584c8219629ULL, 0xf1152cba10093a66ULL,
+        0x068001c6b2159ccbULL, 0x3d70f13c6cda0800ULL, 0x0e6b6746a322b956ULL,
+        0x83a494319d8c770bULL, 0x0faecf64a8553e9aULL, 0xa34919222c39b1bcULL,
+        0x0c63850d89e71c6fULL, 0x585f0bee92e53dc8ULL, 0x10f222b13b4fa5deULL,
+        0x61573114f94252f2ULL, 0x09d59c311fba6c27ULL, 0x014effa7da49ed4eULL,
+        0x4a400a1bc1c31d26ULL, 0xc9091c047b484972ULL, 0x3989f341ec2230ccULL,
+        0xdcb03a98b3aee41eULL, 0x4a54a676a33a95e1ULL, 0xe499b7753951ef7cULL,
+        0x2f43b1d1061d8b48ULL, 0xc3313bdc68ceb146ULL, 0x5159f6bc0e99227fULL,
+        0x98128e6d9c05efcaULL, 0x15ea32b27f77815bULL, 0xe882c054e2654eecULL,
+        0x003d2cdb8faee8c6ULL, 0xb416dd333a9fe1dfULL, 0x73f6746aefcfc98bULL,
+        0x93dc114c10a38d70ULL, 0x05055941657845eaULL, 0x2ed7351347349334ULL,
+        0x26fb1ee2c69ae690ULL, 0xa4575d10dc5b28e0ULL, 0x3395b11295e485ebULL,
+        0xe840f198a224551cULL, 0x78e6e5a431d941d4ULL, 0xa1fee3ceab27f391ULL,
+        0x07d35b3c5698d0dcULL, 0x983c67fca9174a29ULL, 0x2bb6bbae72b5144aULL,
+        0xa7730b8d13ce58efULL, 0x51b5272883de1998ULL, 0xb334e128bb55e260ULL,
+        0x1cacf5fbbe1b9974ULL, 0x71a9df4bb743de60ULL, 0x5176fe545c2d0d7aULL,
+        0xbe592ecf1a16d672ULL, 0x27aa8a30c3efe460ULL, 0x4c78a32f47991e06ULL,
+        0x383459294312f26aULL, 0x97ba789127f1490cULL, 0x51c9aa8a3abd1ef1ULL,
+        0xcc7355188121e50fULL, 0x0ecb3a178ae334c1ULL, 0x84879a5e574b7160ULL,
+        0x0765298f6389e8f3ULL, 0x5c6750435539bb22ULL, 0x11a05cf056c937b5ULL,
+        0xb5dc2172dbfb7662ULL, 0x3ffc17915d9f40e8ULL, 0xbc7904daf3b431b0ULL,
+        0x71f2088490930a7cULL, 0xa89505fd9efb53c4ULL, 0x02e194afd61c5671ULL,
+        0x99a97f4abf35fcecULL, 0x26830aad30fae96fULL, 0x4b2abc16b25cf0b0ULL,
+        0x07ec6fffa1cafbdbULL, 0xf38188fde97a280cULL, 0x121335701afff64dULL,
+        0xea5ef38b4e672a64ULL, 0x477edbcae3eabf03ULL, 0xa32813cc0e0d244dULL,
+        0x13346d2af4972eefULL, 0xcbc18357af1cfa9aULL, 0x561b630316e73fa6ULL,
+        0xe9dfb53249249305ULL, 0x5d2b9dd1479312eeULL, 0x3458008119b56d04ULL,
+        0x50e6790b49801385ULL, 0x5bb9febe2349492bULL, 0x0c2813954299098fULL,
+        0xf747b0c890a071d5ULL, 0x417e8f82cc028d77ULL, 0xa134fee611d804f8ULL,
+        0x24c99ee9a0408761ULL, 0x3ebb224e727137f3ULL, 0x0686022073ceb846ULL,
+        0xa05e901fb82ad7daULL, 0x0ece7dc43ab470fcULL, 0x2d334ecc58f7d6a3ULL,
+        0x23166fadacc54e40ULL, 0x9c3a4472f839556eULL, 0x071717ab5267a4adULL,
+        0xb6600ac351ba3ea0ULL, 0x30ec748313bb63d4ULL, 0xb5374e39287b23ccULL,
+        0x074d75e784238aebULL, 0x77315879243914a4ULL, 0x3bbb1971490865f1ULL,
+        0xa355c21f4fbe02d3ULL, 0x0027f4bb38c8f402ULL, 0xeef8708e652bc5f0ULL,
+        0x7b9aa56cf9440050ULL, 0x113ac03c16cfc924ULL, 0x395db36d3e4bef9fULL,
+        0x5d826fabcaa597aeULL, 0x2a77d3c58786d7e0ULL, 0x85996859a3ba19d4ULL,
+        0x01e7e3c904c2d97fULL, 0x34f90b9b98d51fd0ULL, 0x243aa97fd2e99bb7ULL,
+        0x40a0cebc4f65c1e8ULL, 0x46d3922ed4a5503eULL, 0x446e7ecaf1f9c0a4ULL,
+        0x49dc11558bc2e6aeULL, 0xe7a9f20881793af8ULL, 0x5771cc4bc98103f1ULL,
+        0x2446ea6e718fce90ULL, 0x25d14aca7f7da198ULL, 0x4347af186f9af964ULL,
+        0x10cb44fc9146363aULL, 0x8a35587afce476b4ULL, 0x575144662fee3d3aULL,
+        0x69f41177a6bc7a05ULL, 0x02ff8c38d6b3c898ULL, 0x57c73589a226ca40ULL,
+        0x732f6b5baae66683ULL, 0x00c008bbedd4bb34ULL, 0x7412ff09524d6cadULL,
+        0xb8fd0b5ad8c145a8ULL, 0x74bd9f94b6cdc7dfULL, 0x68233b317ca6c19cULL,
+        0x314b9c2c08b15c54ULL, 0x5bd1ad72072ebd08ULL, 0x6610e6a6c07030e4ULL,
+        0xa4fc38e885ead7ceULL, 0x36975d1ca439e034ULL, 0xa358f0fe358ffb1aULL,
+        0x38e247ad663acf7dULL, 0x77daed3643b5deb8ULL, 0x5507c2aeae1ec3d0ULL,
+        0xfdec226c73acf775ULL, 0x1b87ff5f5033492dULL, 0xa832dee545d9033fULL,
+        0x1cee43a61e41783bULL, 0xdff82b2e2d822f69ULL, 0x2bbc9a376cb38cf2ULL,
+        0x117b1cdaf765dc02ULL, 0x26a407f5682be270ULL, 0x8eb664cf5634af28ULL,
+        0x17cb4513bec68551ULL, 0xb0df6527900cbfd0ULL, 0x335a2dc79c5afdfcULL,
+        0xa2f0ca4cd38dca88ULL, 0x1c370713b81a2de1ULL, 0x849d5df654d1adfcULL,
+        0x2fd1f7675ae14e44ULL, 0x4ff64dfc02247f7bULL, 0x3a2bcf40e395a48dULL,
+        0x436248c821b187c1ULL, 0x29f4337b1c7104c0ULL, 0xfc317c46e6630ec4ULL,
+        0x2774bccc4e3264c7ULL, 0x2d03218d9d5bee23ULL, 0x36a0ed04d659058aULL,
+        0x452484461573cab6ULL, 0x0708edf87ed6272bULL, 0xf07960a1587446cbULL,
+        0x3660167b067d84e0ULL, 0x65990a6993ddf8c4ULL, 0x0b197cd3d0b40b3fULL,
+        0x1dcec4ab619f3a05ULL, 0x722ab223a84f9182ULL, 0x0822d61a81e7c38fULL,
+        0x3d22ad75da563201ULL, 0x93cef6979fd35e0fULL, 0x05c3c25ae598b14cULL,
+        0x1338df97dd496377ULL, 0x15bc324dc9c20acfULL, 0x96397c6127e6e8cfULL,
+        0x004d01069ef2050fULL, 0x2fcf2e27893fdcbcULL, 0x072f77c3e44f4a5cULL,
+        0x5eb1d80b3fe44918ULL, 0x1f59e7c28cc21f22ULL, 0x3390ce5df055c1f8ULL,
+        0x4c0ef11df92cb6bfULL, 0x50f82f9e0848c900ULL, 0x08d0fde3ffc0ae38ULL,
+        0xbd8d0089a3fbfb73ULL, 0x118ba5b0f311ef59ULL, 0x9be9a8407b926a61ULL,
+        0x4ea04fbb21318f63ULL, 0xa1c8e7bb07b871ffULL, 0x1253a7262d5d3b02ULL,
+        0x13e997a0512e5b29ULL, 0x54318460ce9055baULL, 0x4e1d8a4db0054798ULL,
+        0x0b235226e2cade32ULL, 0x2588732c1476b315ULL, 0x16a378750ba8ac68ULL,
+        0xba0b116c04448731ULL, 0x4dd02bd47694c2f1ULL, 0x16d6797b218b6b25ULL,
+        0x769eb3709cfbf936ULL, 0x197746a0ce396f38ULL, 0x7d17ad8465961d6eULL,
+        0xfe58f4998ae19bb4ULL, 0x36df24305233ce69ULL, 0xb88a4eb008f4ee72ULL,
+        0x302b2eb923334787ULL, 0x15a4e3edbe13d448ULL, 0x39a4bf64dd7730ceULL,
+        0xedf25421b31090c4ULL, 0x4d547fc131be3b69ULL, 0x2b316e120ca3b90eULL,
+        0x0faf2357bf18a169ULL, 0x71f34b54ee2c1d62ULL, 0x18eaf6e5c93a3824ULL,
+        0x7e168ba03c1b4c18ULL, 0x1a534dd586d9e871ULL, 0xa2cccd307f5f8c38ULL,
+        0x2999a6fb4dce30f6ULL, 0x8f6d3b02c1d549a6ULL, 0x5cf7f90d817aac5aULL,
+        0xd2a4ceefe66c8170ULL, 0x11560edc4ca959feULL, 0x89e517e6f0dc464dULL,
+        0x75bb8972dddd2085ULL, 0x13859ed1e459d65aULL, 0x057114653326fa84ULL,
+        0xe2e6f465173cc86cULL, 0x0ada4076497d7de4ULL, 0xa856fa10ec6dbf8aULL,
+        0x41505d9a7c25d875ULL, 0x3091b6278382eccdULL, 0x055737185b2c3f13ULL,
+        0x2f4df8ecd6f9c632ULL, 0x0633e89c33552d98ULL, 0xf7673724d16db440ULL,
+        0x7331bd08e636c391ULL, 0x0252f29672fee426ULL, 0x1fc384946b6b9ddeULL,
+        0x03460c12c901443aULL, 0x003a0792e10abcdaULL, 0x8dbec31f624e37d0ULL,
+        0x667420d5bfe4dcbeULL, 0xfbfa30e874ed7641ULL, 0x46d1ae14db7ecef6ULL,
+        0x216bd7e8f5448768ULL, 0x32bcd40d3d69cc88ULL, 0x2e991dbc39b65abeULL,
+        0x0e8fb123a502f553ULL, 0x3d2d486b2c7560c0ULL, 0x09aba1db3079fe03ULL,
+        0xcb540c59398c9bceULL, 0x363970e5339ed600ULL, 0x2caee457c28af00eULL,
+        0x005e7d7ee47f41a0ULL, 0x69fad3eb10f44100ULL, 0x048109388c75beb3ULL,
+        0x253dddf96c7a6fb8ULL, 0x4c47f705b9d47d09ULL, 0x6cec894228b5e978ULL,
+        0x04044bb9f8ff45c2ULL, 0x079e75704d775caeULL, 0x073bd54d2a9e2c33ULL,
+        0xcec7289270a364fbULL, 0x19e7486f19cd9e4eULL, 0xb50ac15b86b76608ULL,
+        0x0620cf81f165c812ULL, 0x63eaaf13be7b11d4ULL, 0x0e0cf831948248c2ULL,
+        0xf0412df8f46e7957ULL, 0x671c1fe752517e3fULL, 0x8841bfb04dd3f540ULL,
+        0x122de4142249f353ULL, 0x40a4959fb0e76870ULL, 0x25cfd3d4b4bbc459ULL,
+        0x78a07c82930c60d0ULL, 0x12c2de24d4cbc969ULL, 0x85d44866096ad7f4ULL,
+        0x1fd917ca66b2007bULL, 0x01fbbb0751764764ULL, 0x3d2a4953c6fe0fdcULL,
+        0xcc1489c5737afd94ULL, 0x1817c5b6a5346f41ULL, 0xe605a6a7e9985644ULL,
+        0x3c50412328ff1946ULL, 0xd8c7fd65817f1291ULL, 0x0bd66975ab66339bULL,
+        0x2baf8fa1c7d10fa9ULL, 0x24abdf06ddef848dULL, 0x14df0c9b2ea4f6c2ULL,
+        0x2be950edfd2cb1f7ULL, 0x21911e21094178b6ULL, 0x0fa54d518a93b379ULL,
+        0xb52508e0ac01ab42ULL, 0x0e035b5fd8cb79beULL, 0x1c1c6d1a3b3c8648ULL,
+        0x286037b42ea9871cULL, 0xfe67bf311e48a340ULL, 0x02324131e932a472ULL,
+        0x2486dc2dd919e2deULL, 0x008aec7f1da1d2ebULL, 0x63269ba0e8d3eb3aULL,
+        0x23c0f11154adb62fULL, 0xc6052393ecd4c018ULL, 0x523585b7d2f5b9fcULL,
+        0xf7e6f8c1e87564c9ULL, 0x09eb9fe5dd32c1a3ULL, 0x4d4f86886e055472ULL,
+        0x67ea17b58a37966bULL, 0x3d3ce8c23b1ed1a8ULL, 0x0df97c5ac48857ceULL,
+        0x9b6992623759eb12ULL, 0x275aa9551ae091f2ULL, 0x08855e19ac5e62e5ULL,
+        0x1155fffe0ae083ccULL, 0xbc9c78db7c570240ULL, 0x074560c447dd2418ULL,
+        0x3bf78d330bcf1e70ULL, 0x49867cd4b7ed134bULL, 0x8e6eee0cb4470accULL,
+        0x1dabafdf59233dd6ULL, 0xea3a50d844fc3fb8ULL, 0x4f03f4454764cb87ULL,
+        0x1f2f41cc36c9e6ecULL, 0x53cba4df42963441ULL, 0x10883b70a88d91fbULL,
+        0x62b1fc77d4eb9481ULL, 0x893d8f2604b362e1ULL, 0x0933b7855368b440ULL,
+        0x9351b545703b2fceULL, 0x59c1d489b9bdd3b4ULL, 0xe72a9c4311417b18ULL,
+        0x5355df77e88eb226ULL, 0xe802c37aa963d7e1ULL, 0x381c3747bd6c3bc3ULL,
+        0x378565573444258cULL, 0x37848b1e52b43c18ULL, 0x5da2cd32bdce12b6ULL,
+        0x13166c5da615f6fdULL, 0xa51ef95efcc66ac8ULL, 0x640c95e473f1e541ULL,
+        0x6ec68def1f217500ULL, 0x49ce3543c76a4079ULL, 0x5fc6fd3cddc706b5ULL,
+        0x05c3c0f0f6a1fb0dULL, 0xe7820c0996ad1bddULL, 0x21f0d752a088f35cULL,
+        0x755405b51d6fc4a0ULL, 0x7ec7649ca4b0e351ULL, 0x3d2b6a46a251f790ULL,
+        0x23e1176b19f418adULL, 0x06056575efe8ac05ULL, 0x0f75981b6966e477ULL,
+        0x06e87ec41ad437e4ULL, 0x43f6c255d5e1cb84ULL, 0xe4e67d1120ceb580ULL,
+        0x2cd67b9e12c26d7bULL, 0xcd00b5ff7fd187f1ULL, 0x3f6cd40accdc4106ULL,
+        0x3e895c835459b330ULL, 0x0814d53a217c0850ULL, 0xc9111fe78bc3a62dULL,
+        0x719967e351473204ULL, 0xe757707d24282aa4ULL, 0x7226b7f5607f98e6ULL,
+        0x7b268ffae3c08d96ULL, 0x16d3917c8b86020eULL, 0x5128bca51c49ea64ULL,
+        0x345ffea02bb1698dULL, 0x9460f5111fe4fbc8ULL, 0x60dd1aa5762852cbULL,
+        0xbb7440ed3c81667cULL, 0x0a4b12affa7f6f5cULL, 0x95cbcb0ae03861b6ULL,
+        0x07ab3b0591db6070ULL, 0xc6476a4c3de78982ULL, 0x204e82e8623ad725ULL,
+        0x569a5b4e8ac2a5ccULL, 0x425a1d77d72ebae2ULL, 0xcdaad5551ab33830ULL,
+        0x0b7c68fd8422939eULL, 0x46d9a01f53ec3020ULL, 0x102871edbb29e852ULL,
+        0x7a8e8084039075a5ULL, 0x40eaede8615e376aULL, 0x4dc67d757a1c751fULL,
+        0x1176ef33063f9145ULL, 0x4ea230285b1c8156ULL, 0x6b2aa46ce0027392ULL,
+        0x32b13230fba1b068ULL, 0x0e69796851bb984fULL, 0xb749f4542db698c0ULL,
+        0x19ad0241ffffd49cULL, 0x2f41e92ef6caff52ULL, 0x4d0b068576747439ULL,
+        0x14d607aef7463e00ULL, 0x1443d00d85fb440eULL, 0x529b43bf68688780ULL,
+        0x21133a6bc3a3e378ULL, 0x865b6436dae0e7e5ULL, 0x6b4fe83dc1d6defcULL,
+        0x03a5858a0ca0be46ULL, 0x1e841b187e67f312ULL, 0x61ee22ef40a66940ULL,
+        0x0494bd2e9e741ef8ULL, 0x4eb59e323010e72cULL, 0x19f2abcfb749810eULL,
+        0xb30f1e4f994ef9bcULL, 0x53cf6cdd51bd2d96ULL, 0x263943036497a514ULL,
+        0x0d4b52170aa2edbaULL, 0x0c4758a1c7b4f758ULL, 0x178dadb1b502b51aULL,
+        0x1ddbb20a602eb57aULL, 0x1fc2e2564a9f27fdULL, 0xd5f8c50a0e3d6f90ULL,
+        0x0081da3bbe72ac09ULL, 0xcf140d002ccdb200ULL, 0x0ae8389f09b017feULL,
+        0x17cc9ffdc03f4440ULL, 0x04eb921d704bcdddULL, 0x139a0ce4cdc521abULL,
+        0x0bfce00c145cb0f0ULL, 0x99925ff132eff707ULL, 0x063f6e5da50c3d35ULL,
+        0xa0c25dea3f0e6e29ULL, 0x0c7a9048cc8e040fULL,
+    };
+
+    const size_t padded = RoundUpTo(kCLMulNum, N);
+    auto expected_lower = AllocateAligned<T>(padded);
+    auto expected_upper = AllocateAligned<T>(padded);
+    memcpy(expected_lower.get(), kCLMulLower, kCLMulNum * sizeof(T));
+    memcpy(expected_upper.get(), kCLMulUpper, kCLMulNum * sizeof(T));
+    const size_t padding_size = (padded - kCLMulNum) * sizeof(T);
+    memset(expected_lower.get() + kCLMulNum, 0, padding_size);
+    memset(expected_upper.get() + kCLMulNum, 0, padding_size);
+
+    // Random inputs in each lane
+    RandomState rng;
+    for (size_t rep = 0; rep < kCLMulNum / N; ++rep) {
+      for (size_t i = 0; i < N; ++i) {
+        in1[i] = Random64(&rng);
+        in2[i] = Random64(&rng);
+      }
+
+      const auto a = Load(d, in1.get());
+      const auto b = Load(d, in2.get());
+#if HWY_PRINT_CLMUL_GOLDEN
+      Store(CLMulLower(a, b), d, expected_lower.get() + rep * N);
+      Store(CLMulUpper(a, b), d, expected_upper.get() + rep * N);
+#else
+      HWY_ASSERT_VEC_EQ(d, expected_lower.get() + rep * N, CLMulLower(a, b));
+      HWY_ASSERT_VEC_EQ(d, expected_upper.get() + rep * N, CLMulUpper(a, b));
+#endif
+    }
+
+#if HWY_PRINT_CLMUL_GOLDEN
+    // RVV lacks PRIu64, so print 32-bit halves.
+    for (size_t i = 0; i < kCLMulNum; ++i) {
+      printf("0x%08x%08xULL,", static_cast<uint32_t>(expected_lower[i] >> 32),
+             static_cast<uint32_t>(expected_lower[i] & 0xFFFFFFFFU));
+    }
+    printf("\n");
+    for (size_t i = 0; i < kCLMulNum; ++i) {
+      printf("0x%08x%08xULL,", static_cast<uint32_t>(expected_upper[i] >> 32),
+             static_cast<uint32_t>(expected_upper[i] & 0xFFFFFFFFU));
+    }
+#endif  // HWY_PRINT_CLMUL_GOLDEN
+#else
+    (void)d;
+#endif
+  }
+};
+
+HWY_NOINLINE void TestAllCLMul() { ForGE128Vectors<TestCLMul>()(uint64_t()); }
+
+// NOLINTNEXTLINE(google-readability-namespace-comments)
+}  // namespace HWY_NAMESPACE
+}  // namespace hwy
+HWY_AFTER_NAMESPACE();
+
+#if HWY_ONCE
+
+namespace hwy {
+HWY_BEFORE_TEST(HwyCryptoTest);
+HWY_EXPORT_AND_TEST_P(HwyCryptoTest, TestAllAES);
+HWY_EXPORT_AND_TEST_P(HwyCryptoTest, TestAllCLMul);
+}  // namespace hwy
+
+// Ought not to be necessary, but without this, no tests run on RVV.
+int main(int argc, char **argv) {
+  ::testing::InitGoogleTest(&argc, argv);
+  return RUN_ALL_TESTS();
+}
+
+#endif
diff --git a/third_party/highway/hwy/tests/logical_test.cc b/third_party/highway/hwy/tests/logical_test.cc
index d4a447f6133c1..c7bc1dfdd6921 100644
--- a/third_party/highway/hwy/tests/logical_test.cc
+++ b/third_party/highway/hwy/tests/logical_test.cc
@@ -16,12 +16,12 @@
 #include <stdint.h>
 #include <string.h>  // memcmp
 
+#include "hwy/aligned_allocator.h"
 #include "hwy/base.h"
 
 #undef HWY_TARGET_INCLUDE
 #define HWY_TARGET_INCLUDE "tests/logical_test.cc"
 #include "hwy/foreach_target.h"
-
 #include "hwy/highway.h"
 #include "hwy/tests/test_util-inl.h"
 
@@ -160,302 +160,6 @@ HWY_NOINLINE void TestAllCopySign() {
   ForFloatTypes(ForPartialVectors<TestCopySign>());
 }
 
-struct TestFirstN {
-  template <class T, class D>
-  HWY_NOINLINE void operator()(T /*unused*/, D d) {
-    const size_t N = Lanes(d);
-    auto mask_lanes = AllocateAligned<T>(N);
-
-    // NOTE: reverse polarity (mask is true iff mask_lanes[i] == 0) because we
-    // cannot reliably compare against all bits set (NaN for float types).
-    const T off = 1;
-
-    for (size_t len = 0; len <= N; ++len) {
-      for (size_t i = 0; i < N; ++i) {
-        mask_lanes[i] = i < len ? T(0) : off;
-      }
-      const auto mask = Eq(Load(d, mask_lanes.get()), Zero(d));
-      HWY_ASSERT_MASK_EQ(d, mask, FirstN(d, len));
-    }
-  }
-};
-
-HWY_NOINLINE void TestAllFirstN() {
-  ForAllTypes(ForPartialVectors<TestFirstN>());
-}
-
-struct TestIfThenElse {
-  template <class T, class D>
-  HWY_NOINLINE void operator()(T /*unused*/, D d) {
-    RandomState rng;
-
-    const size_t N = Lanes(d);
-    auto in1 = AllocateAligned<T>(N);
-    auto in2 = AllocateAligned<T>(N);
-    auto mask_lanes = AllocateAligned<T>(N);
-    auto expected = AllocateAligned<T>(N);
-
-    // NOTE: reverse polarity (mask is true iff lane == 0) because we cannot
-    // reliably compare against all bits set (NaN for float types).
-    const T off = 1;
-
-    // Each lane should have a chance of having mask=true.
-    for (size_t rep = 0; rep < 50; ++rep) {
-      for (size_t i = 0; i < N; ++i) {
-        in1[i] = static_cast<T>(Random32(&rng));
-        in2[i] = static_cast<T>(Random32(&rng));
-        mask_lanes[i] = (Random32(&rng) & 1024) ? off : T(0);
-      }
-
-      const auto v1 = Load(d, in1.get());
-      const auto v2 = Load(d, in2.get());
-      const auto mask = Eq(Load(d, mask_lanes.get()), Zero(d));
-
-      for (size_t i = 0; i < N; ++i) {
-        expected[i] = (mask_lanes[i] == off) ? in2[i] : in1[i];
-      }
-      HWY_ASSERT_VEC_EQ(d, expected.get(), IfThenElse(mask, v1, v2));
-
-      for (size_t i = 0; i < N; ++i) {
-        expected[i] = mask_lanes[i] ? T(0) : in1[i];
-      }
-      HWY_ASSERT_VEC_EQ(d, expected.get(), IfThenElseZero(mask, v1));
-
-      for (size_t i = 0; i < N; ++i) {
-        expected[i] = mask_lanes[i] ? in2[i] : T(0);
-      }
-      HWY_ASSERT_VEC_EQ(d, expected.get(), IfThenZeroElse(mask, v2));
-    }
-  }
-};
-
-HWY_NOINLINE void TestAllIfThenElse() {
-  ForAllTypes(ForPartialVectors<TestIfThenElse>());
-}
-
-struct TestMaskVec {
-  template <class T, class D>
-  HWY_NOINLINE void operator()(T /*unused*/, D d) {
-    RandomState rng;
-
-    const size_t N = Lanes(d);
-    auto mask_lanes = AllocateAligned<T>(N);
-
-    // Each lane should have a chance of having mask=true.
-    for (size_t rep = 0; rep < 100; ++rep) {
-      for (size_t i = 0; i < N; ++i) {
-        mask_lanes[i] = static_cast<T>(Random32(&rng) & 1);
-      }
-
-      const auto mask = RebindMask(d, Eq(Load(d, mask_lanes.get()), Zero(d)));
-      HWY_ASSERT_MASK_EQ(d, mask, MaskFromVec(VecFromMask(d, mask)));
-    }
-  }
-};
-
-HWY_NOINLINE void TestAllMaskVec() {
-  const ForPartialVectors<TestMaskVec> test;
-
-  test(uint16_t());
-  test(int16_t());
-  // TODO(janwas): float16_t - cannot compare yet
-
-  test(uint32_t());
-  test(int32_t());
-  test(float());
-
-#if HWY_CAP_INTEGER64
-  test(uint64_t());
-  test(int64_t());
-#endif
-#if HWY_CAP_FLOAT64
-  test(double());
-#endif
-}
-
-struct TestCompress {
-  template <class T, class D>
-  HWY_NOINLINE void operator()(T /*unused*/, D d) {
-    RandomState rng;
-
-    using TU = MakeUnsigned<T>;
-    const Rebind<TU, D> du;
-    const size_t N = Lanes(d);
-    auto in_lanes = AllocateAligned<T>(N);
-    auto mask_lanes = AllocateAligned<TU>(N);
-    auto expected = AllocateAligned<T>(N);
-    auto actual = AllocateAligned<T>(N);
-
-    // Each lane should have a chance of having mask=true.
-    for (size_t rep = 0; rep < 100; ++rep) {
-      size_t expected_pos = 0;
-      for (size_t i = 0; i < N; ++i) {
-        const uint64_t bits = Random32(&rng);
-        in_lanes[i] = T();  // cannot initialize float16_t directly.
-        CopyBytes<sizeof(T)>(&bits, &in_lanes[i]);
-        mask_lanes[i] = static_cast<TU>(Random32(&rng) & 1);
-        if (mask_lanes[i] == 0) {  // Zero means true (easier to compare)
-          expected[expected_pos++] = in_lanes[i];
-        }
-      }
-
-      const auto in = Load(d, in_lanes.get());
-      const auto mask = RebindMask(d, Eq(Load(du, mask_lanes.get()), Zero(du)));
-
-      Store(Compress(in, mask), d, actual.get());
-      // Upper lanes are undefined.
-      for (size_t i = 0; i < expected_pos; ++i) {
-        HWY_ASSERT(memcmp(&actual[i], &expected[i], sizeof(T)) == 0);
-      }
-
-      // Also check CompressStore in the same way.
-      memset(actual.get(), 0, N * sizeof(T));
-      const size_t num_written = CompressStore(in, mask, d, actual.get());
-      HWY_ASSERT_EQ(expected_pos, num_written);
-      for (size_t i = 0; i < expected_pos; ++i) {
-        HWY_ASSERT(memcmp(&actual[i], &expected[i], sizeof(T)) == 0);
-      }
-    }
-  }
-};
-
-#if 0
-namespace detail {  // for code folding
-void PrintCompress16x8Tables() {
-  constexpr size_t N = 8;  // 128-bit SIMD
-  for (uint64_t code = 0; code < 1ull << N; ++code) {
-    std::array<uint8_t, N> indices{0};
-    size_t pos = 0;
-    for (size_t i = 0; i < N; ++i) {
-      if (code & (1ull << i)) {
-        indices[pos++] = i;
-      }
-    }
-
-    // Doubled (for converting lane to byte indices)
-    for (size_t i = 0; i < N; ++i) {
-      printf("%d,", 2 * indices[i]);
-    }
-  }
-  printf("\n");
-}
-
-// Compressed to nibbles
-void PrintCompress32x8Tables() {
-  constexpr size_t N = 8;  // AVX2
-  for (uint64_t code = 0; code < 1ull << N; ++code) {
-    std::array<uint32_t, N> indices{0};
-    size_t pos = 0;
-    for (size_t i = 0; i < N; ++i) {
-      if (code & (1ull << i)) {
-        indices[pos++] = i;
-      }
-    }
-
-    // Convert to nibbles
-    uint64_t packed = 0;
-    for (size_t i = 0; i < N; ++i) {
-      HWY_ASSERT(indices[i] < 16);
-      packed += indices[i] << (i * 4);
-    }
-
-    HWY_ASSERT(packed < (1ull << 32));
-    printf("0x%08x,", static_cast<uint32_t>(packed));
-  }
-  printf("\n");
-}
-
-// Pairs of 32-bit lane indices
-void PrintCompress64x4Tables() {
-  constexpr size_t N = 4;  // AVX2
-  for (uint64_t code = 0; code < 1ull << N; ++code) {
-    std::array<uint32_t, N> indices{0};
-    size_t pos = 0;
-    for (size_t i = 0; i < N; ++i) {
-      if (code & (1ull << i)) {
-        indices[pos++] = i;
-      }
-    }
-
-    for (size_t i = 0; i < N; ++i) {
-      printf("%d,%d,", 2 * indices[i], 2 * indices[i] + 1);
-    }
-  }
-  printf("\n");
-}
-
-// 4-tuple of byte indices
-void PrintCompress32x4Tables() {
-  using T = uint32_t;
-  constexpr size_t N = 4;  // SSE4
-  for (uint64_t code = 0; code < 1ull << N; ++code) {
-    std::array<uint32_t, N> indices{0};
-    size_t pos = 0;
-    for (size_t i = 0; i < N; ++i) {
-      if (code & (1ull << i)) {
-        indices[pos++] = i;
-      }
-    }
-
-    for (size_t i = 0; i < N; ++i) {
-      for (size_t idx_byte = 0; idx_byte < sizeof(T); ++idx_byte) {
-        printf("%zu,", sizeof(T) * indices[i] + idx_byte);
-      }
-    }
-  }
-  printf("\n");
-}
-
-// 8-tuple of byte indices
-void PrintCompress64x2Tables() {
-  using T = uint64_t;
-  constexpr size_t N = 2;  // SSE4
-  for (uint64_t code = 0; code < 1ull << N; ++code) {
-    std::array<uint32_t, N> indices{0};
-    size_t pos = 0;
-    for (size_t i = 0; i < N; ++i) {
-      if (code & (1ull << i)) {
-        indices[pos++] = i;
-      }
-    }
-
-    for (size_t i = 0; i < N; ++i) {
-      for (size_t idx_byte = 0; idx_byte < sizeof(T); ++idx_byte) {
-        printf("%zu,", sizeof(T) * indices[i] + idx_byte);
-      }
-    }
-  }
-  printf("\n");
-}
-}  // namespace detail
-#endif
-
-HWY_NOINLINE void TestAllCompress() {
-  // detail::PrintCompress32x8Tables();
-  // detail::PrintCompress64x4Tables();
-  // detail::PrintCompress32x4Tables();
-  // detail::PrintCompress64x2Tables();
-  // detail::PrintCompress16x8Tables();
-
-  const ForPartialVectors<TestCompress> test;
-
-  test(uint16_t());
-  test(int16_t());
-  test(float16_t());
-
-  test(uint32_t());
-  test(int32_t());
-  test(float());
-
-#if HWY_CAP_INTEGER64
-  test(uint64_t());
-  test(int64_t());
-#endif
-#if HWY_CAP_FLOAT64
-  test(double());
-#endif
-}
-
 struct TestZeroIfNegative {
   template <class T, class D>
   HWY_NOINLINE void operator()(T /*unused*/, D d) {
@@ -482,7 +186,7 @@ struct TestBroadcastSignBit {
     const auto s0 = Zero(d);
     const auto s1 = Set(d, -1);  // all bit set
     const auto vpos = And(Iota(d, 0), Set(d, LimitsMax<T>()));
-    const auto vneg = s1 - vpos;
+    const auto vneg = Sub(s1, vpos);
 
     HWY_ASSERT_VEC_EQ(d, s0, BroadcastSignBit(vpos));
     HWY_ASSERT_VEC_EQ(d, s0, BroadcastSignBit(Set(d, LimitsMax<T>())));
@@ -507,18 +211,18 @@ struct TestTestBit {
       const auto bit3 = Set(d, 1ull << ((i + 2) % kNumBits));
       const auto bits12 = Or(bit1, bit2);
       const auto bits23 = Or(bit2, bit3);
-      HWY_ASSERT(AllTrue(TestBit(bit1, bit1)));
-      HWY_ASSERT(AllTrue(TestBit(bits12, bit1)));
-      HWY_ASSERT(AllTrue(TestBit(bits12, bit2)));
-
-      HWY_ASSERT(AllFalse(TestBit(bits12, bit3)));
-      HWY_ASSERT(AllFalse(TestBit(bits23, bit1)));
-      HWY_ASSERT(AllFalse(TestBit(bit1, bit2)));
-      HWY_ASSERT(AllFalse(TestBit(bit2, bit1)));
-      HWY_ASSERT(AllFalse(TestBit(bit1, bit3)));
-      HWY_ASSERT(AllFalse(TestBit(bit3, bit1)));
-      HWY_ASSERT(AllFalse(TestBit(bit2, bit3)));
-      HWY_ASSERT(AllFalse(TestBit(bit3, bit2)));
+      HWY_ASSERT(AllTrue(d, TestBit(bit1, bit1)));
+      HWY_ASSERT(AllTrue(d, TestBit(bits12, bit1)));
+      HWY_ASSERT(AllTrue(d, TestBit(bits12, bit2)));
+
+      HWY_ASSERT(AllFalse(d, TestBit(bits12, bit3)));
+      HWY_ASSERT(AllFalse(d, TestBit(bits23, bit1)));
+      HWY_ASSERT(AllFalse(d, TestBit(bit1, bit2)));
+      HWY_ASSERT(AllFalse(d, TestBit(bit2, bit1)));
+      HWY_ASSERT(AllFalse(d, TestBit(bit1, bit3)));
+      HWY_ASSERT(AllFalse(d, TestBit(bit3, bit1)));
+      HWY_ASSERT(AllFalse(d, TestBit(bit2, bit3)));
+      HWY_ASSERT(AllFalse(d, TestBit(bit3, bit2)));
     }
   }
 };
@@ -527,198 +231,54 @@ HWY_NOINLINE void TestAllTestBit() {
   ForIntegerTypes(ForPartialVectors<TestTestBit>());
 }
 
-struct TestAllTrueFalse {
+struct TestPopulationCount {
   template <class T, class D>
   HWY_NOINLINE void operator()(T /*unused*/, D d) {
-    const auto zero = Zero(d);
-    auto v = zero;
-
-    const size_t N = Lanes(d);
-    auto lanes = AllocateAligned<T>(N);
-    std::fill(lanes.get(), lanes.get() + N, T(0));
-
-    auto mask_lanes = AllocateAligned<T>(N);
-
-    HWY_ASSERT(AllTrue(Eq(v, zero)));
-    HWY_ASSERT(!AllFalse(Eq(v, zero)));
-
-    // Single lane implies AllFalse = !AllTrue. Otherwise, there are multiple
-    // lanes and one is nonzero.
-    const bool expected_all_false = (N != 1);
-
-    // Set each lane to nonzero and back to zero
-    for (size_t i = 0; i < N; ++i) {
-      lanes[i] = T(1);
-      v = Load(d, lanes.get());
-
-      // GCC 10.2.1 workaround: AllTrue(Eq(v, zero)) is true but should not be.
-      // Assigning to an lvalue is insufficient but storing to memory prevents
-      // the bug; so does Print of VecFromMask(d, Eq(v, zero)).
-      Store(VecFromMask(d, Eq(v, zero)), d, mask_lanes.get());
-      HWY_ASSERT(!AllTrue(MaskFromVec(Load(d, mask_lanes.get()))));
-
-      HWY_ASSERT(expected_all_false ^ AllFalse(Eq(v, zero)));
-
-      lanes[i] = T(-1);
-      v = Load(d, lanes.get());
-      HWY_ASSERT(!AllTrue(Eq(v, zero)));
-      HWY_ASSERT(expected_all_false ^ AllFalse(Eq(v, zero)));
-
-      // Reset to all zero
-      lanes[i] = T(0);
-      v = Load(d, lanes.get());
-      HWY_ASSERT(AllTrue(Eq(v, zero)));
-      HWY_ASSERT(!AllFalse(Eq(v, zero)));
-    }
-  }
-};
-
-HWY_NOINLINE void TestAllAllTrueFalse() {
-  ForAllTypes(ForPartialVectors<TestAllTrueFalse>());
-}
-
-class TestStoreMaskBits {
- public:
-  template <class T, class D>
-  HWY_NOINLINE void operator()(T /*t*/, D d) {
-    // TODO(janwas): remove once implemented (cast or vse1)
-#if HWY_TARGET != HWY_RVV
-    RandomState rng;
-    const size_t N = Lanes(d);
-    auto lanes = AllocateAligned<T>(N);
-    const size_t expected_bytes = (N + 7) / 8;
-    auto bits = AllocateAligned<uint8_t>(expected_bytes);
-
-    for (size_t rep = 0; rep < 100; ++rep) {
-      // Generate random mask pattern.
-      for (size_t i = 0; i < N; ++i) {
-        lanes[i] = static_cast<T>((rng() & 1024) ? 1 : 0);
-      }
-      const auto mask = Load(d, lanes.get()) == Zero(d);
-
-      const size_t bytes_written = StoreMaskBits(mask, bits.get());
-
-      HWY_ASSERT_EQ(expected_bytes, bytes_written);
-      size_t i = 0;
-      // Stored bits must match original mask
-      for (; i < N; ++i) {
-        const bool bit = (bits[i / 8] & (1 << (i % 8))) != 0;
-        HWY_ASSERT_EQ(bit, lanes[i] == 0);
-      }
-      // Any partial bits in the last byte must be zero
-      for (; i < 8 * bytes_written; ++i) {
-        const int bit = (bits[i / 8] & (1 << (i % 8)));
-        HWY_ASSERT_EQ(bit, 0);
-      }
-    }
+#if HWY_TARGET != HWY_RVV && defined(NDEBUG)
+    constexpr size_t kNumTests = 1 << 20;
 #else
-    (void)d;
+    constexpr size_t kNumTests = 1 << 14;
 #endif
-  }
-};
-
-HWY_NOINLINE void TestAllStoreMaskBits() {
-  ForAllTypes(ForPartialVectors<TestStoreMaskBits>());
-}
-
-struct TestCountTrue {
-  template <class T, class D>
-  HWY_NOINLINE void operator()(T /*unused*/, D d) {
-    const size_t N = Lanes(d);
-    // For all combinations of zero/nonzero state of subset of lanes:
-    const size_t max_lanes = std::min(N, size_t(10));
-
-    auto lanes = AllocateAligned<T>(N);
-    std::fill(lanes.get(), lanes.get() + N, T(1));
-
-    for (size_t code = 0; code < (1ull << max_lanes); ++code) {
-      // Number of zeros written = number of mask lanes that are true.
-      size_t expected = 0;
-      for (size_t i = 0; i < max_lanes; ++i) {
-        lanes[i] = T(1);
-        if (code & (1ull << i)) {
-          ++expected;
-          lanes[i] = T(0);
-        }
+    RandomState rng;
+    size_t N = Lanes(d);
+    auto data = AllocateAligned<T>(N);
+    auto popcnt = AllocateAligned<T>(N);
+    for (size_t i = 0; i < kNumTests / N; i++) {
+      for (size_t i = 0; i < N; i++) {
+        data[i] = static_cast<T>(rng());
+        popcnt[i] = static_cast<T>(PopCount(data[i]));
       }
-
-      const auto mask = Eq(Load(d, lanes.get()), Zero(d));
-      const size_t actual = CountTrue(mask);
-      HWY_ASSERT_EQ(expected, actual);
+      HWY_ASSERT_VEC_EQ(d, popcnt.get(), PopulationCount(Load(d, data.get())));
     }
   }
 };
 
-HWY_NOINLINE void TestAllCountTrue() {
-  ForAllTypes(ForPartialVectors<TestCountTrue>());
+HWY_NOINLINE void TestAllPopulationCount() {
+  ForUnsignedTypes(ForPartialVectors<TestPopulationCount>());
 }
 
-struct TestLogicalMask {
-  template <class T, class D>
-  HWY_NOINLINE void operator()(T /*unused*/, D d) {
-    const auto m0 = MaskFalse(d);
-    const auto m_all = MaskTrue(d);
-
-    const size_t N = Lanes(d);
-    auto lanes = AllocateAligned<T>(N);
-    std::fill(lanes.get(), lanes.get() + N, T(1));
-
-    HWY_ASSERT_MASK_EQ(d, m0, Not(m_all));
-    HWY_ASSERT_MASK_EQ(d, m_all, Not(m0));
-
-    // For all combinations of zero/nonzero state of subset of lanes:
-    const size_t max_lanes = std::min(N, size_t(6));
-    for (size_t code = 0; code < (1ull << max_lanes); ++code) {
-      for (size_t i = 0; i < max_lanes; ++i) {
-        lanes[i] = T(1);
-        if (code & (1ull << i)) {
-          lanes[i] = T(0);
-        }
-      }
-
-      const auto m = Eq(Load(d, lanes.get()), Zero(d));
-
-      HWY_ASSERT_MASK_EQ(d, m0, Xor(m, m));
-      HWY_ASSERT_MASK_EQ(d, m0, AndNot(m, m));
-      HWY_ASSERT_MASK_EQ(d, m0, AndNot(m_all, m));
-
-      HWY_ASSERT_MASK_EQ(d, m, Or(m, m));
-      HWY_ASSERT_MASK_EQ(d, m, Or(m0, m));
-      HWY_ASSERT_MASK_EQ(d, m, Or(m, m0));
-      HWY_ASSERT_MASK_EQ(d, m, Xor(m0, m));
-      HWY_ASSERT_MASK_EQ(d, m, Xor(m, m0));
-      HWY_ASSERT_MASK_EQ(d, m, And(m, m));
-      HWY_ASSERT_MASK_EQ(d, m, And(m_all, m));
-      HWY_ASSERT_MASK_EQ(d, m, And(m, m_all));
-      HWY_ASSERT_MASK_EQ(d, m, AndNot(m0, m));
-    }
-  }
-};
-
-HWY_NOINLINE void TestAllLogicalMask() {
-  ForAllTypes(ForPartialVectors<TestLogicalMask>());
-}
 // NOLINTNEXTLINE(google-readability-namespace-comments)
 }  // namespace HWY_NAMESPACE
 }  // namespace hwy
 HWY_AFTER_NAMESPACE();
 
 #if HWY_ONCE
+
 namespace hwy {
 HWY_BEFORE_TEST(HwyLogicalTest);
 HWY_EXPORT_AND_TEST_P(HwyLogicalTest, TestAllLogicalInteger);
 HWY_EXPORT_AND_TEST_P(HwyLogicalTest, TestAllLogicalFloat);
 HWY_EXPORT_AND_TEST_P(HwyLogicalTest, TestAllCopySign);
-HWY_EXPORT_AND_TEST_P(HwyLogicalTest, TestAllFirstN);
-HWY_EXPORT_AND_TEST_P(HwyLogicalTest, TestAllIfThenElse);
-HWY_EXPORT_AND_TEST_P(HwyLogicalTest, TestAllMaskVec);
-HWY_EXPORT_AND_TEST_P(HwyLogicalTest, TestAllCompress);
 HWY_EXPORT_AND_TEST_P(HwyLogicalTest, TestAllZeroIfNegative);
 HWY_EXPORT_AND_TEST_P(HwyLogicalTest, TestAllBroadcastSignBit);
 HWY_EXPORT_AND_TEST_P(HwyLogicalTest, TestAllTestBit);
-HWY_EXPORT_AND_TEST_P(HwyLogicalTest, TestAllAllTrueFalse);
-HWY_EXPORT_AND_TEST_P(HwyLogicalTest, TestAllStoreMaskBits);
-HWY_EXPORT_AND_TEST_P(HwyLogicalTest, TestAllCountTrue);
-HWY_EXPORT_AND_TEST_P(HwyLogicalTest, TestAllLogicalMask);
+HWY_EXPORT_AND_TEST_P(HwyLogicalTest, TestAllPopulationCount);
 }  // namespace hwy
+
+// Ought not to be necessary, but without this, no tests run on RVV.
+int main(int argc, char **argv) {
+  ::testing::InitGoogleTest(&argc, argv);
+  return RUN_ALL_TESTS();
+}
+
 #endif
diff --git a/third_party/highway/hwy/tests/mask_test.cc b/third_party/highway/hwy/tests/mask_test.cc
new file mode 100644
index 0000000000000..b9ab5203e8358
--- /dev/null
+++ b/third_party/highway/hwy/tests/mask_test.cc
@@ -0,0 +1,435 @@
+// Copyright 2019 Google LLC
+//
+// Licensed under the Apache License, Version 2.0 (the "License");
+// you may not use this file except in compliance with the License.
+// You may obtain a copy of the License at
+//
+//      http://www.apache.org/licenses/LICENSE-2.0
+//
+// Unless required by applicable law or agreed to in writing, software
+// distributed under the License is distributed on an "AS IS" BASIS,
+// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+// See the License for the specific language governing permissions and
+// limitations under the License.
+
+#include <stddef.h>
+#include <stdint.h>
+#include <string.h>  // memcmp
+
+#include "hwy/base.h"
+
+#undef HWY_TARGET_INCLUDE
+#define HWY_TARGET_INCLUDE "tests/mask_test.cc"
+#include "hwy/foreach_target.h"
+
+#include "hwy/highway.h"
+#include "hwy/tests/test_util-inl.h"
+
+HWY_BEFORE_NAMESPACE();
+namespace hwy {
+namespace HWY_NAMESPACE {
+
+// All types.
+struct TestFromVec {
+  template <typename T, class D>
+  HWY_NOINLINE void operator()(T /*unused*/, D d) {
+    const size_t N = Lanes(d);
+    auto lanes = AllocateAligned<T>(N);
+
+    memset(lanes.get(), 0, N * sizeof(T));
+    const auto actual_false = MaskFromVec(Load(d, lanes.get()));
+    HWY_ASSERT_MASK_EQ(d, MaskFalse(d), actual_false);
+
+    memset(lanes.get(), 0xFF, N * sizeof(T));
+    const auto actual_true = MaskFromVec(Load(d, lanes.get()));
+    HWY_ASSERT_MASK_EQ(d, MaskTrue(d), actual_true);
+  }
+};
+
+HWY_NOINLINE void TestAllFromVec() {
+  ForAllTypes(ForPartialVectors<TestFromVec>());
+}
+
+struct TestFirstN {
+  template <class T, class D>
+  HWY_NOINLINE void operator()(T /*unused*/, D d) {
+    const size_t N = Lanes(d);
+    auto mask_lanes = AllocateAligned<T>(N);
+
+    // GCC workaround: we previously used zero to indicate true because we can
+    // safely compare with that value. However, that hits an ICE for u64x1 on
+    // GCC 8.3 but not 8.4, even if the implementation of operator== is
+    // simplified to return zero. Using MaskFromVec avoids this, and requires
+    // FF..FF and 0 constants.
+    T on;
+    memset(&on, 0xFF, sizeof(on));
+    const T off = 0;
+
+    for (size_t len = 0; len <= N; ++len) {
+      for (size_t i = 0; i < N; ++i) {
+        mask_lanes[i] = i < len ? on : off;
+      }
+      const auto mask_vals = Load(d, mask_lanes.get());
+      const auto mask = MaskFromVec(mask_vals);
+      HWY_ASSERT_MASK_EQ(d, mask, FirstN(d, len));
+    }
+  }
+};
+
+HWY_NOINLINE void TestAllFirstN() {
+  ForAllTypes(ForPartialVectors<TestFirstN>());
+}
+
+struct TestIfThenElse {
+  template <class T, class D>
+  HWY_NOINLINE void operator()(T /*unused*/, D d) {
+    RandomState rng;
+
+    using TI = MakeSigned<T>;  // For mask > 0 comparison
+    const Rebind<TI, D> di;
+    const size_t N = Lanes(d);
+    auto in1 = AllocateAligned<T>(N);
+    auto in2 = AllocateAligned<T>(N);
+    auto bool_lanes = AllocateAligned<TI>(N);
+    auto expected = AllocateAligned<T>(N);
+
+    // Each lane should have a chance of having mask=true.
+    for (size_t rep = 0; rep < 50; ++rep) {
+      for (size_t i = 0; i < N; ++i) {
+        in1[i] = static_cast<T>(Random32(&rng));
+        in2[i] = static_cast<T>(Random32(&rng));
+        bool_lanes[i] = (Random32(&rng) & 16) ? TI(1) : TI(0);
+      }
+
+      const auto v1 = Load(d, in1.get());
+      const auto v2 = Load(d, in2.get());
+      const auto mask = RebindMask(d, Gt(Load(di, bool_lanes.get()), Zero(di)));
+
+      for (size_t i = 0; i < N; ++i) {
+        expected[i] = bool_lanes[i] ? in1[i] : in2[i];
+      }
+      HWY_ASSERT_VEC_EQ(d, expected.get(), IfThenElse(mask, v1, v2));
+
+      for (size_t i = 0; i < N; ++i) {
+        expected[i] = bool_lanes[i] ? in1[i] : T(0);
+      }
+      HWY_ASSERT_VEC_EQ(d, expected.get(), IfThenElseZero(mask, v1));
+
+      for (size_t i = 0; i < N; ++i) {
+        expected[i] = bool_lanes[i] ? T(0) : in2[i];
+      }
+      HWY_ASSERT_VEC_EQ(d, expected.get(), IfThenZeroElse(mask, v2));
+    }
+  }
+};
+
+HWY_NOINLINE void TestAllIfThenElse() {
+  ForAllTypes(ForPartialVectors<TestIfThenElse>());
+}
+
+struct TestMaskVec {
+  template <class T, class D>
+  HWY_NOINLINE void operator()(T /*unused*/, D d) {
+    RandomState rng;
+
+    using TI = MakeSigned<T>;  // For mask > 0 comparison
+    const Rebind<TI, D> di;
+    const size_t N = Lanes(d);
+    auto bool_lanes = AllocateAligned<TI>(N);
+
+    // Each lane should have a chance of having mask=true.
+    for (size_t rep = 0; rep < 100; ++rep) {
+      for (size_t i = 0; i < N; ++i) {
+        bool_lanes[i] = (Random32(&rng) & 1024) ? TI(1) : TI(0);
+      }
+
+      const auto mask = RebindMask(d, Gt(Load(di, bool_lanes.get()), Zero(di)));
+      HWY_ASSERT_MASK_EQ(d, mask, MaskFromVec(VecFromMask(d, mask)));
+    }
+  }
+};
+
+HWY_NOINLINE void TestAllMaskVec() {
+  const ForPartialVectors<TestMaskVec> test;
+
+  test(uint16_t());
+  test(int16_t());
+  // TODO(janwas): float16_t - cannot compare yet
+
+  test(uint32_t());
+  test(int32_t());
+  test(float());
+
+#if HWY_CAP_INTEGER64
+  test(uint64_t());
+  test(int64_t());
+#endif
+#if HWY_CAP_FLOAT64
+  test(double());
+#endif
+}
+
+struct TestAllTrueFalse {
+  template <class T, class D>
+  HWY_NOINLINE void operator()(T /*unused*/, D d) {
+    const auto zero = Zero(d);
+    auto v = zero;
+
+    const size_t N = Lanes(d);
+    auto lanes = AllocateAligned<T>(N);
+    std::fill(lanes.get(), lanes.get() + N, T(0));
+
+    auto mask_lanes = AllocateAligned<T>(N);
+
+    HWY_ASSERT(AllTrue(d, Eq(v, zero)));
+    HWY_ASSERT(!AllFalse(d, Eq(v, zero)));
+
+    // Single lane implies AllFalse = !AllTrue. Otherwise, there are multiple
+    // lanes and one is nonzero.
+    const bool expected_all_false = (N != 1);
+
+    // Set each lane to nonzero and back to zero
+    for (size_t i = 0; i < N; ++i) {
+      lanes[i] = T(1);
+      v = Load(d, lanes.get());
+
+      // GCC 10.2.1 workaround: AllTrue(Eq(v, zero)) is true but should not be.
+      // Assigning to an lvalue is insufficient but storing to memory prevents
+      // the bug; so does Print of VecFromMask(d, Eq(v, zero)).
+      Store(VecFromMask(d, Eq(v, zero)), d, mask_lanes.get());
+      HWY_ASSERT(!AllTrue(d, MaskFromVec(Load(d, mask_lanes.get()))));
+
+      HWY_ASSERT(expected_all_false ^ AllFalse(d, Eq(v, zero)));
+
+      lanes[i] = T(-1);
+      v = Load(d, lanes.get());
+      HWY_ASSERT(!AllTrue(d, Eq(v, zero)));
+      HWY_ASSERT(expected_all_false ^ AllFalse(d, Eq(v, zero)));
+
+      // Reset to all zero
+      lanes[i] = T(0);
+      v = Load(d, lanes.get());
+      HWY_ASSERT(AllTrue(d, Eq(v, zero)));
+      HWY_ASSERT(!AllFalse(d, Eq(v, zero)));
+    }
+  }
+};
+
+HWY_NOINLINE void TestAllAllTrueFalse() {
+  ForAllTypes(ForPartialVectors<TestAllTrueFalse>());
+}
+
+class TestStoreMaskBits {
+ public:
+  template <class T, class D>
+  HWY_NOINLINE void operator()(T /*t*/, D /*d*/) {
+    // TODO(janwas): remove once implemented (cast or vse1)
+#if HWY_TARGET != HWY_RVV
+    RandomState rng;
+    using TI = MakeSigned<T>;  // For mask > 0 comparison
+    const Rebind<TI, D> di;
+    const size_t N = Lanes(di);
+    auto bool_lanes = AllocateAligned<TI>(N);
+
+    const Half<Half<Half<HWY_FULL(uint8_t)>>> d_bits;
+    const size_t expected_num_bytes = (N + 7) / 8;
+    auto expected = AllocateAligned<uint8_t>(expected_num_bytes);
+    auto actual = AllocateAligned<uint8_t>(expected_num_bytes);
+
+    for (size_t rep = 0; rep < 100; ++rep) {
+      // Generate random mask pattern.
+      for (size_t i = 0; i < N; ++i) {
+        bool_lanes[i] = static_cast<TI>((rng() & 1024) ? 1 : 0);
+      }
+      const auto bools = Load(di, bool_lanes.get());
+      const auto mask = Gt(bools, Zero(di));
+
+      const size_t bytes_written = StoreMaskBits(di, mask, actual.get());
+      if (bytes_written != expected_num_bytes) {
+        fprintf(stderr, "%s expected %zu bytes, actual %zu\n",
+                TypeName(T(), N).c_str(), expected_num_bytes, bytes_written);
+
+        HWY_ASSERT(false);
+      }
+
+      memset(expected.get(), 0, expected_num_bytes);
+      for (size_t i = 0; i < N; ++i) {
+        expected[i / 8] |= bool_lanes[i] << (i % 8);
+      }
+
+      size_t i = 0;
+      // Stored bits must match original mask
+      for (; i < N; ++i) {
+        const TI is_set = (actual[i / 8] & (1 << (i % 8))) ? 1 : 0;
+        if (is_set != bool_lanes[i]) {
+          fprintf(stderr, "%s lane %zu: expected %d, actual %d\n",
+                  TypeName(T(), N).c_str(), i, int(bool_lanes[i]), int(is_set));
+          Print(di, "bools", bools, 0, N);
+          Print(d_bits, "expected bytes", Load(d_bits, expected.get()), 0,
+                expected_num_bytes);
+          Print(d_bits, "actual bytes", Load(d_bits, actual.get()), 0,
+                expected_num_bytes);
+
+          HWY_ASSERT(false);
+        }
+      }
+      // Any partial bits in the last byte must be zero
+      for (; i < 8 * bytes_written; ++i) {
+        const int bit = (actual[i / 8] & (1 << (i % 8)));
+        if (bit != 0) {
+          fprintf(stderr, "%s: bit #%zu should be zero\n",
+                  TypeName(T(), N).c_str(), i);
+          Print(di, "bools", bools, 0, N);
+          Print(d_bits, "expected bytes", Load(d_bits, expected.get()), 0,
+                expected_num_bytes);
+          Print(d_bits, "actual bytes", Load(d_bits, actual.get()), 0,
+                expected_num_bytes);
+
+          HWY_ASSERT(false);
+        }
+      }
+    }
+#endif
+  }
+};
+
+HWY_NOINLINE void TestAllStoreMaskBits() {
+  ForAllTypes(ForPartialVectors<TestStoreMaskBits>());
+}
+
+struct TestCountTrue {
+  template <class T, class D>
+  HWY_NOINLINE void operator()(T /*unused*/, D d) {
+    using TI = MakeSigned<T>;  // For mask > 0 comparison
+    const Rebind<TI, D> di;
+    const size_t N = Lanes(di);
+    auto bool_lanes = AllocateAligned<TI>(N);
+    memset(bool_lanes.get(), 0, N * sizeof(TI));
+
+    // For all combinations of zero/nonzero state of subset of lanes:
+    const size_t max_lanes = HWY_MIN(N, size_t(10));
+
+    for (size_t code = 0; code < (1ull << max_lanes); ++code) {
+      // Number of zeros written = number of mask lanes that are true.
+      size_t expected = 0;
+      for (size_t i = 0; i < max_lanes; ++i) {
+        const bool is_true = (code & (1ull << i)) != 0;
+        bool_lanes[i] = is_true ? TI(1) : TI(0);
+        expected += is_true;
+      }
+
+      const auto mask = RebindMask(d, Gt(Load(di, bool_lanes.get()), Zero(di)));
+      const size_t actual = CountTrue(d, mask);
+      HWY_ASSERT_EQ(expected, actual);
+    }
+  }
+};
+
+HWY_NOINLINE void TestAllCountTrue() {
+  ForAllTypes(ForPartialVectors<TestCountTrue>());
+}
+
+struct TestFindFirstTrue {
+  template <class T, class D>
+  HWY_NOINLINE void operator()(T /*unused*/, D d) {
+    using TI = MakeSigned<T>;  // For mask > 0 comparison
+    const Rebind<TI, D> di;
+    const size_t N = Lanes(di);
+    auto bool_lanes = AllocateAligned<TI>(N);
+    memset(bool_lanes.get(), 0, N * sizeof(TI));
+
+    // For all combinations of zero/nonzero state of subset of lanes:
+    const size_t max_lanes = HWY_MIN(N, size_t(10));
+
+    HWY_ASSERT_EQ(intptr_t(-1), FindFirstTrue(d, MaskFalse(d)));
+    HWY_ASSERT_EQ(intptr_t(0), FindFirstTrue(d, MaskTrue(d)));
+
+    for (size_t code = 1; code < (1ull << max_lanes); ++code) {
+      for (size_t i = 0; i < max_lanes; ++i) {
+        bool_lanes[i] = (code & (1ull << i)) ? TI(1) : TI(0);
+      }
+
+      const intptr_t expected =
+          static_cast<intptr_t>(Num0BitsBelowLS1Bit_Nonzero32(code));
+      const auto mask = RebindMask(d, Gt(Load(di, bool_lanes.get()), Zero(di)));
+      const intptr_t actual = FindFirstTrue(d, mask);
+      HWY_ASSERT_EQ(expected, actual);
+    }
+  }
+};
+
+HWY_NOINLINE void TestAllFindFirstTrue() {
+  ForAllTypes(ForPartialVectors<TestFindFirstTrue>());
+}
+
+struct TestLogicalMask {
+  template <class T, class D>
+  HWY_NOINLINE void operator()(T /*unused*/, D d) {
+    const auto m0 = MaskFalse(d);
+    const auto m_all = MaskTrue(d);
+
+    using TI = MakeSigned<T>;  // For mask > 0 comparison
+    const Rebind<TI, D> di;
+    const size_t N = Lanes(di);
+    auto bool_lanes = AllocateAligned<TI>(N);
+    memset(bool_lanes.get(), 0, N * sizeof(TI));
+
+    HWY_ASSERT_MASK_EQ(d, m0, Not(m_all));
+    HWY_ASSERT_MASK_EQ(d, m_all, Not(m0));
+
+    // For all combinations of zero/nonzero state of subset of lanes:
+    const size_t max_lanes = HWY_MIN(N, size_t(6));
+    for (size_t code = 0; code < (1ull << max_lanes); ++code) {
+      for (size_t i = 0; i < max_lanes; ++i) {
+        bool_lanes[i] = (code & (1ull << i)) ? TI(1) : TI(0);
+      }
+
+      const auto m = RebindMask(d, Gt(Load(di, bool_lanes.get()), Zero(di)));
+
+      HWY_ASSERT_MASK_EQ(d, m0, Xor(m, m));
+      HWY_ASSERT_MASK_EQ(d, m0, AndNot(m, m));
+      HWY_ASSERT_MASK_EQ(d, m0, AndNot(m_all, m));
+
+      HWY_ASSERT_MASK_EQ(d, m, Or(m, m));
+      HWY_ASSERT_MASK_EQ(d, m, Or(m0, m));
+      HWY_ASSERT_MASK_EQ(d, m, Or(m, m0));
+      HWY_ASSERT_MASK_EQ(d, m, Xor(m0, m));
+      HWY_ASSERT_MASK_EQ(d, m, Xor(m, m0));
+      HWY_ASSERT_MASK_EQ(d, m, And(m, m));
+      HWY_ASSERT_MASK_EQ(d, m, And(m_all, m));
+      HWY_ASSERT_MASK_EQ(d, m, And(m, m_all));
+      HWY_ASSERT_MASK_EQ(d, m, AndNot(m0, m));
+    }
+  }
+};
+
+HWY_NOINLINE void TestAllLogicalMask() {
+  ForAllTypes(ForPartialVectors<TestLogicalMask>());
+}
+// NOLINTNEXTLINE(google-readability-namespace-comments)
+}  // namespace HWY_NAMESPACE
+}  // namespace hwy
+HWY_AFTER_NAMESPACE();
+
+#if HWY_ONCE
+
+namespace hwy {
+HWY_BEFORE_TEST(HwyMaskTest);
+HWY_EXPORT_AND_TEST_P(HwyMaskTest, TestAllFromVec);
+HWY_EXPORT_AND_TEST_P(HwyMaskTest, TestAllFirstN);
+HWY_EXPORT_AND_TEST_P(HwyMaskTest, TestAllIfThenElse);
+HWY_EXPORT_AND_TEST_P(HwyMaskTest, TestAllMaskVec);
+HWY_EXPORT_AND_TEST_P(HwyMaskTest, TestAllAllTrueFalse);
+HWY_EXPORT_AND_TEST_P(HwyMaskTest, TestAllStoreMaskBits);
+HWY_EXPORT_AND_TEST_P(HwyMaskTest, TestAllCountTrue);
+HWY_EXPORT_AND_TEST_P(HwyMaskTest, TestAllFindFirstTrue);
+HWY_EXPORT_AND_TEST_P(HwyMaskTest, TestAllLogicalMask);
+}  // namespace hwy
+
+// Ought not to be necessary, but without this, no tests run on RVV.
+int main(int argc, char **argv) {
+  ::testing::InitGoogleTest(&argc, argv);
+  return RUN_ALL_TESTS();
+}
+
+#endif
diff --git a/third_party/highway/hwy/tests/memory_test.cc b/third_party/highway/hwy/tests/memory_test.cc
index 31bd7dbce52d1..f20e48d108b6c 100644
--- a/third_party/highway/hwy/tests/memory_test.cc
+++ b/third_party/highway/hwy/tests/memory_test.cc
@@ -12,6 +12,12 @@
 // See the License for the specific language governing permissions and
 // limitations under the License.
 
+// Ensure incompabilities with Windows macros (e.g. #define StoreFence) are
+// detected. Must come before Highway headers.
+#if defined(_WIN32) || defined(_WIN64)
+#include <windows.h>
+#endif
+
 #include <stddef.h>
 #include <stdint.h>
 
@@ -281,8 +287,8 @@ struct TestScatter {
       std::fill(expected.get(), expected.get() + range, T(0));
       std::fill(actual.get(), actual.get() + range, T(0));
       for (size_t i = 0; i < N; ++i) {
-        offsets[i] =
-            static_cast<Offset>(Random32(&rng) % (max_bytes - sizeof(T)));
+        // Must be aligned
+        offsets[i] = static_cast<Offset>((Random32(&rng) % range) * sizeof(T));
         CopyBytes<sizeof(T)>(
             bytes.get() + i * sizeof(T),
             reinterpret_cast<uint8_t*>(expected.get()) + offsets[i]);
@@ -334,11 +340,12 @@ struct TestGather {
     using Offset = MakeSigned<T>;
 
     const size_t N = Lanes(d);
+    const size_t range = 4 * N;                  // number of items to gather
+    const size_t max_bytes = range * sizeof(T);  // upper bound on offset
 
     RandomState rng;
 
     // Data to be gathered from
-    const size_t max_bytes = 4 * N * sizeof(T);  // upper bound on offset
     auto bytes = AllocateAligned<uint8_t>(max_bytes);
     for (size_t i = 0; i < max_bytes; ++i) {
       bytes[i] = static_cast<uint8_t>(Random32(&rng) & 0xFF);
@@ -351,8 +358,8 @@ struct TestGather {
     for (size_t rep = 0; rep < 100; ++rep) {
       // Offsets
       for (size_t i = 0; i < N; ++i) {
-        offsets[i] =
-            static_cast<Offset>(Random32(&rng) % (max_bytes - sizeof(T)));
+        // Must be aligned
+        offsets[i] = static_cast<Offset>((Random32(&rng) % range) * sizeof(T));
         CopyBytes<sizeof(T)>(bytes.get() + offsets[i], &expected[i]);
       }
 
@@ -401,6 +408,7 @@ HWY_NOINLINE void TestAllCache() {
 HWY_AFTER_NAMESPACE();
 
 #if HWY_ONCE
+
 namespace hwy {
 HWY_BEFORE_TEST(HwyMemoryTest);
 HWY_EXPORT_AND_TEST_P(HwyMemoryTest, TestAllLoadStore);
@@ -412,4 +420,11 @@ HWY_EXPORT_AND_TEST_P(HwyMemoryTest, TestAllScatter);
 HWY_EXPORT_AND_TEST_P(HwyMemoryTest, TestAllGather);
 HWY_EXPORT_AND_TEST_P(HwyMemoryTest, TestAllCache);
 }  // namespace hwy
+
+// Ought not to be necessary, but without this, no tests run on RVV.
+int main(int argc, char** argv) {
+  ::testing::InitGoogleTest(&argc, argv);
+  return RUN_ALL_TESTS();
+}
+
 #endif
diff --git a/third_party/highway/hwy/tests/swizzle_test.cc b/third_party/highway/hwy/tests/swizzle_test.cc
index 565dc115e4abf..f7ebbc59407d9 100644
--- a/third_party/highway/hwy/tests/swizzle_test.cc
+++ b/third_party/highway/hwy/tests/swizzle_test.cc
@@ -26,202 +26,34 @@ HWY_BEFORE_NAMESPACE();
 namespace hwy {
 namespace HWY_NAMESPACE {
 
-struct TestShiftBytes {
+struct TestGetLane {
   template <class T, class D>
   HWY_NOINLINE void operator()(T /*unused*/, D d) {
-    // Scalar does not define Shift*Bytes.
-#if HWY_TARGET != HWY_SCALAR || HWY_IDE
-    const Repartition<uint8_t, D> du8;
-    const size_t N8 = Lanes(du8);
-
-    // Zero remains zero
-    const auto v0 = Zero(d);
-    HWY_ASSERT_VEC_EQ(d, v0, ShiftLeftBytes<1>(v0));
-    HWY_ASSERT_VEC_EQ(d, v0, ShiftRightBytes<1>(v0));
-
-    // Zero after shifting out the high/low byte
-    auto bytes = AllocateAligned<uint8_t>(N8);
-    std::fill(bytes.get(), bytes.get() + N8, 0);
-    bytes[N8 - 1] = 0x7F;
-    const auto vhi = BitCast(d, Load(du8, bytes.get()));
-    bytes[N8 - 1] = 0;
-    bytes[0] = 0x7F;
-    const auto vlo = BitCast(d, Load(du8, bytes.get()));
-    HWY_ASSERT_VEC_EQ(d, v0, ShiftLeftBytes<1>(vhi));
-    HWY_ASSERT_VEC_EQ(d, v0, ShiftRightBytes<1>(vlo));
-
-    // Check expected result with Iota
-    const size_t N = Lanes(d);
-    auto in = AllocateAligned<T>(N);
-    const uint8_t* in_bytes = reinterpret_cast<const uint8_t*>(in.get());
-    const auto v = BitCast(d, Iota(du8, 1));
-    Store(v, d, in.get());
-
-    auto expected = AllocateAligned<T>(N);
-    uint8_t* expected_bytes = reinterpret_cast<uint8_t*>(expected.get());
-
-    const size_t kBlockSize = HWY_MIN(N8, 16);
-    for (size_t block = 0; block < N8; block += kBlockSize) {
-      expected_bytes[block] = 0;
-      memcpy(expected_bytes + block + 1, in_bytes + block, kBlockSize - 1);
-    }
-    HWY_ASSERT_VEC_EQ(d, expected.get(), ShiftLeftBytes<1>(v));
-
-    for (size_t block = 0; block < N8; block += kBlockSize) {
-      memcpy(expected_bytes + block, in_bytes + block + 1, kBlockSize - 1);
-      expected_bytes[block + kBlockSize - 1] = 0;
-    }
-    HWY_ASSERT_VEC_EQ(d, expected.get(), ShiftRightBytes<1>(v));
-#else
-    (void)d;
-#endif  // #if HWY_TARGET != HWY_SCALAR
-  }
-};
-
-HWY_NOINLINE void TestAllShiftBytes() {
-  ForIntegerTypes(ForGE128Vectors<TestShiftBytes>());
-}
-
-struct TestShiftLanes {
-  template <class T, class D>
-  HWY_NOINLINE void operator()(T /*unused*/, D d) {
-    // Scalar does not define Shift*Lanes.
-#if HWY_TARGET != HWY_SCALAR || HWY_IDE
     const auto v = Iota(d, T(1));
-    const size_t N = Lanes(d);
-    auto expected = AllocateAligned<T>(N);
-
-    HWY_ASSERT_VEC_EQ(d, v, ShiftLeftLanes<0>(v));
-    HWY_ASSERT_VEC_EQ(d, v, ShiftRightLanes<0>(v));
-
-    constexpr size_t kLanesPerBlock = 16 / sizeof(T);
-
-    for (size_t i = 0; i < N; ++i) {
-      expected[i] = (i % kLanesPerBlock) == 0 ? T(0) : T(i);
-    }
-    HWY_ASSERT_VEC_EQ(d, expected.get(), ShiftLeftLanes<1>(v));
-
-    for (size_t i = 0; i < N; ++i) {
-      expected[i] =
-          (i % kLanesPerBlock) == (kLanesPerBlock - 1) ? T(0) : T(2 + i);
-    }
-    HWY_ASSERT_VEC_EQ(d, expected.get(), ShiftRightLanes<1>(v));
-#else
-    (void)d;
-#endif  // #if HWY_TARGET != HWY_SCALAR
+    HWY_ASSERT_EQ(T(1), GetLane(v));
   }
 };
 
-HWY_NOINLINE void TestAllShiftLanes() {
-  ForAllTypes(ForGE128Vectors<TestShiftLanes>());
-}
-
-template <typename D, int kLane>
-struct TestBroadcastR {
-  HWY_NOINLINE void operator()() const {
-// TODO(janwas): fix failure
-#if HWY_TARGET != HWY_WASM
-    using T = typename D::T;
-    const D d;
-    const size_t N = Lanes(d);
-    auto in_lanes = AllocateAligned<T>(N);
-    std::fill(in_lanes.get(), in_lanes.get() + N, T(0));
-    const size_t blockN = HWY_MIN(N * sizeof(T), 16) / sizeof(T);
-    // Need to set within each 128-bit block
-    for (size_t block = 0; block < N; block += blockN) {
-      in_lanes[block + kLane] = static_cast<T>(block + 1);
-    }
-    const auto in = Load(d, in_lanes.get());
-    auto expected = AllocateAligned<T>(N);
-    for (size_t block = 0; block < N; block += blockN) {
-      for (size_t i = 0; i < blockN; ++i) {
-        expected[block + i] = T(block + 1);
-      }
-    }
-    HWY_ASSERT_VEC_EQ(d, expected.get(), Broadcast<kLane>(in));
-
-    TestBroadcastR<D, kLane - 1>()();
-#endif
-  }
-};
-
-template <class D>
-struct TestBroadcastR<D, -1> {
-  void operator()() const {}
-};
-
-struct TestBroadcast {
-  template <class T, class D>
-  HWY_NOINLINE void operator()(T /*unused*/, D d) {
-    TestBroadcastR<D, HWY_MIN(MaxLanes(d), 16 / sizeof(T)) - 1>()();
-  }
-};
-
-HWY_NOINLINE void TestAllBroadcast() {
-  const ForPartialVectors<TestBroadcast> test;
-  // No u8.
-  test(uint16_t());
-  test(uint32_t());
-#if HWY_CAP_INTEGER64
-  test(uint64_t());
-#endif
-
-  // No i8.
-  test(int16_t());
-  test(int32_t());
-#if HWY_CAP_INTEGER64
-  test(int64_t());
-#endif
-
-  ForFloatTypes(test);
+HWY_NOINLINE void TestAllGetLane() {
+  ForAllTypes(ForPartialVectors<TestGetLane>());
 }
 
-struct TestTableLookupBytes {
+struct TestOddEven {
   template <class T, class D>
   HWY_NOINLINE void operator()(T /*unused*/, D d) {
-    RandomState rng;
     const size_t N = Lanes(d);
-    const size_t N8 = Lanes(Repartition<uint8_t, D>());
-    auto in_bytes = AllocateAligned<uint8_t>(N8);
-    for (size_t i = 0; i < N8; ++i) {
-      in_bytes[i] = Random32(&rng) & 0xFF;
-    }
-    const auto in =
-        BitCast(d, Load(d, reinterpret_cast<const T*>(in_bytes.get())));
-
-    // Enough test data; for larger vectors, upper lanes will be zero.
-    const uint8_t index_bytes_source[64] = {
-        // Same index as source, multiple outputs from same input,
-        // unused input (9), ascending/descending and nonconsecutive neighbors.
-        0,  2,  1, 2, 15, 12, 13, 14, 6,  7,  8,  5,  4,  3,  10, 11,
-        11, 10, 3, 4, 5,  8,  7,  6,  14, 13, 12, 15, 2,  1,  2,  0,
-        4,  3,  2, 2, 5,  6,  7,  7,  15, 15, 15, 15, 15, 15, 0,  1};
-    auto index_bytes = AllocateAligned<uint8_t>(N8);
-    for (size_t i = 0; i < N8; ++i) {
-      index_bytes[i] = (i < 64) ? index_bytes_source[i] : 0;
-      // Avoid undefined results / asan error for scalar by capping indices.
-      if (index_bytes[i] >= N * sizeof(T)) {
-        index_bytes[i] = static_cast<uint8_t>(N * sizeof(T) - 1);
-      }
-    }
-    const auto indices = Load(d, reinterpret_cast<const T*>(index_bytes.get()));
+    const auto even = Iota(d, 1);
+    const auto odd = Iota(d, 1 + N);
     auto expected = AllocateAligned<T>(N);
-    uint8_t* expected_bytes = reinterpret_cast<uint8_t*>(expected.get());
-
-    // Byte indices wrap around
-    const size_t mod = HWY_MIN(N8, 256);
-    for (size_t block = 0; block < N8; block += 16) {
-      for (size_t i = 0; i < 16 && (block + i) < N8; ++i) {
-        const uint8_t index = index_bytes[block + i];
-        expected_bytes[block + i] = in_bytes[(block + index) % mod];
-      }
+    for (size_t i = 0; i < N; ++i) {
+      expected[i] = static_cast<T>(1 + i + ((i & 1) ? N : 0));
     }
-    HWY_ASSERT_VEC_EQ(d, expected.get(), TableLookupBytes(in, indices));
+    HWY_ASSERT_VEC_EQ(d, expected.get(), OddEven(odd, even));
   }
 };
 
-HWY_NOINLINE void TestAllTableLookupBytes() {
-  ForIntegerTypes(ForPartialVectors<TestTableLookupBytes>());
+HWY_NOINLINE void TestAllOddEven() {
+  ForAllTypes(ForShrinkableVectors<TestOddEven>());
 }
 
 struct TestTableLookupLanes {
@@ -294,330 +126,204 @@ HWY_NOINLINE void TestAllTableLookupLanes() {
   test(float());
 }
 
-struct TestInterleave {
+struct TestCompress {
   template <class T, class D>
   HWY_NOINLINE void operator()(T /*unused*/, D d) {
-    using TU = MakeUnsigned<T>;
+    RandomState rng;
+
+    using TI = MakeSigned<T>;  // For mask > 0 comparison
+    const Rebind<TI, D> di;
     const size_t N = Lanes(d);
-    auto even_lanes = AllocateAligned<T>(N);
-    auto odd_lanes = AllocateAligned<T>(N);
+    auto in_lanes = AllocateAligned<T>(N);
+    auto mask_lanes = AllocateAligned<TI>(N);
     auto expected = AllocateAligned<T>(N);
-    for (size_t i = 0; i < N; ++i) {
-      even_lanes[i] = static_cast<T>(2 * i + 0);
-      odd_lanes[i] = static_cast<T>(2 * i + 1);
-    }
-    const auto even = Load(d, even_lanes.get());
-    const auto odd = Load(d, odd_lanes.get());
-
-    const size_t blockN = 16 / sizeof(T);
-    for (size_t i = 0; i < Lanes(d); ++i) {
-      const size_t block = i / blockN;
-      const size_t index = (i % blockN) + block * 2 * blockN;
-      expected[i] = static_cast<T>(index & LimitsMax<TU>());
-    }
-    HWY_ASSERT_VEC_EQ(d, expected.get(), InterleaveLower(even, odd));
+    auto actual = AllocateAligned<T>(N);
 
-    for (size_t i = 0; i < Lanes(d); ++i) {
-      const size_t block = i / blockN;
-      expected[i] = T((i % blockN) + block * 2 * blockN + blockN);
-    }
-    HWY_ASSERT_VEC_EQ(d, expected.get(), InterleaveUpper(even, odd));
-  }
-};
+    // Each lane should have a chance of having mask=true.
+    for (size_t rep = 0; rep < 100; ++rep) {
+      size_t expected_pos = 0;
+      for (size_t i = 0; i < N; ++i) {
+        const uint64_t bits = Random32(&rng);
+        in_lanes[i] = T();  // cannot initialize float16_t directly.
+        CopyBytes<sizeof(T)>(&bits, &in_lanes[i]);
+        mask_lanes[i] = (Random32(&rng) & 1024) ? TI(1) : TI(0);
+        if (mask_lanes[i] > 0) {
+          expected[expected_pos++] = in_lanes[i];
+        }
+      }
 
-HWY_NOINLINE void TestAllInterleave() {
-  // Not supported by HWY_SCALAR: Interleave(f32, f32) would return f32x2.
-  ForAllTypes(ForGE128Vectors<TestInterleave>());
-}
+      const auto in = Load(d, in_lanes.get());
+      const auto mask = RebindMask(d, Gt(Load(di, mask_lanes.get()), Zero(di)));
+
+      Store(Compress(in, mask), d, actual.get());
+      // Upper lanes are undefined. Modified from AssertVecEqual.
+      for (size_t i = 0; i < expected_pos; ++i) {
+        if (!IsEqual(expected[i], actual[i])) {
+          fprintf(stderr, "Mismatch at i=%zu of %zu:\n\n", i, expected_pos);
+          Print(di, "mask", Load(di, mask_lanes.get()), 0, N);
+          Print(d, "in", in, 0, N);
+          Print(d, "expect", Load(d, expected.get()), 0, N);
+          Print(d, "actual", Load(d, actual.get()), 0, N);
+          HWY_ASSERT(false);
+        }
+      }
 
-struct TestZipLower {
-  template <class T, class D>
-  HWY_NOINLINE void operator()(T /*unused*/, D d) {
-    using WideT = MakeWide<T>;
-    static_assert(sizeof(T) * 2 == sizeof(WideT), "Must be double-width");
-    static_assert(IsSigned<T>() == IsSigned<WideT>(), "Must have same sign");
-    const size_t N = Lanes(d);
-    auto even_lanes = AllocateAligned<T>(N);
-    auto odd_lanes = AllocateAligned<T>(N);
-    for (size_t i = 0; i < N; ++i) {
-      even_lanes[i] = static_cast<T>(2 * i + 0);
-      odd_lanes[i] = static_cast<T>(2 * i + 1);
-    }
-    const auto even = Load(d, even_lanes.get());
-    const auto odd = Load(d, odd_lanes.get());
-
-    const Repartition<WideT, D> dw;
-    auto expected = AllocateAligned<WideT>(Lanes(dw));
-    const WideT blockN = static_cast<WideT>(16 / sizeof(WideT));
-    for (size_t i = 0; i < Lanes(dw); ++i) {
-      const size_t block = i / blockN;
-      // Value of least-significant lane in lo-vector.
-      const WideT lo =
-          static_cast<WideT>(2 * (i % blockN) + 4 * block * blockN);
-      const WideT kBits = static_cast<WideT>(sizeof(T) * 8);
-      expected[i] =
-          static_cast<WideT>((static_cast<WideT>(lo + 1) << kBits) + lo);
+      // Also check CompressStore in the same way.
+      memset(actual.get(), 0, N * sizeof(T));
+      const size_t num_written = CompressStore(in, mask, d, actual.get());
+      HWY_ASSERT_EQ(expected_pos, num_written);
+      for (size_t i = 0; i < expected_pos; ++i) {
+        if (!IsEqual(expected[i], actual[i])) {
+          fprintf(stderr, "Mismatch at i=%zu of %zu:\n\n", i, expected_pos);
+          Print(di, "mask", Load(di, mask_lanes.get()), 0, N);
+          Print(d, "in", in, 0, N);
+          Print(d, "expect", Load(d, expected.get()), 0, N);
+          Print(d, "actual", Load(d, actual.get()), 0, N);
+          HWY_ASSERT(false);
+        }
+      }
     }
-    HWY_ASSERT_VEC_EQ(dw, expected.get(), ZipLower(even, odd));
   }
 };
 
-struct TestZipUpper {
-  template <class T, class D>
-  HWY_NOINLINE void operator()(T /*unused*/, D d) {
-    using WideT = MakeWide<T>;
-    static_assert(sizeof(T) * 2 == sizeof(WideT), "Must be double-width");
-    static_assert(IsSigned<T>() == IsSigned<WideT>(), "Must have same sign");
-    const size_t N = Lanes(d);
-    auto even_lanes = AllocateAligned<T>(N);
-    auto odd_lanes = AllocateAligned<T>(N);
-    for (size_t i = 0; i < Lanes(d); ++i) {
-      even_lanes[i] = static_cast<T>(2 * i + 0);
-      odd_lanes[i] = static_cast<T>(2 * i + 1);
+// For regenerating tables used in the implementation
+#if 0
+namespace detail {  // for code folding
+void PrintCompress16x8Tables() {
+  constexpr size_t N = 8;  // 128-bit SIMD
+  for (uint64_t code = 0; code < 1ull << N; ++code) {
+    std::array<uint8_t, N> indices{0};
+    size_t pos = 0;
+    for (size_t i = 0; i < N; ++i) {
+      if (code & (1ull << i)) {
+        indices[pos++] = i;
+      }
     }
-    const auto even = Load(d, even_lanes.get());
-    const auto odd = Load(d, odd_lanes.get());
-
-    const Repartition<WideT, D> dw;
-    auto expected = AllocateAligned<WideT>(Lanes(dw));
-
-    constexpr WideT blockN = static_cast<WideT>(16 / sizeof(WideT));
-    for (size_t i = 0; i < Lanes(dw); ++i) {
-      const size_t block = i / blockN;
-      const WideT lo =
-          static_cast<WideT>(2 * (i % blockN) + 4 * block * blockN);
-      const WideT kBits = static_cast<WideT>(sizeof(T) * 8);
-      expected[i] = static_cast<WideT>(
-          (static_cast<WideT>(lo + 2 * blockN + 1) << kBits) + lo + 2 * blockN);
+
+    // Doubled (for converting lane to byte indices)
+    for (size_t i = 0; i < N; ++i) {
+      printf("%d,", 2 * indices[i]);
     }
-    HWY_ASSERT_VEC_EQ(dw, expected.get(), ZipUpper(even, odd));
   }
-};
-
-HWY_NOINLINE void TestAllZip() {
-  const ForPartialVectors<TestZipLower, 2> lower_unsigned;
-  // TODO(janwas): fix
-#if HWY_TARGET != HWY_RVV
-  lower_unsigned(uint8_t());
-#endif
-  lower_unsigned(uint16_t());
-#if HWY_CAP_INTEGER64
-  lower_unsigned(uint32_t());  // generates u64
-#endif
-
-  const ForPartialVectors<TestZipLower, 2> lower_signed;
-#if HWY_TARGET != HWY_RVV
-  lower_signed(int8_t());
-#endif
-  lower_signed(int16_t());
-#if HWY_CAP_INTEGER64
-  lower_signed(int32_t());  // generates i64
-#endif
-
-  const ForGE128Vectors<TestZipUpper> upper_unsigned;
-#if HWY_TARGET != HWY_RVV
-  upper_unsigned(uint8_t());
-#endif
-  upper_unsigned(uint16_t());
-#if HWY_CAP_INTEGER64
-  upper_unsigned(uint32_t());  // generates u64
-#endif
-
-  const ForGE128Vectors<TestZipUpper> upper_signed;
-#if HWY_TARGET != HWY_RVV
-  upper_signed(int8_t());
-#endif
-  upper_signed(int16_t());
-#if HWY_CAP_INTEGER64
-  upper_signed(int32_t());  // generates i64
-#endif
-
-  // No float - concatenating f32 does not result in a f64
+  printf("\n");
 }
 
-class TestSpecialShuffle32 {
- public:
-  template <class T, class D>
-  HWY_NOINLINE void operator()(T /*unused*/, D d) {
-    const auto v = Iota(d, 0);
-
-#define VERIFY_LANES_32(d, v, i3, i2, i1, i0) \
-  VerifyLanes32((d), (v), (i3), (i2), (i1), (i0), __FILE__, __LINE__)
+// Compressed to nibbles
+void PrintCompress32x8Tables() {
+  constexpr size_t N = 8;  // AVX2
+  for (uint64_t code = 0; code < 1ull << N; ++code) {
+    std::array<uint32_t, N> indices{0};
+    size_t pos = 0;
+    for (size_t i = 0; i < N; ++i) {
+      if (code & (1ull << i)) {
+        indices[pos++] = i;
+      }
+    }
 
-    VERIFY_LANES_32(d, Shuffle2301(v), 2, 3, 0, 1);
-    VERIFY_LANES_32(d, Shuffle1032(v), 1, 0, 3, 2);
-    VERIFY_LANES_32(d, Shuffle0321(v), 0, 3, 2, 1);
-    VERIFY_LANES_32(d, Shuffle2103(v), 2, 1, 0, 3);
-    VERIFY_LANES_32(d, Shuffle0123(v), 0, 1, 2, 3);
+    // Convert to nibbles
+    uint64_t packed = 0;
+    for (size_t i = 0; i < N; ++i) {
+      HWY_ASSERT(indices[i] < 16);
+      packed += indices[i] << (i * 4);
+    }
 
-#undef VERIFY_LANES_32
+    HWY_ASSERT(packed < (1ull << 32));
+    printf("0x%08x,", static_cast<uint32_t>(packed));
   }
+  printf("\n");
+}
 
- private:
-  template <class D, class V>
-  HWY_NOINLINE void VerifyLanes32(D d, V v, const int i3, const int i2,
-                                  const int i1, const int i0,
-                                  const char* filename, const int line) {
-    using T = typename D::T;
-    const size_t N = Lanes(d);
-    auto lanes = AllocateAligned<T>(N);
-    Store(v, d, lanes.get());
-    const std::string name = TypeName(lanes[0], N);
-    constexpr size_t kBlockN = 16 / sizeof(T);
-    for (int block = 0; block < static_cast<int>(N); block += kBlockN) {
-      AssertEqual(T(block + i3), lanes[block + 3], name, filename, line);
-      AssertEqual(T(block + i2), lanes[block + 2], name, filename, line);
-      AssertEqual(T(block + i1), lanes[block + 1], name, filename, line);
-      AssertEqual(T(block + i0), lanes[block + 0], name, filename, line);
+// Pairs of 32-bit lane indices
+void PrintCompress64x4Tables() {
+  constexpr size_t N = 4;  // AVX2
+  for (uint64_t code = 0; code < 1ull << N; ++code) {
+    std::array<uint32_t, N> indices{0};
+    size_t pos = 0;
+    for (size_t i = 0; i < N; ++i) {
+      if (code & (1ull << i)) {
+        indices[pos++] = i;
+      }
     }
-  }
-};
 
-class TestSpecialShuffle64 {
- public:
-  template <class T, class D>
-  HWY_NOINLINE void operator()(T /*unused*/, D d) {
-    const auto v = Iota(d, 0);
-    VerifyLanes64(d, Shuffle01(v), 0, 1, __FILE__, __LINE__);
-  }
-
- private:
-  template <class D, class V>
-  HWY_NOINLINE void VerifyLanes64(D d, V v, const int i1, const int i0,
-                                  const char* filename, const int line) {
-    using T = typename D::T;
-    const size_t N = Lanes(d);
-    auto lanes = AllocateAligned<T>(N);
-    Store(v, d, lanes.get());
-    const std::string name = TypeName(lanes[0], N);
-    constexpr size_t kBlockN = 16 / sizeof(T);
-    for (int block = 0; block < static_cast<int>(N); block += kBlockN) {
-      AssertEqual(T(block + i1), lanes[block + 1], name, filename, line);
-      AssertEqual(T(block + i0), lanes[block + 0], name, filename, line);
+    for (size_t i = 0; i < N; ++i) {
+      printf("%d,%d,", 2 * indices[i], 2 * indices[i] + 1);
     }
   }
-};
-
-HWY_NOINLINE void TestAllSpecialShuffles() {
-  const ForGE128Vectors<TestSpecialShuffle32> test32;
-  test32(uint32_t());
-  test32(int32_t());
-  test32(float());
-
-#if HWY_CAP_INTEGER64
-  const ForGE128Vectors<TestSpecialShuffle64> test64;
-  test64(uint64_t());
-  test64(int64_t());
-#endif
-
-#if HWY_CAP_FLOAT64
-  const ForGE128Vectors<TestSpecialShuffle64> test_d;
-  test_d(double());
-#endif
+  printf("\n");
 }
 
-struct TestConcatHalves {
-  template <class T, class D>
-  HWY_NOINLINE void operator()(T /*unused*/, D d) {
-    // TODO(janwas): fix
-#if HWY_TARGET != HWY_RVV
-    // Construct inputs such that interleaved halves == iota.
-    const auto expected = Iota(d, 1);
-
-    const size_t N = Lanes(d);
-    auto lo = AllocateAligned<T>(N);
-    auto hi = AllocateAligned<T>(N);
-    size_t i;
-    for (i = 0; i < N / 2; ++i) {
-      lo[i] = static_cast<T>(1 + i);
-      hi[i] = static_cast<T>(lo[i] + T(N) / 2);
-    }
-    for (; i < N; ++i) {
-      lo[i] = hi[i] = 0;
+// 4-tuple of byte indices
+void PrintCompress32x4Tables() {
+  using T = uint32_t;
+  constexpr size_t N = 4;  // SSE4
+  for (uint64_t code = 0; code < 1ull << N; ++code) {
+    std::array<uint32_t, N> indices{0};
+    size_t pos = 0;
+    for (size_t i = 0; i < N; ++i) {
+      if (code & (1ull << i)) {
+        indices[pos++] = i;
+      }
     }
 
-    HWY_ASSERT_VEC_EQ(d, expected,
-                      ConcatLowerLower(Load(d, hi.get()), Load(d, lo.get())));
-
-    // Same for high blocks.
-    for (i = 0; i < N / 2; ++i) {
-      lo[i] = hi[i] = 0;
+    for (size_t i = 0; i < N; ++i) {
+      for (size_t idx_byte = 0; idx_byte < sizeof(T); ++idx_byte) {
+        printf("%zu,", sizeof(T) * indices[i] + idx_byte);
+      }
     }
-    for (; i < N; ++i) {
-      lo[i] = static_cast<T>(1 + i - N / 2);
-      hi[i] = static_cast<T>(lo[i] + T(N) / 2);
+  }
+  printf("\n");
+}
+
+// 8-tuple of byte indices
+void PrintCompress64x2Tables() {
+  using T = uint64_t;
+  constexpr size_t N = 2;  // SSE4
+  for (uint64_t code = 0; code < 1ull << N; ++code) {
+    std::array<uint32_t, N> indices{0};
+    size_t pos = 0;
+    for (size_t i = 0; i < N; ++i) {
+      if (code & (1ull << i)) {
+        indices[pos++] = i;
+      }
     }
 
-    HWY_ASSERT_VEC_EQ(d, expected,
-                      ConcatUpperUpper(Load(d, hi.get()), Load(d, lo.get())));
-#else
-    (void)d;
-#endif
+    for (size_t i = 0; i < N; ++i) {
+      for (size_t idx_byte = 0; idx_byte < sizeof(T); ++idx_byte) {
+        printf("%zu,", sizeof(T) * indices[i] + idx_byte);
+      }
+    }
   }
-};
-
-HWY_NOINLINE void TestAllConcatHalves() {
-  ForAllTypes(ForGE128Vectors<TestConcatHalves>());
+  printf("\n");
 }
-
-struct TestConcatLowerUpper {
-  template <class T, class D>
-  HWY_NOINLINE void operator()(T /*unused*/, D d) {
-    // TODO(janwas): fix
-#if HWY_TARGET != HWY_RVV
-    const size_t N = Lanes(d);
-    // Middle part of Iota(1) == Iota(1 + N / 2).
-    const auto lo = Iota(d, 1);
-    const auto hi = Iota(d, 1 + N);
-    HWY_ASSERT_VEC_EQ(d, Iota(d, 1 + N / 2), ConcatLowerUpper(hi, lo));
-#else
-    (void)d;
+}  // namespace detail
 #endif
-  }
-};
 
-HWY_NOINLINE void TestAllConcatLowerUpper() {
-  ForAllTypes(ForGE128Vectors<TestConcatLowerUpper>());
-}
+HWY_NOINLINE void TestAllCompress() {
+  // detail::PrintCompress32x8Tables();
+  // detail::PrintCompress64x4Tables();
+  // detail::PrintCompress32x4Tables();
+  // detail::PrintCompress64x2Tables();
+  // detail::PrintCompress16x8Tables();
 
-struct TestConcatUpperLower {
-  template <class T, class D>
-  HWY_NOINLINE void operator()(T /*unused*/, D d) {
-    const size_t N = Lanes(d);
-    const auto lo = Iota(d, 1);
-    const auto hi = Iota(d, 1 + N);
-    auto expected = AllocateAligned<T>(N);
-    size_t i = 0;
-    for (; i < N / 2; ++i) {
-      expected[i] = static_cast<T>(1 + i);
-    }
-    for (; i < N; ++i) {
-      expected[i] = static_cast<T>(1 + i + N);
-    }
-    HWY_ASSERT_VEC_EQ(d, expected.get(), ConcatUpperLower(hi, lo));
-  }
-};
+  const ForPartialVectors<TestCompress> test;
 
-HWY_NOINLINE void TestAllConcatUpperLower() {
-  ForAllTypes(ForGE128Vectors<TestConcatUpperLower>());
-}
+  test(uint16_t());
+  test(int16_t());
+#if HWY_CAP_FLOAT16
+  test(float16_t());
+#endif
 
-struct TestOddEven {
-  template <class T, class D>
-  HWY_NOINLINE void operator()(T /*unused*/, D d) {
-    const size_t N = Lanes(d);
-    const auto even = Iota(d, 1);
-    const auto odd = Iota(d, 1 + N);
-    auto expected = AllocateAligned<T>(N);
-    for (size_t i = 0; i < N; ++i) {
-      expected[i] = static_cast<T>(1 + i + ((i & 1) ? N : 0));
-    }
-    HWY_ASSERT_VEC_EQ(d, expected.get(), OddEven(odd, even));
-  }
-};
+  test(uint32_t());
+  test(int32_t());
+  test(float());
 
-HWY_NOINLINE void TestAllOddEven() {
-  ForAllTypes(ForGE128Vectors<TestOddEven>());
+#if HWY_CAP_INTEGER64
+  test(uint64_t());
+  test(int64_t());
+#endif
+#if HWY_CAP_FLOAT64
+  test(double());
+#endif
 }
 
 // NOLINTNEXTLINE(google-readability-namespace-comments)
@@ -626,19 +332,19 @@ HWY_NOINLINE void TestAllOddEven() {
 HWY_AFTER_NAMESPACE();
 
 #if HWY_ONCE
+
 namespace hwy {
 HWY_BEFORE_TEST(HwySwizzleTest);
-HWY_EXPORT_AND_TEST_P(HwySwizzleTest, TestAllShiftBytes);
-HWY_EXPORT_AND_TEST_P(HwySwizzleTest, TestAllShiftLanes);
-HWY_EXPORT_AND_TEST_P(HwySwizzleTest, TestAllBroadcast);
-HWY_EXPORT_AND_TEST_P(HwySwizzleTest, TestAllTableLookupBytes);
-HWY_EXPORT_AND_TEST_P(HwySwizzleTest, TestAllTableLookupLanes);
-HWY_EXPORT_AND_TEST_P(HwySwizzleTest, TestAllInterleave);
-HWY_EXPORT_AND_TEST_P(HwySwizzleTest, TestAllZip);
-HWY_EXPORT_AND_TEST_P(HwySwizzleTest, TestAllSpecialShuffles);
-HWY_EXPORT_AND_TEST_P(HwySwizzleTest, TestAllConcatHalves);
-HWY_EXPORT_AND_TEST_P(HwySwizzleTest, TestAllConcatLowerUpper);
-HWY_EXPORT_AND_TEST_P(HwySwizzleTest, TestAllConcatUpperLower);
+HWY_EXPORT_AND_TEST_P(HwySwizzleTest, TestAllGetLane);
 HWY_EXPORT_AND_TEST_P(HwySwizzleTest, TestAllOddEven);
+HWY_EXPORT_AND_TEST_P(HwySwizzleTest, TestAllTableLookupLanes);
+HWY_EXPORT_AND_TEST_P(HwySwizzleTest, TestAllCompress);
 }  // namespace hwy
+
+// Ought not to be necessary, but without this, no tests run on RVV.
+int main(int argc, char **argv) {
+  ::testing::InitGoogleTest(&argc, argv);
+  return RUN_ALL_TESTS();
+}
+
 #endif
diff --git a/third_party/highway/hwy/tests/test_util-inl.h b/third_party/highway/hwy/tests/test_util-inl.h
index f62ebeeddd014..db9775aafbcaa 100644
--- a/third_party/highway/hwy/tests/test_util-inl.h
+++ b/third_party/highway/hwy/tests/test_util-inl.h
@@ -25,14 +25,13 @@
 
 #include <cstddef>
 #include <string>
-#include <utility>  // std::forward
+#include <utility>  // std::tuple
 
+#include "gtest/gtest.h"
 #include "hwy/aligned_allocator.h"
 #include "hwy/base.h"
 #include "hwy/highway.h"
 
-#include "gtest/gtest.h"
-
 namespace hwy {
 
 // The maximum vector size used in tests when defining test data. DEPRECATED.
@@ -196,6 +195,10 @@ static HWY_INLINE uint32_t Random32(RandomState* rng) {
   return static_cast<uint32_t>((*rng)());
 }
 
+static HWY_INLINE uint64_t Random64(RandomState* rng) {
+  return (*rng)();
+}
+
 // Prevents the compiler from eliding the computations that led to "output".
 // Works by indicating to the compiler that "output" is being read and modified.
 // The +r constraint avoids unnecessary writes to memory, but only works for
@@ -270,20 +273,41 @@ HWY_BEFORE_NAMESPACE();
 namespace hwy {
 namespace HWY_NAMESPACE {
 
+template <typename T, HWY_IF_LANE_SIZE(T, 1)>
+HWY_NOINLINE void PrintValue(T value) {
+  uint8_t byte;
+  CopyBytes<1>(&value, &byte);  // endian-safe: we ensured sizeof(T)=1.
+  fprintf(stderr, "0x%02X,", byte);
+}
+
+#if HWY_CAP_FLOAT16
+HWY_NOINLINE void PrintValue(float16_t value) {
+  uint16_t bits;
+  CopyBytes<2>(&value, &bits);
+  fprintf(stderr, "0x%02X,", bits);
+}
+#endif
+
+template <typename T, HWY_IF_NOT_LANE_SIZE(T, 1)>
+HWY_NOINLINE void PrintValue(T value) {
+  fprintf(stderr, "%g,", double(value));
+}
+
 // Prints lanes around `lane`, in memory order.
 template <class D>
-HWY_NOINLINE void Print(const D d, const char* caption, const Vec<D> v,
-                        intptr_t lane = 0) {
+HWY_NOINLINE void Print(const D d, const char* caption,
+                        const decltype(Zero(d)) v, intptr_t lane = 0,
+                        size_t max_lanes = 7) {
   using T = TFromD<D>;
   const size_t N = Lanes(d);
   auto lanes = AllocateAligned<T>(N);
   Store(v, d, lanes.get());
-  const size_t begin = static_cast<size_t>(std::max<intptr_t>(0, lane - 2));
-  const size_t end = std::min(begin + 7, N);
+  const size_t begin = static_cast<size_t>(HWY_MAX(0, lane - 2));
+  const size_t end = HWY_MIN(begin + max_lanes, N);
   fprintf(stderr, "%s %s [%zu+ ->]:\n  ", TypeName(T(), N).c_str(), caption,
           begin);
   for (size_t i = begin; i < end; ++i) {
-    fprintf(stderr, "%g,", double(lanes[i]));
+    PrintValue(lanes[i]);
   }
   if (begin >= end) fprintf(stderr, "(out of bounds)");
   fprintf(stderr, "\n");
@@ -324,12 +348,12 @@ MakeUnsigned<TF> ComputeUlpDelta(TF x, TF y) {
   const TU ux = BitCast<TU>(x);
   const TU uy = BitCast<TU>(y);
   // Avoid unsigned->signed cast: 2's complement is only guaranteed by C++20.
-  return std::max(ux, uy) - std::min(ux, uy);
+  return HWY_MAX(ux, uy) - HWY_MIN(ux, uy);
 }
 
 template <typename T, HWY_IF_NOT_FLOAT(T)>
 HWY_NOINLINE bool IsEqual(const T expected, const T actual) {
-  return expected == actual;
+  return memcmp(&expected, &actual, sizeof(T)) == 0;
 }
 
 template <typename T, HWY_IF_FLOAT(T)>
@@ -396,29 +420,68 @@ HWY_NOINLINE void AssertVecEqual(D d, const TFromD<D>* expected, Vec<D> actual,
   AssertVecEqual(d, LoadU(d, expected), actual, filename, line);
 }
 
+// Only checks the valid mask elements (those whose index < Lanes(d)).
 template <class D>
 HWY_NOINLINE void AssertMaskEqual(D d, Mask<D> a, Mask<D> b,
                                   const char* filename, int line) {
   AssertVecEqual(d, VecFromMask(d, a), VecFromMask(d, b), filename, line);
 
   const std::string type_name = TypeName(TFromD<D>(), Lanes(d));
-  AssertEqual(CountTrue(a), CountTrue(b), type_name, filename, line, 0);
-  AssertEqual(AllTrue(a), AllTrue(b), type_name, filename, line, 0);
-  AssertEqual(AllFalse(a), AllFalse(b), type_name, filename, line, 0);
+  AssertEqual(CountTrue(d, a), CountTrue(d, b), type_name, filename, line, 0);
+  AssertEqual(AllTrue(d, a), AllTrue(d, b), type_name, filename, line, 0);
+  AssertEqual(AllFalse(d, a), AllFalse(d, b), type_name, filename, line, 0);
 
-  // TODO(janwas): StoreMaskBits
+  // TODO(janwas): remove RVV once implemented (cast or vse1)
+#if HWY_TARGET != HWY_RVV && HWY_TARGET != HWY_SCALAR
+  const size_t N = Lanes(d);
+  const Repartition<uint8_t, D> d8;
+  const size_t N8 = Lanes(d8);
+  auto bits_a = AllocateAligned<uint8_t>(N8);
+  auto bits_b = AllocateAligned<uint8_t>(N8);
+  memset(bits_a.get(), 0, N8);
+  memset(bits_b.get(), 0, N8);
+  const size_t num_bytes_a = StoreMaskBits(d, a, bits_a.get());
+  const size_t num_bytes_b = StoreMaskBits(d, b, bits_b.get());
+  AssertEqual(num_bytes_a, num_bytes_b, type_name, filename, line, 0);
+  size_t i = 0;
+  // First check whole bytes (if that many elements are still valid)
+  for (; i < N / 8; ++i) {
+    if (bits_a[i] != bits_b[i]) {
+      fprintf(stderr, "Mismatch in byte %zu: %d != %d\n", i, bits_a[i],
+              bits_b[i]);
+      Print(d8, "expect", Load(d8, bits_a.get()), 0, N8);
+      Print(d8, "actual", Load(d8, bits_b.get()), 0, N8);
+      hwy::Abort(filename, line, "Masks not equal");
+    }
+  }
+  // Then the valid bit(s) in the last byte.
+  const size_t remainder = N % 8;
+  if (remainder != 0) {
+    const int mask = (1 << remainder) - 1;
+    const int valid_a = bits_a[i] & mask;
+    const int valid_b = bits_b[i] & mask;
+    if (valid_a != valid_b) {
+      fprintf(stderr, "Mismatch in last byte %zu: %d != %d\n", i, valid_a,
+              valid_b);
+      Print(d8, "expect", Load(d8, bits_a.get()), 0, N8);
+      Print(d8, "actual", Load(d8, bits_b.get()), 0, N8);
+      hwy::Abort(filename, line, "Masks not equal");
+    }
+  }
+#endif
 }
 
+// Only sets valid elements (those whose index < Lanes(d)). This helps catch
+// tests that are not masking off the (undefined) upper mask elements.
 template <class D>
 HWY_NOINLINE Mask<D> MaskTrue(const D d) {
-  const auto v0 = Zero(d);
-  return Eq(v0, v0);
+  return FirstN(d, Lanes(d));
 }
 
 template <class D>
 HWY_NOINLINE Mask<D> MaskFalse(const D d) {
-  // Lt is only for signed types and we cannot yet cast mask types.
-  return Eq(Zero(d), Set(d, 1));
+  const auto zero = Zero(RebindToSigned<D>());
+  return RebindMask(d, Lt(zero, zero));
 }
 
 #ifndef HWY_ASSERT_EQ
@@ -439,14 +502,39 @@ HWY_NOINLINE Mask<D> MaskFalse(const D d) {
 
 // Helpers for instantiating tests with combinations of lane types / counts.
 
-// For all powers of two in [kMinLanes, N * kMinLanes] (so that recursion stops
-// at N == 0)
-template <typename T, size_t N, size_t kMinLanes, class Test>
+template <bool valid>
+struct CallTestIf {
+  template <typename T, size_t N, class Test>
+  static void Do() {
+    const Simd<T, N> d;
+    // Skip invalid fractions (e.g. 1/8th of u32x4).
+    if (Lanes(d) == 0) return;
+    Test()(T(), d);
+  }
+};
+// Avoids instantiating tests for invalid N (a smaller fraction than 1/8th).
+template <>
+struct CallTestIf<false> {
+  template <typename T, size_t N, class Test>
+  static void Do() {
+    // Should only happen with scalable vectors.
+    HWY_ASSERT(HWY_TARGET == HWY_RVV || HWY_TARGET == HWY_SVE ||
+               HWY_TARGET == HWY_SVE2);
+  }
+};
+
+// For all powers of two in [kMinLanes, kMul * kMinLanes] (so that recursion
+// stops at kMul == 0)
+template <typename T, size_t kMul, size_t kMinLanes, class Test>
 struct ForeachSizeR {
   static void Do() {
-    static_assert(N != 0, "End of recursion");
-    Test()(T(), Simd<T, N * kMinLanes>());
-    ForeachSizeR<T, N / 2, kMinLanes, Test>::Do();
+    static_assert(kMul != 0, "End of recursion");
+    constexpr size_t N = kMul * kMinLanes;
+    constexpr bool kIsExact = N * sizeof(T) <= 16;
+    constexpr bool kIsRatio = N >= HWY_LANES(T) / 8;
+    CallTestIf<kIsExact || kIsRatio>::template Do<T, N, Test>();
+
+    ForeachSizeR<T, kMul / 2, kMinLanes, Test>::Do();
   }
 };
 
@@ -458,60 +546,122 @@ struct ForeachSizeR<T, 0, kMinLanes, Test> {
 
 // These adapters may be called directly, or via For*Types:
 
-// Calls Test for all powers of two in [kMinLanes, HWY_LANES(T) / kDivLanes].
-template <class Test, size_t kDivLanes = 1, size_t kMinLanes = 1>
+// Calls Test for all power of two N in [1, Lanes(d)]. This is the default
+// for ops that do not narrow nor widen their input, nor require 128 bits.
+template <class Test>
 struct ForPartialVectors {
   template <typename T>
   void operator()(T /*unused*/) const {
 #if HWY_TARGET == HWY_RVV
-    // Only m1..8 for now, can ignore kMaxLanes because HWY_*_LANES are full.
-    ForeachSizeR<T, 8 / kDivLanes, HWY_LANES(T), Test>::Do();
+    // Only m1..8 until we support fractional LMUL.
+    ForeachSizeR<T, 8, HWY_LANES(T), Test>::Do();
 #else
-    ForeachSizeR<T, HWY_LANES(T) / kDivLanes / kMinLanes, kMinLanes,
-                 Test>::Do();
+    ForeachSizeR<T, HWY_LANES(T), 1, Test>::Do();
 #endif
   }
 };
 
-// Calls Test for all vectors that can be demoted log2(kFactor) times.
-template <class Test, size_t kFactor>
-struct ForDemoteVectors {
+// Calls Test for all power of two N in [16 / sizeof(T), Lanes(d)]. This is for
+// ops that require at least 128 bits, e.g. AES or 64x64 = 128 mul.
+template <class Test>
+struct ForGE128Vectors {
   template <typename T>
   void operator()(T /*unused*/) const {
 #if HWY_TARGET == HWY_RVV
-    // Only m1..8 for now.
-    ForeachSizeR<T, 8 / kFactor, kFactor * HWY_LANES(T), Test>::Do();
+    ForeachSizeR<T, 8, HWY_LANES(T), Test>::Do();
 #else
-    ForeachSizeR<T, HWY_LANES(T), 1, Test>::Do();
+    ForeachSizeR<T, HWY_LANES(T) / (16 / sizeof(T)), (16 / sizeof(T)),
+                 Test>::Do();
 #endif
   }
 };
 
-// Calls Test for all powers of two in [128 bits, max bits].
+// Calls Test for all power of two N in [8 / sizeof(T), Lanes(d)]. This is for
+// ops that require at least 64 bits, e.g. casts.
 template <class Test>
-struct ForGE128Vectors {
+struct ForGE64Vectors {
   template <typename T>
   void operator()(T /*unused*/) const {
 #if HWY_TARGET == HWY_RVV
     ForeachSizeR<T, 8, HWY_LANES(T), Test>::Do();
 #else
-    ForeachSizeR<T, HWY_LANES(T) / (16 / sizeof(T)), (16 / sizeof(T)),
+    ForeachSizeR<T, HWY_LANES(T) / (8 / sizeof(T)), (8 / sizeof(T)),
                  Test>::Do();
-
 #endif
   }
 };
 
-// Calls Test for all vectors that can be expanded by kFactor.
+// Calls Test for all power of two N in [1, Lanes(d) / kFactor]. This is for
+// ops that widen their input, e.g. Combine (not supported by HWY_SCALAR).
 template <class Test, size_t kFactor = 2>
 struct ForExtendableVectors {
   template <typename T>
   void operator()(T /*unused*/) const {
-#if HWY_TARGET == HWY_RVV
+#if HWY_TARGET == HWY_SCALAR
+    // not supported
+#elif HWY_TARGET == HWY_RVV
     ForeachSizeR<T, 8 / kFactor, HWY_LANES(T), Test>::Do();
+    // TODO(janwas): also capped
+    // ForeachSizeR<T, (16 / sizeof(T)) / kFactor, 1, Test>::Do();
+#elif HWY_TARGET == HWY_SVE || HWY_TARGET == HWY_SVE2
+    // Capped
+    ForeachSizeR<T, (16 / sizeof(T)) / kFactor, 1, Test>::Do();
+    // Fractions
+    ForeachSizeR<T, 8 / kFactor, HWY_LANES(T) / 8, Test>::Do();
 #else
-    ForeachSizeR<T, HWY_LANES(T) / kFactor / (16 / sizeof(T)), (16 / sizeof(T)),
-                 Test>::Do();
+    ForeachSizeR<T, HWY_LANES(T) / kFactor, 1, Test>::Do();
+#endif
+  }
+};
+
+// Calls Test for all N that can be promoted (not the same as Extendable because
+// HWY_SCALAR has one lane). Also used for ZipLower, but not ZipUpper.
+template <class Test, size_t kFactor = 2>
+struct ForPromoteVectors {
+  template <typename T>
+  void operator()(T /*unused*/) const {
+#if HWY_TARGET == HWY_SCALAR
+    ForeachSizeR<T, 1, 1, Test>::Do();
+#else
+    return ForExtendableVectors<Test, kFactor>()(T());
+#endif
+  }
+};
+
+// Calls Test for all power of two N in [kFactor, Lanes(d)]. This is for ops
+// that narrow their input, e.g. UpperHalf.
+template <class Test, size_t kFactor = 2>
+struct ForShrinkableVectors {
+  template <typename T>
+  void operator()(T /*unused*/) const {
+#if HWY_TARGET == HWY_RVV
+    // Only m1..8 until we support fractional LMUL.
+    ForeachSizeR<T, 8 / kFactor, kFactor * HWY_LANES(T), Test>::Do();
+#elif HWY_TARGET == HWY_SVE || HWY_TARGET == HWY_SVE2
+    ForeachSizeR<T, 8 / kFactor, kFactor * HWY_LANES(T) / 8, Test>::Do();
+#elif HWY_TARGET == HWY_SCALAR
+    // not supported
+#else
+    ForeachSizeR<T, HWY_LANES(T) / kFactor, kFactor, Test>::Do();
+#endif
+  }
+};
+
+// Calls Test for all N than can be demoted (not the same as Shrinkable because
+// HWY_SCALAR has one lane). Also used for LowerHalf, but not UpperHalf.
+template <class Test, size_t kFactor = 2>
+struct ForDemoteVectors {
+  template <typename T>
+  void operator()(T /*unused*/) const {
+#if HWY_TARGET == HWY_RVV
+    // Only m1..8 until we support fractional LMUL.
+    ForeachSizeR<T, 8 / kFactor, kFactor * HWY_LANES(T), Test>::Do();
+#elif HWY_TARGET == HWY_SVE || HWY_TARGET == HWY_SVE2
+    ForeachSizeR<T, 8 / kFactor, kFactor * HWY_LANES(T) / 8, Test>::Do();
+#elif HWY_TARGET == HWY_SCALAR
+    ForeachSizeR<T, 1, 1, Test>::Do();
+#else
+    ForeachSizeR<T, HWY_LANES(T) / kFactor, kFactor, Test>::Do();
 #endif
   }
 };
diff --git a/third_party/highway/hwy/tests/test_util_test.cc b/third_party/highway/hwy/tests/test_util_test.cc
index b0f5edf52afe7..140e2d975f013 100644
--- a/third_party/highway/hwy/tests/test_util_test.cc
+++ b/third_party/highway/hwy/tests/test_util_test.cc
@@ -94,9 +94,17 @@ HWY_NOINLINE void TestAllEqual() {
 HWY_AFTER_NAMESPACE();
 
 #if HWY_ONCE
+
 namespace hwy {
 HWY_BEFORE_TEST(TestUtilTest);
 HWY_EXPORT_AND_TEST_P(TestUtilTest, TestAllName);
 HWY_EXPORT_AND_TEST_P(TestUtilTest, TestAllEqual);
 }  // namespace hwy
+
+// Ought not to be necessary, but without this, no tests run on RVV.
+int main(int argc, char **argv) {
+  ::testing::InitGoogleTest(&argc, argv);
+  return RUN_ALL_TESTS();
+}
+
 #endif
diff --git a/third_party/highway/run_tests.sh b/third_party/highway/run_tests.sh
old mode 100644
new mode 100755
diff --git a/third_party/highway/test.py b/third_party/highway/test.py
deleted file mode 100644
index f0e5da31acdb9..0000000000000
--- a/third_party/highway/test.py
+++ /dev/null
@@ -1,131 +0,0 @@
-#!/usr/bin/env python3
-"""Helper for running tests for all platforms.
-
-One-time setup:
-sudo apt-get install npm
-sudo npm install -g npm jsvu
-
-Usage:
-third_party/highway/test.py [--test=memory_test]
-
-"""
-
-import os
-import sys
-import tarfile
-import tempfile
-import argparse
-import subprocess
-
-
-def run_subprocess(args, work_dir):
-  """Runs subprocess and checks for success."""
-  process = subprocess.Popen(args, cwd=work_dir)
-  process.communicate()
-  assert process.returncode == 0
-
-
-def print_status(name):
-  print("=" * 60, name)
-
-
-def run_blaze_tests(work_dir, target, desired_config, config_name, blazerc,
-                    config):
-  """Builds and runs via blaze or returns 0 if skipped."""
-  if desired_config is not None and desired_config != config_name:
-    return 0
-  print_status(config_name)
-  default_config = ["-c", "opt", "--copt=-DHWY_COMPILE_ALL_ATTAINABLE"]
-  args = ["blaze"] + blazerc + ["test", ":" + target] + config + default_config
-  run_subprocess(args, work_dir)
-  return 1
-
-
-def run_wasm_tests(work_dir, target, desired_config, config_name, options):
-  """Runs wasm via blaze/v8, or returns 0 if skipped."""
-  if desired_config is not None and desired_config != config_name:
-    return 0
-  args = [options.v8, "--no-liftoff", "--experimental-wasm-simd"]
-  # Otherwise v8 returns 0 even after compile failures!
-  args.append("--no-wasm-async-compilation")
-  if options.profile:
-    args.append("--perf-basic-prof-only-functions")
-
-  num_tests_run = 0
-  skipped = []
-
-  # Build (no blazerc to avoid failures caused by local .blazerc)
-  run_subprocess([
-      "blaze", "--blazerc=/dev/null", "build", "--config=wasm",
-      "--features=wasm_simd", ":" + target
-  ], work_dir)
-
-  path = "blaze-bin/third_party/highway/"
-  TEST_SUFFIX = "_test"
-  for test in os.listdir(path):
-    # Only test binaries (avoids non-tar files)
-    if not test.endswith(TEST_SUFFIX):
-      continue
-
-    # Skip directories
-    tar_pathname = os.path.join(path, test)
-    if os.path.isdir(tar_pathname):
-      continue
-
-    # Only the desired test (if given)
-    if options.test is not None and options.test != test:
-      skipped.append(test[0:-len(TEST_SUFFIX)])
-      continue
-
-    with tempfile.TemporaryDirectory() as extract_dir:
-      with tarfile.open(tar_pathname, mode="r:") as tar:
-        tar.extractall(extract_dir)
-
-      test_args = args + [os.path.join(extract_dir, test) + ".js"]
-      run_subprocess(test_args, extract_dir)
-      num_tests_run += 1
-
-  print("Finished", num_tests_run, "; skipped", ",".join(skipped))
-  assert (num_tests_run != 0)
-  return 1
-
-
-def main(args):
-  parser = argparse.ArgumentParser(description="Run test(s)")
-  parser.add_argument("--v8", help="Pathname to v8 (default ~/jsvu/v8)")
-  parser.add_argument("--test", help="Which test to run (defaults to all)")
-  parser.add_argument("--config", help="Which config to run (defaults to all)")
-  parser.add_argument("--profile", action="store_true", help="Enable profiling")
-  options = parser.parse_args(args)
-  if options.v8 is None:
-    options.v8 = os.path.join(os.getenv("HOME"), ".jsvu", "v8")
-
-  work_dir = os.path.dirname(os.path.realpath(__file__))
-  target = "hwy_ops_tests" if options.test is None else options.test
-
-  num_config = 0
-  num_config += run_blaze_tests(work_dir, target, options.config, "x86", [], [])
-  num_config += run_blaze_tests(
-      work_dir, target, options.config, "rvv",
-      ["--blazerc=../../third_party/unsupported_toolchains/mpu/blazerc"],
-      ["--config=mpu64_gcc"])
-  num_config += run_blaze_tests(work_dir, target, options.config, "arm8", [],
-                                ["--config=android_arm64"])
-  num_config += run_blaze_tests(work_dir, target, options.config, "arm7", [], [
-      "--config=android_arm", "--copt=-mfpu=neon-vfpv4",
-      "--copt=-mfloat-abi=softfp"
-  ])
-  num_config += run_blaze_tests(work_dir, target, options.config, "msvc", [],
-                                ["--config=msvc"])
-  num_config += run_wasm_tests(work_dir, target, options.config, "wasm",
-                               options)
-
-  if num_config == 0:
-    print_status("ERROR: unknown --config=%s, omit to see valid names" %
-                 (options.config,))
-  else:
-    print_status("done")
-
-
-if __name__ == "__main__":
-  main(sys.argv[1:])
From 4e8f7069ac2b8078897ee1fff9e6b4ec3d247f9e Mon Sep 17 00:00:00 2001
From: Makoto Kato <m_kato@ga2.so-net.ne.jp>
Date: Sun, 15 Aug 2021 01:23:35 +0900
Subject: [PATCH] Export some header files after upgrading highway.

---
 media/highway/moz.build | 3 +++
 1 file changed, 3 insertions(+)

diff --git a/media/highway/moz.build b/media/highway/moz.build
index b64522554d5e7..d4465797931e9 100644
--- a/media/highway/moz.build
+++ b/media/highway/moz.build
@@ -18,6 +18,8 @@ EXPORTS.hwy += [
     "/third_party/highway/hwy/aligned_allocator.h",
     "/third_party/highway/hwy/base.h",
     "/third_party/highway/hwy/cache_control.h",
+    "/third_party/highway/hwy/detect_compiler_arch.h",
+    "/third_party/highway/hwy/detect_targets.h",
     "/third_party/highway/hwy/foreach_target.h",
     "/third_party/highway/hwy/highway.h",
     "/third_party/highway/hwy/targets.h",
@@ -25,6 +27,7 @@ EXPORTS.hwy += [
 
 EXPORTS.hwy.ops += [
     "/third_party/highway/hwy/ops/arm_neon-inl.h",
+    "/third_party/highway/hwy/ops/generic_ops-inl.h",
     "/third_party/highway/hwy/ops/rvv-inl.h",
     "/third_party/highway/hwy/ops/scalar-inl.h",
     "/third_party/highway/hwy/ops/set_macros-inl.h",
