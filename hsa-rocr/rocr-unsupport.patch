diff --git a/runtime/hsa-runtime/core/runtime/amd_aql_queue.cpp b/runtime/hsa-runtime/core/runtime/amd_aql_queue.cpp
index 47a6f387..9607107f 100644
--- a/runtime/hsa-runtime/core/runtime/amd_aql_queue.cpp
+++ b/runtime/hsa-runtime/core/runtime/amd_aql_queue.cpp
@@ -1666,7 +1666,13 @@ void AqlQueue::ExecutePM4(uint32_t* cmd_data, size_t cmd_size_b, hsa_fence_scope
   memcpy(&queue_slot[1], &slot_data[1], slot_size_b - sizeof(uint32_t));
   if (core::Runtime::runtime_singleton_->flag().dev_mem_queue() && !agent_->is_xgmi_cpu_gpu()) {
     // Ensure the packet body is written as header may get reordered when writing over PCIE
+#if defined(_M_X64)
     _mm_sfence();
+#elif defined(__loongarch_lp64)
+    asm("dbar 0");
+#elif defined(__riscv)
+    asm volatile("fence w, w");
+#endif
   }
   atomic::Store(&queue_slot[0], slot_data[0], std::memory_order_release);
 
diff --git a/runtime/hsa-runtime/core/runtime/amd_blit_kernel.cpp b/runtime/hsa-runtime/core/runtime/amd_blit_kernel.cpp
index 36d21fa1..92b8f980 100644
--- a/runtime/hsa-runtime/core/runtime/amd_blit_kernel.cpp
+++ b/runtime/hsa-runtime/core/runtime/amd_blit_kernel.cpp
@@ -1274,7 +1274,13 @@ void BlitKernel::PopulateQueue(uint64_t index, uint64_t code_handle, void* args,
   std::atomic_thread_fence(std::memory_order_release);
   if (core::Runtime::runtime_singleton_->flag().dev_mem_queue() && !queue_->needsPcieOrdering()) {
     // Ensure the packet body is written as header may get reordered when writing over PCIE
+#if defined(_M_X64)
     _mm_sfence();
+#elif defined(__loongarch_lp64)
+    asm("dbar 0");
+#elif defined(__riscv)
+    asm volatile("fence w, w");
+#endif
   }
   queue_buffer[index & queue_bitmask_].header = kDispatchPacketHeader;
 
diff --git a/runtime/hsa-runtime/core/runtime/intercept_queue.cpp b/runtime/hsa-runtime/core/runtime/intercept_queue.cpp
index a86dabb3..2d35991d 100644
--- a/runtime/hsa-runtime/core/runtime/intercept_queue.cpp
+++ b/runtime/hsa-runtime/core/runtime/intercept_queue.cpp
@@ -258,7 +258,13 @@ uint64_t InterceptQueue::Submit(const AqlPacket* packets, uint64_t count) {
       ring[barrier & mask].barrier_and.completion_signal = Signal::Convert(async_doorbell_);
       if (Runtime::runtime_singleton_->flag().dev_mem_queue() && !needsPcieOrdering()) {
         // Ensure the packet body is written as header may get reordered when writing over PCIE
-        _mm_sfence();
+#if defined(_M_X64)
+    _mm_sfence();
+#elif defined(__loongarch_lp64)
+    asm("dbar 0");
+#elif defined(__riscv)
+    asm volatile("fence w, w");
+#endif
       }
       atomic::Store(&ring[barrier & mask].barrier_and.header, kBarrierHeader,
                     std::memory_order_release);
@@ -305,7 +311,13 @@ uint64_t InterceptQueue::Submit(const AqlPacket* packets, uint64_t count) {
       if (write_index != 0) {
         if (Runtime::runtime_singleton_->flag().dev_mem_queue() && !needsPcieOrdering()) {
           // Ensure the packet body is written as header may get reordered when writing over PCIE
+#if defined(_M_X64)
           _mm_sfence();
+#elif defined(__loongarch_lp64)
+          asm("dbar 0");
+#elif defined(__riscv)
+          asm volatile("fence w, w");
+#endif
         }
         atomic::Store(&ring[write & mask].packet.header, packets[first_written_packet_index].packet.header,
                       std::memory_order_release);
@@ -374,7 +386,13 @@ void InterceptQueue::StoreRelaxed(hsa_signal_value_t value) {
     handler.first(&ring[i & mask], 1, i, handler.second, PacketWriter);
     if (Runtime::runtime_singleton_->flag().dev_mem_queue() && !needsPcieOrdering()) {
       // Ensure the packet body is written as header may get reordered when writing over PCIE
+#if defined(_M_X64)
       _mm_sfence();
+#elif defined(__loongarch_lp64)
+      asm("dbar 0");
+#elif defined(__riscv)
+      asm volatile("fence w, w");
+#endif
     }
     // Invalidate consumed packet.
     atomic::Store(&ring[i & mask].packet.header, kInvalidHeader, std::memory_order_release);
diff --git a/runtime/hsa-runtime/core/util/lnx/os_linux.cpp b/runtime/hsa-runtime/core/util/lnx/os_linux.cpp
index 4d629798..efb0fc73 100644
--- a/runtime/hsa-runtime/core/util/lnx/os_linux.cpp
+++ b/runtime/hsa-runtime/core/util/lnx/os_linux.cpp
@@ -65,7 +65,7 @@
 #include <cpuid.h>
 #endif
 
-#ifdef __GLIBC__
+#if defined(__GLIBC__) && !defined(__riscv)
 #define ABS_ADDR(base, ptr) (ptr)
 #else
 #define ABS_ADDR(base, ptr) ((base) + (ptr))
diff --git a/runtime/hsa-runtime/core/util/locks.h b/runtime/hsa-runtime/core/util/locks.h
index 6c0de49a..052d04ac 100644
--- a/runtime/hsa-runtime/core/util/locks.h
+++ b/runtime/hsa-runtime/core/util/locks.h
@@ -72,7 +72,11 @@ class HybridMutex {
     while (!lock_.compare_exchange_strong(old, 1)) {
       cnt--;
       if (cnt > maxSpinIterPause) {
+#if defined(_M_X64)
         _mm_pause();
+#else
+        os::YieldThread();
+#endif
       } else if (cnt-- > maxSpinIterYield) {
         os::YieldThread();
       } else {
diff --git a/runtime/hsa-runtime/core/util/utils.h b/runtime/hsa-runtime/core/util/utils.h
index 66c2028a..7b400578 100644
--- a/runtime/hsa-runtime/core/util/utils.h
+++ b/runtime/hsa-runtime/core/util/utils.h
@@ -354,6 +354,7 @@ static __forceinline std::string& trim(std::string& s) { return ltrim(rtrim(s));
 /// @param: offset(Input), offset of base address to flush
 /// @param: len(Input), length of buffer to flush
 inline void FlushCpuCache(const void* base, size_t offset, size_t len) {
+#if defined(_M_X64)
   static long cacheline_size = 0;
 
   if (!cacheline_size) {
@@ -369,6 +370,9 @@ inline void FlushCpuCache(const void* base, size_t offset, size_t len) {
     _mm_clflush((const void*)cur);
     cur += cacheline_size;
   } while (cur <= (const char*)lastline);
+#elif defined(__riscv)
+  asm volatile("fence rw, rw");
+#endif
 }
 
 }  // namespace rocr
