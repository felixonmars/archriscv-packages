diff --git PKGBUILD PKGBUILD
index c00541d6..4bcd1646 100644
--- PKGBUILD
+++ PKGBUILD
@@ -12,7 +12,7 @@ url="https://www.mozilla.org/firefox/"
 depends=(gtk3 libxt mime-types dbus-glib ffmpeg nss ttf-font libpulse)
 makedepends=(unzip zip diffutils yasm mesa imake inetutils xorg-server-xvfb
              autoconf2.13 rust clang llvm jack nodejs cbindgen nasm
-             python-setuptools python-psutil python-zstandard lld dump_syms)
+             python-setuptools python-psutil python-zstandard dump_syms)
 optdepends=('networkmanager: Location detection via available WiFi networks'
             'libnotify: Notification integration'
             'pulseaudio: Audio support'
@@ -23,11 +23,15 @@ options=(!emptydirs !makeflags !strip)
 source=(https://archive.mozilla.org/pub/firefox/releases/$pkgver/source/firefox-$pkgver.source.tar.xz{,.asc}
         0001-Use-remoting-name-for-GDK-application-names.patch
         0002-Bug-1731495-Don-t-typecheck-the-pipewire-session_han.patch
+        cc-crate-riscv64-clang-support.patch
+        makotokato-riscv64-support-and-zenithal-backported.patch
         $pkgname.desktop identity-icons-brand.svg)
 sha256sums=('9096b22e162cd299080d5eef8f3627a71a594ceba2b89e3000f2c3e8ea603eb1'
             'SKIP'
             'd7c7a65c4b7ec9ea40df129724ffb369d3f775b0514e3c267c52eec6d284b5e6'
             '8f313d96c845723f54996d660a201d747dfa8da791f19a827aba55cb81261e38'
+            'b02ecd2476832a243426ac6d69803d8b15c8509da3e442a3f0c08b17e49f4e8a'
+            'bd572ba962ff55530c09b4477578a68f2cd1a408a3051bf82189fadd64ab86a1'
             '298eae9de76ec53182f38d5c549d0379569916eebf62149f9d7f4a7edef36abf'
             'a9b8b4a0a1f4a7b4af77d5fc70c2686d624038909263c795ecc81e0aec7711e9')
 validpgpkeys=('14F26682D0916CDD81E37B6D61B7B526D98F0353') # Mozilla Software Releases <release@mozilla.com>
@@ -50,6 +54,8 @@ prepare() {
 
   # https://bugzilla.mozilla.org/show_bug.cgi?id=1530052
   patch -Np1 -i ../0001-Use-remoting-name-for-GDK-application-names.patch
+  patch -Np1 -i ../cc-crate-riscv64-clang-support.patch
+  patch -Np1 -i ../makotokato-riscv64-support-and-zenithal-backported.patch
 
   # https://bugzilla.mozilla.org/show_bug.cgi?id=1731495
   patch -Np1 -i ../0002-Bug-1731495-Don-t-typecheck-the-pipewire-session_han.patch
@@ -62,13 +68,20 @@ ac_add_options --enable-application=browser
 mk_add_options MOZ_OBJDIR=${PWD@Q}/obj
 
 ac_add_options --prefix=/usr
-ac_add_options --enable-release
+# release mode incurs TEST-UNEXPECTED-FAIL | check_networking
+# refer to config/makefiles/rust.mk#L408
+ac_add_options --disable-release
 ac_add_options --enable-hardening
-ac_add_options --enable-optimize
 ac_add_options --enable-rust-simd
-ac_add_options --enable-linker=lld
-ac_add_options --disable-elf-hack
+# lld is broken
+ac_add_options --enable-linker=bfd
 ac_add_options --disable-bootstrap
+ac_add_options --disable-jit
+
+ac_add_options --enable-optimize
+ac_add_options --disable-debug
+# rustc with opt-level 2 would segfault when compiling neqo-transport
+export RUSTC_OPT_LEVEL=1
 
 # Branding
 ac_add_options --enable-official-branding
@@ -76,7 +89,8 @@ ac_add_options --enable-update-channel=release
 ac_add_options --with-distribution-id=org.archlinux
 ac_add_options --with-unsigned-addon-scopes=app,system
 ac_add_options --allow-addon-sideload
-export MOZILLA_OFFICIAL=1
+# see check_network above
+# export MOZILLA_OFFICIAL=1
 export MOZ_APP_REMOTINGNAME=${pkgname//-/}
 
 # Keys
@@ -91,7 +105,7 @@ ac_add_options --with-system-nss
 # Features
 ac_add_options --enable-alsa
 ac_add_options --enable-jack
-ac_add_options --enable-crashreporter
+# crashreporter not ported to riscv64
 ac_add_options --disable-updater
 ac_add_options --disable-tests
 END
@@ -109,39 +123,41 @@ build() {
   ulimit -n 4096
 
   # Do 3-tier PGO
-  echo "Building instrumented browser..."
-  cat >.mozconfig ../mozconfig - <<END
-ac_add_options --enable-profile-generate=cross
-END
-  ./mach build
-
-  echo "Profiling instrumented browser..."
-  ./mach package
-  LLVM_PROFDATA=llvm-profdata \
-    JARLOG_FILE="$PWD/jarlog" \
-    xvfb-run -s "-screen 0 1920x1080x24 -nolisten local" \
-    ./mach python build/pgo/profileserver.py
-
-  stat -c "Profile data found (%s bytes)" merged.profdata
-  test -s merged.profdata
-
-  stat -c "Jar log found (%s bytes)" jarlog
-  test -s jarlog
-
-  echo "Removing instrumented browser..."
-  ./mach clobber
-
-  echo "Building optimized browser..."
-  cat >.mozconfig ../mozconfig - <<END
-ac_add_options --enable-lto=cross
-ac_add_options --enable-profile-use=cross
-ac_add_options --with-pgo-profile-path=${PWD@Q}/merged.profdata
-ac_add_options --with-pgo-jarlog=${PWD@Q}/jarlog
-END
+  # disable PGO
+#  echo "Building instrumented browser..."
+   cat >.mozconfig ../mozconfig
+#  cat >.mozconfig ../mozconfig - <<END
+#ac_add_options --enable-profile-generate=cross
+#END
   ./mach build
 
-  echo "Building symbol archive..."
-  ./mach buildsymbols
+#  echo "Profiling instrumented browser..."
+#  ./mach package
+#  LLVM_PROFDATA=llvm-profdata \
+#    JARLOG_FILE="$PWD/jarlog" \
+#    xvfb-run -s "-screen 0 1920x1080x24 -nolisten local" \
+#    ./mach python build/pgo/profileserver.py
+#
+#  stat -c "Profile data found (%s bytes)" merged.profdata
+#  test -s merged.profdata
+#
+#  stat -c "Jar log found (%s bytes)" jarlog
+#  test -s jarlog
+#
+#  echo "Removing instrumented browser..."
+#  ./mach clobber
+#
+#  echo "Building optimized browser..."
+#  cat >.mozconfig ../mozconfig - <<END
+#ac_add_options --enable-lto=cross
+#ac_add_options --enable-profile-use=cross
+#ac_add_options --with-pgo-profile-path=${PWD@Q}/merged.profdata
+#ac_add_options --with-pgo-jarlog=${PWD@Q}/jarlog
+#END
+#  ./mach build
+#
+#  echo "Building symbol archive..."
+#  ./mach buildsymbols
 }
 
 package() {
@@ -209,12 +225,12 @@ END
     ln -srfv "$pkgdir/usr/lib/libnssckbi.so" "$nssckbi"
   fi
 
-  export SOCORRO_SYMBOL_UPLOAD_TOKEN_FILE="$startdir/.crash-stats-api.token"
-  if [[ -f $SOCORRO_SYMBOL_UPLOAD_TOKEN_FILE ]]; then
-    make -C obj uploadsymbols
-  else
-    cp -fvt "$startdir" obj/dist/*crashreporter-symbols-full.tar.zst
-  fi
+  #export SOCORRO_SYMBOL_UPLOAD_TOKEN_FILE="$startdir/.crash-stats-api.token"
+  #if [[ -f $SOCORRO_SYMBOL_UPLOAD_TOKEN_FILE ]]; then
+  #  make -C obj uploadsymbols
+  #else
+  #  cp -fvt "$startdir" obj/dist/*crashreporter-symbols-full.tar.zst
+  #fi
 }
 
 # vim:set sw=2 et:
diff --git cc-crate-riscv64-clang-support.patch cc-crate-riscv64-clang-support.patch
new file mode 100644
index 00000000..ec7f3205
--- /dev/null
+++ cc-crate-riscv64-clang-support.patch
@@ -0,0 +1,24 @@
+diff --git a/third_party/rust/cc/src/lib.rs b/third_party/rust/cc/src/lib.rs
+index 9d133a0..9232483 100644
+--- a/third_party/rust/cc/src/lib.rs
++++ b/third_party/rust/cc/src/lib.rs
+@@ -1484,6 +1484,10 @@ impl Build {
+                                 .into(),
+                             );
+                         }
++                    } else if target.starts_with("riscv64gc-") {
++                        cmd.args.push(
++                            format!("--target={}", target.replace("riscv64gc", "riscv64")).into(),
++                        );
+                     } else {
+                         cmd.args.push(format!("--target={}", target).into());
+                     }
+diff --git a/third_party/rust/cc/.cargo-checksum.json b/third_party/rust/cc/.cargo-checksum.json
+index 62e6d49..bdf9fd5 100644
+--- a/third_party/rust/cc/.cargo-checksum.json
++++ b/third_party/rust/cc/.cargo-checksum.json
+@@ -1 +1 @@
+-{"files":{"Cargo.lock":"24720bf62cfad67ca24dfc9192a8f1c11a0f262655c087795605f188cee5c5f0","Cargo.toml":"84ef3b052c7b9ba469573df3ee45d89d426f2cd30d350f43198f115b9c5691fc","LICENSE-APACHE":"a60eea817514531668d7e00765731449fe14d059d3249e0bc93b36de45f759f2","LICENSE-MIT":"378f5840b258e2779c39418f3f2d7b2ba96f1c7917dd6be0713f88305dbda397","README.md":"9916275542d23bfa0815b1f48d4546e514739fadc79775500de6a81cf17aac09","src/bin/gcc-shim.rs":"b77907875029494b6288841c3aed2e4939ed40708c7f597fca5c9e2570490ca6","src/com.rs":"bcdaf1c28b71e6ef889c6b08d1ce9d7c0761344a677f523bc4c3cd297957f804","src/lib.rs":"20d349f8528f191a4cf04a5a42daaaa8085c4e00885c78456ebada92dc39b7fb","src/registry.rs":"3cc1b5a50879fa751572878ae1d0afbfc960c11665258492754b2c8bccb0ff5d","src/setup_config.rs":"7014103587d3382eac599cb76f016e2609b8140970861b2237982d1db24af265","src/vs_instances.rs":"2d3f8278a803b0e7052f4eeb1979b29f963dd0143f4458e2cb5f33c4e5f0963b","src/winapi.rs":"ea8b7edbb9ff87957254f465c2334e714c5d6b3b19a8d757c48ea7ca0881c50c","src/windows_registry.rs":"090b5de68e19dab9e1884175dc2a6866f697bd043d6b3a0d4b3836c9d6812569","tests/cc_env.rs":"e02b3b0824ad039b47e4462c5ef6dbe6c824c28e7953af94a0f28f7b5158042e","tests/cflags.rs":"57f06eb5ce1557e5b4a032d0c4673e18fbe6f8d26c1deb153126e368b96b41b3","tests/cxxflags.rs":"c2c6c6d8a0d7146616fa1caed26876ee7bc9fcfffd525eb4743593cade5f3371","tests/support/mod.rs":"16274867f23871e9b07614eda4c7344da13d1751fed63d4f633857e40be86394","tests/test.rs":"65c073e0e2cf4aa0433066102788e9f57442719e6f32f5ad5248aa7132bb4597"},"package":"4a72c244c1ff497a746a7e1fb3d14bd08420ecda70c8f25c7112f2781652d787"}
+\ No newline at end of file
++{"files":{"Cargo.lock":"24720bf62cfad67ca24dfc9192a8f1c11a0f262655c087795605f188cee5c5f0","Cargo.toml":"84ef3b052c7b9ba469573df3ee45d89d426f2cd30d350f43198f115b9c5691fc","LICENSE-APACHE":"a60eea817514531668d7e00765731449fe14d059d3249e0bc93b36de45f759f2","LICENSE-MIT":"378f5840b258e2779c39418f3f2d7b2ba96f1c7917dd6be0713f88305dbda397","README.md":"9916275542d23bfa0815b1f48d4546e514739fadc79775500de6a81cf17aac09","src/bin/gcc-shim.rs":"b77907875029494b6288841c3aed2e4939ed40708c7f597fca5c9e2570490ca6","src/com.rs":"bcdaf1c28b71e6ef889c6b08d1ce9d7c0761344a677f523bc4c3cd297957f804","src/lib.rs":"849a6131ca0a803f0ed62c8afd68b43dfbb350cb29d1d28e5f52a3d26a605f61","src/registry.rs":"3cc1b5a50879fa751572878ae1d0afbfc960c11665258492754b2c8bccb0ff5d","src/setup_config.rs":"7014103587d3382eac599cb76f016e2609b8140970861b2237982d1db24af265","src/vs_instances.rs":"2d3f8278a803b0e7052f4eeb1979b29f963dd0143f4458e2cb5f33c4e5f0963b","src/winapi.rs":"ea8b7edbb9ff87957254f465c2334e714c5d6b3b19a8d757c48ea7ca0881c50c","src/windows_registry.rs":"090b5de68e19dab9e1884175dc2a6866f697bd043d6b3a0d4b3836c9d6812569","tests/cc_env.rs":"e02b3b0824ad039b47e4462c5ef6dbe6c824c28e7953af94a0f28f7b5158042e","tests/cflags.rs":"57f06eb5ce1557e5b4a032d0c4673e18fbe6f8d26c1deb153126e368b96b41b3","tests/cxxflags.rs":"c2c6c6d8a0d7146616fa1caed26876ee7bc9fcfffd525eb4743593cade5f3371","tests/support/mod.rs":"16274867f23871e9b07614eda4c7344da13d1751fed63d4f633857e40be86394","tests/test.rs":"65c073e0e2cf4aa0433066102788e9f57442719e6f32f5ad5248aa7132bb4597"},"package":"4a72c244c1ff497a746a7e1fb3d14bd08420ecda70c8f25c7112f2781652d787"}
+\ No newline at end of file
diff --git makotokato-riscv64-support-and-zenithal-backported.patch makotokato-riscv64-support-and-zenithal-backported.patch
new file mode 100644
index 00000000..b1fd80ed
--- /dev/null
+++ makotokato-riscv64-support-and-zenithal-backported.patch
@@ -0,0 +1,30914 @@
+From: Zenithal <i@zenithal.me>
+
+Some changes would be rejected/ignored when patching firefox 92 src,
+so I commented out some blocks, see these #-leading block below
+
+From d994f662c2060f7a328d69603aff242ef4f679ff Mon Sep 17 00:00:00 2001
+From: Makoto Kato <m_kato@ga2.so-net.ne.jp>
+Date: Wed, 9 Jun 2021 10:04:18 +0000
+Subject: [PATCH] hashglobe fix
+
+---
+ servo/components/hashglobe/src/alloc.rs | 1 +
+ 1 file changed, 1 insertion(+)
+
+#diff --git a/servo/components/hashglobe/src/alloc.rs b/servo/components/hashglobe/src/alloc.rs
+#index b1c7a6eca5ee6..6c447107cce86 100644
+#--- a/servo/components/hashglobe/src/alloc.rs
+#+++ b/servo/components/hashglobe/src/alloc.rs
+#@@ -18,6 +18,7 @@ const MIN_ALIGN: usize = 8;
+#     target_arch = "x86_64",
+#     target_arch = "aarch64",
+#     target_arch = "mips64",
+#+    target_arch = "riscv64",
+#     target_arch = "s390x",
+#     target_arch = "sparc64"
+# )))]
+From c8fb2a3779d78de22ead674a5d435d64fa0a3ced Mon Sep 17 00:00:00 2001
+From: Makoto Kato <m_kato@ga2.so-net.ne.jp>
+Date: Wed, 9 Jun 2021 10:13:10 +0000
+Subject: [PATCH] Add ARCH_CPU_* for riscv
+
+---
+ ipc/chromium/src/build/build_config.h             |  6 ++++++
+ security/sandbox/chromium/build/build_config.h    | 12 ++++++++++++
+ third_party/libwebrtc/webrtc/build/build_config.h |  3 +++
+ 3 files changed, 21 insertions(+)
+
+diff --git a/ipc/chromium/src/build/build_config.h b/ipc/chromium/src/build/build_config.h
+index f57319089c3dd..7d0de63d6f6b9 100644
+--- a/ipc/chromium/src/build/build_config.h
++++ b/ipc/chromium/src/build/build_config.h
+@@ -122,6 +122,12 @@
+ #  define ARCH_CPU_ARM_FAMILY 1
+ #  define ARCH_CPU_ARM64 1
+ #  define ARCH_CPU_64_BITS 1
++#elif defined(__riscv) && __riscv_xlen == 32
++#  define ARCH_CPU_RISCV 1
++#  define ARCH_CPU_32_BITS 1
++#elif defined(__riscv) && __riscv_xlen == 64
++#  define ARCH_CPU_RISCV 1
++#  define ARCH_CPU_64_BITS 1
+ #else
+ #  error Please add support for your architecture in build/build_config.h
+ #endif
+#diff --git a/security/sandbox/chromium/build/build_config.h b/security/sandbox/chromium/build/build_config.h
+#index d3cdd2db4a697..ca9461e240d23 100644
+#--- a/security/sandbox/chromium/build/build_config.h
+#+++ b/security/sandbox/chromium/build/build_config.h
+#@@ -166,6 +166,18 @@
+# #define ARCH_CPU_32_BITS 1
+# #define ARCH_CPU_BIG_ENDIAN 1
+# #endif
+#+#elif defined(__riscv)
+#+#if __riscv_xlen == 32
+#+#  define ARCH_CPU_RISCV_FAMILY 1
+#+#  define ARCH_CPU_RISCV32 1
+#+#  define ARCH_CPU_32_BITS 1
+#+#  define ARCH_CPU_LITTLE_ENDIAN 1
+#+#elif __riscv_xlen == 64
+#+#  define ARCH_CPU_RISCV_FAMILY 1
+#+#  define ARCH_CPU_RISCV64 1
+#+#  define ARCH_CPU_64_BITS 1
+#+#  define ARCH_CPU_LITTLE_ENDIAN 1
+#+#endif
+# #else
+# #error Please add support for your architecture in build/build_config.h
+# #endif
+#diff --git a/third_party/libwebrtc/webrtc/build/build_config.h b/third_party/libwebrtc/webrtc/build/build_config.h
+#index 229d1f411cfd2..a9bc015c48fcd 100644
+#--- a/third_party/libwebrtc/webrtc/build/build_config.h
+#+++ b/third_party/libwebrtc/webrtc/build/build_config.h
+#@@ -171,6 +171,9 @@
+# #define ARCH_CPU_ARM_FAMILY 1
+# #define ARCH_CPU_ARM64 1
+# #define ARCH_CPU_64_BITS 1
+#+#elif defined(__riscv) && __riscv_xlen == 64
+#+#define ARCH_CPU_RISCV 1
+#+#define ARCH_CPU_64_BITS 1
+# #else
+# #error Please add support for your architecture in build/build_config.h
+# #endif
+From be9cbd86b4c121dbdb626f8c373fd809f25bc23e Mon Sep 17 00:00:00 2001
+From: Makoto Kato <m_kato@ga2.so-net.ne.jp>
+Date: Sun, 13 Jun 2021 04:06:39 +0000
+Subject: [PATCH] Update authenticator-rs
+
+---
+ .cargo/config.in                              |    5 +
+ Cargo.lock                                    |    3 +-
+ .../rust/authenticator/.cargo-checksum.json   |    2 +-
+ third_party/rust/authenticator/.clippy.toml   |    2 +
+ third_party/rust/authenticator/.flake8        |    4 +
+ .../authenticator/.pre-commit-config.yaml     |   42 +
+ third_party/rust/authenticator/.travis.yml    |   42 +
+ third_party/rust/authenticator/Cargo.lock     | 1603 -----------------
+ third_party/rust/authenticator/Cargo.toml     |  131 +-
+ third_party/rust/authenticator/build.rs       |    2 +
+ .../authenticator/src/linux/hidwrapper.rs     |    3 +
+ .../authenticator/src/linux/ioctl_riscv64.rs  |    5 +
+ toolkit/library/rust/shared/Cargo.toml        |    2 +-
+ 13 files changed, 154 insertions(+), 1692 deletions(-)
+ create mode 100644 third_party/rust/authenticator/.clippy.toml
+ create mode 100644 third_party/rust/authenticator/.flake8
+ create mode 100644 third_party/rust/authenticator/.pre-commit-config.yaml
+ create mode 100644 third_party/rust/authenticator/.travis.yml
+ delete mode 100644 third_party/rust/authenticator/Cargo.lock
+ create mode 100644 third_party/rust/authenticator/src/linux/ioctl_riscv64.rs
+
+diff --git a/.cargo/config.in b/.cargo/config.in
+index ad853d744f13a..e4a922765889b 100644
+--- a/.cargo/config.in
++++ b/.cargo/config.in
+@@ -47,6 +47,11 @@ git = "https://github.com/mozilla-spidermonkey/jsparagus"
+ replace-with = "vendored-sources"
+ rev = "d5d8c00ebd3281d12e0be5dfddbb69f791f836f1"
+ 
++[source."https://github.com/makotokato/authenticator-rs"]
++git = "https://github.com/makotokato/authenticator-rs"
++replace-with = "vendored-sources"
++rev = "eed8919d50559f4959e2d7d2af7b4d48869b5366"
++
+ [source."https://github.com/kvark/spirv_cross"]
+ branch = "wgpu5"
+ git = "https://github.com/kvark/spirv_cross"
+diff --git a/Cargo.lock b/Cargo.lock
+index 7e17939fad48b..8519d3d0e95a6 100644
+--- a/Cargo.lock
++++ b/Cargo.lock
+@@ -203,8 +203,7 @@ dependencies = [
+ [[package]]
+ name = "authenticator"
+ version = "0.3.1"
+-source = "registry+https://github.com/rust-lang/crates.io-index"
+-checksum = "08cee7a0952628fde958e149507c2bb321ab4fccfafd225da0b20adc956ef88a"
++source = "git+https://github.com/makotokato/authenticator-rs?rev=eed8919d50559f4959e2d7d2af7b4d48869b5366#eed8919d50559f4959e2d7d2af7b4d48869b5366"
+ dependencies = [
+  "bitflags",
+  "core-foundation",
+diff --git a/third_party/rust/authenticator/.cargo-checksum.json b/third_party/rust/authenticator/.cargo-checksum.json
+index ce451ad09df4f..9791345c9de54 100644
+--- a/third_party/rust/authenticator/.cargo-checksum.json
++++ b/third_party/rust/authenticator/.cargo-checksum.json
+@@ -1 +1 @@
+-{"files":{"Cargo.lock":"abaed4932db2206e5fdb7cb73a8c100f6c91fc84a8f33e8763677040ae8ea9bf","Cargo.toml":"9b56d5495021e7cd8ab7e019cceda45e906a2a3629a68e9019c6e5cb682dbc43","Cross.toml":"8d132da818d48492aa9f4b78a348f0df3adfae45d988d42ebd6be8a5adadb6c3","LICENSE":"e866c8f5864d4cacfe403820e722e9dc03fe3c7565efa5e4dad9051d827bb92a","README.md":"c87d9c7cc44f1dd4ef861a3a9f8cd2eb68aedd3814768871f5fb63c2070806cd","build.rs":"bc308b771ae9741d775370e3efe45e9cca166fd1d0335f4214b00497042ccc55","examples/main.rs":"d899646fa396776d0bb66efb86099ffb195566ecdb6fc4c1765ae3d54d696a8d","rustfmt.toml":"ceb6615363d6fff16426eb56f5727f98a7f7ed459ba9af735b1d8b672e2c3b9b","src/authenticatorservice.rs":"9fc5bcdd1e4f32e58ae920f96f40619a870b0a1b8d05db650803b2402a37fbf9","src/capi.rs":"1d3145ce81293bec697b0d385357fb1b0b495b0c356e2da5e6f15d028d328c70","src/consts.rs":"3dbcdfced6241822062e1aa2e6c8628af5f539ea18ee41edab51a3d33ebb77c6","src/errors.rs":"de89e57435ed1f9ff10f1f2d997a5b29d61cb215551e0ab40861a08ca52d1447","src/freebsd/device.rs":"595df4b3f66b90dd73f8df67e1a2ba9a20c0b5fd893afbadbec564aa34f89981","src/freebsd/mod.rs":"42dcb57fbeb00140003a8ad39acac9b547062b8f281a3fa5deb5f92a6169dde6","src/freebsd/monitor.rs":"c10b154632fbedc3dca27197f7fc890c3d50ac1744b927e9f1e44a9e8a13506e","src/freebsd/transaction.rs":"bfb92dcf2edeb5d620a019907fff1025eb36ef322055e78649a3055b074fa851","src/freebsd/uhid.rs":"84f564d337637c1cd107ccc536b8fce2230628e144e4031e8db4d7163c9c0cb3","src/hidproto.rs":"362fc8e24b94ba431aad5ee0002f5a3364badd937c706c0ae119a5a7a2abc7c2","src/lib.rs":"12f62285a3d33347f95236b71341462a76ea1ded67651fc96ba25d7bd1dd8298","src/linux/device.rs":"d27c5f877cf96b97668579ac5db0f2685f7c969e7a5d0ddc68043eb16bfcddb8","src/linux/hidraw.rs":"ed55caa40fd518d67bb67d5af08f9adcab34f89e0ca591142d45b87f172926dd","src/linux/hidwrapper.h":"72785db3a9b27ea72b6cf13a958fee032af54304522d002f56322473978a20f9","src/linux/hidwrapper.rs":"4be65676cf3220929700bf4906938dcbd1538ba53d40c60b08f9ba8890c910f6","src/linux/ioctl_aarch64le.rs":"2d8b265cd39a9f46816f83d5a5df0701c13eb842bc609325bad42ce50add3bf0","src/linux/ioctl_armle.rs":"2d8b265cd39a9f46816f83d5a5df0701c13eb842bc609325bad42ce50add3bf0","src/linux/ioctl_mips64le.rs":"fbda309934ad8bda689cd4fb5c0ca696fe26dedb493fe9d5a5322c3047d474fd","src/linux/ioctl_mipsbe.rs":"fbda309934ad8bda689cd4fb5c0ca696fe26dedb493fe9d5a5322c3047d474fd","src/linux/ioctl_mipsle.rs":"fbda309934ad8bda689cd4fb5c0ca696fe26dedb493fe9d5a5322c3047d474fd","src/linux/ioctl_powerpc64be.rs":"fbda309934ad8bda689cd4fb5c0ca696fe26dedb493fe9d5a5322c3047d474fd","src/linux/ioctl_powerpc64le.rs":"fbda309934ad8bda689cd4fb5c0ca696fe26dedb493fe9d5a5322c3047d474fd","src/linux/ioctl_powerpcbe.rs":"fbda309934ad8bda689cd4fb5c0ca696fe26dedb493fe9d5a5322c3047d474fd","src/linux/ioctl_s390xbe.rs":"2d8b265cd39a9f46816f83d5a5df0701c13eb842bc609325bad42ce50add3bf0","src/linux/ioctl_x86.rs":"2d8b265cd39a9f46816f83d5a5df0701c13eb842bc609325bad42ce50add3bf0","src/linux/ioctl_x86_64.rs":"2d8b265cd39a9f46816f83d5a5df0701c13eb842bc609325bad42ce50add3bf0","src/linux/mod.rs":"446e435126d2a58f167f648dd95cba28e8ac9c17f1f799e1eaeab80ea800fc57","src/linux/monitor.rs":"9ef4e22fdcf005dd5201b42595d958ea462998c75dbfc68c8a403e7be64328e4","src/linux/transaction.rs":"bfb92dcf2edeb5d620a019907fff1025eb36ef322055e78649a3055b074fa851","src/macos/device.rs":"cc97b773254a89526164987e4b8e4181910fc3decb32acf51ca86c596ad0147b","src/macos/iokit.rs":"7dc4e7bbf8e42e2fcde0cee8e48d14d6234a5a910bd5d3c4e966d8ba6b73992f","src/macos/mod.rs":"333e561554fc901d4f6092f6e4c85823e2b0c4ff31c9188d0e6d542b71a0a07c","src/macos/monitor.rs":"d059861b4739c9272fa305b6dd91ebeb08530bd0e70a013dd999565d6f06fb30","src/macos/transaction.rs":"935b4bc79b0e50a984604a1ada96a7ef723cc283b7d33ca07f3150b1752b99f7","src/manager.rs":"5a4cdc26b9fde20e1a3dc2389f15d38d9153109bfee5119c092fbfdbd19bad8d","src/netbsd/device.rs":"3a99a989a7a8411ddb9893c371644076662a3b488d40b436601c27fd92fdf159","src/netbsd/fd.rs":"260f1a8ae04896c0eb35ab0914e11ca9291e7317a086c94328aa219c0e1fc1d2","src/netbsd/mod.rs":"b1c52aa29537330cebe67427062d6c94871cab2a9b0c04b2305d686f07e88fd5","src/netbsd/monitor.rs":"dfd68e026c52271b68a3a9263837c793127e9d54ed19b748ef6d13ab4c44e09a","src/netbsd/transaction.rs":"9334a832a57e717a981c13c364ed4ee80ce9798460fc6c8954723d2fcf20585a","src/netbsd/uhid.rs":"154a4587767f151e3f846cc0b79f615d5137de67afed84f19176f27ac9097908","src/openbsd/device.rs":"ae1c8de90bb515a12d571372a30322fadb5122bc69ab71caf154452caa8a644f","src/openbsd/mod.rs":"514274d414042ff84b3667a41a736e78581e22fda87ccc97c2bc05617e381a30","src/openbsd/monitor.rs":"5eb071dd3719ea305eac21ec20596463f63790f8cd1f908a59e3f9cb0b71b5ad","src/openbsd/transaction.rs":"2380c9430f4c95a1fefaaab729d8ece0d149674708d705a71dd5d2513d9e1a4c","src/statecallback.rs":"6b16f97176db1ae3fc3851fe8394e4ffc324bc6fe59313845ac3a88132fd52f1","src/statemachine.rs":"27e2655411ebc1077c200f0aa2ba429ca656fc7dd6f90e08b51492b59ec72e61","src/stub/device.rs":"5e378147e113e20160a45d395b717bd3deecb327247c24b6735035f7d50861b7","src/stub/mod.rs":"6a7fec504a52d403b0241b18cd8b95088a31807571f4c0a67e4055afc74f4453","src/stub/transaction.rs":"4a2ccb2d72070a8bc61442254e063278c68212d5565ba5bfe4d47cacebf5bd1c","src/u2fhid-capi.h":"10f2658df774bb7f7f197a9f217b9e20d67b232b60a554e8ee3c3f71480ea1f6","src/u2fprotocol.rs":"72120773a948ffd667b5976c26ae27a4327769d97b0eef7a3b1e6b2b4bbb46a9","src/u2ftypes.rs":"a02d2c29790c5edfec9af320b1d4bcb93be0bbf02b881fa5aa403cfb687a25ae","src/util.rs":"d2042b2db4864f2b1192606c3251709361de7fb7521e1519190ef26a77de8e64","src/virtualdevices/mod.rs":"2c7df7691d5c150757304241351612aed4260d65b70ab0f483edbc1a5cfb5674","src/virtualdevices/software_u2f.rs":"1b86b94c6eadec6a22dffdd2b003c5324247c6412eeddb28a6094feb1c523f8e","src/virtualdevices/webdriver/mod.rs":"4a36e6dfa9f45f941d863b4039bfbcfa8eaca660bd6ed78aeb1a2962db64be5a","src/virtualdevices/webdriver/testtoken.rs":"7146e02f1a5dad2c8827dd11c12ee408c0e42a0706ac65f139998feffd42570f","src/virtualdevices/webdriver/virtualmanager.rs":"a55a28995c81b5affb0a74207b6dd556d272086a554676df2e675fe991d730a9","src/virtualdevices/webdriver/web_api.rs":"27206ee09c83fe25b34cad62174e42383defd8c8a5e917d30691412aacdae08f","src/windows/device.rs":"bc3f9587677c185a624c0aae7537baf9f780484ab8337929db994800b9064ba9","src/windows/mod.rs":"218e7f2fe91ecb390c12bba5a5ffdad2c1f0b22861c937f4d386262e5b3dd617","src/windows/monitor.rs":"3804dc67de46a1a6b7925c83e0df95d94ddfa1aa53a88fc845f4ff26aede57f8","src/windows/transaction.rs":"ee639f28b2dcdb7e00c922d8762fe6aa33def8c7aaeb46ec93e3a772407a9d86","src/windows/winapi.rs":"de92afb17df26216161138f18eb3b9162f3fb2cdeb74aa78173afe804ba02e00","testing/cross/powerpc64le-unknown-linux-gnu.Dockerfile":"d7463ff4376e3e0ca3fed879fab4aa975c4c0a3e7924c5b88aef9381a5d013de","testing/cross/x86_64-unknown-linux-gnu.Dockerfile":"11c79c04b07a171b0c9b63ef75fa75f33263ce76e3c1eda0879a3e723ebd0c24","testing/run_cross.sh":"cc2a7e0359f210eba2e7121f81eb8ab0125cea6e0d0f2698177b0fe2ad0c33d8","webdriver-tools/requirements.txt":"8236aa3dedad886f213c9b778fec80b037212d30e640b458984110211d546005","webdriver-tools/webdriver-driver.py":"82327c26ba271d1689acc87b612ab8436cb5475f0a3c0dba7baa06e7f6f5e19c"},"package":"08cee7a0952628fde958e149507c2bb321ab4fccfafd225da0b20adc956ef88a"}
+\ No newline at end of file
++{"files":{".clippy.toml":"86011295a6e2cea043b8002238f9c96b39f17aa8241aa079f44bb6e71eb62421",".flake8":"04f55f4a3c02b50dfa568ce4f7c6a47a9374b6483256811f8be702d1382576cd",".pre-commit-config.yaml":"b7920a17d5a378c7702f9c39bf5156bb8c4ea15d8691217e0a5a8e8f571b4cf7",".travis.yml":"883be088379477e7fa6f3d06b1c8d59dc41da61b6c15d2675c62113341e7b2d5","Cargo.toml":"e7334212220a6d8ca01996888275cc0d11d098e36db1bf4c5b7429051897bf3f","Cross.toml":"8d132da818d48492aa9f4b78a348f0df3adfae45d988d42ebd6be8a5adadb6c3","LICENSE":"e866c8f5864d4cacfe403820e722e9dc03fe3c7565efa5e4dad9051d827bb92a","README.md":"c87d9c7cc44f1dd4ef861a3a9f8cd2eb68aedd3814768871f5fb63c2070806cd","build.rs":"a459ee1ace052f9692817b15c702cb6e5a6dac7c7dfe74fa075662dbcf808dbe","examples/main.rs":"d899646fa396776d0bb66efb86099ffb195566ecdb6fc4c1765ae3d54d696a8d","rustfmt.toml":"ceb6615363d6fff16426eb56f5727f98a7f7ed459ba9af735b1d8b672e2c3b9b","src/authenticatorservice.rs":"9fc5bcdd1e4f32e58ae920f96f40619a870b0a1b8d05db650803b2402a37fbf9","src/capi.rs":"1d3145ce81293bec697b0d385357fb1b0b495b0c356e2da5e6f15d028d328c70","src/consts.rs":"3dbcdfced6241822062e1aa2e6c8628af5f539ea18ee41edab51a3d33ebb77c6","src/errors.rs":"de89e57435ed1f9ff10f1f2d997a5b29d61cb215551e0ab40861a08ca52d1447","src/freebsd/device.rs":"595df4b3f66b90dd73f8df67e1a2ba9a20c0b5fd893afbadbec564aa34f89981","src/freebsd/mod.rs":"42dcb57fbeb00140003a8ad39acac9b547062b8f281a3fa5deb5f92a6169dde6","src/freebsd/monitor.rs":"c10b154632fbedc3dca27197f7fc890c3d50ac1744b927e9f1e44a9e8a13506e","src/freebsd/transaction.rs":"bfb92dcf2edeb5d620a019907fff1025eb36ef322055e78649a3055b074fa851","src/freebsd/uhid.rs":"84f564d337637c1cd107ccc536b8fce2230628e144e4031e8db4d7163c9c0cb3","src/hidproto.rs":"362fc8e24b94ba431aad5ee0002f5a3364badd937c706c0ae119a5a7a2abc7c2","src/lib.rs":"12f62285a3d33347f95236b71341462a76ea1ded67651fc96ba25d7bd1dd8298","src/linux/device.rs":"d27c5f877cf96b97668579ac5db0f2685f7c969e7a5d0ddc68043eb16bfcddb8","src/linux/hidraw.rs":"ed55caa40fd518d67bb67d5af08f9adcab34f89e0ca591142d45b87f172926dd","src/linux/hidwrapper.h":"72785db3a9b27ea72b6cf13a958fee032af54304522d002f56322473978a20f9","src/linux/hidwrapper.rs":"753c7459dbb73befdd186b6269ac33f7a4537b4c935928f50f2b2131756e787d","src/linux/ioctl_aarch64le.rs":"2d8b265cd39a9f46816f83d5a5df0701c13eb842bc609325bad42ce50add3bf0","src/linux/ioctl_armle.rs":"2d8b265cd39a9f46816f83d5a5df0701c13eb842bc609325bad42ce50add3bf0","src/linux/ioctl_mips64le.rs":"fbda309934ad8bda689cd4fb5c0ca696fe26dedb493fe9d5a5322c3047d474fd","src/linux/ioctl_mipsbe.rs":"fbda309934ad8bda689cd4fb5c0ca696fe26dedb493fe9d5a5322c3047d474fd","src/linux/ioctl_mipsle.rs":"fbda309934ad8bda689cd4fb5c0ca696fe26dedb493fe9d5a5322c3047d474fd","src/linux/ioctl_powerpc64be.rs":"fbda309934ad8bda689cd4fb5c0ca696fe26dedb493fe9d5a5322c3047d474fd","src/linux/ioctl_powerpc64le.rs":"fbda309934ad8bda689cd4fb5c0ca696fe26dedb493fe9d5a5322c3047d474fd","src/linux/ioctl_powerpcbe.rs":"fbda309934ad8bda689cd4fb5c0ca696fe26dedb493fe9d5a5322c3047d474fd","src/linux/ioctl_riscv64.rs":"2d8b265cd39a9f46816f83d5a5df0701c13eb842bc609325bad42ce50add3bf0","src/linux/ioctl_s390xbe.rs":"2d8b265cd39a9f46816f83d5a5df0701c13eb842bc609325bad42ce50add3bf0","src/linux/ioctl_x86.rs":"2d8b265cd39a9f46816f83d5a5df0701c13eb842bc609325bad42ce50add3bf0","src/linux/ioctl_x86_64.rs":"2d8b265cd39a9f46816f83d5a5df0701c13eb842bc609325bad42ce50add3bf0","src/linux/mod.rs":"446e435126d2a58f167f648dd95cba28e8ac9c17f1f799e1eaeab80ea800fc57","src/linux/monitor.rs":"9ef4e22fdcf005dd5201b42595d958ea462998c75dbfc68c8a403e7be64328e4","src/linux/transaction.rs":"bfb92dcf2edeb5d620a019907fff1025eb36ef322055e78649a3055b074fa851","src/macos/device.rs":"cc97b773254a89526164987e4b8e4181910fc3decb32acf51ca86c596ad0147b","src/macos/iokit.rs":"7dc4e7bbf8e42e2fcde0cee8e48d14d6234a5a910bd5d3c4e966d8ba6b73992f","src/macos/mod.rs":"333e561554fc901d4f6092f6e4c85823e2b0c4ff31c9188d0e6d542b71a0a07c","src/macos/monitor.rs":"d059861b4739c9272fa305b6dd91ebeb08530bd0e70a013dd999565d6f06fb30","src/macos/transaction.rs":"935b4bc79b0e50a984604a1ada96a7ef723cc283b7d33ca07f3150b1752b99f7","src/manager.rs":"5a4cdc26b9fde20e1a3dc2389f15d38d9153109bfee5119c092fbfdbd19bad8d","src/netbsd/device.rs":"3a99a989a7a8411ddb9893c371644076662a3b488d40b436601c27fd92fdf159","src/netbsd/fd.rs":"260f1a8ae04896c0eb35ab0914e11ca9291e7317a086c94328aa219c0e1fc1d2","src/netbsd/mod.rs":"b1c52aa29537330cebe67427062d6c94871cab2a9b0c04b2305d686f07e88fd5","src/netbsd/monitor.rs":"dfd68e026c52271b68a3a9263837c793127e9d54ed19b748ef6d13ab4c44e09a","src/netbsd/transaction.rs":"9334a832a57e717a981c13c364ed4ee80ce9798460fc6c8954723d2fcf20585a","src/netbsd/uhid.rs":"154a4587767f151e3f846cc0b79f615d5137de67afed84f19176f27ac9097908","src/openbsd/device.rs":"ae1c8de90bb515a12d571372a30322fadb5122bc69ab71caf154452caa8a644f","src/openbsd/mod.rs":"514274d414042ff84b3667a41a736e78581e22fda87ccc97c2bc05617e381a30","src/openbsd/monitor.rs":"5eb071dd3719ea305eac21ec20596463f63790f8cd1f908a59e3f9cb0b71b5ad","src/openbsd/transaction.rs":"2380c9430f4c95a1fefaaab729d8ece0d149674708d705a71dd5d2513d9e1a4c","src/statecallback.rs":"6b16f97176db1ae3fc3851fe8394e4ffc324bc6fe59313845ac3a88132fd52f1","src/statemachine.rs":"27e2655411ebc1077c200f0aa2ba429ca656fc7dd6f90e08b51492b59ec72e61","src/stub/device.rs":"5e378147e113e20160a45d395b717bd3deecb327247c24b6735035f7d50861b7","src/stub/mod.rs":"6a7fec504a52d403b0241b18cd8b95088a31807571f4c0a67e4055afc74f4453","src/stub/transaction.rs":"4a2ccb2d72070a8bc61442254e063278c68212d5565ba5bfe4d47cacebf5bd1c","src/u2fhid-capi.h":"10f2658df774bb7f7f197a9f217b9e20d67b232b60a554e8ee3c3f71480ea1f6","src/u2fprotocol.rs":"72120773a948ffd667b5976c26ae27a4327769d97b0eef7a3b1e6b2b4bbb46a9","src/u2ftypes.rs":"a02d2c29790c5edfec9af320b1d4bcb93be0bbf02b881fa5aa403cfb687a25ae","src/util.rs":"d2042b2db4864f2b1192606c3251709361de7fb7521e1519190ef26a77de8e64","src/virtualdevices/mod.rs":"2c7df7691d5c150757304241351612aed4260d65b70ab0f483edbc1a5cfb5674","src/virtualdevices/software_u2f.rs":"1b86b94c6eadec6a22dffdd2b003c5324247c6412eeddb28a6094feb1c523f8e","src/virtualdevices/webdriver/mod.rs":"4a36e6dfa9f45f941d863b4039bfbcfa8eaca660bd6ed78aeb1a2962db64be5a","src/virtualdevices/webdriver/testtoken.rs":"7146e02f1a5dad2c8827dd11c12ee408c0e42a0706ac65f139998feffd42570f","src/virtualdevices/webdriver/virtualmanager.rs":"a55a28995c81b5affb0a74207b6dd556d272086a554676df2e675fe991d730a9","src/virtualdevices/webdriver/web_api.rs":"27206ee09c83fe25b34cad62174e42383defd8c8a5e917d30691412aacdae08f","src/windows/device.rs":"bc3f9587677c185a624c0aae7537baf9f780484ab8337929db994800b9064ba9","src/windows/mod.rs":"218e7f2fe91ecb390c12bba5a5ffdad2c1f0b22861c937f4d386262e5b3dd617","src/windows/monitor.rs":"3804dc67de46a1a6b7925c83e0df95d94ddfa1aa53a88fc845f4ff26aede57f8","src/windows/transaction.rs":"ee639f28b2dcdb7e00c922d8762fe6aa33def8c7aaeb46ec93e3a772407a9d86","src/windows/winapi.rs":"de92afb17df26216161138f18eb3b9162f3fb2cdeb74aa78173afe804ba02e00","testing/cross/powerpc64le-unknown-linux-gnu.Dockerfile":"d7463ff4376e3e0ca3fed879fab4aa975c4c0a3e7924c5b88aef9381a5d013de","testing/cross/x86_64-unknown-linux-gnu.Dockerfile":"11c79c04b07a171b0c9b63ef75fa75f33263ce76e3c1eda0879a3e723ebd0c24","testing/run_cross.sh":"cc2a7e0359f210eba2e7121f81eb8ab0125cea6e0d0f2698177b0fe2ad0c33d8","webdriver-tools/requirements.txt":"8236aa3dedad886f213c9b778fec80b037212d30e640b458984110211d546005","webdriver-tools/webdriver-driver.py":"82327c26ba271d1689acc87b612ab8436cb5475f0a3c0dba7baa06e7f6f5e19c"},"package":null}
+\ No newline at end of file
+diff --git a/third_party/rust/authenticator/.clippy.toml b/third_party/rust/authenticator/.clippy.toml
+new file mode 100644
+index 0000000000000..844d0757e91f4
+--- /dev/null
++++ b/third_party/rust/authenticator/.clippy.toml
+@@ -0,0 +1,2 @@
++type-complexity-threshold = 384
++too-many-arguments-threshold = 8
+diff --git a/third_party/rust/authenticator/.flake8 b/third_party/rust/authenticator/.flake8
+new file mode 100644
+index 0000000000000..5a725c9b4ce65
+--- /dev/null
++++ b/third_party/rust/authenticator/.flake8
+@@ -0,0 +1,4 @@
++[flake8]
++# See http://pep8.readthedocs.io/en/latest/intro.html#configuration
++ignore = E121, E123, E126, E129, E133, E203, E226, E241, E242, E704, W503, E402, E741
++max-line-length = 99
+diff --git a/third_party/rust/authenticator/.pre-commit-config.yaml b/third_party/rust/authenticator/.pre-commit-config.yaml
+new file mode 100644
+index 0000000000000..e0ceb8ea5473c
+--- /dev/null
++++ b/third_party/rust/authenticator/.pre-commit-config.yaml
+@@ -0,0 +1,42 @@
++- repo: git://github.com/pre-commit/pre-commit-hooks
++  rev: HEAD
++  hooks:
++    - id: flake8
++    - id: check-ast
++    - id: detect-private-key
++    - id: detect-aws-credentials
++    - id: check-merge-conflict
++    - id: end-of-file-fixer
++    - id: requirements-txt-fixer
++    - id: trailing-whitespace
++- repo: local
++  hooks:
++    - id: rustfmt
++      name: Check rustfmt
++      language: system
++      entry: cargo fmt -- --check
++      pass_filenames: false
++      files: '.rs$'
++- repo: local
++  hooks:
++    - id: tests
++      name: Run tests
++      language: system
++      entry: cargo test --all-targets --all-features
++      pass_filenames: false
++      files: '.rs$'
++- repo: local
++  hooks:
++    - id: clippy
++      name: Check clippy
++      language: system
++      entry: cargo clippy --all-targets -- -A renamed_and_removed_lints -A clippy::new-ret-no-self -D warnings
++      pass_filenames: false
++      files: '.rs$'
++- repo: local
++  hooks:
++    - id: black
++      name: Check black
++      language: system
++      entry: black
++      files: '.py$'
+diff --git a/third_party/rust/authenticator/.travis.yml b/third_party/rust/authenticator/.travis.yml
+new file mode 100644
+index 0000000000000..70ea5c5581af2
+--- /dev/null
++++ b/third_party/rust/authenticator/.travis.yml
+@@ -0,0 +1,42 @@
++os:
++  - linux
++  - windows
++
++language: rust
++rust:
++  - stable
++  - nightly
++cache: cargo
++
++jobs:
++  allow_failures:
++    - rust: nightly
++
++addons:
++  apt:
++    packages:
++      - build-essential
++      - libudev-dev
++
++install:
++  - rustup component add rustfmt
++  - rustup component add clippy
++
++script:
++- |
++  if [ "$TRAVIS_RUST_VERSION" == "nightly" ] && [ "$TRAVIS_OS_NAME" == "linux" ] ; then
++    export ASAN_OPTIONS="detect_odr_violation=1:leak_check_at_exit=0:detect_leaks=0"
++    export RUSTFLAGS="-Z sanitizer=address"
++  fi
++- |
++  if [ "$TRAVIS_RUST_VERSION" == "stable" ] && [ "$TRAVIS_OS_NAME" == "linux" ] ; then
++    echo "Running rustfmt"
++    cargo fmt --all -- --check
++    echo "Running clippy"
++    cargo clippy --all-targets --all-features -- -A renamed_and_removed_lints -A clippy::new-ret-no-self -D warnings
++
++    rustup install nightly
++    cargo install cargo-fuzz
++    cargo +nightly fuzz build
++  fi
++- cargo test --all-targets --all-features
+diff --git a/third_party/rust/authenticator/Cargo.lock b/third_party/rust/authenticator/Cargo.lock
+deleted file mode 100644
+index 9f284b468deaa..0000000000000
+--- a/third_party/rust/authenticator/Cargo.lock
++++ /dev/null
+@@ -1,1603 +0,0 @@
+-# This file is automatically @generated by Cargo.
+-# It is not intended for manual editing.
+-[[package]]
+-name = "aho-corasick"
+-version = "0.7.13"
+-source = "registry+https://github.com/rust-lang/crates.io-index"
+-dependencies = [
+- "memchr 2.3.3 (registry+https://github.com/rust-lang/crates.io-index)",
+-]
+-
+-[[package]]
+-name = "ansi_term"
+-version = "0.11.0"
+-source = "registry+https://github.com/rust-lang/crates.io-index"
+-dependencies = [
+- "winapi 0.3.9 (registry+https://github.com/rust-lang/crates.io-index)",
+-]
+-
+-[[package]]
+-name = "assert_matches"
+-version = "1.3.0"
+-source = "registry+https://github.com/rust-lang/crates.io-index"
+-
+-[[package]]
+-name = "atty"
+-version = "0.2.14"
+-source = "registry+https://github.com/rust-lang/crates.io-index"
+-dependencies = [
+- "hermit-abi 0.1.15 (registry+https://github.com/rust-lang/crates.io-index)",
+- "libc 0.2.73 (registry+https://github.com/rust-lang/crates.io-index)",
+- "winapi 0.3.9 (registry+https://github.com/rust-lang/crates.io-index)",
+-]
+-
+-[[package]]
+-name = "authenticator"
+-version = "0.3.1"
+-dependencies = [
+- "assert_matches 1.3.0 (registry+https://github.com/rust-lang/crates.io-index)",
+- "base64 0.10.1 (registry+https://github.com/rust-lang/crates.io-index)",
+- "bindgen 0.51.1 (registry+https://github.com/rust-lang/crates.io-index)",
+- "bitflags 1.2.1 (registry+https://github.com/rust-lang/crates.io-index)",
+- "bytes 0.5.6 (registry+https://github.com/rust-lang/crates.io-index)",
+- "core-foundation 0.9.0 (registry+https://github.com/rust-lang/crates.io-index)",
+- "devd-rs 0.3.1 (registry+https://github.com/rust-lang/crates.io-index)",
+- "env_logger 0.6.2 (registry+https://github.com/rust-lang/crates.io-index)",
+- "getopts 0.2.21 (registry+https://github.com/rust-lang/crates.io-index)",
+- "libc 0.2.73 (registry+https://github.com/rust-lang/crates.io-index)",
+- "libudev 0.2.0 (registry+https://github.com/rust-lang/crates.io-index)",
+- "log 0.4.11 (registry+https://github.com/rust-lang/crates.io-index)",
+- "rand 0.7.3 (registry+https://github.com/rust-lang/crates.io-index)",
+- "runloop 0.1.0 (registry+https://github.com/rust-lang/crates.io-index)",
+- "serde 1.0.116 (registry+https://github.com/rust-lang/crates.io-index)",
+- "serde_json 1.0.57 (registry+https://github.com/rust-lang/crates.io-index)",
+- "sha2 0.8.2 (registry+https://github.com/rust-lang/crates.io-index)",
+- "tokio 0.2.22 (registry+https://github.com/rust-lang/crates.io-index)",
+- "warp 0.2.5 (registry+https://github.com/rust-lang/crates.io-index)",
+- "winapi 0.3.9 (registry+https://github.com/rust-lang/crates.io-index)",
+-]
+-
+-[[package]]
+-name = "autocfg"
+-version = "0.1.7"
+-source = "registry+https://github.com/rust-lang/crates.io-index"
+-
+-[[package]]
+-name = "autocfg"
+-version = "1.0.1"
+-source = "registry+https://github.com/rust-lang/crates.io-index"
+-
+-[[package]]
+-name = "base64"
+-version = "0.10.1"
+-source = "registry+https://github.com/rust-lang/crates.io-index"
+-dependencies = [
+- "byteorder 1.3.4 (registry+https://github.com/rust-lang/crates.io-index)",
+-]
+-
+-[[package]]
+-name = "base64"
+-version = "0.12.3"
+-source = "registry+https://github.com/rust-lang/crates.io-index"
+-
+-[[package]]
+-name = "bindgen"
+-version = "0.51.1"
+-source = "registry+https://github.com/rust-lang/crates.io-index"
+-dependencies = [
+- "bitflags 1.2.1 (registry+https://github.com/rust-lang/crates.io-index)",
+- "cexpr 0.3.6 (registry+https://github.com/rust-lang/crates.io-index)",
+- "cfg-if 0.1.10 (registry+https://github.com/rust-lang/crates.io-index)",
+- "clang-sys 0.28.1 (registry+https://github.com/rust-lang/crates.io-index)",
+- "clap 2.33.1 (registry+https://github.com/rust-lang/crates.io-index)",
+- "env_logger 0.6.2 (registry+https://github.com/rust-lang/crates.io-index)",
+- "lazy_static 1.4.0 (registry+https://github.com/rust-lang/crates.io-index)",
+- "log 0.4.11 (registry+https://github.com/rust-lang/crates.io-index)",
+- "peeking_take_while 0.1.2 (registry+https://github.com/rust-lang/crates.io-index)",
+- "proc-macro2 1.0.19 (registry+https://github.com/rust-lang/crates.io-index)",
+- "quote 1.0.7 (registry+https://github.com/rust-lang/crates.io-index)",
+- "regex 1.3.9 (registry+https://github.com/rust-lang/crates.io-index)",
+- "rustc-hash 1.1.0 (registry+https://github.com/rust-lang/crates.io-index)",
+- "shlex 0.1.1 (registry+https://github.com/rust-lang/crates.io-index)",
+- "which 3.1.1 (registry+https://github.com/rust-lang/crates.io-index)",
+-]
+-
+-[[package]]
+-name = "bitflags"
+-version = "1.2.1"
+-source = "registry+https://github.com/rust-lang/crates.io-index"
+-
+-[[package]]
+-name = "block-buffer"
+-version = "0.7.3"
+-source = "registry+https://github.com/rust-lang/crates.io-index"
+-dependencies = [
+- "block-padding 0.1.5 (registry+https://github.com/rust-lang/crates.io-index)",
+- "byte-tools 0.3.1 (registry+https://github.com/rust-lang/crates.io-index)",
+- "byteorder 1.3.4 (registry+https://github.com/rust-lang/crates.io-index)",
+- "generic-array 0.12.3 (registry+https://github.com/rust-lang/crates.io-index)",
+-]
+-
+-[[package]]
+-name = "block-buffer"
+-version = "0.9.0"
+-source = "registry+https://github.com/rust-lang/crates.io-index"
+-dependencies = [
+- "generic-array 0.14.4 (registry+https://github.com/rust-lang/crates.io-index)",
+-]
+-
+-[[package]]
+-name = "block-padding"
+-version = "0.1.5"
+-source = "registry+https://github.com/rust-lang/crates.io-index"
+-dependencies = [
+- "byte-tools 0.3.1 (registry+https://github.com/rust-lang/crates.io-index)",
+-]
+-
+-[[package]]
+-name = "buf_redux"
+-version = "0.8.4"
+-source = "registry+https://github.com/rust-lang/crates.io-index"
+-dependencies = [
+- "memchr 2.3.3 (registry+https://github.com/rust-lang/crates.io-index)",
+- "safemem 0.3.3 (registry+https://github.com/rust-lang/crates.io-index)",
+-]
+-
+-[[package]]
+-name = "byte-tools"
+-version = "0.3.1"
+-source = "registry+https://github.com/rust-lang/crates.io-index"
+-
+-[[package]]
+-name = "byteorder"
+-version = "1.3.4"
+-source = "registry+https://github.com/rust-lang/crates.io-index"
+-
+-[[package]]
+-name = "bytes"
+-version = "0.5.6"
+-source = "registry+https://github.com/rust-lang/crates.io-index"
+-dependencies = [
+- "serde 1.0.116 (registry+https://github.com/rust-lang/crates.io-index)",
+-]
+-
+-[[package]]
+-name = "cc"
+-version = "1.0.58"
+-source = "registry+https://github.com/rust-lang/crates.io-index"
+-
+-[[package]]
+-name = "cexpr"
+-version = "0.3.6"
+-source = "registry+https://github.com/rust-lang/crates.io-index"
+-dependencies = [
+- "nom 4.2.3 (registry+https://github.com/rust-lang/crates.io-index)",
+-]
+-
+-[[package]]
+-name = "cfg-if"
+-version = "0.1.10"
+-source = "registry+https://github.com/rust-lang/crates.io-index"
+-
+-[[package]]
+-name = "clang-sys"
+-version = "0.28.1"
+-source = "registry+https://github.com/rust-lang/crates.io-index"
+-dependencies = [
+- "glob 0.3.0 (registry+https://github.com/rust-lang/crates.io-index)",
+- "libc 0.2.73 (registry+https://github.com/rust-lang/crates.io-index)",
+- "libloading 0.5.2 (registry+https://github.com/rust-lang/crates.io-index)",
+-]
+-
+-[[package]]
+-name = "clap"
+-version = "2.33.1"
+-source = "registry+https://github.com/rust-lang/crates.io-index"
+-dependencies = [
+- "ansi_term 0.11.0 (registry+https://github.com/rust-lang/crates.io-index)",
+- "atty 0.2.14 (registry+https://github.com/rust-lang/crates.io-index)",
+- "bitflags 1.2.1 (registry+https://github.com/rust-lang/crates.io-index)",
+- "strsim 0.8.0 (registry+https://github.com/rust-lang/crates.io-index)",
+- "textwrap 0.11.0 (registry+https://github.com/rust-lang/crates.io-index)",
+- "unicode-width 0.1.8 (registry+https://github.com/rust-lang/crates.io-index)",
+- "vec_map 0.8.2 (registry+https://github.com/rust-lang/crates.io-index)",
+-]
+-
+-[[package]]
+-name = "cloudabi"
+-version = "0.0.3"
+-source = "registry+https://github.com/rust-lang/crates.io-index"
+-dependencies = [
+- "bitflags 1.2.1 (registry+https://github.com/rust-lang/crates.io-index)",
+-]
+-
+-[[package]]
+-name = "core-foundation"
+-version = "0.9.0"
+-source = "registry+https://github.com/rust-lang/crates.io-index"
+-dependencies = [
+- "core-foundation-sys 0.8.0 (registry+https://github.com/rust-lang/crates.io-index)",
+- "libc 0.2.73 (registry+https://github.com/rust-lang/crates.io-index)",
+-]
+-
+-[[package]]
+-name = "core-foundation-sys"
+-version = "0.8.0"
+-source = "registry+https://github.com/rust-lang/crates.io-index"
+-
+-[[package]]
+-name = "cpuid-bool"
+-version = "0.1.2"
+-source = "registry+https://github.com/rust-lang/crates.io-index"
+-
+-[[package]]
+-name = "devd-rs"
+-version = "0.3.1"
+-source = "registry+https://github.com/rust-lang/crates.io-index"
+-dependencies = [
+- "libc 0.2.73 (registry+https://github.com/rust-lang/crates.io-index)",
+- "nom 5.1.2 (registry+https://github.com/rust-lang/crates.io-index)",
+-]
+-
+-[[package]]
+-name = "digest"
+-version = "0.8.1"
+-source = "registry+https://github.com/rust-lang/crates.io-index"
+-dependencies = [
+- "generic-array 0.12.3 (registry+https://github.com/rust-lang/crates.io-index)",
+-]
+-
+-[[package]]
+-name = "digest"
+-version = "0.9.0"
+-source = "registry+https://github.com/rust-lang/crates.io-index"
+-dependencies = [
+- "generic-array 0.14.4 (registry+https://github.com/rust-lang/crates.io-index)",
+-]
+-
+-[[package]]
+-name = "dtoa"
+-version = "0.4.6"
+-source = "registry+https://github.com/rust-lang/crates.io-index"
+-
+-[[package]]
+-name = "env_logger"
+-version = "0.6.2"
+-source = "registry+https://github.com/rust-lang/crates.io-index"
+-dependencies = [
+- "atty 0.2.14 (registry+https://github.com/rust-lang/crates.io-index)",
+- "humantime 1.3.0 (registry+https://github.com/rust-lang/crates.io-index)",
+- "log 0.4.11 (registry+https://github.com/rust-lang/crates.io-index)",
+- "regex 1.3.9 (registry+https://github.com/rust-lang/crates.io-index)",
+- "termcolor 1.1.0 (registry+https://github.com/rust-lang/crates.io-index)",
+-]
+-
+-[[package]]
+-name = "fake-simd"
+-version = "0.1.2"
+-source = "registry+https://github.com/rust-lang/crates.io-index"
+-
+-[[package]]
+-name = "fnv"
+-version = "1.0.7"
+-source = "registry+https://github.com/rust-lang/crates.io-index"
+-
+-[[package]]
+-name = "fuchsia-cprng"
+-version = "0.1.1"
+-source = "registry+https://github.com/rust-lang/crates.io-index"
+-
+-[[package]]
+-name = "fuchsia-zircon"
+-version = "0.3.3"
+-source = "registry+https://github.com/rust-lang/crates.io-index"
+-dependencies = [
+- "bitflags 1.2.1 (registry+https://github.com/rust-lang/crates.io-index)",
+- "fuchsia-zircon-sys 0.3.3 (registry+https://github.com/rust-lang/crates.io-index)",
+-]
+-
+-[[package]]
+-name = "fuchsia-zircon-sys"
+-version = "0.3.3"
+-source = "registry+https://github.com/rust-lang/crates.io-index"
+-
+-[[package]]
+-name = "futures"
+-version = "0.3.5"
+-source = "registry+https://github.com/rust-lang/crates.io-index"
+-dependencies = [
+- "futures-channel 0.3.5 (registry+https://github.com/rust-lang/crates.io-index)",
+- "futures-core 0.3.5 (registry+https://github.com/rust-lang/crates.io-index)",
+- "futures-io 0.3.5 (registry+https://github.com/rust-lang/crates.io-index)",
+- "futures-sink 0.3.5 (registry+https://github.com/rust-lang/crates.io-index)",
+- "futures-task 0.3.5 (registry+https://github.com/rust-lang/crates.io-index)",
+- "futures-util 0.3.5 (registry+https://github.com/rust-lang/crates.io-index)",
+-]
+-
+-[[package]]
+-name = "futures-channel"
+-version = "0.3.5"
+-source = "registry+https://github.com/rust-lang/crates.io-index"
+-dependencies = [
+- "futures-core 0.3.5 (registry+https://github.com/rust-lang/crates.io-index)",
+- "futures-sink 0.3.5 (registry+https://github.com/rust-lang/crates.io-index)",
+-]
+-
+-[[package]]
+-name = "futures-core"
+-version = "0.3.5"
+-source = "registry+https://github.com/rust-lang/crates.io-index"
+-
+-[[package]]
+-name = "futures-io"
+-version = "0.3.5"
+-source = "registry+https://github.com/rust-lang/crates.io-index"
+-
+-[[package]]
+-name = "futures-sink"
+-version = "0.3.5"
+-source = "registry+https://github.com/rust-lang/crates.io-index"
+-
+-[[package]]
+-name = "futures-task"
+-version = "0.3.5"
+-source = "registry+https://github.com/rust-lang/crates.io-index"
+-dependencies = [
+- "once_cell 1.4.1 (registry+https://github.com/rust-lang/crates.io-index)",
+-]
+-
+-[[package]]
+-name = "futures-util"
+-version = "0.3.5"
+-source = "registry+https://github.com/rust-lang/crates.io-index"
+-dependencies = [
+- "futures-core 0.3.5 (registry+https://github.com/rust-lang/crates.io-index)",
+- "futures-sink 0.3.5 (registry+https://github.com/rust-lang/crates.io-index)",
+- "futures-task 0.3.5 (registry+https://github.com/rust-lang/crates.io-index)",
+- "pin-project 0.4.23 (registry+https://github.com/rust-lang/crates.io-index)",
+- "pin-utils 0.1.0 (registry+https://github.com/rust-lang/crates.io-index)",
+- "slab 0.4.2 (registry+https://github.com/rust-lang/crates.io-index)",
+-]
+-
+-[[package]]
+-name = "generic-array"
+-version = "0.12.3"
+-source = "registry+https://github.com/rust-lang/crates.io-index"
+-dependencies = [
+- "typenum 1.12.0 (registry+https://github.com/rust-lang/crates.io-index)",
+-]
+-
+-[[package]]
+-name = "generic-array"
+-version = "0.14.4"
+-source = "registry+https://github.com/rust-lang/crates.io-index"
+-dependencies = [
+- "typenum 1.12.0 (registry+https://github.com/rust-lang/crates.io-index)",
+- "version_check 0.9.2 (registry+https://github.com/rust-lang/crates.io-index)",
+-]
+-
+-[[package]]
+-name = "getopts"
+-version = "0.2.21"
+-source = "registry+https://github.com/rust-lang/crates.io-index"
+-dependencies = [
+- "unicode-width 0.1.8 (registry+https://github.com/rust-lang/crates.io-index)",
+-]
+-
+-[[package]]
+-name = "getrandom"
+-version = "0.1.14"
+-source = "registry+https://github.com/rust-lang/crates.io-index"
+-dependencies = [
+- "cfg-if 0.1.10 (registry+https://github.com/rust-lang/crates.io-index)",
+- "libc 0.2.73 (registry+https://github.com/rust-lang/crates.io-index)",
+- "wasi 0.9.0+wasi-snapshot-preview1 (registry+https://github.com/rust-lang/crates.io-index)",
+-]
+-
+-[[package]]
+-name = "glob"
+-version = "0.3.0"
+-source = "registry+https://github.com/rust-lang/crates.io-index"
+-
+-[[package]]
+-name = "h2"
+-version = "0.2.6"
+-source = "registry+https://github.com/rust-lang/crates.io-index"
+-dependencies = [
+- "bytes 0.5.6 (registry+https://github.com/rust-lang/crates.io-index)",
+- "fnv 1.0.7 (registry+https://github.com/rust-lang/crates.io-index)",
+- "futures-core 0.3.5 (registry+https://github.com/rust-lang/crates.io-index)",
+- "futures-sink 0.3.5 (registry+https://github.com/rust-lang/crates.io-index)",
+- "futures-util 0.3.5 (registry+https://github.com/rust-lang/crates.io-index)",
+- "http 0.2.1 (registry+https://github.com/rust-lang/crates.io-index)",
+- "indexmap 1.6.0 (registry+https://github.com/rust-lang/crates.io-index)",
+- "slab 0.4.2 (registry+https://github.com/rust-lang/crates.io-index)",
+- "tokio 0.2.22 (registry+https://github.com/rust-lang/crates.io-index)",
+- "tokio-util 0.3.1 (registry+https://github.com/rust-lang/crates.io-index)",
+- "tracing 0.1.19 (registry+https://github.com/rust-lang/crates.io-index)",
+-]
+-
+-[[package]]
+-name = "hashbrown"
+-version = "0.9.0"
+-source = "registry+https://github.com/rust-lang/crates.io-index"
+-
+-[[package]]
+-name = "headers"
+-version = "0.3.2"
+-source = "registry+https://github.com/rust-lang/crates.io-index"
+-dependencies = [
+- "base64 0.12.3 (registry+https://github.com/rust-lang/crates.io-index)",
+- "bitflags 1.2.1 (registry+https://github.com/rust-lang/crates.io-index)",
+- "bytes 0.5.6 (registry+https://github.com/rust-lang/crates.io-index)",
+- "headers-core 0.2.0 (registry+https://github.com/rust-lang/crates.io-index)",
+- "http 0.2.1 (registry+https://github.com/rust-lang/crates.io-index)",
+- "mime 0.3.16 (registry+https://github.com/rust-lang/crates.io-index)",
+- "sha-1 0.8.2 (registry+https://github.com/rust-lang/crates.io-index)",
+- "time 0.1.44 (registry+https://github.com/rust-lang/crates.io-index)",
+-]
+-
+-[[package]]
+-name = "headers-core"
+-version = "0.2.0"
+-source = "registry+https://github.com/rust-lang/crates.io-index"
+-dependencies = [
+- "http 0.2.1 (registry+https://github.com/rust-lang/crates.io-index)",
+-]
+-
+-[[package]]
+-name = "hermit-abi"
+-version = "0.1.15"
+-source = "registry+https://github.com/rust-lang/crates.io-index"
+-dependencies = [
+- "libc 0.2.73 (registry+https://github.com/rust-lang/crates.io-index)",
+-]
+-
+-[[package]]
+-name = "http"
+-version = "0.2.1"
+-source = "registry+https://github.com/rust-lang/crates.io-index"
+-dependencies = [
+- "bytes 0.5.6 (registry+https://github.com/rust-lang/crates.io-index)",
+- "fnv 1.0.7 (registry+https://github.com/rust-lang/crates.io-index)",
+- "itoa 0.4.6 (registry+https://github.com/rust-lang/crates.io-index)",
+-]
+-
+-[[package]]
+-name = "http-body"
+-version = "0.3.1"
+-source = "registry+https://github.com/rust-lang/crates.io-index"
+-dependencies = [
+- "bytes 0.5.6 (registry+https://github.com/rust-lang/crates.io-index)",
+- "http 0.2.1 (registry+https://github.com/rust-lang/crates.io-index)",
+-]
+-
+-[[package]]
+-name = "httparse"
+-version = "1.3.4"
+-source = "registry+https://github.com/rust-lang/crates.io-index"
+-
+-[[package]]
+-name = "humantime"
+-version = "1.3.0"
+-source = "registry+https://github.com/rust-lang/crates.io-index"
+-dependencies = [
+- "quick-error 1.2.3 (registry+https://github.com/rust-lang/crates.io-index)",
+-]
+-
+-[[package]]
+-name = "hyper"
+-version = "0.13.7"
+-source = "registry+https://github.com/rust-lang/crates.io-index"
+-dependencies = [
+- "bytes 0.5.6 (registry+https://github.com/rust-lang/crates.io-index)",
+- "futures-channel 0.3.5 (registry+https://github.com/rust-lang/crates.io-index)",
+- "futures-core 0.3.5 (registry+https://github.com/rust-lang/crates.io-index)",
+- "futures-util 0.3.5 (registry+https://github.com/rust-lang/crates.io-index)",
+- "h2 0.2.6 (registry+https://github.com/rust-lang/crates.io-index)",
+- "http 0.2.1 (registry+https://github.com/rust-lang/crates.io-index)",
+- "http-body 0.3.1 (registry+https://github.com/rust-lang/crates.io-index)",
+- "httparse 1.3.4 (registry+https://github.com/rust-lang/crates.io-index)",
+- "itoa 0.4.6 (registry+https://github.com/rust-lang/crates.io-index)",
+- "pin-project 0.4.23 (registry+https://github.com/rust-lang/crates.io-index)",
+- "socket2 0.3.15 (registry+https://github.com/rust-lang/crates.io-index)",
+- "time 0.1.44 (registry+https://github.com/rust-lang/crates.io-index)",
+- "tokio 0.2.22 (registry+https://github.com/rust-lang/crates.io-index)",
+- "tower-service 0.3.0 (registry+https://github.com/rust-lang/crates.io-index)",
+- "tracing 0.1.19 (registry+https://github.com/rust-lang/crates.io-index)",
+- "want 0.3.0 (registry+https://github.com/rust-lang/crates.io-index)",
+-]
+-
+-[[package]]
+-name = "idna"
+-version = "0.2.0"
+-source = "registry+https://github.com/rust-lang/crates.io-index"
+-dependencies = [
+- "matches 0.1.8 (registry+https://github.com/rust-lang/crates.io-index)",
+- "unicode-bidi 0.3.4 (registry+https://github.com/rust-lang/crates.io-index)",
+- "unicode-normalization 0.1.13 (registry+https://github.com/rust-lang/crates.io-index)",
+-]
+-
+-[[package]]
+-name = "indexmap"
+-version = "1.6.0"
+-source = "registry+https://github.com/rust-lang/crates.io-index"
+-dependencies = [
+- "autocfg 1.0.1 (registry+https://github.com/rust-lang/crates.io-index)",
+- "hashbrown 0.9.0 (registry+https://github.com/rust-lang/crates.io-index)",
+-]
+-
+-[[package]]
+-name = "input_buffer"
+-version = "0.3.1"
+-source = "registry+https://github.com/rust-lang/crates.io-index"
+-dependencies = [
+- "bytes 0.5.6 (registry+https://github.com/rust-lang/crates.io-index)",
+-]
+-
+-[[package]]
+-name = "iovec"
+-version = "0.1.4"
+-source = "registry+https://github.com/rust-lang/crates.io-index"
+-dependencies = [
+- "libc 0.2.73 (registry+https://github.com/rust-lang/crates.io-index)",
+-]
+-
+-[[package]]
+-name = "itoa"
+-version = "0.4.6"
+-source = "registry+https://github.com/rust-lang/crates.io-index"
+-
+-[[package]]
+-name = "kernel32-sys"
+-version = "0.2.2"
+-source = "registry+https://github.com/rust-lang/crates.io-index"
+-dependencies = [
+- "winapi 0.2.8 (registry+https://github.com/rust-lang/crates.io-index)",
+- "winapi-build 0.1.1 (registry+https://github.com/rust-lang/crates.io-index)",
+-]
+-
+-[[package]]
+-name = "lazy_static"
+-version = "1.4.0"
+-source = "registry+https://github.com/rust-lang/crates.io-index"
+-
+-[[package]]
+-name = "libc"
+-version = "0.2.73"
+-source = "registry+https://github.com/rust-lang/crates.io-index"
+-
+-[[package]]
+-name = "libloading"
+-version = "0.5.2"
+-source = "registry+https://github.com/rust-lang/crates.io-index"
+-dependencies = [
+- "cc 1.0.58 (registry+https://github.com/rust-lang/crates.io-index)",
+- "winapi 0.3.9 (registry+https://github.com/rust-lang/crates.io-index)",
+-]
+-
+-[[package]]
+-name = "libudev"
+-version = "0.2.0"
+-source = "registry+https://github.com/rust-lang/crates.io-index"
+-dependencies = [
+- "libc 0.2.73 (registry+https://github.com/rust-lang/crates.io-index)",
+- "libudev-sys 0.1.4 (registry+https://github.com/rust-lang/crates.io-index)",
+-]
+-
+-[[package]]
+-name = "libudev-sys"
+-version = "0.1.4"
+-source = "registry+https://github.com/rust-lang/crates.io-index"
+-dependencies = [
+- "libc 0.2.73 (registry+https://github.com/rust-lang/crates.io-index)",
+- "pkg-config 0.3.18 (registry+https://github.com/rust-lang/crates.io-index)",
+-]
+-
+-[[package]]
+-name = "log"
+-version = "0.4.11"
+-source = "registry+https://github.com/rust-lang/crates.io-index"
+-dependencies = [
+- "cfg-if 0.1.10 (registry+https://github.com/rust-lang/crates.io-index)",
+-]
+-
+-[[package]]
+-name = "matches"
+-version = "0.1.8"
+-source = "registry+https://github.com/rust-lang/crates.io-index"
+-
+-[[package]]
+-name = "memchr"
+-version = "2.3.3"
+-source = "registry+https://github.com/rust-lang/crates.io-index"
+-
+-[[package]]
+-name = "mime"
+-version = "0.3.16"
+-source = "registry+https://github.com/rust-lang/crates.io-index"
+-
+-[[package]]
+-name = "mime_guess"
+-version = "2.0.3"
+-source = "registry+https://github.com/rust-lang/crates.io-index"
+-dependencies = [
+- "mime 0.3.16 (registry+https://github.com/rust-lang/crates.io-index)",
+- "unicase 2.6.0 (registry+https://github.com/rust-lang/crates.io-index)",
+-]
+-
+-[[package]]
+-name = "mio"
+-version = "0.6.22"
+-source = "registry+https://github.com/rust-lang/crates.io-index"
+-dependencies = [
+- "cfg-if 0.1.10 (registry+https://github.com/rust-lang/crates.io-index)",
+- "fuchsia-zircon 0.3.3 (registry+https://github.com/rust-lang/crates.io-index)",
+- "fuchsia-zircon-sys 0.3.3 (registry+https://github.com/rust-lang/crates.io-index)",
+- "iovec 0.1.4 (registry+https://github.com/rust-lang/crates.io-index)",
+- "kernel32-sys 0.2.2 (registry+https://github.com/rust-lang/crates.io-index)",
+- "libc 0.2.73 (registry+https://github.com/rust-lang/crates.io-index)",
+- "log 0.4.11 (registry+https://github.com/rust-lang/crates.io-index)",
+- "miow 0.2.1 (registry+https://github.com/rust-lang/crates.io-index)",
+- "net2 0.2.35 (registry+https://github.com/rust-lang/crates.io-index)",
+- "slab 0.4.2 (registry+https://github.com/rust-lang/crates.io-index)",
+- "winapi 0.2.8 (registry+https://github.com/rust-lang/crates.io-index)",
+-]
+-
+-[[package]]
+-name = "miow"
+-version = "0.2.1"
+-source = "registry+https://github.com/rust-lang/crates.io-index"
+-dependencies = [
+- "kernel32-sys 0.2.2 (registry+https://github.com/rust-lang/crates.io-index)",
+- "net2 0.2.35 (registry+https://github.com/rust-lang/crates.io-index)",
+- "winapi 0.2.8 (registry+https://github.com/rust-lang/crates.io-index)",
+- "ws2_32-sys 0.2.1 (registry+https://github.com/rust-lang/crates.io-index)",
+-]
+-
+-[[package]]
+-name = "multipart"
+-version = "0.17.0"
+-source = "registry+https://github.com/rust-lang/crates.io-index"
+-dependencies = [
+- "buf_redux 0.8.4 (registry+https://github.com/rust-lang/crates.io-index)",
+- "httparse 1.3.4 (registry+https://github.com/rust-lang/crates.io-index)",
+- "log 0.4.11 (registry+https://github.com/rust-lang/crates.io-index)",
+- "mime 0.3.16 (registry+https://github.com/rust-lang/crates.io-index)",
+- "mime_guess 2.0.3 (registry+https://github.com/rust-lang/crates.io-index)",
+- "quick-error 1.2.3 (registry+https://github.com/rust-lang/crates.io-index)",
+- "rand 0.6.5 (registry+https://github.com/rust-lang/crates.io-index)",
+- "safemem 0.3.3 (registry+https://github.com/rust-lang/crates.io-index)",
+- "tempfile 3.1.0 (registry+https://github.com/rust-lang/crates.io-index)",
+- "twoway 0.1.8 (registry+https://github.com/rust-lang/crates.io-index)",
+-]
+-
+-[[package]]
+-name = "net2"
+-version = "0.2.35"
+-source = "registry+https://github.com/rust-lang/crates.io-index"
+-dependencies = [
+- "cfg-if 0.1.10 (registry+https://github.com/rust-lang/crates.io-index)",
+- "libc 0.2.73 (registry+https://github.com/rust-lang/crates.io-index)",
+- "winapi 0.3.9 (registry+https://github.com/rust-lang/crates.io-index)",
+-]
+-
+-[[package]]
+-name = "nom"
+-version = "4.2.3"
+-source = "registry+https://github.com/rust-lang/crates.io-index"
+-dependencies = [
+- "memchr 2.3.3 (registry+https://github.com/rust-lang/crates.io-index)",
+- "version_check 0.1.5 (registry+https://github.com/rust-lang/crates.io-index)",
+-]
+-
+-[[package]]
+-name = "nom"
+-version = "5.1.2"
+-source = "registry+https://github.com/rust-lang/crates.io-index"
+-dependencies = [
+- "memchr 2.3.3 (registry+https://github.com/rust-lang/crates.io-index)",
+- "version_check 0.9.2 (registry+https://github.com/rust-lang/crates.io-index)",
+-]
+-
+-[[package]]
+-name = "once_cell"
+-version = "1.4.1"
+-source = "registry+https://github.com/rust-lang/crates.io-index"
+-
+-[[package]]
+-name = "opaque-debug"
+-version = "0.2.3"
+-source = "registry+https://github.com/rust-lang/crates.io-index"
+-
+-[[package]]
+-name = "opaque-debug"
+-version = "0.3.0"
+-source = "registry+https://github.com/rust-lang/crates.io-index"
+-
+-[[package]]
+-name = "peeking_take_while"
+-version = "0.1.2"
+-source = "registry+https://github.com/rust-lang/crates.io-index"
+-
+-[[package]]
+-name = "percent-encoding"
+-version = "2.1.0"
+-source = "registry+https://github.com/rust-lang/crates.io-index"
+-
+-[[package]]
+-name = "pin-project"
+-version = "0.4.23"
+-source = "registry+https://github.com/rust-lang/crates.io-index"
+-dependencies = [
+- "pin-project-internal 0.4.23 (registry+https://github.com/rust-lang/crates.io-index)",
+-]
+-
+-[[package]]
+-name = "pin-project-internal"
+-version = "0.4.23"
+-source = "registry+https://github.com/rust-lang/crates.io-index"
+-dependencies = [
+- "proc-macro2 1.0.19 (registry+https://github.com/rust-lang/crates.io-index)",
+- "quote 1.0.7 (registry+https://github.com/rust-lang/crates.io-index)",
+- "syn 1.0.41 (registry+https://github.com/rust-lang/crates.io-index)",
+-]
+-
+-[[package]]
+-name = "pin-project-lite"
+-version = "0.1.7"
+-source = "registry+https://github.com/rust-lang/crates.io-index"
+-
+-[[package]]
+-name = "pin-utils"
+-version = "0.1.0"
+-source = "registry+https://github.com/rust-lang/crates.io-index"
+-
+-[[package]]
+-name = "pkg-config"
+-version = "0.3.18"
+-source = "registry+https://github.com/rust-lang/crates.io-index"
+-
+-[[package]]
+-name = "ppv-lite86"
+-version = "0.2.8"
+-source = "registry+https://github.com/rust-lang/crates.io-index"
+-
+-[[package]]
+-name = "proc-macro2"
+-version = "1.0.19"
+-source = "registry+https://github.com/rust-lang/crates.io-index"
+-dependencies = [
+- "unicode-xid 0.2.1 (registry+https://github.com/rust-lang/crates.io-index)",
+-]
+-
+-[[package]]
+-name = "quick-error"
+-version = "1.2.3"
+-source = "registry+https://github.com/rust-lang/crates.io-index"
+-
+-[[package]]
+-name = "quote"
+-version = "1.0.7"
+-source = "registry+https://github.com/rust-lang/crates.io-index"
+-dependencies = [
+- "proc-macro2 1.0.19 (registry+https://github.com/rust-lang/crates.io-index)",
+-]
+-
+-[[package]]
+-name = "rand"
+-version = "0.6.5"
+-source = "registry+https://github.com/rust-lang/crates.io-index"
+-dependencies = [
+- "autocfg 0.1.7 (registry+https://github.com/rust-lang/crates.io-index)",
+- "libc 0.2.73 (registry+https://github.com/rust-lang/crates.io-index)",
+- "rand_chacha 0.1.1 (registry+https://github.com/rust-lang/crates.io-index)",
+- "rand_core 0.4.2 (registry+https://github.com/rust-lang/crates.io-index)",
+- "rand_hc 0.1.0 (registry+https://github.com/rust-lang/crates.io-index)",
+- "rand_isaac 0.1.1 (registry+https://github.com/rust-lang/crates.io-index)",
+- "rand_jitter 0.1.4 (registry+https://github.com/rust-lang/crates.io-index)",
+- "rand_os 0.1.3 (registry+https://github.com/rust-lang/crates.io-index)",
+- "rand_pcg 0.1.2 (registry+https://github.com/rust-lang/crates.io-index)",
+- "rand_xorshift 0.1.1 (registry+https://github.com/rust-lang/crates.io-index)",
+- "winapi 0.3.9 (registry+https://github.com/rust-lang/crates.io-index)",
+-]
+-
+-[[package]]
+-name = "rand"
+-version = "0.7.3"
+-source = "registry+https://github.com/rust-lang/crates.io-index"
+-dependencies = [
+- "getrandom 0.1.14 (registry+https://github.com/rust-lang/crates.io-index)",
+- "libc 0.2.73 (registry+https://github.com/rust-lang/crates.io-index)",
+- "rand_chacha 0.2.2 (registry+https://github.com/rust-lang/crates.io-index)",
+- "rand_core 0.5.1 (registry+https://github.com/rust-lang/crates.io-index)",
+- "rand_hc 0.2.0 (registry+https://github.com/rust-lang/crates.io-index)",
+-]
+-
+-[[package]]
+-name = "rand_chacha"
+-version = "0.1.1"
+-source = "registry+https://github.com/rust-lang/crates.io-index"
+-dependencies = [
+- "autocfg 0.1.7 (registry+https://github.com/rust-lang/crates.io-index)",
+- "rand_core 0.3.1 (registry+https://github.com/rust-lang/crates.io-index)",
+-]
+-
+-[[package]]
+-name = "rand_chacha"
+-version = "0.2.2"
+-source = "registry+https://github.com/rust-lang/crates.io-index"
+-dependencies = [
+- "ppv-lite86 0.2.8 (registry+https://github.com/rust-lang/crates.io-index)",
+- "rand_core 0.5.1 (registry+https://github.com/rust-lang/crates.io-index)",
+-]
+-
+-[[package]]
+-name = "rand_core"
+-version = "0.3.1"
+-source = "registry+https://github.com/rust-lang/crates.io-index"
+-dependencies = [
+- "rand_core 0.4.2 (registry+https://github.com/rust-lang/crates.io-index)",
+-]
+-
+-[[package]]
+-name = "rand_core"
+-version = "0.4.2"
+-source = "registry+https://github.com/rust-lang/crates.io-index"
+-
+-[[package]]
+-name = "rand_core"
+-version = "0.5.1"
+-source = "registry+https://github.com/rust-lang/crates.io-index"
+-dependencies = [
+- "getrandom 0.1.14 (registry+https://github.com/rust-lang/crates.io-index)",
+-]
+-
+-[[package]]
+-name = "rand_hc"
+-version = "0.1.0"
+-source = "registry+https://github.com/rust-lang/crates.io-index"
+-dependencies = [
+- "rand_core 0.3.1 (registry+https://github.com/rust-lang/crates.io-index)",
+-]
+-
+-[[package]]
+-name = "rand_hc"
+-version = "0.2.0"
+-source = "registry+https://github.com/rust-lang/crates.io-index"
+-dependencies = [
+- "rand_core 0.5.1 (registry+https://github.com/rust-lang/crates.io-index)",
+-]
+-
+-[[package]]
+-name = "rand_isaac"
+-version = "0.1.1"
+-source = "registry+https://github.com/rust-lang/crates.io-index"
+-dependencies = [
+- "rand_core 0.3.1 (registry+https://github.com/rust-lang/crates.io-index)",
+-]
+-
+-[[package]]
+-name = "rand_jitter"
+-version = "0.1.4"
+-source = "registry+https://github.com/rust-lang/crates.io-index"
+-dependencies = [
+- "libc 0.2.73 (registry+https://github.com/rust-lang/crates.io-index)",
+- "rand_core 0.4.2 (registry+https://github.com/rust-lang/crates.io-index)",
+- "winapi 0.3.9 (registry+https://github.com/rust-lang/crates.io-index)",
+-]
+-
+-[[package]]
+-name = "rand_os"
+-version = "0.1.3"
+-source = "registry+https://github.com/rust-lang/crates.io-index"
+-dependencies = [
+- "cloudabi 0.0.3 (registry+https://github.com/rust-lang/crates.io-index)",
+- "fuchsia-cprng 0.1.1 (registry+https://github.com/rust-lang/crates.io-index)",
+- "libc 0.2.73 (registry+https://github.com/rust-lang/crates.io-index)",
+- "rand_core 0.4.2 (registry+https://github.com/rust-lang/crates.io-index)",
+- "rdrand 0.4.0 (registry+https://github.com/rust-lang/crates.io-index)",
+- "winapi 0.3.9 (registry+https://github.com/rust-lang/crates.io-index)",
+-]
+-
+-[[package]]
+-name = "rand_pcg"
+-version = "0.1.2"
+-source = "registry+https://github.com/rust-lang/crates.io-index"
+-dependencies = [
+- "autocfg 0.1.7 (registry+https://github.com/rust-lang/crates.io-index)",
+- "rand_core 0.4.2 (registry+https://github.com/rust-lang/crates.io-index)",
+-]
+-
+-[[package]]
+-name = "rand_xorshift"
+-version = "0.1.1"
+-source = "registry+https://github.com/rust-lang/crates.io-index"
+-dependencies = [
+- "rand_core 0.3.1 (registry+https://github.com/rust-lang/crates.io-index)",
+-]
+-
+-[[package]]
+-name = "rdrand"
+-version = "0.4.0"
+-source = "registry+https://github.com/rust-lang/crates.io-index"
+-dependencies = [
+- "rand_core 0.3.1 (registry+https://github.com/rust-lang/crates.io-index)",
+-]
+-
+-[[package]]
+-name = "redox_syscall"
+-version = "0.1.57"
+-source = "registry+https://github.com/rust-lang/crates.io-index"
+-
+-[[package]]
+-name = "regex"
+-version = "1.3.9"
+-source = "registry+https://github.com/rust-lang/crates.io-index"
+-dependencies = [
+- "aho-corasick 0.7.13 (registry+https://github.com/rust-lang/crates.io-index)",
+- "memchr 2.3.3 (registry+https://github.com/rust-lang/crates.io-index)",
+- "regex-syntax 0.6.18 (registry+https://github.com/rust-lang/crates.io-index)",
+- "thread_local 1.0.1 (registry+https://github.com/rust-lang/crates.io-index)",
+-]
+-
+-[[package]]
+-name = "regex-syntax"
+-version = "0.6.18"
+-source = "registry+https://github.com/rust-lang/crates.io-index"
+-
+-[[package]]
+-name = "remove_dir_all"
+-version = "0.5.3"
+-source = "registry+https://github.com/rust-lang/crates.io-index"
+-dependencies = [
+- "winapi 0.3.9 (registry+https://github.com/rust-lang/crates.io-index)",
+-]
+-
+-[[package]]
+-name = "runloop"
+-version = "0.1.0"
+-source = "registry+https://github.com/rust-lang/crates.io-index"
+-
+-[[package]]
+-name = "rustc-hash"
+-version = "1.1.0"
+-source = "registry+https://github.com/rust-lang/crates.io-index"
+-
+-[[package]]
+-name = "ryu"
+-version = "1.0.5"
+-source = "registry+https://github.com/rust-lang/crates.io-index"
+-
+-[[package]]
+-name = "safemem"
+-version = "0.3.3"
+-source = "registry+https://github.com/rust-lang/crates.io-index"
+-
+-[[package]]
+-name = "scoped-tls"
+-version = "1.0.0"
+-source = "registry+https://github.com/rust-lang/crates.io-index"
+-
+-[[package]]
+-name = "serde"
+-version = "1.0.116"
+-source = "registry+https://github.com/rust-lang/crates.io-index"
+-dependencies = [
+- "serde_derive 1.0.116 (registry+https://github.com/rust-lang/crates.io-index)",
+-]
+-
+-[[package]]
+-name = "serde_derive"
+-version = "1.0.116"
+-source = "registry+https://github.com/rust-lang/crates.io-index"
+-dependencies = [
+- "proc-macro2 1.0.19 (registry+https://github.com/rust-lang/crates.io-index)",
+- "quote 1.0.7 (registry+https://github.com/rust-lang/crates.io-index)",
+- "syn 1.0.41 (registry+https://github.com/rust-lang/crates.io-index)",
+-]
+-
+-[[package]]
+-name = "serde_json"
+-version = "1.0.57"
+-source = "registry+https://github.com/rust-lang/crates.io-index"
+-dependencies = [
+- "itoa 0.4.6 (registry+https://github.com/rust-lang/crates.io-index)",
+- "ryu 1.0.5 (registry+https://github.com/rust-lang/crates.io-index)",
+- "serde 1.0.116 (registry+https://github.com/rust-lang/crates.io-index)",
+-]
+-
+-[[package]]
+-name = "serde_urlencoded"
+-version = "0.6.1"
+-source = "registry+https://github.com/rust-lang/crates.io-index"
+-dependencies = [
+- "dtoa 0.4.6 (registry+https://github.com/rust-lang/crates.io-index)",
+- "itoa 0.4.6 (registry+https://github.com/rust-lang/crates.io-index)",
+- "serde 1.0.116 (registry+https://github.com/rust-lang/crates.io-index)",
+- "url 2.1.1 (registry+https://github.com/rust-lang/crates.io-index)",
+-]
+-
+-[[package]]
+-name = "sha-1"
+-version = "0.8.2"
+-source = "registry+https://github.com/rust-lang/crates.io-index"
+-dependencies = [
+- "block-buffer 0.7.3 (registry+https://github.com/rust-lang/crates.io-index)",
+- "digest 0.8.1 (registry+https://github.com/rust-lang/crates.io-index)",
+- "fake-simd 0.1.2 (registry+https://github.com/rust-lang/crates.io-index)",
+- "opaque-debug 0.2.3 (registry+https://github.com/rust-lang/crates.io-index)",
+-]
+-
+-[[package]]
+-name = "sha-1"
+-version = "0.9.1"
+-source = "registry+https://github.com/rust-lang/crates.io-index"
+-dependencies = [
+- "block-buffer 0.9.0 (registry+https://github.com/rust-lang/crates.io-index)",
+- "cfg-if 0.1.10 (registry+https://github.com/rust-lang/crates.io-index)",
+- "cpuid-bool 0.1.2 (registry+https://github.com/rust-lang/crates.io-index)",
+- "digest 0.9.0 (registry+https://github.com/rust-lang/crates.io-index)",
+- "opaque-debug 0.3.0 (registry+https://github.com/rust-lang/crates.io-index)",
+-]
+-
+-[[package]]
+-name = "sha2"
+-version = "0.8.2"
+-source = "registry+https://github.com/rust-lang/crates.io-index"
+-dependencies = [
+- "block-buffer 0.7.3 (registry+https://github.com/rust-lang/crates.io-index)",
+- "digest 0.8.1 (registry+https://github.com/rust-lang/crates.io-index)",
+- "fake-simd 0.1.2 (registry+https://github.com/rust-lang/crates.io-index)",
+- "opaque-debug 0.2.3 (registry+https://github.com/rust-lang/crates.io-index)",
+-]
+-
+-[[package]]
+-name = "shlex"
+-version = "0.1.1"
+-source = "registry+https://github.com/rust-lang/crates.io-index"
+-
+-[[package]]
+-name = "slab"
+-version = "0.4.2"
+-source = "registry+https://github.com/rust-lang/crates.io-index"
+-
+-[[package]]
+-name = "socket2"
+-version = "0.3.15"
+-source = "registry+https://github.com/rust-lang/crates.io-index"
+-dependencies = [
+- "cfg-if 0.1.10 (registry+https://github.com/rust-lang/crates.io-index)",
+- "libc 0.2.73 (registry+https://github.com/rust-lang/crates.io-index)",
+- "redox_syscall 0.1.57 (registry+https://github.com/rust-lang/crates.io-index)",
+- "winapi 0.3.9 (registry+https://github.com/rust-lang/crates.io-index)",
+-]
+-
+-[[package]]
+-name = "strsim"
+-version = "0.8.0"
+-source = "registry+https://github.com/rust-lang/crates.io-index"
+-
+-[[package]]
+-name = "syn"
+-version = "1.0.41"
+-source = "registry+https://github.com/rust-lang/crates.io-index"
+-dependencies = [
+- "proc-macro2 1.0.19 (registry+https://github.com/rust-lang/crates.io-index)",
+- "quote 1.0.7 (registry+https://github.com/rust-lang/crates.io-index)",
+- "unicode-xid 0.2.1 (registry+https://github.com/rust-lang/crates.io-index)",
+-]
+-
+-[[package]]
+-name = "tempfile"
+-version = "3.1.0"
+-source = "registry+https://github.com/rust-lang/crates.io-index"
+-dependencies = [
+- "cfg-if 0.1.10 (registry+https://github.com/rust-lang/crates.io-index)",
+- "libc 0.2.73 (registry+https://github.com/rust-lang/crates.io-index)",
+- "rand 0.7.3 (registry+https://github.com/rust-lang/crates.io-index)",
+- "redox_syscall 0.1.57 (registry+https://github.com/rust-lang/crates.io-index)",
+- "remove_dir_all 0.5.3 (registry+https://github.com/rust-lang/crates.io-index)",
+- "winapi 0.3.9 (registry+https://github.com/rust-lang/crates.io-index)",
+-]
+-
+-[[package]]
+-name = "termcolor"
+-version = "1.1.0"
+-source = "registry+https://github.com/rust-lang/crates.io-index"
+-dependencies = [
+- "winapi-util 0.1.5 (registry+https://github.com/rust-lang/crates.io-index)",
+-]
+-
+-[[package]]
+-name = "textwrap"
+-version = "0.11.0"
+-source = "registry+https://github.com/rust-lang/crates.io-index"
+-dependencies = [
+- "unicode-width 0.1.8 (registry+https://github.com/rust-lang/crates.io-index)",
+-]
+-
+-[[package]]
+-name = "thread_local"
+-version = "1.0.1"
+-source = "registry+https://github.com/rust-lang/crates.io-index"
+-dependencies = [
+- "lazy_static 1.4.0 (registry+https://github.com/rust-lang/crates.io-index)",
+-]
+-
+-[[package]]
+-name = "time"
+-version = "0.1.44"
+-source = "registry+https://github.com/rust-lang/crates.io-index"
+-dependencies = [
+- "libc 0.2.73 (registry+https://github.com/rust-lang/crates.io-index)",
+- "wasi 0.10.0+wasi-snapshot-preview1 (registry+https://github.com/rust-lang/crates.io-index)",
+- "winapi 0.3.9 (registry+https://github.com/rust-lang/crates.io-index)",
+-]
+-
+-[[package]]
+-name = "tinyvec"
+-version = "0.3.4"
+-source = "registry+https://github.com/rust-lang/crates.io-index"
+-
+-[[package]]
+-name = "tokio"
+-version = "0.2.22"
+-source = "registry+https://github.com/rust-lang/crates.io-index"
+-dependencies = [
+- "bytes 0.5.6 (registry+https://github.com/rust-lang/crates.io-index)",
+- "fnv 1.0.7 (registry+https://github.com/rust-lang/crates.io-index)",
+- "futures-core 0.3.5 (registry+https://github.com/rust-lang/crates.io-index)",
+- "iovec 0.1.4 (registry+https://github.com/rust-lang/crates.io-index)",
+- "lazy_static 1.4.0 (registry+https://github.com/rust-lang/crates.io-index)",
+- "memchr 2.3.3 (registry+https://github.com/rust-lang/crates.io-index)",
+- "mio 0.6.22 (registry+https://github.com/rust-lang/crates.io-index)",
+- "pin-project-lite 0.1.7 (registry+https://github.com/rust-lang/crates.io-index)",
+- "slab 0.4.2 (registry+https://github.com/rust-lang/crates.io-index)",
+- "tokio-macros 0.2.5 (registry+https://github.com/rust-lang/crates.io-index)",
+-]
+-
+-[[package]]
+-name = "tokio-macros"
+-version = "0.2.5"
+-source = "registry+https://github.com/rust-lang/crates.io-index"
+-dependencies = [
+- "proc-macro2 1.0.19 (registry+https://github.com/rust-lang/crates.io-index)",
+- "quote 1.0.7 (registry+https://github.com/rust-lang/crates.io-index)",
+- "syn 1.0.41 (registry+https://github.com/rust-lang/crates.io-index)",
+-]
+-
+-[[package]]
+-name = "tokio-tungstenite"
+-version = "0.11.0"
+-source = "registry+https://github.com/rust-lang/crates.io-index"
+-dependencies = [
+- "futures-util 0.3.5 (registry+https://github.com/rust-lang/crates.io-index)",
+- "log 0.4.11 (registry+https://github.com/rust-lang/crates.io-index)",
+- "pin-project 0.4.23 (registry+https://github.com/rust-lang/crates.io-index)",
+- "tokio 0.2.22 (registry+https://github.com/rust-lang/crates.io-index)",
+- "tungstenite 0.11.1 (registry+https://github.com/rust-lang/crates.io-index)",
+-]
+-
+-[[package]]
+-name = "tokio-util"
+-version = "0.3.1"
+-source = "registry+https://github.com/rust-lang/crates.io-index"
+-dependencies = [
+- "bytes 0.5.6 (registry+https://github.com/rust-lang/crates.io-index)",
+- "futures-core 0.3.5 (registry+https://github.com/rust-lang/crates.io-index)",
+- "futures-sink 0.3.5 (registry+https://github.com/rust-lang/crates.io-index)",
+- "log 0.4.11 (registry+https://github.com/rust-lang/crates.io-index)",
+- "pin-project-lite 0.1.7 (registry+https://github.com/rust-lang/crates.io-index)",
+- "tokio 0.2.22 (registry+https://github.com/rust-lang/crates.io-index)",
+-]
+-
+-[[package]]
+-name = "tower-service"
+-version = "0.3.0"
+-source = "registry+https://github.com/rust-lang/crates.io-index"
+-
+-[[package]]
+-name = "tracing"
+-version = "0.1.19"
+-source = "registry+https://github.com/rust-lang/crates.io-index"
+-dependencies = [
+- "cfg-if 0.1.10 (registry+https://github.com/rust-lang/crates.io-index)",
+- "log 0.4.11 (registry+https://github.com/rust-lang/crates.io-index)",
+- "tracing-core 0.1.16 (registry+https://github.com/rust-lang/crates.io-index)",
+-]
+-
+-[[package]]
+-name = "tracing-core"
+-version = "0.1.16"
+-source = "registry+https://github.com/rust-lang/crates.io-index"
+-dependencies = [
+- "lazy_static 1.4.0 (registry+https://github.com/rust-lang/crates.io-index)",
+-]
+-
+-[[package]]
+-name = "tracing-futures"
+-version = "0.2.4"
+-source = "registry+https://github.com/rust-lang/crates.io-index"
+-dependencies = [
+- "pin-project 0.4.23 (registry+https://github.com/rust-lang/crates.io-index)",
+- "tracing 0.1.19 (registry+https://github.com/rust-lang/crates.io-index)",
+-]
+-
+-[[package]]
+-name = "try-lock"
+-version = "0.2.3"
+-source = "registry+https://github.com/rust-lang/crates.io-index"
+-
+-[[package]]
+-name = "tungstenite"
+-version = "0.11.1"
+-source = "registry+https://github.com/rust-lang/crates.io-index"
+-dependencies = [
+- "base64 0.12.3 (registry+https://github.com/rust-lang/crates.io-index)",
+- "byteorder 1.3.4 (registry+https://github.com/rust-lang/crates.io-index)",
+- "bytes 0.5.6 (registry+https://github.com/rust-lang/crates.io-index)",
+- "http 0.2.1 (registry+https://github.com/rust-lang/crates.io-index)",
+- "httparse 1.3.4 (registry+https://github.com/rust-lang/crates.io-index)",
+- "input_buffer 0.3.1 (registry+https://github.com/rust-lang/crates.io-index)",
+- "log 0.4.11 (registry+https://github.com/rust-lang/crates.io-index)",
+- "rand 0.7.3 (registry+https://github.com/rust-lang/crates.io-index)",
+- "sha-1 0.9.1 (registry+https://github.com/rust-lang/crates.io-index)",
+- "url 2.1.1 (registry+https://github.com/rust-lang/crates.io-index)",
+- "utf-8 0.7.5 (registry+https://github.com/rust-lang/crates.io-index)",
+-]
+-
+-[[package]]
+-name = "twoway"
+-version = "0.1.8"
+-source = "registry+https://github.com/rust-lang/crates.io-index"
+-dependencies = [
+- "memchr 2.3.3 (registry+https://github.com/rust-lang/crates.io-index)",
+-]
+-
+-[[package]]
+-name = "typenum"
+-version = "1.12.0"
+-source = "registry+https://github.com/rust-lang/crates.io-index"
+-
+-[[package]]
+-name = "unicase"
+-version = "2.6.0"
+-source = "registry+https://github.com/rust-lang/crates.io-index"
+-dependencies = [
+- "version_check 0.9.2 (registry+https://github.com/rust-lang/crates.io-index)",
+-]
+-
+-[[package]]
+-name = "unicode-bidi"
+-version = "0.3.4"
+-source = "registry+https://github.com/rust-lang/crates.io-index"
+-dependencies = [
+- "matches 0.1.8 (registry+https://github.com/rust-lang/crates.io-index)",
+-]
+-
+-[[package]]
+-name = "unicode-normalization"
+-version = "0.1.13"
+-source = "registry+https://github.com/rust-lang/crates.io-index"
+-dependencies = [
+- "tinyvec 0.3.4 (registry+https://github.com/rust-lang/crates.io-index)",
+-]
+-
+-[[package]]
+-name = "unicode-width"
+-version = "0.1.8"
+-source = "registry+https://github.com/rust-lang/crates.io-index"
+-
+-[[package]]
+-name = "unicode-xid"
+-version = "0.2.1"
+-source = "registry+https://github.com/rust-lang/crates.io-index"
+-
+-[[package]]
+-name = "url"
+-version = "2.1.1"
+-source = "registry+https://github.com/rust-lang/crates.io-index"
+-dependencies = [
+- "idna 0.2.0 (registry+https://github.com/rust-lang/crates.io-index)",
+- "matches 0.1.8 (registry+https://github.com/rust-lang/crates.io-index)",
+- "percent-encoding 2.1.0 (registry+https://github.com/rust-lang/crates.io-index)",
+-]
+-
+-[[package]]
+-name = "urlencoding"
+-version = "1.1.1"
+-source = "registry+https://github.com/rust-lang/crates.io-index"
+-
+-[[package]]
+-name = "utf-8"
+-version = "0.7.5"
+-source = "registry+https://github.com/rust-lang/crates.io-index"
+-
+-[[package]]
+-name = "vec_map"
+-version = "0.8.2"
+-source = "registry+https://github.com/rust-lang/crates.io-index"
+-
+-[[package]]
+-name = "version_check"
+-version = "0.1.5"
+-source = "registry+https://github.com/rust-lang/crates.io-index"
+-
+-[[package]]
+-name = "version_check"
+-version = "0.9.2"
+-source = "registry+https://github.com/rust-lang/crates.io-index"
+-
+-[[package]]
+-name = "want"
+-version = "0.3.0"
+-source = "registry+https://github.com/rust-lang/crates.io-index"
+-dependencies = [
+- "log 0.4.11 (registry+https://github.com/rust-lang/crates.io-index)",
+- "try-lock 0.2.3 (registry+https://github.com/rust-lang/crates.io-index)",
+-]
+-
+-[[package]]
+-name = "warp"
+-version = "0.2.5"
+-source = "registry+https://github.com/rust-lang/crates.io-index"
+-dependencies = [
+- "bytes 0.5.6 (registry+https://github.com/rust-lang/crates.io-index)",
+- "futures 0.3.5 (registry+https://github.com/rust-lang/crates.io-index)",
+- "headers 0.3.2 (registry+https://github.com/rust-lang/crates.io-index)",
+- "http 0.2.1 (registry+https://github.com/rust-lang/crates.io-index)",
+- "hyper 0.13.7 (registry+https://github.com/rust-lang/crates.io-index)",
+- "log 0.4.11 (registry+https://github.com/rust-lang/crates.io-index)",
+- "mime 0.3.16 (registry+https://github.com/rust-lang/crates.io-index)",
+- "mime_guess 2.0.3 (registry+https://github.com/rust-lang/crates.io-index)",
+- "multipart 0.17.0 (registry+https://github.com/rust-lang/crates.io-index)",
+- "pin-project 0.4.23 (registry+https://github.com/rust-lang/crates.io-index)",
+- "scoped-tls 1.0.0 (registry+https://github.com/rust-lang/crates.io-index)",
+- "serde 1.0.116 (registry+https://github.com/rust-lang/crates.io-index)",
+- "serde_json 1.0.57 (registry+https://github.com/rust-lang/crates.io-index)",
+- "serde_urlencoded 0.6.1 (registry+https://github.com/rust-lang/crates.io-index)",
+- "tokio 0.2.22 (registry+https://github.com/rust-lang/crates.io-index)",
+- "tokio-tungstenite 0.11.0 (registry+https://github.com/rust-lang/crates.io-index)",
+- "tower-service 0.3.0 (registry+https://github.com/rust-lang/crates.io-index)",
+- "tracing 0.1.19 (registry+https://github.com/rust-lang/crates.io-index)",
+- "tracing-futures 0.2.4 (registry+https://github.com/rust-lang/crates.io-index)",
+- "urlencoding 1.1.1 (registry+https://github.com/rust-lang/crates.io-index)",
+-]
+-
+-[[package]]
+-name = "wasi"
+-version = "0.9.0+wasi-snapshot-preview1"
+-source = "registry+https://github.com/rust-lang/crates.io-index"
+-
+-[[package]]
+-name = "wasi"
+-version = "0.10.0+wasi-snapshot-preview1"
+-source = "registry+https://github.com/rust-lang/crates.io-index"
+-
+-[[package]]
+-name = "which"
+-version = "3.1.1"
+-source = "registry+https://github.com/rust-lang/crates.io-index"
+-dependencies = [
+- "libc 0.2.73 (registry+https://github.com/rust-lang/crates.io-index)",
+-]
+-
+-[[package]]
+-name = "winapi"
+-version = "0.2.8"
+-source = "registry+https://github.com/rust-lang/crates.io-index"
+-
+-[[package]]
+-name = "winapi"
+-version = "0.3.9"
+-source = "registry+https://github.com/rust-lang/crates.io-index"
+-dependencies = [
+- "winapi-i686-pc-windows-gnu 0.4.0 (registry+https://github.com/rust-lang/crates.io-index)",
+- "winapi-x86_64-pc-windows-gnu 0.4.0 (registry+https://github.com/rust-lang/crates.io-index)",
+-]
+-
+-[[package]]
+-name = "winapi-build"
+-version = "0.1.1"
+-source = "registry+https://github.com/rust-lang/crates.io-index"
+-
+-[[package]]
+-name = "winapi-i686-pc-windows-gnu"
+-version = "0.4.0"
+-source = "registry+https://github.com/rust-lang/crates.io-index"
+-
+-[[package]]
+-name = "winapi-util"
+-version = "0.1.5"
+-source = "registry+https://github.com/rust-lang/crates.io-index"
+-dependencies = [
+- "winapi 0.3.9 (registry+https://github.com/rust-lang/crates.io-index)",
+-]
+-
+-[[package]]
+-name = "winapi-x86_64-pc-windows-gnu"
+-version = "0.4.0"
+-source = "registry+https://github.com/rust-lang/crates.io-index"
+-
+-[[package]]
+-name = "ws2_32-sys"
+-version = "0.2.1"
+-source = "registry+https://github.com/rust-lang/crates.io-index"
+-dependencies = [
+- "winapi 0.2.8 (registry+https://github.com/rust-lang/crates.io-index)",
+- "winapi-build 0.1.1 (registry+https://github.com/rust-lang/crates.io-index)",
+-]
+-
+-[metadata]
+-"checksum aho-corasick 0.7.13 (registry+https://github.com/rust-lang/crates.io-index)" = "043164d8ba5c4c3035fec9bbee8647c0261d788f3474306f93bb65901cae0e86"
+-"checksum ansi_term 0.11.0 (registry+https://github.com/rust-lang/crates.io-index)" = "ee49baf6cb617b853aa8d93bf420db2383fab46d314482ca2803b40d5fde979b"
+-"checksum assert_matches 1.3.0 (registry+https://github.com/rust-lang/crates.io-index)" = "7deb0a829ca7bcfaf5da70b073a8d128619259a7be8216a355e23f00763059e5"
+-"checksum atty 0.2.14 (registry+https://github.com/rust-lang/crates.io-index)" = "d9b39be18770d11421cdb1b9947a45dd3f37e93092cbf377614828a319d5fee8"
+-"checksum autocfg 0.1.7 (registry+https://github.com/rust-lang/crates.io-index)" = "1d49d90015b3c36167a20fe2810c5cd875ad504b39cff3d4eae7977e6b7c1cb2"
+-"checksum autocfg 1.0.1 (registry+https://github.com/rust-lang/crates.io-index)" = "cdb031dd78e28731d87d56cc8ffef4a8f36ca26c38fe2de700543e627f8a464a"
+-"checksum base64 0.10.1 (registry+https://github.com/rust-lang/crates.io-index)" = "0b25d992356d2eb0ed82172f5248873db5560c4721f564b13cb5193bda5e668e"
+-"checksum base64 0.12.3 (registry+https://github.com/rust-lang/crates.io-index)" = "3441f0f7b02788e948e47f457ca01f1d7e6d92c693bc132c22b087d3141c03ff"
+-"checksum bindgen 0.51.1 (registry+https://github.com/rust-lang/crates.io-index)" = "ebd71393f1ec0509b553aa012b9b58e81dadbdff7130bd3b8cba576e69b32f75"
+-"checksum bitflags 1.2.1 (registry+https://github.com/rust-lang/crates.io-index)" = "cf1de2fe8c75bc145a2f577add951f8134889b4795d47466a54a5c846d691693"
+-"checksum block-buffer 0.7.3 (registry+https://github.com/rust-lang/crates.io-index)" = "c0940dc441f31689269e10ac70eb1002a3a1d3ad1390e030043662eb7fe4688b"
+-"checksum block-buffer 0.9.0 (registry+https://github.com/rust-lang/crates.io-index)" = "4152116fd6e9dadb291ae18fc1ec3575ed6d84c29642d97890f4b4a3417297e4"
+-"checksum block-padding 0.1.5 (registry+https://github.com/rust-lang/crates.io-index)" = "fa79dedbb091f449f1f39e53edf88d5dbe95f895dae6135a8d7b881fb5af73f5"
+-"checksum buf_redux 0.8.4 (registry+https://github.com/rust-lang/crates.io-index)" = "b953a6887648bb07a535631f2bc00fbdb2a2216f135552cb3f534ed136b9c07f"
+-"checksum byte-tools 0.3.1 (registry+https://github.com/rust-lang/crates.io-index)" = "e3b5ca7a04898ad4bcd41c90c5285445ff5b791899bb1b0abdd2a2aa791211d7"
+-"checksum byteorder 1.3.4 (registry+https://github.com/rust-lang/crates.io-index)" = "08c48aae112d48ed9f069b33538ea9e3e90aa263cfa3d1c24309612b1f7472de"
+-"checksum bytes 0.5.6 (registry+https://github.com/rust-lang/crates.io-index)" = "0e4cec68f03f32e44924783795810fa50a7035d8c8ebe78580ad7e6c703fba38"
+-"checksum cc 1.0.58 (registry+https://github.com/rust-lang/crates.io-index)" = "f9a06fb2e53271d7c279ec1efea6ab691c35a2ae67ec0d91d7acec0caf13b518"
+-"checksum cexpr 0.3.6 (registry+https://github.com/rust-lang/crates.io-index)" = "fce5b5fb86b0c57c20c834c1b412fd09c77c8a59b9473f86272709e78874cd1d"
+-"checksum cfg-if 0.1.10 (registry+https://github.com/rust-lang/crates.io-index)" = "4785bdd1c96b2a846b2bd7cc02e86b6b3dbf14e7e53446c4f54c92a361040822"
+-"checksum clang-sys 0.28.1 (registry+https://github.com/rust-lang/crates.io-index)" = "81de550971c976f176130da4b2978d3b524eaa0fd9ac31f3ceb5ae1231fb4853"
+-"checksum clap 2.33.1 (registry+https://github.com/rust-lang/crates.io-index)" = "bdfa80d47f954d53a35a64987ca1422f495b8d6483c0fe9f7117b36c2a792129"
+-"checksum cloudabi 0.0.3 (registry+https://github.com/rust-lang/crates.io-index)" = "ddfc5b9aa5d4507acaf872de71051dfd0e309860e88966e1051e462a077aac4f"
+-"checksum core-foundation 0.9.0 (registry+https://github.com/rust-lang/crates.io-index)" = "3b5ed8e7e76c45974e15e41bfa8d5b0483cd90191639e01d8f5f1e606299d3fb"
+-"checksum core-foundation-sys 0.8.0 (registry+https://github.com/rust-lang/crates.io-index)" = "9a21fa21941700a3cd8fcb4091f361a6a712fac632f85d9f487cc892045d55c6"
+-"checksum cpuid-bool 0.1.2 (registry+https://github.com/rust-lang/crates.io-index)" = "8aebca1129a03dc6dc2b127edd729435bbc4a37e1d5f4d7513165089ceb02634"
+-"checksum devd-rs 0.3.1 (registry+https://github.com/rust-lang/crates.io-index)" = "1945ccb7caedabdfb9347766ead740fb1e0582b7425598325f546adbd832cce1"
+-"checksum digest 0.8.1 (registry+https://github.com/rust-lang/crates.io-index)" = "f3d0c8c8752312f9713efd397ff63acb9f85585afbf179282e720e7704954dd5"
+-"checksum digest 0.9.0 (registry+https://github.com/rust-lang/crates.io-index)" = "d3dd60d1080a57a05ab032377049e0591415d2b31afd7028356dbf3cc6dcb066"
+-"checksum dtoa 0.4.6 (registry+https://github.com/rust-lang/crates.io-index)" = "134951f4028bdadb9b84baf4232681efbf277da25144b9b0ad65df75946c422b"
+-"checksum env_logger 0.6.2 (registry+https://github.com/rust-lang/crates.io-index)" = "aafcde04e90a5226a6443b7aabdb016ba2f8307c847d524724bd9b346dd1a2d3"
+-"checksum fake-simd 0.1.2 (registry+https://github.com/rust-lang/crates.io-index)" = "e88a8acf291dafb59c2d96e8f59828f3838bb1a70398823ade51a84de6a6deed"
+-"checksum fnv 1.0.7 (registry+https://github.com/rust-lang/crates.io-index)" = "3f9eec918d3f24069decb9af1554cad7c880e2da24a9afd88aca000531ab82c1"
+-"checksum fuchsia-cprng 0.1.1 (registry+https://github.com/rust-lang/crates.io-index)" = "a06f77d526c1a601b7c4cdd98f54b5eaabffc14d5f2f0296febdc7f357c6d3ba"
+-"checksum fuchsia-zircon 0.3.3 (registry+https://github.com/rust-lang/crates.io-index)" = "2e9763c69ebaae630ba35f74888db465e49e259ba1bc0eda7d06f4a067615d82"
+-"checksum fuchsia-zircon-sys 0.3.3 (registry+https://github.com/rust-lang/crates.io-index)" = "3dcaa9ae7725d12cdb85b3ad99a434db70b468c09ded17e012d86b5c1010f7a7"
+-"checksum futures 0.3.5 (registry+https://github.com/rust-lang/crates.io-index)" = "1e05b85ec287aac0dc34db7d4a569323df697f9c55b99b15d6b4ef8cde49f613"
+-"checksum futures-channel 0.3.5 (registry+https://github.com/rust-lang/crates.io-index)" = "f366ad74c28cca6ba456d95e6422883cfb4b252a83bed929c83abfdbbf2967d5"
+-"checksum futures-core 0.3.5 (registry+https://github.com/rust-lang/crates.io-index)" = "59f5fff90fd5d971f936ad674802482ba441b6f09ba5e15fd8b39145582ca399"
+-"checksum futures-io 0.3.5 (registry+https://github.com/rust-lang/crates.io-index)" = "de27142b013a8e869c14957e6d2edeef89e97c289e69d042ee3a49acd8b51789"
+-"checksum futures-sink 0.3.5 (registry+https://github.com/rust-lang/crates.io-index)" = "3f2032893cb734c7a05d85ce0cc8b8c4075278e93b24b66f9de99d6eb0fa8acc"
+-"checksum futures-task 0.3.5 (registry+https://github.com/rust-lang/crates.io-index)" = "bdb66b5f09e22019b1ab0830f7785bcea8e7a42148683f99214f73f8ec21a626"
+-"checksum futures-util 0.3.5 (registry+https://github.com/rust-lang/crates.io-index)" = "8764574ff08b701a084482c3c7031349104b07ac897393010494beaa18ce32c6"
+-"checksum generic-array 0.12.3 (registry+https://github.com/rust-lang/crates.io-index)" = "c68f0274ae0e023facc3c97b2e00f076be70e254bc851d972503b328db79b2ec"
+-"checksum generic-array 0.14.4 (registry+https://github.com/rust-lang/crates.io-index)" = "501466ecc8a30d1d3b7fc9229b122b2ce8ed6e9d9223f1138d4babb253e51817"
+-"checksum getopts 0.2.21 (registry+https://github.com/rust-lang/crates.io-index)" = "14dbbfd5c71d70241ecf9e6f13737f7b5ce823821063188d7e46c41d371eebd5"
+-"checksum getrandom 0.1.14 (registry+https://github.com/rust-lang/crates.io-index)" = "7abc8dd8451921606d809ba32e95b6111925cd2906060d2dcc29c070220503eb"
+-"checksum glob 0.3.0 (registry+https://github.com/rust-lang/crates.io-index)" = "9b919933a397b79c37e33b77bb2aa3dc8eb6e165ad809e58ff75bc7db2e34574"
+-"checksum h2 0.2.6 (registry+https://github.com/rust-lang/crates.io-index)" = "993f9e0baeed60001cf565546b0d3dbe6a6ad23f2bd31644a133c641eccf6d53"
+-"checksum hashbrown 0.9.0 (registry+https://github.com/rust-lang/crates.io-index)" = "00d63df3d41950fb462ed38308eea019113ad1508da725bbedcd0fa5a85ef5f7"
+-"checksum headers 0.3.2 (registry+https://github.com/rust-lang/crates.io-index)" = "ed18eb2459bf1a09ad2d6b1547840c3e5e62882fa09b9a6a20b1de8e3228848f"
+-"checksum headers-core 0.2.0 (registry+https://github.com/rust-lang/crates.io-index)" = "e7f66481bfee273957b1f20485a4ff3362987f85b2c236580d81b4eb7a326429"
+-"checksum hermit-abi 0.1.15 (registry+https://github.com/rust-lang/crates.io-index)" = "3deed196b6e7f9e44a2ae8d94225d80302d81208b1bb673fd21fe634645c85a9"
+-"checksum http 0.2.1 (registry+https://github.com/rust-lang/crates.io-index)" = "28d569972648b2c512421b5f2a405ad6ac9666547189d0c5477a3f200f3e02f9"
+-"checksum http-body 0.3.1 (registry+https://github.com/rust-lang/crates.io-index)" = "13d5ff830006f7646652e057693569bfe0d51760c0085a071769d142a205111b"
+-"checksum httparse 1.3.4 (registry+https://github.com/rust-lang/crates.io-index)" = "cd179ae861f0c2e53da70d892f5f3029f9594be0c41dc5269cd371691b1dc2f9"
+-"checksum humantime 1.3.0 (registry+https://github.com/rust-lang/crates.io-index)" = "df004cfca50ef23c36850aaaa59ad52cc70d0e90243c3c7737a4dd32dc7a3c4f"
+-"checksum hyper 0.13.7 (registry+https://github.com/rust-lang/crates.io-index)" = "3e68a8dd9716185d9e64ea473ea6ef63529252e3e27623295a0378a19665d5eb"
+-"checksum idna 0.2.0 (registry+https://github.com/rust-lang/crates.io-index)" = "02e2673c30ee86b5b96a9cb52ad15718aa1f966f5ab9ad54a8b95d5ca33120a9"
+-"checksum indexmap 1.6.0 (registry+https://github.com/rust-lang/crates.io-index)" = "55e2e4c765aa53a0424761bf9f41aa7a6ac1efa87238f59560640e27fca028f2"
+-"checksum input_buffer 0.3.1 (registry+https://github.com/rust-lang/crates.io-index)" = "19a8a95243d5a0398cae618ec29477c6e3cb631152be5c19481f80bc71559754"
+-"checksum iovec 0.1.4 (registry+https://github.com/rust-lang/crates.io-index)" = "b2b3ea6ff95e175473f8ffe6a7eb7c00d054240321b84c57051175fe3c1e075e"
+-"checksum itoa 0.4.6 (registry+https://github.com/rust-lang/crates.io-index)" = "dc6f3ad7b9d11a0c00842ff8de1b60ee58661048eb8049ed33c73594f359d7e6"
+-"checksum kernel32-sys 0.2.2 (registry+https://github.com/rust-lang/crates.io-index)" = "7507624b29483431c0ba2d82aece8ca6cdba9382bff4ddd0f7490560c056098d"
+-"checksum lazy_static 1.4.0 (registry+https://github.com/rust-lang/crates.io-index)" = "e2abad23fbc42b3700f2f279844dc832adb2b2eb069b2df918f455c4e18cc646"
+-"checksum libc 0.2.73 (registry+https://github.com/rust-lang/crates.io-index)" = "bd7d4bd64732af4bf3a67f367c27df8520ad7e230c5817b8ff485864d80242b9"
+-"checksum libloading 0.5.2 (registry+https://github.com/rust-lang/crates.io-index)" = "f2b111a074963af1d37a139918ac6d49ad1d0d5e47f72fd55388619691a7d753"
+-"checksum libudev 0.2.0 (registry+https://github.com/rust-lang/crates.io-index)" = "ea626d3bdf40a1c5aee3bcd4f40826970cae8d80a8fec934c82a63840094dcfe"
+-"checksum libudev-sys 0.1.4 (registry+https://github.com/rust-lang/crates.io-index)" = "3c8469b4a23b962c1396b9b451dda50ef5b283e8dd309d69033475fa9b334324"
+-"checksum log 0.4.11 (registry+https://github.com/rust-lang/crates.io-index)" = "4fabed175da42fed1fa0746b0ea71f412aa9d35e76e95e59b192c64b9dc2bf8b"
+-"checksum matches 0.1.8 (registry+https://github.com/rust-lang/crates.io-index)" = "7ffc5c5338469d4d3ea17d269fa8ea3512ad247247c30bd2df69e68309ed0a08"
+-"checksum memchr 2.3.3 (registry+https://github.com/rust-lang/crates.io-index)" = "3728d817d99e5ac407411fa471ff9800a778d88a24685968b36824eaf4bee400"
+-"checksum mime 0.3.16 (registry+https://github.com/rust-lang/crates.io-index)" = "2a60c7ce501c71e03a9c9c0d35b861413ae925bd979cc7a4e30d060069aaac8d"
+-"checksum mime_guess 2.0.3 (registry+https://github.com/rust-lang/crates.io-index)" = "2684d4c2e97d99848d30b324b00c8fcc7e5c897b7cbb5819b09e7c90e8baf212"
+-"checksum mio 0.6.22 (registry+https://github.com/rust-lang/crates.io-index)" = "fce347092656428bc8eaf6201042cb551b8d67855af7374542a92a0fbfcac430"
+-"checksum miow 0.2.1 (registry+https://github.com/rust-lang/crates.io-index)" = "8c1f2f3b1cf331de6896aabf6e9d55dca90356cc9960cca7eaaf408a355ae919"
+-"checksum multipart 0.17.0 (registry+https://github.com/rust-lang/crates.io-index)" = "8209c33c951f07387a8497841122fc6f712165e3f9bda3e6be4645b58188f676"
+-"checksum net2 0.2.35 (registry+https://github.com/rust-lang/crates.io-index)" = "3ebc3ec692ed7c9a255596c67808dee269f64655d8baf7b4f0638e51ba1d6853"
+-"checksum nom 4.2.3 (registry+https://github.com/rust-lang/crates.io-index)" = "2ad2a91a8e869eeb30b9cb3119ae87773a8f4ae617f41b1eb9c154b2905f7bd6"
+-"checksum nom 5.1.2 (registry+https://github.com/rust-lang/crates.io-index)" = "ffb4262d26ed83a1c0a33a38fe2bb15797329c85770da05e6b828ddb782627af"
+-"checksum once_cell 1.4.1 (registry+https://github.com/rust-lang/crates.io-index)" = "260e51e7efe62b592207e9e13a68e43692a7a279171d6ba57abd208bf23645ad"
+-"checksum opaque-debug 0.2.3 (registry+https://github.com/rust-lang/crates.io-index)" = "2839e79665f131bdb5782e51f2c6c9599c133c6098982a54c794358bf432529c"
+-"checksum opaque-debug 0.3.0 (registry+https://github.com/rust-lang/crates.io-index)" = "624a8340c38c1b80fd549087862da4ba43e08858af025b236e509b6649fc13d5"
+-"checksum peeking_take_while 0.1.2 (registry+https://github.com/rust-lang/crates.io-index)" = "19b17cddbe7ec3f8bc800887bab5e717348c95ea2ca0b1bf0837fb964dc67099"
+-"checksum percent-encoding 2.1.0 (registry+https://github.com/rust-lang/crates.io-index)" = "d4fd5641d01c8f18a23da7b6fe29298ff4b55afcccdf78973b24cf3175fee32e"
+-"checksum pin-project 0.4.23 (registry+https://github.com/rust-lang/crates.io-index)" = "ca4433fff2ae79342e497d9f8ee990d174071408f28f726d6d83af93e58e48aa"
+-"checksum pin-project-internal 0.4.23 (registry+https://github.com/rust-lang/crates.io-index)" = "2c0e815c3ee9a031fdf5af21c10aa17c573c9c6a566328d99e3936c34e36461f"
+-"checksum pin-project-lite 0.1.7 (registry+https://github.com/rust-lang/crates.io-index)" = "282adbf10f2698a7a77f8e983a74b2d18176c19a7fd32a45446139ae7b02b715"
+-"checksum pin-utils 0.1.0 (registry+https://github.com/rust-lang/crates.io-index)" = "8b870d8c151b6f2fb93e84a13146138f05d02ed11c7e7c54f8826aaaf7c9f184"
+-"checksum pkg-config 0.3.18 (registry+https://github.com/rust-lang/crates.io-index)" = "d36492546b6af1463394d46f0c834346f31548646f6ba10849802c9c9a27ac33"
+-"checksum ppv-lite86 0.2.8 (registry+https://github.com/rust-lang/crates.io-index)" = "237a5ed80e274dbc66f86bd59c1e25edc039660be53194b5fe0a482e0f2612ea"
+-"checksum proc-macro2 1.0.19 (registry+https://github.com/rust-lang/crates.io-index)" = "04f5f085b5d71e2188cb8271e5da0161ad52c3f227a661a3c135fdf28e258b12"
+-"checksum quick-error 1.2.3 (registry+https://github.com/rust-lang/crates.io-index)" = "a1d01941d82fa2ab50be1e79e6714289dd7cde78eba4c074bc5a4374f650dfe0"
+-"checksum quote 1.0.7 (registry+https://github.com/rust-lang/crates.io-index)" = "aa563d17ecb180e500da1cfd2b028310ac758de548efdd203e18f283af693f37"
+-"checksum rand 0.6.5 (registry+https://github.com/rust-lang/crates.io-index)" = "6d71dacdc3c88c1fde3885a3be3fbab9f35724e6ce99467f7d9c5026132184ca"
+-"checksum rand 0.7.3 (registry+https://github.com/rust-lang/crates.io-index)" = "6a6b1679d49b24bbfe0c803429aa1874472f50d9b363131f0e89fc356b544d03"
+-"checksum rand_chacha 0.1.1 (registry+https://github.com/rust-lang/crates.io-index)" = "556d3a1ca6600bfcbab7c7c91ccb085ac7fbbcd70e008a98742e7847f4f7bcef"
+-"checksum rand_chacha 0.2.2 (registry+https://github.com/rust-lang/crates.io-index)" = "f4c8ed856279c9737206bf725bf36935d8666ead7aa69b52be55af369d193402"
+-"checksum rand_core 0.3.1 (registry+https://github.com/rust-lang/crates.io-index)" = "7a6fdeb83b075e8266dcc8762c22776f6877a63111121f5f8c7411e5be7eed4b"
+-"checksum rand_core 0.4.2 (registry+https://github.com/rust-lang/crates.io-index)" = "9c33a3c44ca05fa6f1807d8e6743f3824e8509beca625669633be0acbdf509dc"
+-"checksum rand_core 0.5.1 (registry+https://github.com/rust-lang/crates.io-index)" = "90bde5296fc891b0cef12a6d03ddccc162ce7b2aff54160af9338f8d40df6d19"
+-"checksum rand_hc 0.1.0 (registry+https://github.com/rust-lang/crates.io-index)" = "7b40677c7be09ae76218dc623efbf7b18e34bced3f38883af07bb75630a21bc4"
+-"checksum rand_hc 0.2.0 (registry+https://github.com/rust-lang/crates.io-index)" = "ca3129af7b92a17112d59ad498c6f81eaf463253766b90396d39ea7a39d6613c"
+-"checksum rand_isaac 0.1.1 (registry+https://github.com/rust-lang/crates.io-index)" = "ded997c9d5f13925be2a6fd7e66bf1872597f759fd9dd93513dd7e92e5a5ee08"
+-"checksum rand_jitter 0.1.4 (registry+https://github.com/rust-lang/crates.io-index)" = "1166d5c91dc97b88d1decc3285bb0a99ed84b05cfd0bc2341bdf2d43fc41e39b"
+-"checksum rand_os 0.1.3 (registry+https://github.com/rust-lang/crates.io-index)" = "7b75f676a1e053fc562eafbb47838d67c84801e38fc1ba459e8f180deabd5071"
+-"checksum rand_pcg 0.1.2 (registry+https://github.com/rust-lang/crates.io-index)" = "abf9b09b01790cfe0364f52bf32995ea3c39f4d2dd011eac241d2914146d0b44"
+-"checksum rand_xorshift 0.1.1 (registry+https://github.com/rust-lang/crates.io-index)" = "cbf7e9e623549b0e21f6e97cf8ecf247c1a8fd2e8a992ae265314300b2455d5c"
+-"checksum rdrand 0.4.0 (registry+https://github.com/rust-lang/crates.io-index)" = "678054eb77286b51581ba43620cc911abf02758c91f93f479767aed0f90458b2"
+-"checksum redox_syscall 0.1.57 (registry+https://github.com/rust-lang/crates.io-index)" = "41cc0f7e4d5d4544e8861606a285bb08d3e70712ccc7d2b84d7c0ccfaf4b05ce"
+-"checksum regex 1.3.9 (registry+https://github.com/rust-lang/crates.io-index)" = "9c3780fcf44b193bc4d09f36d2a3c87b251da4a046c87795a0d35f4f927ad8e6"
+-"checksum regex-syntax 0.6.18 (registry+https://github.com/rust-lang/crates.io-index)" = "26412eb97c6b088a6997e05f69403a802a92d520de2f8e63c2b65f9e0f47c4e8"
+-"checksum remove_dir_all 0.5.3 (registry+https://github.com/rust-lang/crates.io-index)" = "3acd125665422973a33ac9d3dd2df85edad0f4ae9b00dafb1a05e43a9f5ef8e7"
+-"checksum runloop 0.1.0 (registry+https://github.com/rust-lang/crates.io-index)" = "5d79b4b604167921892e84afbbaad9d5ad74e091bf6c511d9dbfb0593f09fabd"
+-"checksum rustc-hash 1.1.0 (registry+https://github.com/rust-lang/crates.io-index)" = "08d43f7aa6b08d49f382cde6a7982047c3426db949b1424bc4b7ec9ae12c6ce2"
+-"checksum ryu 1.0.5 (registry+https://github.com/rust-lang/crates.io-index)" = "71d301d4193d031abdd79ff7e3dd721168a9572ef3fe51a1517aba235bd8f86e"
+-"checksum safemem 0.3.3 (registry+https://github.com/rust-lang/crates.io-index)" = "ef703b7cb59335eae2eb93ceb664c0eb7ea6bf567079d843e09420219668e072"
+-"checksum scoped-tls 1.0.0 (registry+https://github.com/rust-lang/crates.io-index)" = "ea6a9290e3c9cf0f18145ef7ffa62d68ee0bf5fcd651017e586dc7fd5da448c2"
+-"checksum serde 1.0.116 (registry+https://github.com/rust-lang/crates.io-index)" = "96fe57af81d28386a513cbc6858332abc6117cfdb5999647c6444b8f43a370a5"
+-"checksum serde_derive 1.0.116 (registry+https://github.com/rust-lang/crates.io-index)" = "f630a6370fd8e457873b4bd2ffdae75408bc291ba72be773772a4c2a065d9ae8"
+-"checksum serde_json 1.0.57 (registry+https://github.com/rust-lang/crates.io-index)" = "164eacbdb13512ec2745fb09d51fd5b22b0d65ed294a1dcf7285a360c80a675c"
+-"checksum serde_urlencoded 0.6.1 (registry+https://github.com/rust-lang/crates.io-index)" = "9ec5d77e2d4c73717816afac02670d5c4f534ea95ed430442cad02e7a6e32c97"
+-"checksum sha-1 0.8.2 (registry+https://github.com/rust-lang/crates.io-index)" = "f7d94d0bede923b3cea61f3f1ff57ff8cdfd77b400fb8f9998949e0cf04163df"
+-"checksum sha-1 0.9.1 (registry+https://github.com/rust-lang/crates.io-index)" = "170a36ea86c864a3f16dd2687712dd6646f7019f301e57537c7f4dc9f5916770"
+-"checksum sha2 0.8.2 (registry+https://github.com/rust-lang/crates.io-index)" = "a256f46ea78a0c0d9ff00077504903ac881a1dafdc20da66545699e7776b3e69"
+-"checksum shlex 0.1.1 (registry+https://github.com/rust-lang/crates.io-index)" = "7fdf1b9db47230893d76faad238fd6097fd6d6a9245cd7a4d90dbd639536bbd2"
+-"checksum slab 0.4.2 (registry+https://github.com/rust-lang/crates.io-index)" = "c111b5bd5695e56cffe5129854aa230b39c93a305372fdbb2668ca2394eea9f8"
+-"checksum socket2 0.3.15 (registry+https://github.com/rust-lang/crates.io-index)" = "b1fa70dc5c8104ec096f4fe7ede7a221d35ae13dcd19ba1ad9a81d2cab9a1c44"
+-"checksum strsim 0.8.0 (registry+https://github.com/rust-lang/crates.io-index)" = "8ea5119cdb4c55b55d432abb513a0429384878c15dde60cc77b1c99de1a95a6a"
+-"checksum syn 1.0.41 (registry+https://github.com/rust-lang/crates.io-index)" = "6690e3e9f692504b941dc6c3b188fd28df054f7fb8469ab40680df52fdcc842b"
+-"checksum tempfile 3.1.0 (registry+https://github.com/rust-lang/crates.io-index)" = "7a6e24d9338a0a5be79593e2fa15a648add6138caa803e2d5bc782c371732ca9"
+-"checksum termcolor 1.1.0 (registry+https://github.com/rust-lang/crates.io-index)" = "bb6bfa289a4d7c5766392812c0a1f4c1ba45afa1ad47803c11e1f407d846d75f"
+-"checksum textwrap 0.11.0 (registry+https://github.com/rust-lang/crates.io-index)" = "d326610f408c7a4eb6f51c37c330e496b08506c9457c9d34287ecc38809fb060"
+-"checksum thread_local 1.0.1 (registry+https://github.com/rust-lang/crates.io-index)" = "d40c6d1b69745a6ec6fb1ca717914848da4b44ae29d9b3080cbee91d72a69b14"
+-"checksum time 0.1.44 (registry+https://github.com/rust-lang/crates.io-index)" = "6db9e6914ab8b1ae1c260a4ae7a49b6c5611b40328a735b21862567685e73255"
+-"checksum tinyvec 0.3.4 (registry+https://github.com/rust-lang/crates.io-index)" = "238ce071d267c5710f9d31451efec16c5ee22de34df17cc05e56cbc92e967117"
+-"checksum tokio 0.2.22 (registry+https://github.com/rust-lang/crates.io-index)" = "5d34ca54d84bf2b5b4d7d31e901a8464f7b60ac145a284fba25ceb801f2ddccd"
+-"checksum tokio-macros 0.2.5 (registry+https://github.com/rust-lang/crates.io-index)" = "f0c3acc6aa564495a0f2e1d59fab677cd7f81a19994cfc7f3ad0e64301560389"
+-"checksum tokio-tungstenite 0.11.0 (registry+https://github.com/rust-lang/crates.io-index)" = "6d9e878ad426ca286e4dcae09cbd4e1973a7f8987d97570e2469703dd7f5720c"
+-"checksum tokio-util 0.3.1 (registry+https://github.com/rust-lang/crates.io-index)" = "be8242891f2b6cbef26a2d7e8605133c2c554cd35b3e4948ea892d6d68436499"
+-"checksum tower-service 0.3.0 (registry+https://github.com/rust-lang/crates.io-index)" = "e987b6bf443f4b5b3b6f38704195592cca41c5bb7aedd3c3693c7081f8289860"
+-"checksum tracing 0.1.19 (registry+https://github.com/rust-lang/crates.io-index)" = "6d79ca061b032d6ce30c660fded31189ca0b9922bf483cd70759f13a2d86786c"
+-"checksum tracing-core 0.1.16 (registry+https://github.com/rust-lang/crates.io-index)" = "5bcf46c1f1f06aeea2d6b81f3c863d0930a596c86ad1920d4e5bad6dd1d7119a"
+-"checksum tracing-futures 0.2.4 (registry+https://github.com/rust-lang/crates.io-index)" = "ab7bb6f14721aa00656086e9335d363c5c8747bae02ebe32ea2c7dece5689b4c"
+-"checksum try-lock 0.2.3 (registry+https://github.com/rust-lang/crates.io-index)" = "59547bce71d9c38b83d9c0e92b6066c4253371f15005def0c30d9657f50c7642"
+-"checksum tungstenite 0.11.1 (registry+https://github.com/rust-lang/crates.io-index)" = "f0308d80d86700c5878b9ef6321f020f29b1bb9d5ff3cab25e75e23f3a492a23"
+-"checksum twoway 0.1.8 (registry+https://github.com/rust-lang/crates.io-index)" = "59b11b2b5241ba34be09c3cc85a36e56e48f9888862e19cedf23336d35316ed1"
+-"checksum typenum 1.12.0 (registry+https://github.com/rust-lang/crates.io-index)" = "373c8a200f9e67a0c95e62a4f52fbf80c23b4381c05a17845531982fa99e6b33"
+-"checksum unicase 2.6.0 (registry+https://github.com/rust-lang/crates.io-index)" = "50f37be617794602aabbeee0be4f259dc1778fabe05e2d67ee8f79326d5cb4f6"
+-"checksum unicode-bidi 0.3.4 (registry+https://github.com/rust-lang/crates.io-index)" = "49f2bd0c6468a8230e1db229cff8029217cf623c767ea5d60bfbd42729ea54d5"
+-"checksum unicode-normalization 0.1.13 (registry+https://github.com/rust-lang/crates.io-index)" = "6fb19cf769fa8c6a80a162df694621ebeb4dafb606470b2b2fce0be40a98a977"
+-"checksum unicode-width 0.1.8 (registry+https://github.com/rust-lang/crates.io-index)" = "9337591893a19b88d8d87f2cec1e73fad5cdfd10e5a6f349f498ad6ea2ffb1e3"
+-"checksum unicode-xid 0.2.1 (registry+https://github.com/rust-lang/crates.io-index)" = "f7fe0bb3479651439c9112f72b6c505038574c9fbb575ed1bf3b797fa39dd564"
+-"checksum url 2.1.1 (registry+https://github.com/rust-lang/crates.io-index)" = "829d4a8476c35c9bf0bbce5a3b23f4106f79728039b726d292bb93bc106787cb"
+-"checksum urlencoding 1.1.1 (registry+https://github.com/rust-lang/crates.io-index)" = "c9232eb53352b4442e40d7900465dfc534e8cb2dc8f18656fcb2ac16112b5593"
+-"checksum utf-8 0.7.5 (registry+https://github.com/rust-lang/crates.io-index)" = "05e42f7c18b8f902290b009cde6d651262f956c98bc51bca4cd1d511c9cd85c7"
+-"checksum vec_map 0.8.2 (registry+https://github.com/rust-lang/crates.io-index)" = "f1bddf1187be692e79c5ffeab891132dfb0f236ed36a43c7ed39f1165ee20191"
+-"checksum version_check 0.1.5 (registry+https://github.com/rust-lang/crates.io-index)" = "914b1a6776c4c929a602fafd8bc742e06365d4bcbe48c30f9cca5824f70dc9dd"
+-"checksum version_check 0.9.2 (registry+https://github.com/rust-lang/crates.io-index)" = "b5a972e5669d67ba988ce3dc826706fb0a8b01471c088cb0b6110b805cc36aed"
+-"checksum want 0.3.0 (registry+https://github.com/rust-lang/crates.io-index)" = "1ce8a968cb1cd110d136ff8b819a556d6fb6d919363c61534f6860c7eb172ba0"
+-"checksum warp 0.2.5 (registry+https://github.com/rust-lang/crates.io-index)" = "f41be6df54c97904af01aa23e613d4521eed7ab23537cede692d4058f6449407"
+-"checksum wasi 0.10.0+wasi-snapshot-preview1 (registry+https://github.com/rust-lang/crates.io-index)" = "1a143597ca7c7793eff794def352d41792a93c481eb1042423ff7ff72ba2c31f"
+-"checksum wasi 0.9.0+wasi-snapshot-preview1 (registry+https://github.com/rust-lang/crates.io-index)" = "cccddf32554fecc6acb585f82a32a72e28b48f8c4c1883ddfeeeaa96f7d8e519"
+-"checksum which 3.1.1 (registry+https://github.com/rust-lang/crates.io-index)" = "d011071ae14a2f6671d0b74080ae0cd8ebf3a6f8c9589a2cd45f23126fe29724"
+-"checksum winapi 0.2.8 (registry+https://github.com/rust-lang/crates.io-index)" = "167dc9d6949a9b857f3451275e911c3f44255842c1f7a76f33c55103a909087a"
+-"checksum winapi 0.3.9 (registry+https://github.com/rust-lang/crates.io-index)" = "5c839a674fcd7a98952e593242ea400abe93992746761e38641405d28b00f419"
+-"checksum winapi-build 0.1.1 (registry+https://github.com/rust-lang/crates.io-index)" = "2d315eee3b34aca4797b2da6b13ed88266e6d612562a0c46390af8299fc699bc"
+-"checksum winapi-i686-pc-windows-gnu 0.4.0 (registry+https://github.com/rust-lang/crates.io-index)" = "ac3b87c63620426dd9b991e5ce0329eff545bccbbb34f3be09ff6fb6ab51b7b6"
+-"checksum winapi-util 0.1.5 (registry+https://github.com/rust-lang/crates.io-index)" = "70ec6ce85bb158151cae5e5c87f95a8e97d2c0c4b001223f33a334e3ce5de178"
+-"checksum winapi-x86_64-pc-windows-gnu 0.4.0 (registry+https://github.com/rust-lang/crates.io-index)" = "712e227841d057c1ee1cd2fb22fa7e5a5461ae8e48fa2ca79ec42cfc1931183f"
+-"checksum ws2_32-sys 0.2.1 (registry+https://github.com/rust-lang/crates.io-index)" = "d59cefebd0c892fa2dd6de581e937301d8552cb44489cdff035c6187cb63fa5e"
+diff --git a/third_party/rust/authenticator/Cargo.toml b/third_party/rust/authenticator/Cargo.toml
+index 57d24bd66b948..c49befae2178c 100644
+--- a/third_party/rust/authenticator/Cargo.toml
++++ b/third_party/rust/authenticator/Cargo.toml
+@@ -1,99 +1,60 @@
+-# THIS FILE IS AUTOMATICALLY GENERATED BY CARGO
+-#
+-# When uploading crates to the registry Cargo will automatically
+-# "normalize" Cargo.toml files for maximal compatibility
+-# with all versions of Cargo and also rewrite `path` dependencies
+-# to registry (e.g., crates.io) dependencies
+-#
+-# If you believe there's an error in this file please file an
+-# issue against the rust-lang/cargo repository. If you're
+-# editing this file be aware that the upstream Cargo.toml
+-# will likely look very different (and much more reasonable)
+-
+ [package]
+-edition = "2018"
+ name = "authenticator"
+ version = "0.3.1"
+ authors = ["J.C. Jones <jc@mozilla.com>", "Tim Taubert <ttaubert@mozilla.com>", "Kyle Machulis <kyle@nonpolynomial.com>"]
+-description = "Library for interacting with CTAP1/2 security keys for Web Authentication. Used by Firefox."
+ keywords = ["ctap2", "u2f", "fido", "webauthn"]
+ categories = ["cryptography", "hardware-support", "os"]
+-license = "MPL-2.0"
+ repository = "https://github.com/mozilla/authenticator-rs/"
+-[dependencies.base64]
+-version = "^0.10"
+-optional = true
+-
+-[dependencies.bitflags]
+-version = "1.0"
+-
+-[dependencies.bytes]
+-version = "0.5"
+-features = ["serde"]
+-optional = true
+-
+-[dependencies.libc]
+-version = "0.2"
+-
+-[dependencies.log]
+-version = "0.4"
+-
+-[dependencies.rand]
+-version = "0.7"
+-
+-[dependencies.runloop]
+-version = "0.1.0"
+-
+-[dependencies.serde]
+-version = "1.0"
+-features = ["derive"]
+-optional = true
+-
+-[dependencies.serde_json]
+-version = "1.0"
+-optional = true
+-
+-[dependencies.tokio]
+-version = "0.2"
+-features = ["macros"]
+-optional = true
++license = "MPL-2.0"
++description = "Library for interacting with CTAP1/2 security keys for Web Authentication. Used by Firefox."
++edition = "2018"
+ 
+-[dependencies.warp]
+-version = "0.2.4"
+-optional = true
+-[dev-dependencies.assert_matches]
+-version = "1.2"
++[badges]
++travis-ci = { repository = "mozilla/authenticator-rs", branch = "master" }
++maintenance = { status = "actively-developed" }
+ 
+-[dev-dependencies.base64]
+-version = "^0.10"
++[features]
++binding-recompile = ["bindgen"]
++webdriver = ["base64", "bytes", "warp", "tokio", "serde", "serde_json"]
+ 
+-[dev-dependencies.env_logger]
+-version = "^0.6"
++[target.'cfg(target_os = "linux")'.dependencies]
++libudev = "^0.2"
+ 
+-[dev-dependencies.getopts]
+-version = "^0.2"
++[target.'cfg(target_os = "freebsd")'.dependencies]
++devd-rs = "0.3"
+ 
+-[dev-dependencies.sha2]
+-version = "^0.8.2"
+-[build-dependencies.bindgen]
+-version = "^0.51"
+-optional = true
++[target.'cfg(target_os = "macos")'.dependencies]
++core-foundation = "0.9"
+ 
+-[features]
+-binding-recompile = ["bindgen"]
+-webdriver = ["base64", "bytes", "warp", "tokio", "serde", "serde_json"]
+-[target."cfg(target_os = \"freebsd\")".dependencies.devd-rs]
+-version = "0.3"
+-[target."cfg(target_os = \"linux\")".dependencies.libudev]
+-version = "^0.2"
+-[target."cfg(target_os = \"macos\")".dependencies.core-foundation]
+-version = "0.9"
+-[target."cfg(target_os = \"windows\")".dependencies.winapi]
++[target.'cfg(target_os = "windows")'.dependencies.winapi]
+ version = "^0.3"
+-features = ["handleapi", "hidclass", "hidpi", "hidusage", "setupapi"]
+-[badges.maintenance]
+-status = "actively-developed"
+-
+-[badges.travis-ci]
+-branch = "master"
+-repository = "mozilla/authenticator-rs"
++features = [
++    "handleapi",
++    "hidclass",
++    "hidpi",
++    "hidusage",
++    "setupapi",
++]
++
++[build-dependencies]
++bindgen = { version = "^0.58.1", optional = true }
++
++[dependencies]
++rand = "0.7"
++log = "0.4"
++libc = "0.2"
++runloop = "0.1.0"
++bitflags = "1.0"
++tokio = { version = "0.2", optional = true, features = ["macros"] }
++warp = { version = "0.2.4", optional = true }
++serde = { version = "1.0", optional = true, features = ["derive"] }
++serde_json = { version = "1.0", optional = true }
++bytes = { version = "0.5", optional = true, features = ["serde"] }
++base64 = { version = "^0.10", optional = true }
++
++[dev-dependencies]
++sha2 = "^0.8.2"
++base64 = "^0.10"
++env_logger = "^0.6"
++getopts = "^0.2"
++assert_matches = "1.2"
+diff --git a/third_party/rust/authenticator/build.rs b/third_party/rust/authenticator/build.rs
+index 299e4df6d7331..c972d85b898ea 100644
+--- a/third_party/rust/authenticator/build.rs
++++ b/third_party/rust/authenticator/build.rs
+@@ -45,6 +45,8 @@ fn main() {
+         "ioctl_aarch64be.rs"
+     } else if cfg!(all(target_arch = "s390x", target_endian = "big")) {
+         "ioctl_s390xbe.rs"
++    } else if cfg!(all(target_arch = "riscv64", target_endian = "little")) {
++        "ioctl_riscv64.rs"
+     } else {
+         panic!("architecture not supported");
+     };
+diff --git a/third_party/rust/authenticator/src/linux/hidwrapper.rs b/third_party/rust/authenticator/src/linux/hidwrapper.rs
+index ea1a39051b63a..82aabc6301017 100644
+--- a/third_party/rust/authenticator/src/linux/hidwrapper.rs
++++ b/third_party/rust/authenticator/src/linux/hidwrapper.rs
+@@ -46,3 +46,6 @@ include!("ioctl_aarch64be.rs");
+ 
+ #[cfg(all(target_arch = "s390x", target_endian = "big"))]
+ include!("ioctl_s390xbe.rs");
++
++#[cfg(all(target_arch = "riscv64", target_endian = "little"))]
++include!("ioctl_riscv64.rs");
+diff --git a/third_party/rust/authenticator/src/linux/ioctl_riscv64.rs b/third_party/rust/authenticator/src/linux/ioctl_riscv64.rs
+new file mode 100644
+index 0000000000000..a784e9bf4600b
+--- /dev/null
++++ b/third_party/rust/authenticator/src/linux/ioctl_riscv64.rs
+@@ -0,0 +1,5 @@
++/* automatically generated by rust-bindgen */
++
++pub type __u32 = ::std::os::raw::c_uint;
++pub const _HIDIOCGRDESCSIZE: __u32 = 2147764225;
++pub const _HIDIOCGRDESC: __u32 = 2416199682;
+diff --git a/toolkit/library/rust/shared/Cargo.toml b/toolkit/library/rust/shared/Cargo.toml
+index 47ea0f7c4e00a..2c50e360158d7 100644
+--- a/toolkit/library/rust/shared/Cargo.toml
++++ b/toolkit/library/rust/shared/Cargo.toml
+@@ -25,7 +25,7 @@ cubeb-sys = { version = "0.9", optional = true, features=["gecko-in-tree"] }
+ encoding_glue = { path = "../../../../intl/encoding_glue" }
+ audioipc-client = { git = "https://github.com/mozilla/audioipc-2", rev = "7537bfadad2e981577eb75e4f13662fc517e1a09", optional = true }
+ audioipc-server = { git = "https://github.com/mozilla/audioipc-2", rev = "7537bfadad2e981577eb75e4f13662fc517e1a09", optional = true }
+-authenticator = "0.3.1"
++authenticator = { git = "https://github.com/makotokato/authenticator-rs", rev = "eed8919d50559f4959e2d7d2af7b4d48869b5366" }
+ gkrust_utils = { path = "../../../../xpcom/rust/gkrust_utils" }
+ gecko_logger = { path = "../../../../xpcom/rust/gecko_logger" }
+ rsdparsa_capi = { path = "../../../../dom/media/webrtc/sdp/rsdparsa_capi" }
+From 12413f122e2ea2a29663f5c7e38bef3896b02d9c Mon Sep 17 00:00:00 2001
+From: Makoto Kato <m_kato@ga2.so-net.ne.jp>
+Date: Mon, 14 Jun 2021 08:05:37 +0000
+Subject: [PATCH] Add XPTCAll stub
+
+---
+ xpcom/reflect/xptcall/md/unix/moz.build       |   8 +
+ .../xptcall/md/unix/xptcinvoke_asm_riscv64.S  |  72 ++++++++
+ .../xptcall/md/unix/xptcinvoke_riscv64.cpp    |  89 ++++++++++
+ .../xptcall/md/unix/xptcstubs_asm_riscv64.S   |  48 ++++++
+ .../xptcall/md/unix/xptcstubs_riscv64.cpp     | 154 ++++++++++++++++++
+ 5 files changed, 371 insertions(+)
+ create mode 100644 xpcom/reflect/xptcall/md/unix/xptcinvoke_asm_riscv64.S
+ create mode 100644 xpcom/reflect/xptcall/md/unix/xptcinvoke_riscv64.cpp
+ create mode 100644 xpcom/reflect/xptcall/md/unix/xptcstubs_asm_riscv64.S
+ create mode 100644 xpcom/reflect/xptcall/md/unix/xptcstubs_riscv64.cpp
+
+diff --git a/xpcom/reflect/xptcall/md/unix/moz.build b/xpcom/reflect/xptcall/md/unix/moz.build
+index 1083e26a2fde1..4af0b577bca86 100644
+--- a/xpcom/reflect/xptcall/md/unix/moz.build
++++ b/xpcom/reflect/xptcall/md/unix/moz.build
+@@ -263,6 +263,14 @@ if CONFIG["OS_ARCH"] == "Linux":
+                 "-fno-integrated-as",
+             ]
+ 
++if CONFIG["OS_ARCH"] == "Linux" and  CONFIG["CPU_ARCH"] == "riscv64":
++    SOURCES += [
++        "xptcinvoke_asm_riscv64.S",
++        "xptcinvoke_riscv64.cpp",
++        "xptcstubs_asm_riscv64.S",
++        "xptcstubs_riscv64.cpp",
++    ]
++
+ FINAL_LIBRARY = "xul"
+ 
+ LOCAL_INCLUDES += [
+diff --git a/xpcom/reflect/xptcall/md/unix/xptcinvoke_asm_riscv64.S b/xpcom/reflect/xptcall/md/unix/xptcinvoke_asm_riscv64.S
+new file mode 100644
+index 0000000000000..4b27dd77884bf
+--- /dev/null
++++ b/xpcom/reflect/xptcall/md/unix/xptcinvoke_asm_riscv64.S
+@@ -0,0 +1,72 @@
++/* This Source Code Form is subject to the terms of the Mozilla Public
++ * License, v. 2.0. If a copy of the MPL was not distributed with this
++ * file, You can obtain one at http://mozilla.org/MPL/2.0/. */
++
++    .set NGPREGS,8
++    .set NFPREGS,8
++
++    .text
++    .globl  _NS_InvokeByIndex
++    .type   _NS_InvokeByIndex, @function
++/*
++ * _NS_InvokeByIndex(nsISupports* that, uint32_t methodIndex,
++ *                   uint32_t paramCount, nsXPTCVariant* params)
++ */
++_NS_InvokeByIndex:
++    addi    sp, sp, -32
++    sd      s0, 0(sp)
++    sd      s1, 8(sp)
++    sd      ra, 16(sp)
++
++    mv      s0, a0 
++    mv      s1, a1 
++    mv      s2, sp 
++    mv      a4, sp
++
++    # 16-bytes alignment
++    addiw   a0, a0, 1
++    andi    a0, a0, -2
++    slli    a0, a0, 3
++    sub     sp, sp, a0
++
++    addi    sp, sp, -8*(NGPREGS+NFPREGS)
++    mv      a0, sp
++    addi    a1, sp, 8*NGPREGS
++
++    # extern "C" void invoke_copy_to_stack(uint64_t* gpregs, double* fpregs,
++    #                                 uint32_t paramCount, nsXPTCVariant* s,
++    #                                 uint64_t* d) {
++
++    call    invoke_copy_to_stack
++    addi    sp, sp, 8*(NGPREGS+NFPREGS)
++
++    ld      a1, 8(s0)
++    ld      a2, 16(s0)
++    ld      a3, 24(s0)
++    ld      a4, 32(s0)
++    ld      a5, 40(s0)
++    ld      a6, 48(s0)
++    ld      a7, 56(s0)
++
++    fld     fa0, 64(s0)
++    fld     fa1, 72(s0)
++    fld     fa2, 80(s0)
++    fld     fa3, 88(s0)
++    fld     fa4, 96(s0)
++    fld     fa5, 104(s0)
++    fld     fa6, 112(s0)
++    fld     fa7, 120(s0)
++
++    ld      t0, (s0)
++    slli    t1, s1, 3
++    add     t0, t0, t1
++    ld      t0, [t0]
++    call    t0
++
++    mv      sp, s2
++    ld      s0, 0(sp)
++    ld      s1, 8(sp)
++    addi    sp, sp, 32
++    ret
++    .size   _NS_InvokeByIndex, .-_NS_InvokeByIndex
++    .section        .note.GNU-stack,"",@progbits
+diff --git a/xpcom/reflect/xptcall/md/unix/xptcinvoke_riscv64.cpp b/xpcom/reflect/xptcall/md/unix/xptcinvoke_riscv64.cpp
+new file mode 100644
+index 0000000000000..0d88718802314
+--- /dev/null
++++ b/xpcom/reflect/xptcall/md/unix/xptcinvoke_riscv64.cpp
+@@ -0,0 +1,89 @@
++/* This Source Code Form is subject to the terms of the Mozilla Public
++ * License, v. 2.0. If a copy of the MPL was not distributed with this
++ * file, You can obtain one at http://mozilla.org/MPL/2.0/. */
++
++// Platform specific code to invoke XPCOM methods on native objects
++
++#include "xptcprivate.h"
++
++extern "C" void invoke_copy_to_stack(uint64_t* gpregs, double* fpregs,
++                                     uint32_t paramCount, nsXPTCVariant* s,
++                                     uint64_t* d) {
++  static const uint32_t GPR_COUNT = 8;
++  static const uint32_t FPR_COUNT = 8;
++
++  uint32_t nr_gpr = 0;
++  uint32_t nr_fpr = 0;
++  uint64_t value;
++
++  for (uint32_t i = 0; i < paramCount; i++, s++) {
++    if (s->IsIndirect()) {
++      value = (uint64_t)&s->val;
++    } else {
++      switch (s->type) {
++        case nsXPTType::T_FLOAT:
++          break;
++        case nsXPTType::T_DOUBLE:
++          break;
++        case nsXPTType::T_I8:
++          value = s->val.i8;
++          break;
++        case nsXPTType::T_I16:
++          value = s->val.i16;
++          break;
++        case nsXPTType::T_I32:
++          value = s->val.i32;
++          break;
++        case nsXPTType::T_I64:
++          value = s->val.i64;
++          break;
++        case nsXPTType::T_U8:
++          value = s->val.u8;
++          break;
++        case nsXPTType::T_U16:
++          value = s->val.u16;
++          break;
++        case nsXPTType::T_U32:
++          value = s->val.u32;
++          break;
++        case nsXPTType::T_U64:
++          value = s->val.u64;
++          break;
++        case nsXPTType::T_BOOL:
++          value = s->val.b;
++          break;
++        case nsXPTType::T_CHAR:
++          value = s->val.c;
++          break;
++        case nsXPTType::T_WCHAR:
++          value = s->val.wc;
++          break;
++        default:
++          value = (uint64_t)s->val.p;
++          break;
++      }
++    }
++
++    if (!s->IsIndirect() && s->type == nsXPTType::T_DOUBLE) {
++      if (nr_fpr < FPR_COUNT) {
++        fpregs[nr_fpr++] = s->val.d;
++      } else {
++        *((double*)d) = s->val.d;
++        d++;
++      }
++    } else if (!s->IsIndirect() && s->type == nsXPTType::T_FLOAT) {
++      if (nr_fpr < FPR_COUNT) {
++        fpregs[nr_fpr++] = s->val.d;
++      } else {
++        *((float*)d) = s->val.f;
++        d++;
++      }
++    } else {
++      if (nr_gpr < GPR_COUNT) {
++        gpregs[nr_gpr++] = value;
++      } else {
++        *d++ = value;
++      }
++    }
++  }
++}
+diff --git a/xpcom/reflect/xptcall/md/unix/xptcstubs_asm_riscv64.S b/xpcom/reflect/xptcall/md/unix/xptcstubs_asm_riscv64.S
+new file mode 100644
+index 0000000000000..cd15f9fa47280
+--- /dev/null
++++ b/xpcom/reflect/xptcall/md/unix/xptcstubs_asm_riscv64.S
+@@ -0,0 +1,48 @@
++# This Source Code Form is subject to the terms of the Mozilla Public
++# License, v. 2.0. If a copy of the MPL was not distributed with this
++# file, You can obtain one at http://mozilla.org/MPL/2.0/.
++
++    .set NGPREGS,8
++    .set NFPREGS,8
++
++    .text
++    .globl SharedStub
++    .hidden SharedStub
++    .type  SharedStub,@function
++
++SharedStub:
++    .cfi_startproc
++    addi    sp, sp, -8*(NGPREGS+NFPREGS)-16
++    .cfi_adjust_cfa_offset 8*(NGPREGS+NFPREGS)+16
++    sd      a0, 0(sp)
++    sd      a1, 8(sp)
++    sd      a2, 16(sp)
++    sd      a3, 24(sp)
++    sd      a4, 32(sp)
++    sd      a5, 40(sp)
++    sd      a6, 48(sp)
++    sd      a7, 56(sp)
++    fsd     fa0, 64(sp)
++    fsd     fa1, 72(sp)
++    fsd     fa2, 80(sp)
++    fsd     fa3, 88(sp)
++    fsd     fa4, 96(sp)
++    fsd     fa5, 104(sp)
++    fsd     fa6, 112(sp)
++    fsd     fa7, 128(sp)
++
++    # methodIndex passed from stub
++    mv      a1, t0
++    addi    a2, sp, 8*(NGPREGS+NFPREGS)+16+8
++    mv      a3, sp
++    addi    a4, sp, 64
++
++    call    _PrepareAndDispatch
++
++    addi    sp, sp, 8*(NGPREGS+NFPREGS)+16
++    .cfi_adjust_cfa_offset -8*(NGPREGS+NFPREGS)-16
++    ret
++    .cfi_endproc
++
++    .size SharedStub, . - SharedStub
++    .section .note.GNU-stack, "", @progbits
+diff --git a/xpcom/reflect/xptcall/md/unix/xptcstubs_riscv64.cpp b/xpcom/reflect/xptcall/md/unix/xptcstubs_riscv64.cpp
+new file mode 100644
+index 0000000000000..3b2de5c934d09
+--- /dev/null
++++ b/xpcom/reflect/xptcall/md/unix/xptcstubs_riscv64.cpp
+@@ -0,0 +1,154 @@
++/* -*- Mode: C; tab-width: 8; indent-tabs-mode: nil; c-basic-offset: 2 -*- */
++/* This Source Code Form is subject to the terms of the Mozilla Public
++ * License, v. 2.0. If a copy of the MPL was not distributed with this
++ * file, You can obtain one at http://mozilla.org/MPL/2.0/. */
++
++#include "xptcprivate.h"
++
++extern "C" nsresult ATTRIBUTE_USED PrepareAndDispatch(nsXPTCStubBase* self,
++                                                      uint32_t methodIndex,
++                                                      uint64_t* args,
++                                                      uint64_t* gpregs,
++                                                      double* fpregs) {
++  static const uint32_t GPR_COUNT = 8;
++  static const uint32_t FPR_COUNT = 8;
++  nsXPTCMiniVariant paramBuffer[PARAM_BUFFER_COUNT];
++  const nsXPTMethodInfo* info;
++
++  self->mEntry->GetMethodInfo(uint16_t(methodIndex), &info);
++
++  uint32_t paramCount = info->GetParamCount();
++  const uint8_t indexOfJSContext = info->IndexOfJSContext();
++
++  uint64_t* ap = args;
++  uint32_t nr_gpr = 1;  // skip one GPR register for 'that'
++  uint32_t nr_fpr = 0;
++  uint64_t value;
++
++  for (uint32_t i = 0; i < paramCount; i++) {
++    const nsXPTParamInfo& param = info->GetParam(i);
++    const nsXPTType& type = param.GetType();
++    nsXPTCMiniVariant* dp = &paramBuffer[i];
++
++    if (i == indexOfJSContext) {
++      if (nr_gpr < GPR_COUNT)
++        nr_gpr++;
++      else
++        ap++;
++    }
++
++    if (!param.IsOut() && type == nsXPTType::T_DOUBLE) {
++      if (nr_fpr < FPR_COUNT) {
++        dp->val.d = fpregs[nr_fpr++];
++      } else {
++        dp->val.d = *(double*)ap++;
++      }
++      continue;
++    }
++
++    if (!param.IsOut() && type == nsXPTType::T_FLOAT) {
++      if (nr_fpr < FPR_COUNT) {
++        dp->val.d = fpregs[nr_fpr++];
++      } else {
++        dp->val.f = *(float*)ap++;
++      }
++      continue;
++    }
++
++    if (nr_gpr < GPR_COUNT) {
++      value = gpregs[nr_gpr++];
++    } else {
++      value = *ap++;
++    }
++
++    if (param.IsOut() || !type.IsArithmetic()) {
++      dp->val.p = (void*)value;
++      continue;
++    }
++
++    switch (type) {
++      case nsXPTType::T_I8:
++        dp->val.i8 = (int8_t)value;
++        break;
++      case nsXPTType::T_I16:
++        dp->val.i16 = (int16_t)value;
++        break;
++      case nsXPTType::T_I32:
++        dp->val.i32 = (int32_t)value;
++        break;
++      case nsXPTType::T_I64:
++        dp->val.i64 = (int64_t)value;
++        break;
++      case nsXPTType::T_U8:
++        dp->val.u8 = (uint8_t)value;
++        break;
++      case nsXPTType::T_U16:
++        dp->val.u16 = (uint16_t)value;
++        break;
++      case nsXPTType::T_U32:
++        dp->val.u32 = (uint32_t)value;
++        break;
++      case nsXPTType::T_U64:
++        dp->val.u64 = (uint64_t)value;
++        break;
++      case nsXPTType::T_BOOL:
++        dp->val.b = (bool)(uint8_t)value;
++        break;
++      case nsXPTType::T_CHAR:
++        dp->val.c = (char)value;
++        break;
++      case nsXPTType::T_WCHAR:
++        dp->val.wc = (wchar_t)value;
++        break;
++      default:
++        NS_ERROR("bad type");
++        break;
++    }
++  }
++
++  nsresult result =
++      self->mOuter->CallMethod((uint16_t)methodIndex, info, paramBuffer);
++
++  return result;
++}
++
++// Load t0 with the constant 'n' and branch to SharedStub().
++#define STUB_ENTRY(n)                                               \
++  __asm__(                                                          \
++      ".text\n\t"                                                   \
++      ".if "#n" < 10 \n\t"                                          \
++      ".globl  _ZN14nsXPTCStubBase5Stub"#n"Ev \n\t"                                 \
++      ".hidden _ZN14nsXPTCStubBase5Stub" #n "Ev \n\t"               \
++      ".type   _ZN14nsXPTCStubBase5Stub" #n "Ev,@function \n\n"     \
++      "_ZN14nsXPTCStubBase5Stub"#n"Ev: \n\t"                        \
++      ".elseif "#n" < 100 \n\t"                                                     \
++      ".globl  _ZN14nsXPTCStubBase6Stub"#n"Ev \n\t"                                 \
++      ".hidden _ZN14nsXPTCStubBase6Stub"#n"Ev \n\t"                                 \
++      ".type   _ZN14nsXPTCStubBase6Stub"#n"Ev,@function \n\n"                       \
++      "_ZN14nsXPTCStubBase6Stub"#n"Ev: \n\t"                                        \
++      ".elseif "#n" < 1000 \n\t"                                                    \
++      ".globl  _ZN14nsXPTCStubBase7Stub"#n"Ev \n\t"                                 \
++      ".hidden _ZN14nsXPTCStubBase7Stub"#n"Ev \n\t"                                 \
++      ".type   _ZN14nsXPTCStubBase7Stub"#n"Ev,@function \n\n"                       \
++      "_ZN14nsXPTCStubBase7Stub"#n"Ev: \n\t"                                        \
++      ".else  \n\t"                                                                 \
++      ".err   \"stub number "#n" >= 1000 not yet supported\"\n"                     \
++      ".endif \n\t"                                                                 \
++      "li      t0, "#n" \n\t"                                                       \
++      "j       SharedStub \n"                                                       \
++      ".if "#n" < 10\n\t"                                                           \
++      ".size   _ZN14nsXPTCStubBase5Stub"#n"Ev,.-_ZN14nsXPTCStubBase5Stub"#n"Ev\n\t" \
++      ".elseif "#n" < 100\n\t"                                                      \
++      ".size   _ZN14nsXPTCStubBase6Stub"#n"Ev,.-_ZN14nsXPTCStubBase6Stub"#n"Ev\n\t" \
++      ".else\n\t"                                                                   \
++      ".size   _ZN14nsXPTCStubBase7Stub"#n"Ev,.-_ZN14nsXPTCStubBase7Stub"#n"Ev\n\t" \
++      ".endif"                                                                      \
++);
++
++#define SENTINEL_ENTRY(n)                        \
++  nsresult nsXPTCStubBase::Sentinel##n() {       \
++    NS_ERROR("nsXPTCStubBase::Sentinel called"); \
++    return NS_ERROR_NOT_IMPLEMENTED;             \
++  }
++
++#include "xptcstubsdef.inc"
+From c20ab426530a74731bb9ea9288a6bdb93309c817 Mon Sep 17 00:00:00 2001
+From: Makoto Kato <m_kato@ga2.so-net.ne.jp>
+Date: Mon, 14 Jun 2021 08:06:04 +0000
+Subject: [PATCH] NSS fix
+
+---
+ security/nss/coreconf/config.gypi | 2 +-
+ 1 file changed, 1 insertion(+), 1 deletion(-)
+
+#diff --git a/security/nss/coreconf/config.gypi b/security/nss/coreconf/config.gypi
+#index eec50ddbb74cd..5800f87915389 100644
+#--- a/security/nss/coreconf/config.gypi
+#+++ b/security/nss/coreconf/config.gypi
+#@@ -209,7 +209,7 @@
+#           },
+#         },
+#       }],
+#-      [ 'target_arch=="arm64" or target_arch=="aarch64" or target_arch=="sparc64" or target_arch=="ppc64" or target_arch=="ppc64le" or target_arch=="s390x" or target_arch=="mips64" or target_arch=="e2k"', {
+#+      [ 'target_arch=="arm64" or target_arch=="aarch64" or target_arch=="sparc64" or target_arch=="ppc64" or target_arch=="ppc64le" or target_arch=="s390x" or target_arch=="mips64" or target_arch=="e2k" or target_arch=="riscv64"', {
+#         'defines': [
+#           'NSS_USE_64',
+#         ],
+From 6a8fbc856157600fbbb2a696b300899d54cf7b65 Mon Sep 17 00:00:00 2001
+From: Makoto Kato <m_kato@ga2.so-net.ne.jp>
+Date: Mon, 14 Jun 2021 14:44:26 +0000
+Subject: [PATCH] Update xptcall
+
+---
+ .../xptcall/md/unix/xptcinvoke_asm_riscv64.S  | 30 +++++++++++--------
+ .../xptcall/md/unix/xptcinvoke_riscv64.cpp    |  4 +++
+ .../xptcall/md/unix/xptcstubs_asm_riscv64.S   |  2 +-
+ .../xptcall/md/unix/xptcstubs_riscv64.cpp     | 20 ++++++++-----
+ 4 files changed, 36 insertions(+), 20 deletions(-)
+
+diff --git a/xpcom/reflect/xptcall/md/unix/xptcinvoke_asm_riscv64.S b/xpcom/reflect/xptcall/md/unix/xptcinvoke_asm_riscv64.S
+index 4b27dd77884bf..e7d35d3aa7a9c 100644
+--- a/xpcom/reflect/xptcall/md/unix/xptcinvoke_asm_riscv64.S
++++ b/xpcom/reflect/xptcall/md/unix/xptcinvoke_asm_riscv64.S
+@@ -6,17 +6,20 @@
+     .set NFPREGS,8
+ 
+     .text
+-    .globl  _NS_InvokeByIndex
+-    .type   _NS_InvokeByIndex, @function
++    .globl  NS_InvokeByIndex
++    .type   NS_InvokeByIndex, @function
+ /*
+- * _NS_InvokeByIndex(nsISupports* that, uint32_t methodIndex,
+- *                   uint32_t paramCount, nsXPTCVariant* params)
++ * NS_InvokeByIndex(nsISupports* that, uint32_t methodIndex,
++ *                  uint32_t paramCount, nsXPTCVariant* params)
+  */
+-_NS_InvokeByIndex:
++NS_InvokeByIndex:
++    .cfi_startproc
+     addi    sp, sp, -32
++    .cfi_adjust_cfa_offset 32
+     sd      s0, 0(sp)
+     sd      s1, 8(sp)
+-    sd      ra, 16(sp)
++    sd      s2, 16(sp)
++    sd      ra, 24(sp)
+ 
+     mv      s0, a0 
+     mv      s1, a1 
+@@ -57,16 +60,19 @@ _NS_InvokeByIndex:
+     fld     fa6, 112(s0)
+     fld     fa7, 120(s0)
+ 
+-    ld      t0, (s0)
+-    slli    t1, s1, 3
+-    add     t0, t0, t1
+-    ld      t0, [t0]
+-    call    t0
++    ld      s0, (s0)
++    slli    s1, s1, 3
++    add     s0, s0, s1
++    ld      s0, [s0]
++    call    s0
+ 
+     mv      sp, s2
+     ld      s0, 0(sp)
+     ld      s1, 8(sp)
++    ld      s2, 8(sp)
++    ld      ra, 24(sp)
+     addi    sp, sp, 32
+     ret
+-    .size   _NS_InvokeByIndex, .-_NS_InvokeByIndex
++    .cfi_endproc
++    .size   NS_InvokeByIndex, .-NS_InvokeByIndex
+     .section        .note.GNU-stack,"",@progbits
+diff --git a/xpcom/reflect/xptcall/md/unix/xptcinvoke_riscv64.cpp b/xpcom/reflect/xptcall/md/unix/xptcinvoke_riscv64.cpp
+index 0d88718802314..369511d33a40b 100644
+--- a/xpcom/reflect/xptcall/md/unix/xptcinvoke_riscv64.cpp
++++ b/xpcom/reflect/xptcall/md/unix/xptcinvoke_riscv64.cpp
+@@ -4,6 +4,10 @@
+ 
+ // Platform specific code to invoke XPCOM methods on native objects
+ 
++#if defined(__riscv_float_abi_soft)
++#  error "Not support soft float ABI"
++#endif
++
+ #include "xptcprivate.h"
+ 
+ extern "C" void invoke_copy_to_stack(uint64_t* gpregs, double* fpregs,
+diff --git a/xpcom/reflect/xptcall/md/unix/xptcstubs_asm_riscv64.S b/xpcom/reflect/xptcall/md/unix/xptcstubs_asm_riscv64.S
+index cd15f9fa47280..870abd05fa7ba 100644
+--- a/xpcom/reflect/xptcall/md/unix/xptcstubs_asm_riscv64.S
++++ b/xpcom/reflect/xptcall/md/unix/xptcstubs_asm_riscv64.S
+@@ -37,7 +37,7 @@ SharedStub:
+     mv      a3, sp
+     addi    a4, sp, 64
+ 
+-    call    _PrepareAndDispatch
++    call    PrepareAndDispatch
+ 
+     addi    sp, sp, 8*(NGPREGS+NFPREGS)+16
+     .cfi_adjust_cfa_offset -8*(NGPREGS+NFPREGS)-16
+diff --git a/xpcom/reflect/xptcall/md/unix/xptcstubs_riscv64.cpp b/xpcom/reflect/xptcall/md/unix/xptcstubs_riscv64.cpp
+index 3b2de5c934d09..94c7b4ab8d089 100644
+--- a/xpcom/reflect/xptcall/md/unix/xptcstubs_riscv64.cpp
++++ b/xpcom/reflect/xptcall/md/unix/xptcstubs_riscv64.cpp
+@@ -3,6 +3,10 @@
+  * License, v. 2.0. If a copy of the MPL was not distributed with this
+  * file, You can obtain one at http://mozilla.org/MPL/2.0/. */
+ 
++#if defined(__riscv_float_abi_soft)
++#  error "Not support soft float ABI"
++#endif
++
+ #include "xptcprivate.h"
+ 
+ extern "C" nsresult ATTRIBUTE_USED PrepareAndDispatch(nsXPTCStubBase* self,
+@@ -113,14 +117,15 @@ extern "C" nsresult ATTRIBUTE_USED PrepareAndDispatch(nsXPTCStubBase* self,
+ }
+ 
+ // Load t0 with the constant 'n' and branch to SharedStub().
+-#define STUB_ENTRY(n)                                               \
+-  __asm__(                                                          \
+-      ".text\n\t"                                                   \
+-      ".if "#n" < 10 \n\t"                                          \
++// clang-format off
++#define STUB_ENTRY(n)                                                               \
++  __asm__(                                                                          \
++      ".text\n\t"                                                                   \
++      ".if "#n" < 10 \n\t"                                                          \
+       ".globl  _ZN14nsXPTCStubBase5Stub"#n"Ev \n\t"                                 \
+-      ".hidden _ZN14nsXPTCStubBase5Stub" #n "Ev \n\t"               \
+-      ".type   _ZN14nsXPTCStubBase5Stub" #n "Ev,@function \n\n"     \
+-      "_ZN14nsXPTCStubBase5Stub"#n"Ev: \n\t"                        \
++      ".hidden _ZN14nsXPTCStubBase5Stub"#n"Ev \n\t"                                 \
++      ".type   _ZN14nsXPTCStubBase5Stub"#n"Ev,@function \n\n"                       \
++      "_ZN14nsXPTCStubBase5Stub"#n"Ev: \n\t"                                        \
+       ".elseif "#n" < 100 \n\t"                                                     \
+       ".globl  _ZN14nsXPTCStubBase6Stub"#n"Ev \n\t"                                 \
+       ".hidden _ZN14nsXPTCStubBase6Stub"#n"Ev \n\t"                                 \
+@@ -144,6 +149,7 @@ extern "C" nsresult ATTRIBUTE_USED PrepareAndDispatch(nsXPTCStubBase* self,
+       ".size   _ZN14nsXPTCStubBase7Stub"#n"Ev,.-_ZN14nsXPTCStubBase7Stub"#n"Ev\n\t" \
+       ".endif"                                                                      \
+ );
++// clang-format on
+ 
+ #define SENTINEL_ENTRY(n)                        \
+   nsresult nsXPTCStubBase::Sentinel##n() {       \
+From 75685ab28c13c481a737b26ea95668461b060fac Mon Sep 17 00:00:00 2001
+From: Makoto Kato <m_kato@ga2.so-net.ne.jp>
+Date: Mon, 14 Jun 2021 16:00:08 +0000
+Subject: [PATCH] Fix highway
+
+---
+ third_party/highway/hwy/base.h | 4 ++--
+ 1 file changed, 2 insertions(+), 2 deletions(-)
+
+diff --git a/third_party/highway/hwy/base.h b/third_party/highway/hwy/base.h
+index d87eb34b8eb5d..75fe585e4c296 100644
+--- a/third_party/highway/hwy/base.h
++++ b/third_party/highway/hwy/base.h
+@@ -316,7 +316,7 @@ namespace hwy {
+ #if HWY_ARCH_X86
+ static constexpr HWY_MAYBE_UNUSED size_t kMaxVectorSize = 64;  // AVX-512
+ #define HWY_ALIGN_MAX alignas(64)
+-#elif HWY_ARCH_RVV
++#elif HWY_ARCH_RVV && defined(__riscv_vector)
+ // Not actually an upper bound on the size, but this value prevents crossing a
+ // 4K boundary (relevant on Andes).
+ static constexpr HWY_MAYBE_UNUSED size_t kMaxVectorSize = 4096;
+@@ -333,7 +333,7 @@ static constexpr HWY_MAYBE_UNUSED size_t kMaxVectorSize = 16;
+ // by concatenating base type and bits.
+ 
+ // RVV already has a builtin type and the GCC intrinsics require it.
+-#if HWY_ARCH_RVV && HWY_COMPILER_GCC
++#if HWY_ARCH_RVV && HWY_COMPILER_GCC && defined(__riscv_vector)
+ #define HWY_NATIVE_FLOAT16 1
+ #else
+ #define HWY_NATIVE_FLOAT16 0
+From 7d664f7f9c198322ceaf0ce22f3f42367584d52b Mon Sep 17 00:00:00 2001
+From: Makoto Kato <m_kato@ga2.so-net.ne.jp>
+Date: Tue, 15 Jun 2021 11:25:59 +0000
+Subject: [PATCH] More webrtc fix. But this isn't built yet.
+
+---
+ third_party/libwebrtc/webrtc/build/build_config.h | 3 +++
+ third_party/libwebrtc/webrtc/typedefs.h           | 8 ++++++++
+ 2 files changed, 11 insertions(+)
+
+#diff --git a/third_party/libwebrtc/webrtc/build/build_config.h b/third_party/libwebrtc/webrtc/build/build_config.h
+#index a9bc015c48fcd..272a840180605 100644
+#--- a/third_party/libwebrtc/webrtc/build/build_config.h
+#+++ b/third_party/libwebrtc/webrtc/build/build_config.h
+#@@ -171,6 +171,9 @@
+# #define ARCH_CPU_ARM_FAMILY 1
+# #define ARCH_CPU_ARM64 1
+# #define ARCH_CPU_64_BITS 1
+#+#elif defined(__riscv) && __riscv_xlen == 32
+#+#define ARCH_CPU_RISCV 1
+#+#define ARCH_CPU_32_BITS 1
+# #elif defined(__riscv) && __riscv_xlen == 64
+# #define ARCH_CPU_RISCV 1
+# #define ARCH_CPU_64_BITS 1
+diff --git a/third_party/libwebrtc/webrtc/typedefs.h b/third_party/libwebrtc/webrtc/typedefs.h
+index 60095a926faa2..2b9ca31d6e47c 100644
+--- a/third_party/libwebrtc/webrtc/typedefs.h
++++ b/third_party/libwebrtc/webrtc/typedefs.h
+@@ -120,6 +120,14 @@
+ #define WEBRTC_ARCH_32_BITS 1
+ #define WEBRTC_ARCH_BIG_ENDIAN
+ #define WEBRTC_BIG_ENDIAN
++#elif defined(__riscv) && __riscv_xlen == 64
++#define WEBRTC_ARCH_64_BITS
++#define WEBRTC_ARCH_LITTLE_ENDIAN
++#define WEBRTC_LITTLE_ENDIAN
++#elif defined(__riscv) && __riscv_xlen == 32
++#define WEBRTC_ARCH_32_BITS
++#define WEBRTC_ARCH_LITTLE_ENDIAN
++#define WEBRTC_LITTLE_ENDIAN
+ #elif defined(__pnacl__)
+ #define WEBRTC_ARCH_32_BITS
+ #define WEBRTC_ARCH_LITTLE_ENDIAN
+From 1058e1e849b584a462fa65b9c5d0b539244dd2b3 Mon Sep 17 00:00:00 2001
+From: Makoto Kato <m_kato@ga2.so-net.ne.jp>
+Date: Tue, 15 Jun 2021 11:26:30 +0000
+Subject: [PATCH] Update xptcall.
+
+---
+ .../xptcall/md/unix/xptcinvoke_asm_riscv64.S  | 75 +++++++++++--------
+ .../xptcall/md/unix/xptcstubs_asm_riscv64.S   | 16 +++-
+ 2 files changed, 58 insertions(+), 33 deletions(-)
+
+diff --git a/xpcom/reflect/xptcall/md/unix/xptcinvoke_asm_riscv64.S b/xpcom/reflect/xptcall/md/unix/xptcinvoke_asm_riscv64.S
+index e7d35d3aa7a9c..a0b65c5d92032 100644
+--- a/xpcom/reflect/xptcall/md/unix/xptcinvoke_asm_riscv64.S
++++ b/xpcom/reflect/xptcall/md/unix/xptcinvoke_asm_riscv64.S
+@@ -16,62 +16,77 @@ NS_InvokeByIndex:
+     .cfi_startproc
+     addi    sp, sp, -32
+     .cfi_adjust_cfa_offset 32
+-    sd      s0, 0(sp)
++    sd      s0, 16(sp)
++    .cfi_rel_offset s0, 16
+     sd      s1, 8(sp)
+-    sd      s2, 16(sp)
++    .cfi_rel_offset s1, 8
++    sd      s2, 0(sp)
++    .cfi_rel_offset s0, 0
+     sd      ra, 24(sp)
++    .cfi_rel_offset ra, 24
+ 
+     mv      s0, a0 
+     mv      s1, a1 
+     mv      s2, sp 
+-    mv      a4, sp
++    .cfi_def_cfa_register s2
+ 
+-    # 16-bytes alignment
+-    addiw   a0, a0, 1
++    /* 16-bytes alignment */
++    addiw   a0, a2, 1
+     andi    a0, a0, -2
+     slli    a0, a0, 3
+     sub     sp, sp, a0
++    mv      a4, sp
+ 
+     addi    sp, sp, -8*(NGPREGS+NFPREGS)
+     mv      a0, sp
+     addi    a1, sp, 8*NGPREGS
+ 
+-    # extern "C" void invoke_copy_to_stack(uint64_t* gpregs, double* fpregs,
+-    #                                 uint32_t paramCount, nsXPTCVariant* s,
+-    #                                 uint64_t* d) {
++    /*
++     *  void invoke_copy_to_stack(uint64_t* gpregs, double* fpregs,
++     *                           uint32_t paramCount, nsXPTCVariant* s,
++     *                           uint64_t* d);
++     */
+ 
+     call    invoke_copy_to_stack
+-    addi    sp, sp, 8*(NGPREGS+NFPREGS)
+ 
+-    ld      a1, 8(s0)
+-    ld      a2, 16(s0)
+-    ld      a3, 24(s0)
+-    ld      a4, 32(s0)
+-    ld      a5, 40(s0)
+-    ld      a6, 48(s0)
+-    ld      a7, 56(s0)
++    ld      a0, 0(sp)
++    ld      a1, 8(sp)
++    ld      a2, 16(sp)
++    ld      a3, 24(sp)
++    ld      a4, 32(sp)
++    ld      a5, 40(sp)
++    ld      a6, 48(sp)
++    ld      a7, 56(sp)
+ 
+-    fld     fa0, 64(s0)
+-    fld     fa1, 72(s0)
+-    fld     fa2, 80(s0)
+-    fld     fa3, 88(s0)
+-    fld     fa4, 96(s0)
+-    fld     fa5, 104(s0)
+-    fld     fa6, 112(s0)
+-    fld     fa7, 120(s0)
++    fld     fa0, 64(sp)
++    fld     fa1, 72(sp)
++    fld     fa2, 80(sp)
++    fld     fa3, 88(sp)
++    fld     fa4, 96(sp)
++    fld     fa5, 104(sp)
++    fld     fa6, 112(sp)
++    fld     fa7, 120(sp)
++
++    addi    sp, sp, 8*(NGPREGS+NFPREGS)
+ 
+-    ld      s0, (s0)
+-    slli    s1, s1, 3
++    ld      s0, 0(s0)
++    slliw   s1, s1, 3
+     add     s0, s0, s1
+-    ld      s0, [s0]
+-    call    s0
++    ld      t0, 0(s0)
++    jalr    t0
+ 
+     mv      sp, s2
+-    ld      s0, 0(sp)
++    .cfi_def_cfa_register sp
++    ld      s0, 16(sp)
++    .cfi_restore s0
+     ld      s1, 8(sp)
+-    ld      s2, 8(sp)
++    .cfi_restore s1
++    ld      s2, 0(sp)
++    .cfi_restore s2
+     ld      ra, 24(sp)
++    .cfi_restore ra
+     addi    sp, sp, 32
++    .cfi_adjust_cfa_offset -32
+     ret
+     .cfi_endproc
+     .size   NS_InvokeByIndex, .-NS_InvokeByIndex
+diff --git a/xpcom/reflect/xptcall/md/unix/xptcstubs_asm_riscv64.S b/xpcom/reflect/xptcall/md/unix/xptcstubs_asm_riscv64.S
+index 870abd05fa7ba..83fdfcb4397e9 100644
+--- a/xpcom/reflect/xptcall/md/unix/xptcstubs_asm_riscv64.S
++++ b/xpcom/reflect/xptcall/md/unix/xptcstubs_asm_riscv64.S
+@@ -29,16 +29,26 @@ SharedStub:
+     fsd     fa4, 96(sp)
+     fsd     fa5, 104(sp)
+     fsd     fa6, 112(sp)
+-    fsd     fa7, 128(sp)
++    fsd     fa7, 120(sp)
++    sd      ra, 136(sp)
++    .cfi_rel_offset ra, 136
+ 
+     # methodIndex passed from stub
+     mv      a1, t0
+-    addi    a2, sp, 8*(NGPREGS+NFPREGS)+16+8
++    addi    a2, sp, 8*(NGPREGS+NFPREGS)+16
+     mv      a3, sp
+-    addi    a4, sp, 64
++    addi    a4, sp, 8*NGPREGS
++
++    # nsresult PrepareAndDispatch(nsXPTCStubBase* self,
++    #                             uint32_t methodIndex,
++    #                             uint64_t* args,
++    #                             uint64_t* gpregs,
++    #                             double* fpregs);
+ 
+     call    PrepareAndDispatch
+ 
++    ld      ra, 136(sp)
++    .cfi_restore ra
+     addi    sp, sp, 8*(NGPREGS+NFPREGS)+16
+     .cfi_adjust_cfa_offset -8*(NGPREGS+NFPREGS)-16
+     ret
+From 735487e921dfb35d5a52eb2ff5222d70ad0aefa7 Mon Sep 17 00:00:00 2001
+From: Makoto Kato <m_kato@ga2.so-net.ne.jp>
+Date: Wed, 16 Jun 2021 23:19:59 +0900
+Subject: [PATCH] Add toolchain prefix to strip.
+
+---
+ build/autoconf/toolchain.m4             |  1 -
+ build/moz.configure/toolchain.configure | 14 ++++++++++++++
+ js/src/old-configure.in                 |  2 --
+ old-configure.in                        |  2 --
+ 4 files changed, 14 insertions(+), 5 deletions(-)
+
+diff --git a/build/autoconf/toolchain.m4 b/build/autoconf/toolchain.m4
+index 2a8744610c89c..407d0aa499088 100644
+--- a/build/autoconf/toolchain.m4
++++ b/build/autoconf/toolchain.m4
+@@ -82,7 +82,6 @@ AC_PROG_CXX
+ AC_CHECK_PROGS(RANLIB, "${TOOLCHAIN_PREFIX}ranlib", :)
+ AC_CHECK_PROGS(AS, "${TOOLCHAIN_PREFIX}as", :)
+ AC_CHECK_PROGS(LIPO, "${TOOLCHAIN_PREFIX}lipo", :)
+-AC_CHECK_PROGS(STRIP, "${TOOLCHAIN_PREFIX}strip", :)
+ AC_CHECK_PROGS(OTOOL, "${TOOLCHAIN_PREFIX}otool", :)
+ AC_CHECK_PROGS(INSTALL_NAME_TOOL, "${TOOLCHAIN_PREFIX}install_name_tool", :)
+ AC_CHECK_PROGS(OBJCOPY, "${TOOLCHAIN_PREFIX}objcopy", :)
+diff --git a/build/moz.configure/toolchain.configure b/build/moz.configure/toolchain.configure
+index 218e4ec049d49..69e7c93a3d2fc 100755
+--- a/build/moz.configure/toolchain.configure
++++ b/build/moz.configure/toolchain.configure
+@@ -2690,6 +2690,20 @@ def nm_names(toolchain_prefix, c_compiler):
+ check_prog("NM", nm_names, paths=clang_search_path, when=target_is_linux)
+ 
+ 
++@depends(toolchain_prefix)
++def strip_names(toolchain_prefix):
++    return tuple('%s%s' % (p, "strip") for p in (toolchain_prefix or ()) + ('',))
++
++
++option(env='STRIP', nargs=1, help='Path to the strip program', when=target_is_unix)
++
++strip = check_prog('STRIP', strip_names,
++                   input='STRIP', paths=clang_search_path,
++                   allow_missing=True, when=target_is_unix)
++
++add_old_configure_assignment('STRIP', strip)
++
++
+ option("--enable-cpp-rtti", help="Enable C++ RTTI")
+ 
+ add_old_configure_assignment("_MOZ_USE_RTTI", "1", when="--enable-cpp-rtti")
+diff --git a/js/src/old-configure.in b/js/src/old-configure.in
+index 77652f67529ca..b8ded24428578 100644
+--- a/js/src/old-configure.in
++++ b/js/src/old-configure.in
+@@ -92,7 +92,6 @@ else
+     AC_PROG_CC
+     AC_PROG_CXX
+     MOZ_PATH_PROGS(AS, $AS as, $CC)
+-    AC_CHECK_PROGS(STRIP, strip, :)
+ fi
+ 
+ MOZ_TOOL_VARIABLES
+@@ -515,7 +514,6 @@ case "$target" in
+         WIN32_GUI_EXE_LDFLAGS=-mwindows
+     else
+         TARGET_COMPILER_ABI=msvc
+-        STRIP='echo not_strip'
+         # aarch64 doesn't support subsystems below 6.02
+         if test "$CPU_ARCH" = "aarch64"; then
+             WIN32_SUBSYSTEM_VERSION=6.02
+diff --git a/old-configure.in b/old-configure.in
+index de2642f71d0fd..223230c480d86 100644
+--- a/old-configure.in
++++ b/old-configure.in
+@@ -104,7 +104,6 @@ else
+     esac
+     AC_PROG_CXX
+     MOZ_PATH_PROGS(AS, $AS as, $CC)
+-    AC_CHECK_PROGS(STRIP, strip, :)
+     AC_CHECK_PROGS(OTOOL, otool, :)
+ fi
+ 
+@@ -608,7 +607,6 @@ case "$target" in
+         LDFLAGS="$LDFLAGS -Wl,--no-insert-timestamp"
+     else
+         TARGET_COMPILER_ABI=msvc
+-        STRIP='echo not_strip'
+         # aarch64 doesn't support subsystems below 6.02
+         if test "$CPU_ARCH" = "aarch64"; then
+             WIN32_SUBSYSTEM_VERSION=6.02
+From fc9ed124c89e626cb078f67331feba5d5a3f5563 Mon Sep 17 00:00:00 2001
+From: Makoto Kato <m_kato@ga2.so-net.ne.jp>
+Date: Thu, 17 Jun 2021 19:49:59 +0900
+Subject: [PATCH] Fix xptcinvoke
+
+---
+ xpcom/reflect/xptcall/md/unix/xptcinvoke_asm_riscv64.S | 4 +++-
+ xpcom/reflect/xptcall/md/unix/xptcinvoke_riscv64.cpp   | 4 ++--
+ 2 files changed, 5 insertions(+), 3 deletions(-)
+
+diff --git a/xpcom/reflect/xptcall/md/unix/xptcinvoke_asm_riscv64.S b/xpcom/reflect/xptcall/md/unix/xptcinvoke_asm_riscv64.S
+index a0b65c5d92032..6cd4edafdc2d1 100644
+--- a/xpcom/reflect/xptcall/md/unix/xptcinvoke_asm_riscv64.S
++++ b/xpcom/reflect/xptcall/md/unix/xptcinvoke_asm_riscv64.S
+@@ -49,7 +49,9 @@ NS_InvokeByIndex:
+ 
+     call    invoke_copy_to_stack
+ 
+-    ld      a0, 0(sp)
++    /* this */
++    mv      a0, s0
++
+     ld      a1, 8(sp)
+     ld      a2, 16(sp)
+     ld      a3, 24(sp)
+diff --git a/xpcom/reflect/xptcall/md/unix/xptcinvoke_riscv64.cpp b/xpcom/reflect/xptcall/md/unix/xptcinvoke_riscv64.cpp
+index 369511d33a40b..b4fa8bc404e87 100644
+--- a/xpcom/reflect/xptcall/md/unix/xptcinvoke_riscv64.cpp
++++ b/xpcom/reflect/xptcall/md/unix/xptcinvoke_riscv64.cpp
+@@ -16,9 +16,9 @@ extern "C" void invoke_copy_to_stack(uint64_t* gpregs, double* fpregs,
+   static const uint32_t GPR_COUNT = 8;
+   static const uint32_t FPR_COUNT = 8;
+ 
+-  uint32_t nr_gpr = 0;
++  uint32_t nr_gpr = 1;  // skip one GPR register for "this"
+   uint32_t nr_fpr = 0;
+-  uint64_t value;
++  uint64_t value = 0;
+ 
+   for (uint32_t i = 0; i < paramCount; i++, s++) {
+     if (s->IsIndirect()) {
+From a418c651c88cd2682c4cfe61e9f57b5389078c09 Mon Sep 17 00:00:00 2001
+From: Makoto Kato <m_kato@ga2.so-net.ne.jp>
+Date: Thu, 17 Jun 2021 21:50:49 +0900
+Subject: [PATCH] signal handler
+
+---
+ js/src/wasm/WasmSignalHandlers.cpp | 9 +++++++++
+ 1 file changed, 9 insertions(+)
+
+diff --git a/js/src/wasm/WasmSignalHandlers.cpp b/js/src/wasm/WasmSignalHandlers.cpp
+index 37bc5a9c19273..856368019235d 100644
+--- a/js/src/wasm/WasmSignalHandlers.cpp
++++ b/js/src/wasm/WasmSignalHandlers.cpp
+@@ -154,6 +154,11 @@ using mozilla::DebugOnly;
+ #    define R01_sig(p) ((p)->uc_mcontext.gp_regs[1])
+ #    define R32_sig(p) ((p)->uc_mcontext.gp_regs[32])
+ #  endif
++#  if defined(__linux__) && defined(__riscv) && __riscv_xlen == 64
++#    define EPC_sig(p) ((p)->uc_mcontext.__gregs[0])
++#    define X02_sig(p) ((p)->uc_mcontext.__gregs[2])
++#    define X08_sig(p) ((p)->uc_mcontext.__gregs[8])
++#  endif
+ #elif defined(__NetBSD__)
+ #  define EIP_sig(p) ((p)->uc_mcontext.__gregs[_REG_EIP])
+ #  define EBP_sig(p) ((p)->uc_mcontext.__gregs[_REG_EBP])
+@@ -395,6 +400,10 @@ struct macos_aarch64_context {
+ #  define PC_sig(p) R32_sig(p)
+ #  define SP_sig(p) R01_sig(p)
+ #  define FP_sig(p) R01_sig(p)
++#elif defined(__riscv) && __riscv_xlen == 64
++#  define PC_sig(p) EPC_sig(p)
++#  define SP_sig(p) X02_sig(p)
++#  define FP_sig(p) X08_sig(p)
+ #endif
+ 
+ static void SetContextPC(CONTEXT* context, uint8_t* pc) {
+From 96b8edb87c25998af2c89a82e010c480244b8ab9 Mon Sep 17 00:00:00 2001
+From: Makoto Kato <m_kato@ga2.so-net.ne.jp>
+Date: Thu, 17 Jun 2021 23:37:12 +0900
+Subject: [PATCH] Avoid _Unwind_Backtrace crash.
+
+---
+ .../xptcall/md/unix/xptcinvoke_asm_riscv64.S  | 32 +++++++++----------
+ .../xptcall/md/unix/xptcinvoke_riscv64.cpp    | 10 ++++++
+ 2 files changed, 26 insertions(+), 16 deletions(-)
+
+diff --git a/xpcom/reflect/xptcall/md/unix/xptcinvoke_asm_riscv64.S b/xpcom/reflect/xptcall/md/unix/xptcinvoke_asm_riscv64.S
+index 6cd4edafdc2d1..b2ad16169b467 100644
+--- a/xpcom/reflect/xptcall/md/unix/xptcinvoke_asm_riscv64.S
++++ b/xpcom/reflect/xptcall/md/unix/xptcinvoke_asm_riscv64.S
+@@ -6,13 +6,13 @@
+     .set NFPREGS,8
+ 
+     .text
+-    .globl  NS_InvokeByIndex
+-    .type   NS_InvokeByIndex, @function
++    .globl  _NS_InvokeByIndex
++    .type   _NS_InvokeByIndex, @function
+ /*
+  * NS_InvokeByIndex(nsISupports* that, uint32_t methodIndex,
+  *                  uint32_t paramCount, nsXPTCVariant* params)
+  */
+-NS_InvokeByIndex:
++_NS_InvokeByIndex:
+     .cfi_startproc
+     addi    sp, sp, -32
+     .cfi_adjust_cfa_offset 32
+@@ -21,14 +21,14 @@ NS_InvokeByIndex:
+     sd      s1, 8(sp)
+     .cfi_rel_offset s1, 8
+     sd      s2, 0(sp)
+-    .cfi_rel_offset s0, 0
++    .cfi_rel_offset s2, 0
+     sd      ra, 24(sp)
+     .cfi_rel_offset ra, 24
+ 
+-    mv      s0, a0 
+-    mv      s1, a1 
+-    mv      s2, sp 
+-    .cfi_def_cfa_register s2
++    mv      s2, a0
++    mv      s1, a1
++    mv      s0, sp
++    .cfi_def_cfa_register s0
+ 
+     /* 16-bytes alignment */
+     addiw   a0, a2, 1
+@@ -49,8 +49,8 @@ NS_InvokeByIndex:
+ 
+     call    invoke_copy_to_stack
+ 
+-    /* this */
+-    mv      a0, s0
++    /* 1st argument is this */
++    mv      a0, s2
+ 
+     ld      a1, 8(sp)
+     ld      a2, 16(sp)
+@@ -71,13 +71,13 @@ NS_InvokeByIndex:
+ 
+     addi    sp, sp, 8*(NGPREGS+NFPREGS)
+ 
+-    ld      s0, 0(s0)
++    ld      s2, 0(s2)
+     slliw   s1, s1, 3
+-    add     s0, s0, s1
+-    ld      t0, 0(s0)
++    add     s2, s2, s1
++    ld      t0, 0(s2)
+     jalr    t0
+ 
+-    mv      sp, s2
++    mv      sp, s0
+     .cfi_def_cfa_register sp
+     ld      s0, 16(sp)
+     .cfi_restore s0
+@@ -91,5 +91,5 @@ NS_InvokeByIndex:
+     .cfi_adjust_cfa_offset -32
+     ret
+     .cfi_endproc
+-    .size   NS_InvokeByIndex, .-NS_InvokeByIndex
+-    .section        .note.GNU-stack,"",@progbits
++    .size   _NS_InvokeByIndex, . -_NS_InvokeByIndex
++    .section .note.GNU-stack, "", @progbits
+diff --git a/xpcom/reflect/xptcall/md/unix/xptcinvoke_riscv64.cpp b/xpcom/reflect/xptcall/md/unix/xptcinvoke_riscv64.cpp
+index b4fa8bc404e87..6ff8f28a274bf 100644
+--- a/xpcom/reflect/xptcall/md/unix/xptcinvoke_riscv64.cpp
++++ b/xpcom/reflect/xptcall/md/unix/xptcinvoke_riscv64.cpp
+@@ -91,3 +91,13 @@ extern "C" void invoke_copy_to_stack(uint64_t* gpregs, double* fpregs,
+     }
+   }
+ }
++
++extern "C" nsresult _NS_InvokeByIndex(nsISupports* that, uint32_t methodIndex,
++                                      uint32_t paramCount,
++                                      nsXPTCVariant* params);
++
++EXPORT_XPCOM_API(nsresult)
++NS_InvokeByIndex(nsISupports* that, uint32_t methodIndex, uint32_t paramCount,
++                 nsXPTCVariant* params) {
++  return _NS_InvokeByIndex(that, methodIndex, paramCount, params);
++}
+From 145f6968c46bf805a5213a606a1df98fcaabc2f4 Mon Sep 17 00:00:00 2001
+From: Makoto Kato <m_kato@ga2.so-net.ne.jp>
+Date: Sun, 20 Jun 2021 13:59:10 +0900
+Subject: [PATCH] Don't use libwebrtc include path without MOZ_WEBRTC.
+
+---
+ dom/bindings/moz.build    | 8 ++++++--
+ dom/media/gtest/moz.build | 8 ++++++--
+ ipc/glue/moz.build        | 8 ++++++--
+ 3 files changed, 18 insertions(+), 6 deletions(-)
+
+diff --git a/dom/bindings/moz.build b/dom/bindings/moz.build
+index ca13276f98c86..8b0ef9d816117 100644
+--- a/dom/bindings/moz.build
++++ b/dom/bindings/moz.build
+@@ -94,12 +94,16 @@ LOCAL_INCLUDES += [
+     "/layout/xul/tree",
+     "/media/webrtc/",
+     "/netwerk/base/",
+-    "/third_party/libwebrtc",
+-    "/third_party/libwebrtc/webrtc",
+ ]
+ 
+ LOCAL_INCLUDES += ["/third_party/msgpack/include"]
+ 
++if CONFIG["MOZ_WEBRTC"]:
++    LOCAL_INCLUDES += [
++        "/third_party/libwebrtc",
++        "/third_party/libwebrtc/webrtc",
++    ]
++
+ DEFINES["GOOGLE_PROTOBUF_NO_RTTI"] = True
+ DEFINES["GOOGLE_PROTOBUF_NO_STATIC_INITIALIZER"] = True
+ 
+diff --git a/dom/media/gtest/moz.build b/dom/media/gtest/moz.build
+index aaa5e45972eef..25c457c702636 100644
+--- a/dom/media/gtest/moz.build
++++ b/dom/media/gtest/moz.build
+@@ -11,10 +11,14 @@ DEFINES["ENABLE_SET_CUBEB_BACKEND"] = True
+ LOCAL_INCLUDES += [
+     "/dom/media/mediasink",
+     "/dom/media/webrtc/common/",
+-    "/third_party/libwebrtc",
+-    "/third_party/libwebrtc/webrtc",
+ ]
+ 
++if CONFIG["MOZ_WEBRTC"]:
++    LOCAL_INCLUDES += [
++        "/third_party/libwebrtc",
++        "/third_party/libwebrtc/webrtc",
++    ]
++
+ UNIFIED_SOURCES += [
+     "MockCubeb.cpp",
+     "MockMediaResource.cpp",
+diff --git a/ipc/glue/moz.build b/ipc/glue/moz.build
+index 569ca15046bf1..2b3a1167267be 100644
+--- a/ipc/glue/moz.build
++++ b/ipc/glue/moz.build
+@@ -207,11 +207,15 @@ LOCAL_INCLUDES += [
+     "/dom/indexedDB",
+     "/dom/storage",
+     "/netwerk/base",
+-    "/third_party/libwebrtc",
+-    "/third_party/libwebrtc/webrtc",
+     "/xpcom/build",
+ ]
+ 
++if CONFIG["MOZ_WEBRTC"]:
++    LOCAL_INCLUDES += [
++        "/third_party/libwebrtc",
++        "/third_party/libwebrtc/webrtc",
++    ]
++
+ IPDL_SOURCES = [
+     "InputStreamParams.ipdlh",
+     "IPCStream.ipdlh",
+From 575f97d84204fa8b2a5c3599ef6b0b6b85d73b3f Mon Sep 17 00:00:00 2001
+From: Makoto Kato <m_kato@ga2.so-net.ne.jp>
+Date: Sun, 20 Jun 2021 14:03:13 +0900
+Subject: [PATCH] Update build_config.h
+
+---
+ ipc/chromium/src/build/build_config.h            |  5 +----
+ security/sandbox/chromium/build/build_config.h   | 16 ++++------------
+ .../libwebrtc/webrtc/build/build_config.h        |  5 +----
+ 3 files changed, 6 insertions(+), 20 deletions(-)
+
+diff --git a/ipc/chromium/src/build/build_config.h b/ipc/chromium/src/build/build_config.h
+index 7d0de63d6f6b9..de26701b2d716 100644
+--- a/ipc/chromium/src/build/build_config.h
++++ b/ipc/chromium/src/build/build_config.h
+@@ -122,11 +122,8 @@
+ #  define ARCH_CPU_ARM_FAMILY 1
+ #  define ARCH_CPU_ARM64 1
+ #  define ARCH_CPU_64_BITS 1
+-#elif defined(__riscv) && __riscv_xlen == 32
+-#  define ARCH_CPU_RISCV 1
+-#  define ARCH_CPU_32_BITS 1
+ #elif defined(__riscv) && __riscv_xlen == 64
+-#  define ARCH_CPU_RISCV 1
++#  define ARCH_CPU_RISCV64 1
+ #  define ARCH_CPU_64_BITS 1
+ #else
+ #  error Please add support for your architecture in build/build_config.h
+# diff --git a/security/sandbox/chromium/build/build_config.h b/security/sandbox/chromium/build/build_config.h
+# index ca9461e240d23..48109c4d46af3 100644
+# --- a/security/sandbox/chromium/build/build_config.h
+# +++ b/security/sandbox/chromium/build/build_config.h
+# @@ -166,18 +166,10 @@
+#  #define ARCH_CPU_32_BITS 1
+#  #define ARCH_CPU_BIG_ENDIAN 1
+#  #endif
+# -#elif defined(__riscv)
+# -#if __riscv_xlen == 32
+# -#  define ARCH_CPU_RISCV_FAMILY 1
+# -#  define ARCH_CPU_RISCV32 1
+# -#  define ARCH_CPU_32_BITS 1
+# -#  define ARCH_CPU_LITTLE_ENDIAN 1
+# -#elif __riscv_xlen == 64
+# -#  define ARCH_CPU_RISCV_FAMILY 1
+# -#  define ARCH_CPU_RISCV64 1
+# -#  define ARCH_CPU_64_BITS 1
+# -#  define ARCH_CPU_LITTLE_ENDIAN 1
+# -#endif
+# +#elif defined(__riscv) && __riscv_xlen == 64
+# +#define ARCH_CPU_RISCV64 1
+# +#define ARCH_CPU_64_BITS 1
+# +#define ARCH_CPU_LITTLE_ENDIAN 1
+#  #else
+#  #error Please add support for your architecture in build/build_config.h
+#  #endif
+#diff --git a/third_party/libwebrtc/webrtc/build/build_config.h b/third_party/libwebrtc/webrtc/build/build_config.h
+#index 272a840180605..afb82ecd602c1 100644
+#--- a/third_party/libwebrtc/webrtc/build/build_config.h
+#+++ b/third_party/libwebrtc/webrtc/build/build_config.h
+#@@ -171,8 +171,8 @@
+# #define ARCH_CPU_ARM_FAMILY 1
+# #define ARCH_CPU_ARM64 1
+# #define ARCH_CPU_64_BITS 1
+# #elif defined(__riscv) && __riscv_xlen == 64
+#-#define ARCH_CPU_RISCV 1
+#+#define ARCH_CPU_RISCV64 1
+# #define ARCH_CPU_64_BITS 1
+# #else
+# #error Please add support for your architecture in build/build_config.h
+From 201a107ca4300a19a6d0a8e718ee1560e7d85731 Mon Sep 17 00:00:00 2001
+From: Makoto Kato <m_kato@ga2.so-net.ne.jp>
+Date: Sun, 20 Jun 2021 14:11:49 +0900
+Subject: [PATCH] Update comment of xptcall
+
+---
+ .../xptcall/md/unix/xptcinvoke_asm_riscv64.S     | 16 +++++-----------
+ .../xptcall/md/unix/xptcstubs_asm_riscv64.S      | 15 +++++----------
+ 2 files changed, 10 insertions(+), 21 deletions(-)
+
+diff --git a/xpcom/reflect/xptcall/md/unix/xptcinvoke_asm_riscv64.S b/xpcom/reflect/xptcall/md/unix/xptcinvoke_asm_riscv64.S
+index b2ad16169b467..46065232962cd 100644
+--- a/xpcom/reflect/xptcall/md/unix/xptcinvoke_asm_riscv64.S
++++ b/xpcom/reflect/xptcall/md/unix/xptcinvoke_asm_riscv64.S
+@@ -2,15 +2,15 @@
+  * License, v. 2.0. If a copy of the MPL was not distributed with this
+  * file, You can obtain one at http://mozilla.org/MPL/2.0/. */
+ 
+-    .set NGPREGS,8
+-    .set NFPREGS,8
++    .set NGPREGS, 8
++    .set NFPREGS, 8
+ 
+     .text
+     .globl  _NS_InvokeByIndex
+     .type   _NS_InvokeByIndex, @function
+ /*
+- * NS_InvokeByIndex(nsISupports* that, uint32_t methodIndex,
+- *                  uint32_t paramCount, nsXPTCVariant* params)
++ * _NS_InvokeByIndex(nsISupports* that, uint32_t methodIndex,
++ *                   uint32_t paramCount, nsXPTCVariant* params)
+  */
+ _NS_InvokeByIndex:
+     .cfi_startproc
+@@ -41,12 +41,6 @@ _NS_InvokeByIndex:
+     mv      a0, sp
+     addi    a1, sp, 8*NGPREGS
+ 
+-    /*
+-     *  void invoke_copy_to_stack(uint64_t* gpregs, double* fpregs,
+-     *                           uint32_t paramCount, nsXPTCVariant* s,
+-     *                           uint64_t* d);
+-     */
+-
+     call    invoke_copy_to_stack
+ 
+     /* 1st argument is this */
+@@ -91,5 +85,5 @@ _NS_InvokeByIndex:
+     .cfi_adjust_cfa_offset -32
+     ret
+     .cfi_endproc
+-    .size   _NS_InvokeByIndex, . -_NS_InvokeByIndex
++    .size   _NS_InvokeByIndex, . - _NS_InvokeByIndex
+     .section .note.GNU-stack, "", @progbits
+diff --git a/xpcom/reflect/xptcall/md/unix/xptcstubs_asm_riscv64.S b/xpcom/reflect/xptcall/md/unix/xptcstubs_asm_riscv64.S
+index 83fdfcb4397e9..02bb812d5997e 100644
+--- a/xpcom/reflect/xptcall/md/unix/xptcstubs_asm_riscv64.S
++++ b/xpcom/reflect/xptcall/md/unix/xptcstubs_asm_riscv64.S
+@@ -2,8 +2,8 @@
+ # License, v. 2.0. If a copy of the MPL was not distributed with this
+ # file, You can obtain one at http://mozilla.org/MPL/2.0/.
+ 
+-    .set NGPREGS,8
+-    .set NFPREGS,8
++    .set NGPREGS, 8
++    .set NFPREGS, 8
+ 
+     .text
+     .globl SharedStub
+@@ -12,6 +12,7 @@
+ 
+ SharedStub:
+     .cfi_startproc
++    mv      t1, sp
+     addi    sp, sp, -8*(NGPREGS+NFPREGS)-16
+     .cfi_adjust_cfa_offset 8*(NGPREGS+NFPREGS)+16
+     sd      a0, 0(sp)
+@@ -33,18 +34,12 @@ SharedStub:
+     sd      ra, 136(sp)
+     .cfi_rel_offset ra, 136
+ 
+-    # methodIndex passed from stub
++    /* methodIndex is passed from stub */
+     mv      a1, t0
+-    addi    a2, sp, 8*(NGPREGS+NFPREGS)+16
++    mv      a2, t1
+     mv      a3, sp
+     addi    a4, sp, 8*NGPREGS
+ 
+-    # nsresult PrepareAndDispatch(nsXPTCStubBase* self,
+-    #                             uint32_t methodIndex,
+-    #                             uint64_t* args,
+-    #                             uint64_t* gpregs,
+-    #                             double* fpregs);
+-
+     call    PrepareAndDispatch
+ 
+     ld      ra, 136(sp)
+From 3add6a1ff92a51951e3dea8c7ce92af996336bcd Mon Sep 17 00:00:00 2001
+From: Makoto Kato <m_kato@ga2.so-net.ne.jp>
+Date: Tue, 10 Aug 2021 21:42:25 +0900
+Subject: [PATCH] Update highway to 0.14.0
+
+---
+ media/highway/moz.yaml                        |   86 +-
+ third_party/highway/BUILD                     |   38 +-
+ third_party/highway/CMakeLists.txt            |   57 +-
+ third_party/highway/CMakeLists.txt.in         |    2 +-
+ third_party/highway/README.md                 |   87 +-
+ third_party/highway/debian/changelog          |   16 +
+ third_party/highway/debian/rules              |    0
+ third_party/highway/hwy/aligned_allocator.cc  |   15 +-
+ .../highway/hwy/aligned_allocator_test.cc     |   14 +-
+ third_party/highway/hwy/base.h                |  293 +-
+ third_party/highway/hwy/base_test.cc          |   20 +
+ third_party/highway/hwy/cache_control.h       |   30 +-
+ .../highway/hwy/contrib/image/image.cc        |    2 +-
+ .../highway/hwy/contrib/image/image_test.cc   |   10 +-
+ .../highway/hwy/contrib/math/math-inl.h       |  242 +-
+ .../highway/hwy/contrib/math/math_test.cc     |   41 +-
+ .../highway/hwy/detect_compiler_arch.h        |  188 ++
+ third_party/highway/hwy/detect_targets.h      |  386 +++
+ third_party/highway/hwy/examples/benchmark.cc |    2 +-
+ third_party/highway/hwy/examples/skeleton.cc  |    2 +-
+ .../highway/hwy/examples/skeleton_test.cc     |    8 +
+ third_party/highway/hwy/foreach_target.h      |   24 +-
+ third_party/highway/hwy/highway.h             |   90 +-
+ third_party/highway/hwy/highway_test.cc       |   66 +-
+ third_party/highway/hwy/nanobenchmark.cc      |    6 +-
+ third_party/highway/hwy/nanobenchmark_test.cc |   17 +-
+ third_party/highway/hwy/ops/arm_neon-inl.h    | 2262 +++++++++------
+ third_party/highway/hwy/ops/arm_sve-inl.h     | 2312 ++++++++-------
+ third_party/highway/hwy/ops/generic_ops-inl.h |  324 +++
+ third_party/highway/hwy/ops/rvv-inl.h         |  742 +++--
+ third_party/highway/hwy/ops/scalar-inl.h      |  441 +--
+ third_party/highway/hwy/ops/set_macros-inl.h  |  109 +-
+ third_party/highway/hwy/ops/shared-inl.h      |   11 +
+ third_party/highway/hwy/ops/wasm_128-inl.h    | 1372 +++++----
+ third_party/highway/hwy/ops/x86_128-inl.h     | 2485 +++++++++++------
+ third_party/highway/hwy/ops/x86_256-inl.h     | 1031 ++++---
+ third_party/highway/hwy/ops/x86_512-inl.h     | 1176 +++++---
+ third_party/highway/hwy/targets.cc            |  134 +-
+ third_party/highway/hwy/targets.h             |  317 +--
+ third_party/highway/hwy/targets_test.cc       |   14 +
+ .../highway/hwy/tests/arithmetic_test.cc      |  170 +-
+ .../highway/hwy/tests/blockwise_test.cc       |  655 +++++
+ third_party/highway/hwy/tests/combine_test.cc |  192 +-
+ third_party/highway/hwy/tests/compare_test.cc |   46 +-
+ third_party/highway/hwy/tests/convert_test.cc |  187 +-
+ third_party/highway/hwy/tests/crypto_test.cc  |  549 ++++
+ third_party/highway/hwy/tests/logical_test.cc |  516 +---
+ third_party/highway/hwy/tests/mask_test.cc    |  435 +++
+ third_party/highway/hwy/tests/memory_test.cc  |   25 +-
+ third_party/highway/hwy/tests/swizzle_test.cc |  658 ++---
+ third_party/highway/hwy/tests/test_util-inl.h |  238 +-
+ .../highway/hwy/tests/test_util_test.cc       |    8 +
+ third_party/highway/run_tests.sh              |    0
+ third_party/highway/test.py                   |  131 -
+ 54 files changed, 11454 insertions(+), 6828 deletions(-)
+ mode change 100644 => 100755 third_party/highway/debian/rules
+ create mode 100644 third_party/highway/hwy/detect_compiler_arch.h
+ create mode 100644 third_party/highway/hwy/detect_targets.h
+ create mode 100644 third_party/highway/hwy/ops/generic_ops-inl.h
+ create mode 100644 third_party/highway/hwy/tests/blockwise_test.cc
+ create mode 100644 third_party/highway/hwy/tests/crypto_test.cc
+ create mode 100644 third_party/highway/hwy/tests/mask_test.cc
+ mode change 100644 => 100755 third_party/highway/run_tests.sh
+ delete mode 100644 third_party/highway/test.py
+
+diff --git a/media/highway/moz.yaml b/media/highway/moz.yaml
+index 2381fb69b4f3a..a6d84aef91dbd 100644
+--- a/media/highway/moz.yaml
++++ b/media/highway/moz.yaml
+@@ -1,43 +1,43 @@
+-# Version of this schema
+-schema: 1
+-
+-bugzilla:
+-  # Bugzilla product and component for this directory and subdirectories
+-  product: Core
+-  component: "ImageLib"
+-
+-# Document the source of externally hosted code
+-origin:
+-
+-  # Short name of the package/library
+-  name: highway
+-
+-  description: Performance-portable, length-agnostic SIMD with runtime dispatch
+-
+-  # Full URL for the package's homepage/etc
+-  # Usually different from repository url
+-  url: https://github.com/google/highway
+-
+-  # Human-readable identifier for this version/release
+-  # Generally "version NNN", "tag SSS", "bookmark SSS"
+-  release: commit d9882104caf9fb060d328d62cac9ce0a05a191db (2021-05-21T10:02:16Z).
+-
+-  # Revision to pull in
+-  # Must be a long or short commit SHA (long preferred)
+-  revision: d9882104caf9fb060d328d62cac9ce0a05a191db
+-
+-  # The package's license, where possible using the mnemonic from
+-  # https://spdx.org/licenses/
+-  # Multiple licenses can be specified (as a YAML list)
+-  # A "LICENSE" file must exist containing the full license text
+-  license: Apache-2.0
+-
+-  license-file: LICENSE
+-
+-vendoring:
+-  url: https://github.com/google/highway.git
+-  source-hosting: github
+-  vendor-directory: third_party/highway
+-
+-  exclude:
+-    - g3doc/
++# Version of this schema
++schema: 1
++
++bugzilla:
++  # Bugzilla product and component for this directory and subdirectories
++  product: Core
++  component: "ImageLib"
++
++# Document the source of externally hosted code
++origin:
++
++  # Short name of the package/library
++  name: highway
++
++  description: Performance-portable, length-agnostic SIMD with runtime dispatch
++
++  # Full URL for the package's homepage/etc
++  # Usually different from repository url
++  url: https://github.com/google/highway
++
++  # Human-readable identifier for this version/release
++  # Generally "version NNN", "tag SSS", "bookmark SSS"
++  release: commit 32e48580c1c82bd6233a10dce6021c0f583237d5 (2021-07-29T13:12:33Z).
++
++  # Revision to pull in
++  # Must be a long or short commit SHA (long preferred)
++  revision: 32e48580c1c82bd6233a10dce6021c0f583237d5
++
++  # The package's license, where possible using the mnemonic from
++  # https://spdx.org/licenses/
++  # Multiple licenses can be specified (as a YAML list)
++  # A "LICENSE" file must exist containing the full license text
++  license: Apache-2.0
++
++  license-file: LICENSE
++
++vendoring:
++  url: https://github.com/google/highway.git
++  source-hosting: github
++  vendor-directory: third_party/highway
++
++  exclude:
++    - g3doc/
+diff --git a/third_party/highway/BUILD b/third_party/highway/BUILD
+index e38101f41d661..a846a1d2d6561 100644
+--- a/third_party/highway/BUILD
++++ b/third_party/highway/BUILD
+@@ -41,6 +41,13 @@ selects.config_setting_group(
+     ],
+ )
+ 
++config_setting(
++    name = "emulate_sve",
++    values = {
++        "copt": "-DHWY_EMULATE_SVE",
++    },
++)
++
+ # Additional warnings for Clang OR GCC (skip for MSVC)
+ CLANG_GCC_COPTS = [
+     "-Werror",
+@@ -76,11 +83,10 @@ COPTS = select({
+     "//conditions:default": CLANG_GCC_COPTS + CLANG_ONLY_COPTS,
+ })
+ 
+-# Unused on Bazel builds, where these are not defined/known; Copybara replaces
+-# usages of this with an empty list.
++# Unused on Bazel builds, where this is not defined/known; Copybara replaces
++# usages with an empty list.
+ COMPAT = [
+-    "//buildenv/target:mobile",
+-    "//buildenv/target:vendor",
++    "//buildenv/target:non_prod",  # includes mobile/vendor.
+ ]
+ 
+ # WARNING: changing flags such as HWY_DISABLED_TARGETS may break users without
+@@ -94,19 +100,23 @@ cc_library(
+         "hwy/aligned_allocator.cc",
+         "hwy/targets.cc",
+     ],
++    # Normal headers with include guards
+     hdrs = [
+         "hwy/aligned_allocator.h",
+         "hwy/base.h",
+         "hwy/cache_control.h",
+-        "hwy/highway.h",
++        "hwy/detect_compiler_arch.h",  # private
++        "hwy/detect_targets.h",  # private
+         "hwy/targets.h",
+     ],
+     compatible_with = [],
+     copts = COPTS,
+     textual_hdrs = [
++        "hwy/highway.h",  # public
+         "hwy/foreach_target.h",  # public
+         "hwy/ops/arm_neon-inl.h",
+         "hwy/ops/arm_sve-inl.h",
++        "hwy/ops/generic_ops-inl.h",
+         "hwy/ops/rvv-inl.h",
+         "hwy/ops/scalar-inl.h",
+         "hwy/ops/set_macros-inl.h",
+@@ -116,6 +126,10 @@ cc_library(
+         "hwy/ops/x86_256-inl.h",
+         "hwy/ops/x86_512-inl.h",
+     ],
++    deps = select({
++        ":emulate_sve": ["//third_party/farm_sve"],
++        "//conditions:default": [],
++    }),
+ )
+ 
+ cc_library(
+@@ -186,10 +200,13 @@ HWY_TESTS = [
+     ("hwy/", "highway_test"),
+     ("hwy/", "targets_test"),
+     ("hwy/tests/", "arithmetic_test"),
++    ("hwy/tests/", "blockwise_test"),
+     ("hwy/tests/", "combine_test"),
+     ("hwy/tests/", "compare_test"),
+     ("hwy/tests/", "convert_test"),
++    ("hwy/tests/", "crypto_test"),
+     ("hwy/tests/", "logical_test"),
++    ("hwy/tests/", "mask_test"),
+     ("hwy/tests/", "memory_test"),
+     ("hwy/tests/", "swizzle_test"),
+     ("hwy/tests/", "test_util_test"),
+@@ -204,9 +221,14 @@ HWY_TESTS = [
+             srcs = [
+                 subdir + test + ".cc",
+             ],
+-            copts = COPTS,
+-            # for test_suite. math_test is not yet supported on RVV.
+-            tags = ["hwy_ops_test"] if test != "math_test" else [],
++            copts = COPTS + [
++                # gTest triggers this warning (which is enabled by the
++                # extra-semi in COPTS), so we need to disable it here,
++                # but it's still enabled for :hwy.
++                "-Wno-c++98-compat-extra-semi",
++            ],
++            # for test_suite.
++            tags = ["hwy_ops_test"],
+             deps = [
+                 ":hwy",
+                 ":hwy_test_util",
+diff --git a/third_party/highway/CMakeLists.txt b/third_party/highway/CMakeLists.txt
+index c6ec4012b1dd7..177c961fab708 100644
+--- a/third_party/highway/CMakeLists.txt
++++ b/third_party/highway/CMakeLists.txt
+@@ -19,7 +19,7 @@ if(POLICY CMP0083)
+   cmake_policy(SET CMP0083 NEW)
+ endif()
+ 
+-project(hwy VERSION 0.12.1)  # Keep in sync with highway.h version
++project(hwy VERSION 0.14.0)  # Keep in sync with highway.h version
+ 
+ set(CMAKE_CXX_STANDARD 11)
+ set(CMAKE_CXX_EXTENSIONS OFF)
+@@ -64,12 +64,15 @@ set(HWY_SOURCES
+     hwy/aligned_allocator.h
+     hwy/base.h
+     hwy/cache_control.h
++    hwy/detect_compiler_arch.h  # private
++    hwy/detect_targets.h  # private
+     hwy/foreach_target.h
+     hwy/highway.h
+     hwy/nanobenchmark.cc
+     hwy/nanobenchmark.h
+     hwy/ops/arm_neon-inl.h
+     hwy/ops/arm_sve-inl.h
++    hwy/ops/generic_ops-inl.h
+     hwy/ops/scalar-inl.h
+     hwy/ops/set_macros-inl.h
+     hwy/ops/shared-inl.h
+@@ -107,7 +110,6 @@ else()
+ 
+   if(${CMAKE_CXX_COMPILER_ID} MATCHES "Clang")
+     list(APPEND HWY_FLAGS
+-      -Wc++2a-extensions
+       -Wfloat-overflow-conversion
+       -Wfloat-zero-conversion
+       -Wfor-loop-analysis
+@@ -126,6 +128,9 @@ else()
+       # Use color in messages
+       -fdiagnostics-show-option -fcolor-diagnostics
+     )
++    if (CMAKE_CXX_COMPILER_VERSION VERSION_GREATER_EQUAL 6.0)
++      list(APPEND HWY_FLAGS -Wc++2a-extensions)
++    endif()
+   endif()
+ 
+   if (WIN32)
+@@ -174,6 +179,28 @@ target_compile_options(hwy_contrib PRIVATE ${HWY_FLAGS})
+ set_property(TARGET hwy_contrib PROPERTY POSITION_INDEPENDENT_CODE ON)
+ target_include_directories(hwy_contrib PUBLIC ${CMAKE_CURRENT_LIST_DIR})
+ 
++# -------------------------------------------------------- hwy_list_targets
++# Generate a tool to print the compiled-in targets as defined by the current
++# flags. This tool will print to stderr at build time, after building hwy.
++add_executable(hwy_list_targets hwy/tests/list_targets.cc)
++target_compile_options(hwy_list_targets PRIVATE ${HWY_FLAGS})
++target_include_directories(hwy_list_targets PRIVATE
++  $<TARGET_PROPERTY:hwy,INCLUDE_DIRECTORIES>)
++# TARGET_FILE always returns the path to executable
++# Naked target also not always could be run (due to the lack of '.\' prefix)
++# Thus effective command to run should contain the full path
++# and emulator prefix (if any).
++add_custom_command(TARGET hwy POST_BUILD
++    COMMAND ${CMAKE_CROSSCOMPILING_EMULATOR} $<TARGET_FILE:hwy_list_targets> || (exit 0))
++
++# --------------------------------------------------------
++# The following sections are skipped if this project is added via
++# add_subdirectory into parent project (i.e. used as library).
++# Those sections contain artifacts that are not required for building
++# parent project: tests, examples, benchmarks and installation.
++# --------------------------------------------------------
++if (CMAKE_SOURCE_DIR STREQUAL CMAKE_CURRENT_SOURCE_DIR)
++
+ # -------------------------------------------------------- install library
+ install(TARGETS hwy
+   DESTINATION "${CMAKE_INSTALL_LIBDIR}")
+@@ -207,20 +234,6 @@ foreach (pc libhwy.pc libhwy-contrib.pc libhwy-test.pc)
+       DESTINATION "${CMAKE_INSTALL_LIBDIR}/pkgconfig")
+ endforeach()
+ 
+-# -------------------------------------------------------- hwy_list_targets
+-# Generate a tool to print the compiled-in targets as defined by the current
+-# flags. This tool will print to stderr at build time, after building hwy.
+-add_executable(hwy_list_targets hwy/tests/list_targets.cc)
+-target_compile_options(hwy_list_targets PRIVATE ${HWY_FLAGS})
+-target_include_directories(hwy_list_targets PRIVATE
+-  $<TARGET_PROPERTY:hwy,INCLUDE_DIRECTORIES>)
+-# TARGET_FILE always returns the path to executable
+-# Naked target also not always could be run (due to the lack of '.\' prefix)
+-# Thus effective command to run should contain the full path
+-# and emulator prefix (if any).
+-add_custom_command(TARGET hwy POST_BUILD
+-    COMMAND ${CMAKE_CROSSCOMPILING_EMULATOR} $<TARGET_FILE:hwy_list_targets> || (exit 0))
+-
+ # -------------------------------------------------------- Examples
+ 
+ # Avoids mismatch between GTest's static CRT and our dynamic.
+@@ -292,10 +305,13 @@ set(HWY_TEST_FILES
+   hwy/targets_test.cc
+   hwy/examples/skeleton_test.cc
+   hwy/tests/arithmetic_test.cc
++  hwy/tests/blockwise_test.cc
+   hwy/tests/combine_test.cc
+   hwy/tests/compare_test.cc
+   hwy/tests/convert_test.cc
++  hwy/tests/crypto_test.cc
+   hwy/tests/logical_test.cc
++  hwy/tests/mask_test.cc
+   hwy/tests/memory_test.cc
+   hwy/tests/swizzle_test.cc
+   hwy/tests/test_util_test.cc
+@@ -307,8 +323,11 @@ foreach (TESTFILE IN LISTS HWY_TEST_FILES)
+   get_filename_component(TESTNAME ${TESTFILE} NAME_WE)
+   add_executable(${TESTNAME} ${TESTFILE})
+   target_compile_options(${TESTNAME} PRIVATE ${HWY_FLAGS})
+-  # Test all targets, not just the best/baseline.
+-  target_compile_options(${TESTNAME} PRIVATE -DHWY_COMPILE_ALL_ATTAINABLE=1)
++  # Test all targets, not just the best/baseline. This changes the default
++  # policy to all-attainable; note that setting -DHWY_COMPILE_* directly can
++  # cause compile errors because only one may be set, and other CMakeLists.txt
++  # that include us may set them.
++  target_compile_options(${TESTNAME} PRIVATE -DHWY_IS_TEST=1)
+ 
+   if(HWY_SYSTEM_GTEST)
+     target_link_libraries(${TESTNAME} hwy hwy_contrib GTest::GTest GTest::Main)
+@@ -333,3 +352,5 @@ endforeach ()
+ target_sources(skeleton_test PRIVATE hwy/examples/skeleton.cc)
+ 
+ endif() # BUILD_TESTING
++
++endif() # CMAKE_SOURCE_DIR STREQUAL CMAKE_CURRENT_SOURCE_DIR
+diff --git a/third_party/highway/CMakeLists.txt.in b/third_party/highway/CMakeLists.txt.in
+index f98ccb4ac9780..df401705ee991 100644
+--- a/third_party/highway/CMakeLists.txt.in
++++ b/third_party/highway/CMakeLists.txt.in
+@@ -1,4 +1,4 @@
+-cmake_minimum_required(VERSION 2.8.2)
++cmake_minimum_required(VERSION 2.8.12)
+ 
+ project(googletest-download NONE)
+ 
+diff --git a/third_party/highway/README.md b/third_party/highway/README.md
+index a66bdc5e14224..45256a6a1cab9 100644
+--- a/third_party/highway/README.md
++++ b/third_party/highway/README.md
+@@ -14,8 +14,12 @@ applying the same operation to 'lanes'.
+ 
+ ## Current status
+ 
+-Supported targets: scalar, SSE4, AVX2, AVX-512, NEON (ARMv7 and v8), WASM SIMD.
+-Ports to RVV and SVE/SVE2 are in progress.
++Supported targets: scalar, S-SSE3, SSE4, AVX2, AVX-512, NEON (ARMv7 and v8),
++SVE, WASM SIMD.
++
++SVE is tested using farm_sve (see acknowledgments). SVE2 is implemented but not
++yet validated. A subset of RVV is implemented and tested with GCC and QEMU.
++Work is underway to compile using LLVM, which has different intrinsics with AVL.
+ 
+ Version 0.11 is considered stable enough to use in other projects, and is
+ expected to remain backwards compatible unless serious issues are discovered
+@@ -61,11 +65,6 @@ make -j && make test
+ 
+ Or you can run `run_tests.sh` (`run_tests.bat` on Windows).
+ 
+-To test on all the attainable targets for your platform, use
+-`cmake .. -DCMAKE_CXX_FLAGS="-DHWY_COMPILE_ALL_ATTAINABLE"`. Otherwise, the
+-default configuration skips baseline targets (e.g. scalar) that are superseded
+-by another baseline target.
+-
+ Bazel is also supported for building, but it is not as widely used/tested.
+ 
+ ## Quick start
+@@ -79,10 +78,11 @@ number of instructions per operation.
+ We recommend using full SIMD vectors whenever possible for maximum performance
+ portability. To obtain them, pass a `HWY_FULL(float)` tag to functions such as
+ `Zero/Set/Load`. There is also the option of a vector of up to `N` (a power of
+-two) lanes: `HWY_CAPPED(T, N)`. 128-bit vectors are guaranteed to be available
+-for lanes of type `T` if `HWY_TARGET != HWY_SCALAR` and `N == 16 / sizeof(T)`.
++two <= 16/sizeof(T)) lanes of type `T`: `HWY_CAPPED(T, N)`. If `HWY_TARGET ==
++HWY_SCALAR`, the vector always has one lane. For all other targets, up to
++128-bit vectors are guaranteed to be available.
+ 
+-Functions using Highway must be inside a namespace `namespace HWY_NAMESPACE {`
++Functions using Highway must be inside `namespace HWY_NAMESPACE {`
+ (possibly nested in one or more other namespaces defined by the project), and
+ additionally either prefixed with `HWY_ATTR`, or residing between
+ `HWY_BEFORE_NAMESPACE()` and `HWY_AFTER_NAMESPACE()`.
+@@ -97,7 +97,7 @@ additionally either prefixed with `HWY_ATTR`, or residing between
+ 
+ *   For dynamic dispatch, a table of function pointers is generated via the
+     `HWY_EXPORT` macro that is used by `HWY_DYNAMIC_DISPATCH(func)(args)` to
+-    call the best function pointer for the current CPU supported targets. A
++    call the best function pointer for the current CPU's supported targets. A
+     module is automatically compiled for each target in `HWY_TARGETS` (see
+     [quick-reference](g3doc/quick_reference.md)) if `HWY_TARGET_INCLUDE` is
+     defined and foreach_target.h is included.
+@@ -139,10 +139,7 @@ Highway offers several ways to express loops where `N` need not divide `count`:
+     The template parameter and second function arguments are again not needed.
+ 
+     This avoids duplicating code, and is reasonable if `count` is large.
+-    Otherwise, multiple iterations may be slower than one `LoopBody` variant
+-    with masking, especially because the `HWY_SCALAR` target selected by
+-    `HWY_CAPPED(T, 1)` is slower for some operations due to workarounds for
+-    undefined behavior in C++.
++    If `count` is small, the second loop may be slower than the next option.
+ 
+ *   Process whole vectors as above, followed by a single call to a modified
+     `LoopBody` with masking:
+@@ -157,9 +154,8 @@ Highway offers several ways to express loops where `N` need not divide `count`:
+     }
+     ```
+     Now the template parameter and second function argument can be used inside
+-    `LoopBody` to replace `Load/Store` of full aligned vectors with
+-    `LoadN/StoreN(n)` that affect no more than `1 <= n <= N` aligned elements
+-    (pending implementation).
++    `LoopBody` to 'blend' the new partial vector with previous memory contents:
++    `Store(IfThenElse(FirstN(d, N), partial, prev_full), d, aligned_pointer);`.
+ 
+     This is a good default when it is infeasible to ensure vectors are padded.
+     In contrast to the scalar loop, only a single final iteration is needed.
+@@ -170,7 +166,9 @@ Highway offers several ways to express loops where `N` need not divide `count`:
+     the trouble of using SIMD clearly cares about speed. However, portability,
+     maintainability and readability also matter, otherwise we would write in
+     assembly. We aim for performance within 10-20% of a hand-written assembly
+-    implementation on the development platform.
++    implementation on the development platform. There is no performance gap vs.
++    intrinsics: Highway code can do anything they can. If necessary, you can use
++    platform-specific instructions inside `#if HWY_TARGET == HWY_NEON` etc.
+ 
+ *   The guiding principles of C++ are "pay only for what you use" and "leave no
+     room for a lower-level language below C++". We apply these by defining a
+@@ -192,14 +190,12 @@ Highway offers several ways to express loops where `N` need not divide `count`:
+     blocks) and AVX-512 added two kinds of predicates (writemask and zeromask).
+     To ensure the API reflects hardware realities, we suggest a flexible
+     approach that adds new operations as they become commonly available, with
+-    scalar fallbacks where not supported.
++    fallback implementations where necessary.
+ 
+-*   Masking is not yet widely supported on current CPUs. It is difficult to
+-    define an interface that provides access to all platform features while
+-    retaining performance portability. The P0214R5 proposal lacks support for
+-    AVX-512/ARM SVE zeromasks. We suggest limiting usage of masks to the
+-    `IfThen[Zero]Else[Zero]` functions until the community has gained more
+-    experience with them.
++*   Masking/predication differs between platforms, and it is not clear how
++    important the use cases are beyond the ternary operator `IfThenElse`.
++    AVX-512/ARM SVE zeromasks are useful, but not supported by P0214R5.
++    We provide `IfThen[Zero]Else[Zero]` variants.
+ 
+ *   "Width-agnostic" SIMD is more future-proof than user-specified fixed sizes.
+     For example, valarray-like code can iterate over a 1D array with a
+@@ -209,7 +205,7 @@ Highway offers several ways to express loops where `N` need not divide `count`:
+     RiscV V as well as Agner Fog's
+     [ForwardCom instruction set proposal](https://goo.gl/CFizWu). However, some
+     applications may require fixed sizes, so we also guarantee support for
+-    128-bit vectors in each instruction set.
++    <= 128-bit vectors in each instruction set.
+ 
+ *   The API and its implementation should be usable and efficient with commonly
+     used compilers, including MSVC. For example, we write `ShiftLeft<3>(v)`
+@@ -227,23 +223,23 @@ Highway offers several ways to express loops where `N` need not divide `count`:
+     Therefore, we provide code paths for multiple instruction sets and choose
+     the most suitable at runtime. To reduce overhead, dispatch should be hoisted
+     to higher layers instead of checking inside every low-level function.
+-    Highway supports inlining functions in the same file or in *-inl.h headers.
+-    We generate all code paths from the same source to reduce implementation-
+-    and debugging cost.
++    Highway supports inlining functions in the same file or in `*-inl.h`
++    headers. We generate all code paths from the same source to reduce
++    implementation- and debugging cost.
+ 
+ *   Not every CPU need be supported. For example, pre-SSE4.1 CPUs are
+     increasingly rare and the AVX instruction set is limited to floating-point
+     operations. To reduce code size and compile time, we provide specializations
+-    for SSE4, AVX2 and AVX-512 instruction sets on x86, plus a scalar fallback.
++    for S-SSE3, SSE4, AVX2 and AVX-512 instruction sets on x86, plus a scalar
++    fallback.
+ 
+ *   Access to platform-specific intrinsics is necessary for acceptance in
+     performance-critical projects. We provide conversions to and from intrinsics
+     to allow utilizing specialized platform-specific functionality, and simplify
+     incremental porting of existing code.
+ 
+-*   The core API should be compact and easy to learn. We provide only the few
+-    dozen operations which are necessary and sufficient for most of the 150+
+-    SIMD applications we examined.
++*   The core API should be compact and easy to learn; we provide a concise
++    summary in g3doc/quick_reference.md.
+ 
+ ## Prior API designs
+ 
+@@ -258,6 +254,10 @@ runtime dispatch.
+ 
+ ## Differences versus [P0214R5 proposal](https://goo.gl/zKW4SA)
+ 
++1.  Allowing the use of built-in vector types by relying on non-member
++    functions. By contrast, P0214R5 requires a wrapper class, which does not
++    work for sizeless vector types currently used by ARM SVE and Risc-V.
++
+ 1.  Adding widely used and portable operations such as `AndNot`, `AverageRound`,
+     bit-shift by immediates and `IfThenElse`.
+ 
+@@ -286,13 +286,6 @@ runtime dispatch.
+     static target selection and runtime dispatch for hotspots that may benefit
+     from newer instruction sets if available.
+ 
+-1.  Using built-in PPC vector types without a wrapper class. This leads to much
+-    better code generation with GCC 6.3: https://godbolt.org/z/pd2PNP.
+-    By contrast, P0214R5 requires a wrapper. We avoid this by using only the
+-    member operators provided by the PPC vectors; all other functions and
+-    typedefs are non-members. 2019-04 update: Clang power64le does not have
+-    this issue, so we simplified get_part(d, v) to GetLane(v).
+-
+ 1.  Omitting inefficient or non-performance-portable operations such as `hmax`,
+     `operator[]`, and unsupported integer comparisons. Applications can often
+     replace these operations at lower cost than emulating that exact behavior.
+@@ -301,8 +294,8 @@ runtime dispatch.
+ 
+ 1.  Ensuring signed integer overflow has well-defined semantics (wraparound).
+ 
+-1.  Simple header-only implementation and less than a tenth of the size of the
+-    Vc library from which P0214 was derived (98,000 lines in
++1.  Simple header-only implementation and a fraction of the size of the
++    Vc library from which P0214 was derived (39K, vs. 92K lines in
+     https://github.com/VcDevel/Vc according to the gloc Chrome extension).
+ 
+ 1.  Avoiding hidden performance costs. P0214R5 allows implicit conversions from
+@@ -338,7 +331,7 @@ vectors cannot be default-constructed. We instead use a dedicated 'descriptor'
+ type `Simd` for overloading, abbreviated to `D` for template arguments and
+ `d` in lvalues.
+ 
+-Note that generic function templates are possible (see highway.h).
++Note that generic function templates are possible (see generic_ops-inlz.h).
+ 
+ ## Masks
+ 
+@@ -362,5 +355,11 @@ set, we provide a special `ZeroIfNegative` function.
+ [intro]: g3doc/highway_intro.pdf
+ [instmtx]: g3doc/instruction_matrix.pdf
+ 
++## Acknowledgments
++
++We have used [farm-sve](https://gitlab.inria.fr/bramas/farm-sve) by Berenger
++Bramas; it has proved useful for checking the SVE port on an x86 development
++machine.
++
+ This is not an officially supported Google product.
+ Contact: janwas@google.com
+diff --git a/third_party/highway/debian/changelog b/third_party/highway/debian/changelog
+index 75768968fbea7..14b667c016c95 100644
+--- a/third_party/highway/debian/changelog
++++ b/third_party/highway/debian/changelog
+@@ -1,3 +1,19 @@
++highway (0.14.0-1) UNRELEASED; urgency=medium
++
++  * Add SVE, S-SSE3, AVX3_DL targets
++  * Support partial vectors in all ops
++  * Add PopulationCount, FindFirstTrue, Ne, TableLookupBytesOr0
++  * Add AESRound, CLMul, MulOdd, HWY_CAP_FLOAT16
++
++ -- Jan Wassenberg <janwas@google.com>  Thu, 29 Jul 2021 15:00:00 +0200
++
++highway (0.12.2-1) UNRELEASED; urgency=medium
++
++  * fix scalar-only test and Windows macro conflict with Load/StoreFence
++  * replace deprecated wasm intrinsics
++
++ -- Jan Wassenberg <janwas@google.com>  Mon, 31 May 2021 16:00:00 +0200
++
+ highway (0.12.1-1) UNRELEASED; urgency=medium
+ 
+   * doc updates, ARM GCC support, fix s390/ppc, complete partial vectors
+diff --git a/third_party/highway/debian/rules b/third_party/highway/debian/rules
+old mode 100644
+new mode 100755
+diff --git a/third_party/highway/hwy/aligned_allocator.cc b/third_party/highway/hwy/aligned_allocator.cc
+index bec7c3bb1b701..4fcc364062e71 100644
+--- a/third_party/highway/hwy/aligned_allocator.cc
++++ b/third_party/highway/hwy/aligned_allocator.cc
+@@ -27,10 +27,21 @@
+ namespace hwy {
+ namespace {
+ 
+-constexpr size_t kAlignment = HWY_MAX(HWY_ALIGNMENT, kMaxVectorSize);
++#if HWY_ARCH_RVV && defined(__riscv_vector)
++// Not actually an upper bound on the size, but this value prevents crossing a
++// 4K boundary (relevant on Andes).
++constexpr size_t kAlignment = HWY_MAX(HWY_ALIGNMENT, 4096);
++#else
++constexpr size_t kAlignment = HWY_ALIGNMENT;
++#endif
++
++#if HWY_ARCH_X86
+ // On x86, aliasing can only occur at multiples of 2K, but that's too wasteful
+ // if this is used for single-vector allocations. 256 is more reasonable.
+ constexpr size_t kAlias = kAlignment * 4;
++#else
++constexpr size_t kAlias = kAlignment;
++#endif
+ 
+ #pragma pack(push, 1)
+ struct AllocationHeader {
+@@ -94,7 +105,7 @@ void* AllocateAlignedBytes(const size_t payload_size, AllocPtr alloc_ptr,
+   header->allocated = allocated;
+   header->payload_size = payload_size;
+ 
+-  return HWY_ASSUME_ALIGNED(reinterpret_cast<void*>(payload), kMaxVectorSize);
++  return HWY_ASSUME_ALIGNED(reinterpret_cast<void*>(payload), kAlignment);
+ }
+ 
+ void FreeAlignedBytes(const void* aligned_pointer, FreePtr free_ptr,
+diff --git a/third_party/highway/hwy/aligned_allocator_test.cc b/third_party/highway/hwy/aligned_allocator_test.cc
+index c11033b18c1e1..f729a2865fd10 100644
+--- a/third_party/highway/hwy/aligned_allocator_test.cc
++++ b/third_party/highway/hwy/aligned_allocator_test.cc
+@@ -120,7 +120,7 @@ TEST(AlignedAllocatorTest, AllocDefaultPointers) {
+                                    /*opaque_ptr=*/nullptr);
+   ASSERT_NE(nullptr, ptr);
+   // Make sure the pointer is actually aligned.
+-  EXPECT_EQ(0U, reinterpret_cast<uintptr_t>(ptr) % kMaxVectorSize);
++  EXPECT_EQ(0U, reinterpret_cast<uintptr_t>(ptr) % HWY_ALIGNMENT);
+   char* p = static_cast<char*>(ptr);
+   size_t ret = 0;
+   for (size_t i = 0; i < kSize; i++) {
+@@ -152,7 +152,7 @@ TEST(AlignedAllocatorTest, CustomAlloc) {
+   // We should have only requested one alloc from the allocator.
+   EXPECT_EQ(1U, fake_alloc.PendingAllocs());
+   // Make sure the pointer is actually aligned.
+-  EXPECT_EQ(0U, reinterpret_cast<uintptr_t>(ptr) % kMaxVectorSize);
++  EXPECT_EQ(0U, reinterpret_cast<uintptr_t>(ptr) % HWY_ALIGNMENT);
+   FreeAlignedBytes(ptr, &FakeAllocator::StaticFree, &fake_alloc);
+   EXPECT_EQ(0U, fake_alloc.PendingAllocs());
+ }
+@@ -197,7 +197,7 @@ TEST(AlignedAllocatorTest, MakeUniqueAlignedArray) {
+ TEST(AlignedAllocatorTest, AllocSingleInt) {
+   auto ptr = AllocateAligned<uint32_t>(1);
+   ASSERT_NE(nullptr, ptr.get());
+-  EXPECT_EQ(0U, reinterpret_cast<uintptr_t>(ptr.get()) % kMaxVectorSize);
++  EXPECT_EQ(0U, reinterpret_cast<uintptr_t>(ptr.get()) % HWY_ALIGNMENT);
+   // Force delete of the unique_ptr now to check that it doesn't crash.
+   ptr.reset(nullptr);
+   EXPECT_EQ(nullptr, ptr.get());
+@@ -207,7 +207,7 @@ TEST(AlignedAllocatorTest, AllocMultipleInt) {
+   const size_t kSize = 7777;
+   auto ptr = AllocateAligned<uint32_t>(kSize);
+   ASSERT_NE(nullptr, ptr.get());
+-  EXPECT_EQ(0U, reinterpret_cast<uintptr_t>(ptr.get()) % kMaxVectorSize);
++  EXPECT_EQ(0U, reinterpret_cast<uintptr_t>(ptr.get()) % HWY_ALIGNMENT);
+   // ptr[i] is actually (*ptr.get())[i] which will use the operator[] of the
+   // underlying type chosen by AllocateAligned() for the std::unique_ptr.
+   EXPECT_EQ(&(ptr[0]) + 1, &(ptr[1]));
+@@ -276,3 +276,9 @@ TEST(AlignedAllocatorTest, DefaultInit) {
+ }
+ 
+ }  // namespace hwy
++
++// Ought not to be necessary, but without this, no tests run on RVV.
++int main(int argc, char** argv) {
++  ::testing::InitGoogleTest(&argc, argv);
++  return RUN_ALL_TESTS();
++}
+diff --git a/third_party/highway/hwy/base.h b/third_party/highway/hwy/base.h
+index 75fe585e4c296..1ecd215d4103a 100644
+--- a/third_party/highway/hwy/base.h
++++ b/third_party/highway/hwy/base.h
+@@ -23,72 +23,7 @@
+ #include <atomic>
+ #include <cfloat>
+ 
+-// Add to #if conditions to prevent IDE from graying out code.
+-#if (defined __CDT_PARSER__) || (defined __INTELLISENSE__) || \
+-    (defined Q_CREATOR_RUN) || (defined(__CLANGD__))
+-#define HWY_IDE 1
+-#else
+-#define HWY_IDE 0
+-#endif
+-
+-//------------------------------------------------------------------------------
+-// Detect compiler using predefined macros
+-
+-// clang-cl defines _MSC_VER but doesn't behave like MSVC in other aspects like
+-// used in HWY_DIAGNOSTICS(). We include a check that we are not clang for that
+-// purpose.
+-#if defined(_MSC_VER) && !defined(__clang__)
+-#define HWY_COMPILER_MSVC _MSC_VER
+-#else
+-#define HWY_COMPILER_MSVC 0
+-#endif
+-
+-#ifdef __INTEL_COMPILER
+-#define HWY_COMPILER_ICC __INTEL_COMPILER
+-#else
+-#define HWY_COMPILER_ICC 0
+-#endif
+-
+-#ifdef __GNUC__
+-#define HWY_COMPILER_GCC (__GNUC__ * 100 + __GNUC_MINOR__)
+-#else
+-#define HWY_COMPILER_GCC 0
+-#endif
+-
+-// Clang can masquerade as MSVC/GCC, in which case both are set.
+-#ifdef __clang__
+-#ifdef __APPLE__
+-// Apple LLVM version is unrelated to the actual Clang version, which we need
+-// for enabling workarounds. Use the presence of warning flags to deduce it.
+-// Adapted from https://github.com/simd-everywhere/simde/ simde-detect-clang.h.
+-#if __has_warning("-Wformat-insufficient-args")
+-#define HWY_COMPILER_CLANG 1200
+-#elif __has_warning("-Wimplicit-const-int-float-conversion")
+-#define HWY_COMPILER_CLANG 1100
+-#elif __has_warning("-Wmisleading-indentation")
+-#define HWY_COMPILER_CLANG 1000
+-#elif defined(__FILE_NAME__)
+-#define HWY_COMPILER_CLANG 900
+-#elif __has_warning("-Wextra-semi-stmt") || \
+-    __has_builtin(__builtin_rotateleft32)
+-#define HWY_COMPILER_CLANG 800
+-#elif __has_warning("-Wc++98-compat-extra-semi")
+-#define HWY_COMPILER_CLANG 700
+-#else  // Anything older than 7.0 is not recommended for Highway.
+-#define HWY_COMPILER_CLANG 600
+-#endif  // __has_warning chain
+-#else   // Non-Apple: normal version
+-#define HWY_COMPILER_CLANG (__clang_major__ * 100 + __clang_minor__)
+-#endif
+-#else  // Not clang
+-#define HWY_COMPILER_CLANG 0
+-#endif
+-
+-// More than one may be nonzero, but we want at least one.
+-#if !HWY_COMPILER_MSVC && !HWY_COMPILER_ICC && !HWY_COMPILER_GCC && \
+-    !HWY_COMPILER_CLANG
+-#error "Unsupported compiler"
+-#endif
++#include "hwy/detect_compiler_arch.h"
+ 
+ //------------------------------------------------------------------------------
+ // Compiler-specific definitions
+@@ -140,18 +75,6 @@
+ //------------------------------------------------------------------------------
+ // Builtin/attributes
+ 
+-#ifdef __has_builtin
+-#define HWY_HAS_BUILTIN(name) __has_builtin(name)
+-#else
+-#define HWY_HAS_BUILTIN(name) 0
+-#endif
+-
+-#ifdef __has_attribute
+-#define HWY_HAS_ATTRIBUTE(name) __has_attribute(name)
+-#else
+-#define HWY_HAS_ATTRIBUTE(name) 0
+-#endif
+-
+ // Enables error-checking of format strings.
+ #if HWY_HAS_ATTRIBUTE(__format__)
+ #define HWY_FORMAT(idx_fmt, idx_arg) \
+@@ -175,9 +98,9 @@
+ // are inlined. Support both per-function annotation (HWY_ATTR) for lambdas and
+ // automatic annotation via pragmas.
+ #if HWY_COMPILER_CLANG
+-#define HWY_PUSH_ATTRIBUTES(targets_str)                                     \
++#define HWY_PUSH_ATTRIBUTES(targets_str)                                \
+   HWY_PRAGMA(clang attribute push(__attribute__((target(targets_str))), \
+-                                       apply_to = function))
++                                  apply_to = function))
+ #define HWY_POP_ATTRIBUTES HWY_PRAGMA(clang attribute pop)
+ #elif HWY_COMPILER_GCC
+ #define HWY_PUSH_ATTRIBUTES(targets_str) \
+@@ -188,78 +111,6 @@
+ #define HWY_POP_ATTRIBUTES
+ #endif
+ 
+-//------------------------------------------------------------------------------
+-// Detect architecture using predefined macros
+-
+-#if defined(__i386__) || defined(_M_IX86)
+-#define HWY_ARCH_X86_32 1
+-#else
+-#define HWY_ARCH_X86_32 0
+-#endif
+-
+-#if defined(__x86_64__) || defined(_M_X64)
+-#define HWY_ARCH_X86_64 1
+-#else
+-#define HWY_ARCH_X86_64 0
+-#endif
+-
+-#if HWY_ARCH_X86_32 && HWY_ARCH_X86_64
+-#error "Cannot have both x86-32 and x86-64"
+-#endif
+-
+-#if HWY_ARCH_X86_32 || HWY_ARCH_X86_64
+-#define HWY_ARCH_X86 1
+-#else
+-#define HWY_ARCH_X86 0
+-#endif
+-
+-#if defined(__powerpc64__) || defined(_M_PPC)
+-#define HWY_ARCH_PPC 1
+-#else
+-#define HWY_ARCH_PPC 0
+-#endif
+-
+-#if defined(__ARM_ARCH_ISA_A64) || defined(__aarch64__) || defined(_M_ARM64)
+-#define HWY_ARCH_ARM_A64 1
+-#else
+-#define HWY_ARCH_ARM_A64 0
+-#endif
+-
+-#if defined(__arm__) || defined(_M_ARM)
+-#define HWY_ARCH_ARM_V7 1
+-#else
+-#define HWY_ARCH_ARM_V7 0
+-#endif
+-
+-#if HWY_ARCH_ARM_A64 && HWY_ARCH_ARM_V7
+-#error "Cannot have both A64 and V7"
+-#endif
+-
+-#if HWY_ARCH_ARM_A64 || HWY_ARCH_ARM_V7
+-#define HWY_ARCH_ARM 1
+-#else
+-#define HWY_ARCH_ARM 0
+-#endif
+-
+-#if defined(__EMSCRIPTEN__) || defined(__wasm__) || defined(__WASM__)
+-#define HWY_ARCH_WASM 1
+-#else
+-#define HWY_ARCH_WASM 0
+-#endif
+-
+-#ifdef __riscv
+-#define HWY_ARCH_RVV 1
+-#else
+-#define HWY_ARCH_RVV 0
+-#endif
+-
+-// It is an error to detect multiple architectures at the same time, but OK to
+-// detect none of the above.
+-#if (HWY_ARCH_X86 + HWY_ARCH_PPC + HWY_ARCH_ARM + HWY_ARCH_WASM + \
+-     HWY_ARCH_RVV) > 1
+-#error "Must not detect more than one architecture"
+-#endif
+-
+ //------------------------------------------------------------------------------
+ // Macros
+ 
+@@ -305,24 +156,33 @@
+   } while (0)
+ #endif
+ 
++#if defined(HWY_EMULATE_SVE)
++class FarmFloat16;
++#endif
+ 
+ namespace hwy {
+ 
+ //------------------------------------------------------------------------------
+-// Alignment
++// kMaxVectorSize (undocumented, pending removal)
+ 
+-// Not guaranteed to be an upper bound, but the alignment established by
+-// aligned_allocator is HWY_MAX(HWY_ALIGNMENT, kMaxVectorSize).
+ #if HWY_ARCH_X86
+ static constexpr HWY_MAYBE_UNUSED size_t kMaxVectorSize = 64;  // AVX-512
+-#define HWY_ALIGN_MAX alignas(64)
+ #elif HWY_ARCH_RVV && defined(__riscv_vector)
+-// Not actually an upper bound on the size, but this value prevents crossing a
+-// 4K boundary (relevant on Andes).
++// Not actually an upper bound on the size.
+ static constexpr HWY_MAYBE_UNUSED size_t kMaxVectorSize = 4096;
+-#define HWY_ALIGN_MAX alignas(8)  // only elements need be aligned
+ #else
+ static constexpr HWY_MAYBE_UNUSED size_t kMaxVectorSize = 16;
++#endif
++
++//------------------------------------------------------------------------------
++// Alignment
++
++// For stack-allocated partial arrays or LoadDup128.
++#if HWY_ARCH_X86
++#define HWY_ALIGN_MAX alignas(64)
++#elif HWY_ARCH_RVV && defined(__riscv_vector)
++#define HWY_ALIGN_MAX alignas(8)  // only elements need be aligned
++#else
+ #define HWY_ALIGN_MAX alignas(16)
+ #endif
+ 
+@@ -333,13 +193,16 @@ static constexpr HWY_MAYBE_UNUSED size_t kMaxVectorSize = 16;
+ // by concatenating base type and bits.
+ 
+ // RVV already has a builtin type and the GCC intrinsics require it.
+-#if HWY_ARCH_RVV && HWY_COMPILER_GCC && defined(__riscv_vector)
++#if (HWY_ARCH_RVV && HWY_COMPILER_GCC && defined(__riscv_vector)) || \
++    (HWY_ARCH_ARM && (__ARM_FP & 2))
+ #define HWY_NATIVE_FLOAT16 1
+ #else
+ #define HWY_NATIVE_FLOAT16 0
+ #endif
+ 
+-#if HWY_NATIVE_FLOAT16
++#if defined(HWY_EMULATE_SVE)
++using float16_t = FarmFloat16;
++#elif HWY_NATIVE_FLOAT16
+ using float16_t = __fp16;
+ // Clang does not allow __fp16 arguments, but scalar.h requires LaneType
+ // arguments, so use a wrapper.
+@@ -368,6 +231,21 @@ struct EnableIfT<true, T> {
+ template <bool Condition, class T = void>
+ using EnableIf = typename EnableIfT<Condition, T>::type;
+ 
++template <typename T, typename U>
++struct IsSameT {
++  enum { value = 0 };
++};
++
++template <typename T>
++struct IsSameT<T, T> {
++  enum { value = 1 };
++};
++
++template <typename T, typename U>
++HWY_API constexpr bool IsSame() {
++  return IsSameT<T, U>::value;
++}
++
+ // Insert into template/function arguments to enable this overload only for
+ // vectors of AT MOST this many bits.
+ //
+@@ -377,6 +255,9 @@ using EnableIf = typename EnableIfT<Condition, T>::type;
+ #define HWY_IF_LE128(T, N) hwy::EnableIf<N * sizeof(T) <= 16>* = nullptr
+ #define HWY_IF_LE64(T, N) hwy::EnableIf<N * sizeof(T) <= 8>* = nullptr
+ #define HWY_IF_LE32(T, N) hwy::EnableIf<N * sizeof(T) <= 4>* = nullptr
++#define HWY_IF_GE64(T, N) hwy::EnableIf<N * sizeof(T) >= 8>* = nullptr
++#define HWY_IF_GE128(T, N) hwy::EnableIf<N * sizeof(T) >= 16>* = nullptr
++#define HWY_IF_GT128(T, N) hwy::EnableIf<(N * sizeof(T) > 16)>* = nullptr
+ 
+ #define HWY_IF_UNSIGNED(T) hwy::EnableIf<!IsSigned<T>()>* = nullptr
+ #define HWY_IF_SIGNED(T) \
+@@ -393,18 +274,36 @@ using EnableIf = typename EnableIfT<Condition, T>::type;
+ template <size_t N>
+ struct SizeTag {};
+ 
++template <class T>
++struct RemoveConstT {
++  using type = T;
++};
++template <class T>
++struct RemoveConstT<const T> {
++  using type = T;
++};
++
++template <class T>
++using RemoveConst = typename RemoveConstT<T>::type;
++
+ //------------------------------------------------------------------------------
+ // Type traits
+ 
+ template <typename T>
+ constexpr bool IsFloat() {
+-  return T(1.25) != T(1);
++  // Cannot use T(1.25) != T(1) for float16_t, which can only be converted to or
++  // from a float, not compared.
++  return IsSame<T, float>() || IsSame<T, double>();
+ }
+ 
+ template <typename T>
+ constexpr bool IsSigned() {
+   return T(0) > T(-1);
+ }
++template <>
++constexpr bool IsSigned<float16_t>() {
++  return true;
++}
+ 
+ // Largest/smallest representable integer values.
+ template <typename T>
+@@ -555,6 +454,7 @@ struct Relations<float> {
+   using Signed = int32_t;
+   using Float = float;
+   using Wide = double;
++  using Narrow = float16_t;
+ };
+ template <>
+ struct Relations<double> {
+@@ -564,6 +464,31 @@ struct Relations<double> {
+   using Narrow = float;
+ };
+ 
++template <size_t N>
++struct TypeFromSize;
++template <>
++struct TypeFromSize<1> {
++  using Unsigned = uint8_t;
++  using Signed = int8_t;
++};
++template <>
++struct TypeFromSize<2> {
++  using Unsigned = uint16_t;
++  using Signed = int16_t;
++};
++template <>
++struct TypeFromSize<4> {
++  using Unsigned = uint32_t;
++  using Signed = int32_t;
++  using Float = float;
++};
++template <>
++struct TypeFromSize<8> {
++  using Unsigned = uint64_t;
++  using Signed = int64_t;
++  using Float = double;
++};
++
+ }  // namespace detail
+ 
+ // Aliases for types of a different category, but the same size.
+@@ -580,6 +505,14 @@ using MakeWide = typename detail::Relations<T>::Wide;
+ template <typename T>
+ using MakeNarrow = typename detail::Relations<T>::Narrow;
+ 
++// Obtain type from its size [bytes].
++template <size_t N>
++using UnsignedFromSize = typename detail::TypeFromSize<N>::Unsigned;
++template <size_t N>
++using SignedFromSize = typename detail::TypeFromSize<N>::Signed;
++template <size_t N>
++using FloatFromSize = typename detail::TypeFromSize<N>::Float;
++
+ //------------------------------------------------------------------------------
+ // Helper functions
+ 
+@@ -599,11 +532,21 @@ HWY_API size_t Num0BitsBelowLS1Bit_Nonzero32(const uint32_t x) {
+   unsigned long index;  // NOLINT
+   _BitScanForward(&index, x);
+   return index;
+-#else  // HWY_COMPILER_MSVC
++#else   // HWY_COMPILER_MSVC
+   return static_cast<size_t>(__builtin_ctz(x));
+ #endif  // HWY_COMPILER_MSVC
+ }
+ 
++HWY_API size_t Num0BitsBelowLS1Bit_Nonzero64(const uint64_t x) {
++#if HWY_COMPILER_MSVC
++  unsigned long index;  // NOLINT
++  _BitScanForward64(&index, x);
++  return index;
++#else   // HWY_COMPILER_MSVC
++  return static_cast<size_t>(__builtin_ctzll(x));
++#endif  // HWY_COMPILER_MSVC
++}
++
+ HWY_API size_t PopCount(uint64_t x) {
+ #if HWY_COMPILER_CLANG || HWY_COMPILER_GCC
+   return static_cast<size_t>(__builtin_popcountll(x));
+@@ -623,6 +566,30 @@ HWY_API size_t PopCount(uint64_t x) {
+ #endif
+ }
+ 
++#if HWY_COMPILER_MSVC && HWY_ARCH_X86_64
++#pragma intrinsic(_umul128)
++#endif
++
++// 64 x 64 = 128 bit multiplication
++HWY_API uint64_t Mul128(uint64_t a, uint64_t b, uint64_t* HWY_RESTRICT upper) {
++#if defined(__SIZEOF_INT128__)
++  __uint128_t product = (__uint128_t)a * (__uint128_t)b;
++  *upper = (uint64_t)(product >> 64);
++  return (uint64_t)(product & 0xFFFFFFFFFFFFFFFFULL);
++#elif HWY_COMPILER_MSVC && HWY_ARCH_X86_64
++  return _umul128(a, b, upper);
++#else
++  constexpr uint64_t kLo32 = 0xFFFFFFFFU;
++  const uint64_t lo_lo = (a & kLo32) * (b & kLo32);
++  const uint64_t hi_lo = (a >> 32) * (b & kLo32);
++  const uint64_t lo_hi = (a & kLo32) * (b >> 32);
++  const uint64_t hi_hi = (a >> 32) * (b >> 32);
++  const uint64_t t = (lo_lo >> 32) + (hi_lo & kLo32) + lo_hi;
++  *upper = (hi_lo >> 32) + (t >> 32) + hi_hi;
++  return (t << 32) | (lo_lo & kLo32);
++#endif
++}
++
+ // The source/destination must not overlap/alias.
+ template <size_t kBytes, typename From, typename To>
+ HWY_API void CopyBytes(const From* from, To* to) {
+diff --git a/third_party/highway/hwy/base_test.cc b/third_party/highway/hwy/base_test.cc
+index 19e0b6f5445c4..df04f68f0aecf 100644
+--- a/third_party/highway/hwy/base_test.cc
++++ b/third_party/highway/hwy/base_test.cc
+@@ -90,6 +90,17 @@ HWY_NOINLINE void TestAllType() {
+   ForFloatTypes(TestIsFloat());
+ }
+ 
++struct TestIsSame {
++  template <class T>
++  HWY_NOINLINE void operator()(T /*unused*/) const {
++    static_assert(IsSame<T, T>(), "T == T");
++    static_assert(!IsSame<MakeSigned<T>, MakeUnsigned<T>>(), "S != U");
++    static_assert(!IsSame<MakeUnsigned<T>, MakeSigned<T>>(), "U != S");
++  }
++};
++
++HWY_NOINLINE void TestAllIsSame() { ForAllTypes(TestIsSame()); }
++
+ HWY_NOINLINE void TestAllPopCount() {
+   HWY_ASSERT_EQ(size_t(0), PopCount(0u));
+   HWY_ASSERT_EQ(size_t(1), PopCount(1u));
+@@ -113,11 +124,20 @@ HWY_NOINLINE void TestAllPopCount() {
+ HWY_AFTER_NAMESPACE();
+ 
+ #if HWY_ONCE
++
+ namespace hwy {
+ HWY_BEFORE_TEST(BaseTest);
+ HWY_EXPORT_AND_TEST_P(BaseTest, TestAllLimits);
+ HWY_EXPORT_AND_TEST_P(BaseTest, TestAllLowestHighest);
+ HWY_EXPORT_AND_TEST_P(BaseTest, TestAllType);
++HWY_EXPORT_AND_TEST_P(BaseTest, TestAllIsSame);
+ HWY_EXPORT_AND_TEST_P(BaseTest, TestAllPopCount);
+ }  // namespace hwy
++
++// Ought not to be necessary, but without this, no tests run on RVV.
++int main(int argc, char **argv) {
++  ::testing::InitGoogleTest(&argc, argv);
++  return RUN_ALL_TESTS();
++}
++
+ #endif
+diff --git a/third_party/highway/hwy/cache_control.h b/third_party/highway/hwy/cache_control.h
+index ab0a2347982b9..65f326a5f5f1f 100644
+--- a/third_party/highway/hwy/cache_control.h
++++ b/third_party/highway/hwy/cache_control.h
+@@ -32,6 +32,14 @@
+ #include <emmintrin.h>  // SSE2
+ #endif
+ 
++// Windows.h #defines these, which causes infinite recursion. Temporarily
++// undefine them in this header; these functions are anyway deprecated.
++// TODO(janwas): remove when these functions are removed.
++#pragma push_macro("LoadFence")
++#pragma push_macro("StoreFence")
++#undef LoadFence
++#undef StoreFence
++
+ namespace hwy {
+ 
+ // Even if N*sizeof(T) is smaller, Stream may write a multiple of this size.
+@@ -47,20 +55,28 @@ namespace hwy {
+ // Delays subsequent loads until prior loads are visible. On Intel CPUs, also
+ // serves as a full fence (waits for all prior instructions to complete).
+ // No effect on non-x86.
++// DEPRECATED due to differing behavior across architectures AND vendors.
+ HWY_INLINE HWY_ATTR_CACHE void LoadFence() {
+ #if HWY_ARCH_X86 && !defined(HWY_DISABLE_CACHE_CONTROL)
+   _mm_lfence();
+ #endif
+ }
+ 
+-// Ensures previous weakly-ordered stores are visible. No effect on non-x86.
+-HWY_INLINE HWY_ATTR_CACHE void StoreFence() {
++// Ensures values written by previous `Stream` calls are visible on the current
++// core. This is NOT sufficient for synchronizing across cores; when `Stream`
++// outputs are to be consumed by other core(s), the producer must publish
++// availability (e.g. via mutex or atomic_flag) after `FlushStream`.
++HWY_INLINE HWY_ATTR_CACHE void FlushStream() {
+ #if HWY_ARCH_X86 && !defined(HWY_DISABLE_CACHE_CONTROL)
+   _mm_sfence();
+ #endif
+ }
+ 
+-// Begins loading the cache line containing "p".
++// DEPRECATED, replace with `FlushStream`.
++HWY_INLINE HWY_ATTR_CACHE void StoreFence() { FlushStream(); }
++
++// Optionally begins loading the cache line containing "p" to reduce latency of
++// subsequent actual loads.
+ template <typename T>
+ HWY_INLINE HWY_ATTR_CACHE void Prefetch(const T* p) {
+ #if HWY_ARCH_X86 && !defined(HWY_DISABLE_CACHE_CONTROL)
+@@ -74,7 +90,7 @@ HWY_INLINE HWY_ATTR_CACHE void Prefetch(const T* p) {
+ #endif
+ }
+ 
+-// Invalidates and flushes the cache line containing "p". No effect on non-x86.
++// Invalidates and flushes the cache line containing "p", if possible.
+ HWY_INLINE HWY_ATTR_CACHE void FlushCacheline(const void* p) {
+ #if HWY_ARCH_X86 && !defined(HWY_DISABLE_CACHE_CONTROL)
+   _mm_clflush(p);
+@@ -83,7 +99,7 @@ HWY_INLINE HWY_ATTR_CACHE void FlushCacheline(const void* p) {
+ #endif
+ }
+ 
+-// Reduces power consumption in spin-loops. No effect on non-x86.
++// When called inside a spin-loop, may reduce power consumption.
+ HWY_INLINE HWY_ATTR_CACHE void Pause() {
+ #if HWY_ARCH_X86 && !defined(HWY_DISABLE_CACHE_CONTROL)
+   _mm_pause();
+@@ -92,4 +108,8 @@ HWY_INLINE HWY_ATTR_CACHE void Pause() {
+ 
+ }  // namespace hwy
+ 
++// TODO(janwas): remove when these functions are removed. (See above.)
++#pragma pop_macro("StoreFence")
++#pragma pop_macro("LoadFence")
++
+ #endif  // HIGHWAY_HWY_CACHE_CONTROL_H_
+diff --git a/third_party/highway/hwy/contrib/image/image.cc b/third_party/highway/hwy/contrib/image/image.cc
+index 0dfe739a4914c..0dc0108b1d19e 100644
+--- a/third_party/highway/hwy/contrib/image/image.cc
++++ b/third_party/highway/hwy/contrib/image/image.cc
+@@ -58,7 +58,7 @@ size_t ImageBase::BytesPerRow(const size_t xsize, const size_t sizeof_t) {
+   }
+ 
+   // Round up to vector and cache line size.
+-  const size_t align = std::max<size_t>(vec_size, HWY_ALIGNMENT);
++  const size_t align = HWY_MAX(vec_size, HWY_ALIGNMENT);
+   size_t bytes_per_row = RoundUpTo(valid_bytes, align);
+ 
+   // During the lengthy window before writes are committed to memory, CPUs
+diff --git a/third_party/highway/hwy/contrib/image/image_test.cc b/third_party/highway/hwy/contrib/image/image_test.cc
+index c27e52a195773..4a4e54f76f1cf 100644
+--- a/third_party/highway/hwy/contrib/image/image_test.cc
++++ b/third_party/highway/hwy/contrib/image/image_test.cc
+@@ -96,7 +96,7 @@ struct TestUnalignedT {
+         for (size_t y = 0; y < ysize; ++y) {
+           T* HWY_RESTRICT row = img.MutableRow(y);
+           for (size_t x = 0; x < xsize; ++x) {
+-            accum |= LoadU(d, row + x);
++            accum = Or(accum, LoadU(d, row + x));
+           }
+         }
+ 
+@@ -143,9 +143,17 @@ void TestUnaligned() { ForUnsignedTypes(TestUnalignedT()); }
+ HWY_AFTER_NAMESPACE();
+ 
+ #if HWY_ONCE
++
+ namespace hwy {
+ HWY_BEFORE_TEST(ImageTest);
+ HWY_EXPORT_AND_TEST_P(ImageTest, TestAligned);
+ HWY_EXPORT_AND_TEST_P(ImageTest, TestUnaligned);
+ }  // namespace hwy
++
++// Ought not to be necessary, but without this, no tests run on RVV.
++int main(int argc, char** argv) {
++  ::testing::InitGoogleTest(&argc, argv);
++  return RUN_ALL_TESTS();
++}
++
+ #endif
+diff --git a/third_party/highway/hwy/contrib/math/math-inl.h b/third_party/highway/hwy/contrib/math/math-inl.h
+index 15b80d63bafa2..dd8794be73a97 100644
+--- a/third_party/highway/hwy/contrib/math/math-inl.h
++++ b/third_party/highway/hwy/contrib/math/math-inl.h
+@@ -282,43 +282,49 @@ HWY_INLINE HWY_MAYBE_UNUSED T Estrin(T x, T c0, T c1) {
+ }
+ template <class T>
+ HWY_INLINE HWY_MAYBE_UNUSED T Estrin(T x, T c0, T c1, T c2) {
+-  T x2(x * x);
++  T x2 = Mul(x, x);
+   return MulAdd(x2, c2, MulAdd(c1, x, c0));
+ }
+ template <class T>
+ HWY_INLINE HWY_MAYBE_UNUSED T Estrin(T x, T c0, T c1, T c2, T c3) {
+-  T x2(x * x);
++  T x2 = Mul(x, x);
+   return MulAdd(x2, MulAdd(c3, x, c2), MulAdd(c1, x, c0));
+ }
+ template <class T>
+ HWY_INLINE HWY_MAYBE_UNUSED T Estrin(T x, T c0, T c1, T c2, T c3, T c4) {
+-  T x2(x * x), x4(x2 * x2);
++  T x2 = Mul(x, x);
++  T x4 = Mul(x2, x2);
+   return MulAdd(x4, c4, MulAdd(x2, MulAdd(c3, x, c2), MulAdd(c1, x, c0)));
+ }
+ template <class T>
+ HWY_INLINE HWY_MAYBE_UNUSED T Estrin(T x, T c0, T c1, T c2, T c3, T c4, T c5) {
+-  T x2(x * x), x4(x2 * x2);
++  T x2 = Mul(x, x);
++  T x4 = Mul(x2, x2);
+   return MulAdd(x4, MulAdd(c5, x, c4),
+                 MulAdd(x2, MulAdd(c3, x, c2), MulAdd(c1, x, c0)));
+ }
+ template <class T>
+ HWY_INLINE HWY_MAYBE_UNUSED T Estrin(T x, T c0, T c1, T c2, T c3, T c4, T c5,
+                                      T c6) {
+-  T x2(x * x), x4(x2 * x2);
++  T x2 = Mul(x, x);
++  T x4 = Mul(x2, x2);
+   return MulAdd(x4, MulAdd(x2, c6, MulAdd(c5, x, c4)),
+                 MulAdd(x2, MulAdd(c3, x, c2), MulAdd(c1, x, c0)));
+ }
+ template <class T>
+ HWY_INLINE HWY_MAYBE_UNUSED T Estrin(T x, T c0, T c1, T c2, T c3, T c4, T c5,
+                                      T c6, T c7) {
+-  T x2(x * x), x4(x2 * x2);
++  T x2 = Mul(x, x);
++  T x4 = Mul(x2, x2);
+   return MulAdd(x4, MulAdd(x2, MulAdd(c7, x, c6), MulAdd(c5, x, c4)),
+                 MulAdd(x2, MulAdd(c3, x, c2), MulAdd(c1, x, c0)));
+ }
+ template <class T>
+ HWY_INLINE HWY_MAYBE_UNUSED T Estrin(T x, T c0, T c1, T c2, T c3, T c4, T c5,
+                                      T c6, T c7, T c8) {
+-  T x2(x * x), x4(x2 * x2), x8(x4 * x4);
++  T x2 = Mul(x, x);
++  T x4 = Mul(x2, x2);
++  T x8 = Mul(x4, x4);
+   return MulAdd(x8, c8,
+                 MulAdd(x4, MulAdd(x2, MulAdd(c7, x, c6), MulAdd(c5, x, c4)),
+                        MulAdd(x2, MulAdd(c3, x, c2), MulAdd(c1, x, c0))));
+@@ -326,7 +332,9 @@ HWY_INLINE HWY_MAYBE_UNUSED T Estrin(T x, T c0, T c1, T c2, T c3, T c4, T c5,
+ template <class T>
+ HWY_INLINE HWY_MAYBE_UNUSED T Estrin(T x, T c0, T c1, T c2, T c3, T c4, T c5,
+                                      T c6, T c7, T c8, T c9) {
+-  T x2(x * x), x4(x2 * x2), x8(x4 * x4);
++  T x2 = Mul(x, x);
++  T x4 = Mul(x2, x2);
++  T x8 = Mul(x4, x4);
+   return MulAdd(x8, MulAdd(c9, x, c8),
+                 MulAdd(x4, MulAdd(x2, MulAdd(c7, x, c6), MulAdd(c5, x, c4)),
+                        MulAdd(x2, MulAdd(c3, x, c2), MulAdd(c1, x, c0))));
+@@ -334,7 +342,9 @@ HWY_INLINE HWY_MAYBE_UNUSED T Estrin(T x, T c0, T c1, T c2, T c3, T c4, T c5,
+ template <class T>
+ HWY_INLINE HWY_MAYBE_UNUSED T Estrin(T x, T c0, T c1, T c2, T c3, T c4, T c5,
+                                      T c6, T c7, T c8, T c9, T c10) {
+-  T x2(x * x), x4(x2 * x2), x8(x4 * x4);
++  T x2 = Mul(x, x);
++  T x4 = Mul(x2, x2);
++  T x8 = Mul(x4, x4);
+   return MulAdd(x8, MulAdd(x2, c10, MulAdd(c9, x, c8)),
+                 MulAdd(x4, MulAdd(x2, MulAdd(c7, x, c6), MulAdd(c5, x, c4)),
+                        MulAdd(x2, MulAdd(c3, x, c2), MulAdd(c1, x, c0))));
+@@ -342,7 +352,9 @@ HWY_INLINE HWY_MAYBE_UNUSED T Estrin(T x, T c0, T c1, T c2, T c3, T c4, T c5,
+ template <class T>
+ HWY_INLINE HWY_MAYBE_UNUSED T Estrin(T x, T c0, T c1, T c2, T c3, T c4, T c5,
+                                      T c6, T c7, T c8, T c9, T c10, T c11) {
+-  T x2(x * x), x4(x2 * x2), x8(x4 * x4);
++  T x2 = Mul(x, x);
++  T x4 = Mul(x2, x2);
++  T x8 = Mul(x4, x4);
+   return MulAdd(x8, MulAdd(x2, MulAdd(c11, x, c10), MulAdd(c9, x, c8)),
+                 MulAdd(x4, MulAdd(x2, MulAdd(c7, x, c6), MulAdd(c5, x, c4)),
+                        MulAdd(x2, MulAdd(c3, x, c2), MulAdd(c1, x, c0))));
+@@ -351,7 +363,9 @@ template <class T>
+ HWY_INLINE HWY_MAYBE_UNUSED T Estrin(T x, T c0, T c1, T c2, T c3, T c4, T c5,
+                                      T c6, T c7, T c8, T c9, T c10, T c11,
+                                      T c12) {
+-  T x2(x * x), x4(x2 * x2), x8(x4 * x4);
++  T x2 = Mul(x, x);
++  T x4 = Mul(x2, x2);
++  T x8 = Mul(x4, x4);
+   return MulAdd(
+       x8, MulAdd(x4, c12, MulAdd(x2, MulAdd(c11, x, c10), MulAdd(c9, x, c8))),
+       MulAdd(x4, MulAdd(x2, MulAdd(c7, x, c6), MulAdd(c5, x, c4)),
+@@ -361,7 +375,9 @@ template <class T>
+ HWY_INLINE HWY_MAYBE_UNUSED T Estrin(T x, T c0, T c1, T c2, T c3, T c4, T c5,
+                                      T c6, T c7, T c8, T c9, T c10, T c11,
+                                      T c12, T c13) {
+-  T x2(x * x), x4(x2 * x2), x8(x4 * x4);
++  T x2 = Mul(x, x);
++  T x4 = Mul(x2, x2);
++  T x8 = Mul(x4, x4);
+   return MulAdd(x8,
+                 MulAdd(x4, MulAdd(c13, x, c12),
+                        MulAdd(x2, MulAdd(c11, x, c10), MulAdd(c9, x, c8))),
+@@ -372,7 +388,9 @@ template <class T>
+ HWY_INLINE HWY_MAYBE_UNUSED T Estrin(T x, T c0, T c1, T c2, T c3, T c4, T c5,
+                                      T c6, T c7, T c8, T c9, T c10, T c11,
+                                      T c12, T c13, T c14) {
+-  T x2(x * x), x4(x2 * x2), x8(x4 * x4);
++  T x2 = Mul(x, x);
++  T x4 = Mul(x2, x2);
++  T x8 = Mul(x4, x4);
+   return MulAdd(x8,
+                 MulAdd(x4, MulAdd(x2, c14, MulAdd(c13, x, c12)),
+                        MulAdd(x2, MulAdd(c11, x, c10), MulAdd(c9, x, c8))),
+@@ -383,7 +401,9 @@ template <class T>
+ HWY_INLINE HWY_MAYBE_UNUSED T Estrin(T x, T c0, T c1, T c2, T c3, T c4, T c5,
+                                      T c6, T c7, T c8, T c9, T c10, T c11,
+                                      T c12, T c13, T c14, T c15) {
+-  T x2(x * x), x4(x2 * x2), x8(x4 * x4);
++  T x2 = Mul(x, x);
++  T x4 = Mul(x2, x2);
++  T x8 = Mul(x4, x4);
+   return MulAdd(x8,
+                 MulAdd(x4, MulAdd(x2, MulAdd(c15, x, c14), MulAdd(c13, x, c12)),
+                        MulAdd(x2, MulAdd(c11, x, c10), MulAdd(c9, x, c8))),
+@@ -394,7 +414,10 @@ template <class T>
+ HWY_INLINE HWY_MAYBE_UNUSED T Estrin(T x, T c0, T c1, T c2, T c3, T c4, T c5,
+                                      T c6, T c7, T c8, T c9, T c10, T c11,
+                                      T c12, T c13, T c14, T c15, T c16) {
+-  T x2(x * x), x4(x2 * x2), x8(x4 * x4), x16(x8 * x8);
++  T x2 = Mul(x, x);
++  T x4 = Mul(x2, x2);
++  T x8 = Mul(x4, x4);
++  T x16 = Mul(x8, x8);
+   return MulAdd(
+       x16, c16,
+       MulAdd(x8,
+@@ -407,7 +430,10 @@ template <class T>
+ HWY_INLINE HWY_MAYBE_UNUSED T Estrin(T x, T c0, T c1, T c2, T c3, T c4, T c5,
+                                      T c6, T c7, T c8, T c9, T c10, T c11,
+                                      T c12, T c13, T c14, T c15, T c16, T c17) {
+-  T x2(x * x), x4(x2 * x2), x8(x4 * x4), x16(x8 * x8);
++  T x2 = Mul(x, x);
++  T x4 = Mul(x2, x2);
++  T x8 = Mul(x4, x4);
++  T x16 = Mul(x8, x8);
+   return MulAdd(
+       x16, MulAdd(c17, x, c16),
+       MulAdd(x8,
+@@ -421,7 +447,10 @@ HWY_INLINE HWY_MAYBE_UNUSED T Estrin(T x, T c0, T c1, T c2, T c3, T c4, T c5,
+                                      T c6, T c7, T c8, T c9, T c10, T c11,
+                                      T c12, T c13, T c14, T c15, T c16, T c17,
+                                      T c18) {
+-  T x2(x * x), x4(x2 * x2), x8(x4 * x4), x16(x8 * x8);
++  T x2 = Mul(x, x);
++  T x4 = Mul(x2, x2);
++  T x8 = Mul(x4, x4);
++  T x16 = Mul(x8, x8);
+   return MulAdd(
+       x16, MulAdd(x2, c18, MulAdd(c17, x, c16)),
+       MulAdd(x8,
+@@ -497,8 +526,8 @@ struct AtanImpl<float> {
+     const auto k6 = Set(d, -0.0159569028764963150024414f);
+     const auto k7 = Set(d, +0.00282363896258175373077393f);
+ 
+-    const auto y = (x * x);
+-    return MulAdd(Estrin(y, k0, k1, k2, k3, k4, k5, k6, k7), (y * x), x);
++    const auto y = Mul(x, x);
++    return MulAdd(Estrin(y, k0, k1, k2, k3, k4, k5, k6, k7), Mul(y, x), x);
+   }
+ };
+ 
+@@ -529,10 +558,10 @@ struct AtanImpl<double> {
+     const auto k17 = Set(d, +0.000209850076645816976906797);
+     const auto k18 = Set(d, -1.88796008463073496563746e-5);
+ 
+-    const auto y = (x * x);
++    const auto y = Mul(x, x);
+     return MulAdd(Estrin(y, k0, k1, k2, k3, k4, k5, k6, k7, k8, k9, k10, k11,
+                          k12, k13, k14, k15, k16, k17, k18),
+-                  (y * x), x);
++                  Mul(y, x), x);
+   }
+ };
+ 
+@@ -553,8 +582,8 @@ struct CosSinImpl<float> {
+     const auto k2 = Set(d, -1.981069071916863322258e-4f);
+     const auto k3 = Set(d, +2.6083159809786593541503e-6f);
+ 
+-    const auto y(x * x);
+-    return MulAdd(Estrin(y, k0, k1, k2, k3), (y * x), x);
++    const auto y = Mul(x, x);
++    return MulAdd(Estrin(y, k0, k1, k2, k3), Mul(y, x), x);
+   }
+ 
+   template <class D, class V, class VI32>
+@@ -628,8 +657,8 @@ struct CosSinImpl<double> {
+     const auto k7 = Set(d, +2.81009972710863200091251e-15);
+     const auto k8 = Set(d, -7.97255955009037868891952e-18);
+ 
+-    const auto y(x * x);
+-    return MulAdd(Estrin(y, k0, k1, k2, k3, k4, k5, k6, k7, k8), (y * x), x);
++    const auto y = Mul(x, x);
++    return MulAdd(Estrin(y, k0, k1, k2, k3, k4, k5, k6, k7, k8), Mul(y, x), x);
+   }
+ 
+   template <class D, class V, class VI32>
+@@ -702,7 +731,7 @@ struct ExpImpl<float> {
+     const auto k4 = Set(d, +0.00139304355252534151077271f);
+     const auto k5 = Set(d, +0.000198527617612853646278381f);
+ 
+-    return MulAdd(Estrin(x, k0, k1, k2, k3, k4, k5), (x * x), x);
++    return MulAdd(Estrin(x, k0, k1, k2, k3, k4, k5), Mul(x, x), x);
+   }
+ 
+   // Computes 2^x, where x is an integer.
+@@ -710,14 +739,14 @@ struct ExpImpl<float> {
+   HWY_INLINE Vec<D> Pow2I(D d, VI32 x) {
+     const Rebind<int32_t, D> di32;
+     const VI32 kOffset = Set(di32, 0x7F);
+-    return BitCast(d, ShiftLeft<23>(x + kOffset));
++    return BitCast(d, ShiftLeft<23>(Add(x, kOffset)));
+   }
+ 
+   // Sets the exponent of 'x' to 2^e.
+   template <class D, class V, class VI32>
+   HWY_INLINE V LoadExpShortRange(D d, V x, VI32 e) {
+     const VI32 y = ShiftRight<1>(e);
+-    return x * Pow2I(d, y) * Pow2I(d, e - y);
++    return Mul(Mul(x, Pow2I(d, y)), Pow2I(d, Sub(e, y)));
+   }
+ 
+   template <class D, class V, class VI32>
+@@ -740,7 +769,8 @@ struct LogImpl<float> {
+   HWY_INLINE Vec<Rebind<int32_t, D>> Log2p1NoSubnormal(D /*d*/, V x) {
+     const Rebind<int32_t, D> di32;
+     const Rebind<uint32_t, D> du32;
+-    return BitCast(di32, ShiftRight<23>(BitCast(du32, x))) - Set(di32, 0x7F);
++    const auto kBias = Set(di32, 0x7F);
++    return Sub(BitCast(di32, ShiftRight<23>(BitCast(du32, x))), kBias);
+   }
+ 
+   // Approximates Log(x) over the range [sqrt(2) / 2, sqrt(2)].
+@@ -751,9 +781,9 @@ struct LogImpl<float> {
+     const V k2 = Set(d, 0.28498786688f);
+     const V k3 = Set(d, 0.24279078841f);
+ 
+-    const V x2 = (x * x);
+-    const V x4 = (x2 * x2);
+-    return MulAdd(MulAdd(k2, x4, k0), x2, (MulAdd(k3, x4, k1) * x4));
++    const V x2 = Mul(x, x);
++    const V x4 = Mul(x2, x2);
++    return MulAdd(MulAdd(k2, x4, k0), x2, Mul(MulAdd(k3, x4, k1), x4));
+   }
+ };
+ 
+@@ -781,7 +811,7 @@ struct ExpImpl<double> {
+     const auto k10 = Set(d, +2.08860621107283687536341e-9);
+ 
+     return MulAdd(Estrin(x, k0, k1, k2, k3, k4, k5, k6, k7, k8, k9, k10),
+-                  (x * x), x);
++                  Mul(x, x), x);
+   }
+ 
+   // Computes 2^x, where x is an integer.
+@@ -790,14 +820,14 @@ struct ExpImpl<double> {
+     const Rebind<int32_t, D> di32;
+     const Rebind<int64_t, D> di64;
+     const VI32 kOffset = Set(di32, 0x3FF);
+-    return BitCast(d, ShiftLeft<52>(PromoteTo(di64, x + kOffset)));
++    return BitCast(d, ShiftLeft<52>(PromoteTo(di64, Add(x, kOffset))));
+   }
+ 
+   // Sets the exponent of 'x' to 2^e.
+   template <class D, class V, class VI32>
+   HWY_INLINE V LoadExpShortRange(D d, V x, VI32 e) {
+     const VI32 y = ShiftRight<1>(e);
+-    return (x * Pow2I(d, y) * Pow2I(d, e - y));
++    return Mul(Mul(x, Pow2I(d, y)), Pow2I(d, Sub(e, y)));
+   }
+ 
+   template <class D, class V, class VI32>
+@@ -820,7 +850,8 @@ struct LogImpl<double> {
+   HWY_INLINE Vec<Rebind<int64_t, D>> Log2p1NoSubnormal(D /*d*/, V x) {
+     const Rebind<int64_t, D> di64;
+     const Rebind<uint64_t, D> du64;
+-    return BitCast(di64, ShiftRight<52>(BitCast(du64, x))) - Set(di64, 0x3FF);
++    return Sub(BitCast(di64, ShiftRight<52>(BitCast(du64, x))),
++               Set(di64, 0x3FF));
+   }
+ 
+   // Approximates Log(x) over the range [sqrt(2) / 2, sqrt(2)].
+@@ -834,10 +865,10 @@ struct LogImpl<double> {
+     const V k5 = Set(d, 0.1531383769920937332);
+     const V k6 = Set(d, 0.1479819860511658591);
+ 
+-    const V x2 = (x * x);
+-    const V x4 = (x2 * x2);
++    const V x2 = Mul(x, x);
++    const V x4 = Mul(x2, x2);
+     return MulAdd(MulAdd(MulAdd(MulAdd(k6, x4, k4), x4, k2), x4, k0), x2,
+-                  (MulAdd(MulAdd(k5, x4, k3), x4, k1) * x4));
++                  (Mul(MulAdd(MulAdd(k5, x4, k3), x4, k1), x4)));
+   }
+ };
+ 
+@@ -877,31 +908,32 @@ HWY_INLINE V Log(const D d, V x) {
+   VI exp_bits;
+   V exp;
+   if (kAllowSubnormals == true) {
+-    const auto is_denormal = (x < kMinNormal);
+-    x = IfThenElse(is_denormal, (x * kScale), x);
++    const auto is_denormal = Lt(x, kMinNormal);
++    x = IfThenElse(is_denormal, Mul(x, kScale), x);
+ 
+     // Compute the new exponent.
+-    exp_bits = (BitCast(di, x) + (kExpMask - kMagic));
++    exp_bits = Add(BitCast(di, x), Sub(kExpMask, kMagic));
+     const VI exp_scale =
+         BitCast(di, IfThenElseZero(is_denormal, BitCast(d, kExpScale)));
+     exp = ConvertTo(
+-        d, exp_scale + impl.Log2p1NoSubnormal(d, BitCast(d, exp_bits)));
++        d, Add(exp_scale, impl.Log2p1NoSubnormal(d, BitCast(d, exp_bits))));
+   } else {
+     // Compute the new exponent.
+-    exp_bits = (BitCast(di, x) + (kExpMask - kMagic));
++    exp_bits = Add(BitCast(di, x), Sub(kExpMask, kMagic));
+     exp = ConvertTo(d, impl.Log2p1NoSubnormal(d, BitCast(d, exp_bits)));
+   }
+ 
+   // Renormalize.
+   const V y = Or(And(x, BitCast(d, kLowerBits)),
+-                 BitCast(d, ((exp_bits & kManMask) + kMagic)));
++                 BitCast(d, Add(And(exp_bits, kManMask), kMagic)));
+ 
+   // Approximate and reconstruct.
+-  const V ym1 = (y - kOne);
+-  const V z = (ym1 / (y + kOne));
++  const V ym1 = Sub(y, kOne);
++  const V z = Div(ym1, Add(y, kOne));
+ 
+-  return MulSub(exp, kLn2Hi,
+-                (MulSub(z, (ym1 - impl.LogPoly(d, z)), (exp * kLn2Lo)) - ym1));
++  return MulSub(
++      exp, kLn2Hi,
++      Sub(MulSub(z, Sub(ym1, impl.LogPoly(d, z)), Mul(exp, kLn2Lo)), ym1));
+ }
+ 
+ }  // namespace impl
+@@ -912,22 +944,24 @@ HWY_INLINE V Acos(const D d, V x) {
+ 
+   const V kZero = Zero(d);
+   const V kHalf = Set(d, +0.5);
+-  const V kOne = Set(d, +1.0);
+-  const V kTwo = Set(d, +2.0);
+   const V kPi = Set(d, +3.14159265358979323846264);
+   const V kPiOverTwo = Set(d, +1.57079632679489661923132169);
+ 
+   const V sign_x = And(SignBit(d), x);
+   const V abs_x = Xor(x, sign_x);
+-  const auto mask = (abs_x < kHalf);
+-  const V yy = IfThenElse(mask, (abs_x * abs_x), ((kOne - abs_x) * kHalf));
++  const auto mask = Lt(abs_x, kHalf);
++  const V yy =
++      IfThenElse(mask, Mul(abs_x, abs_x), NegMulAdd(abs_x, kHalf, kHalf));
+   const V y = IfThenElse(mask, abs_x, Sqrt(yy));
+ 
+   impl::AsinImpl<LaneType> impl;
+-  const V t = (impl.AsinPoly(d, yy, y) * (y * yy));
+-  const V z = IfThenElse(mask, (kPiOverTwo - (Xor(y, sign_x) + Xor(t, sign_x))),
+-                         ((t + y) * kTwo));
+-  return IfThenElse(Or(mask, (x >= kZero)), z, (kPi - z));
++  const V t = Mul(impl.AsinPoly(d, yy, y), Mul(y, yy));
++
++  const V t_plus_y = Add(t, y);
++  const V z =
++      IfThenElse(mask, Sub(kPiOverTwo, Add(Xor(y, sign_x), Xor(t, sign_x))),
++                 Add(t_plus_y, t_plus_y));
++  return IfThenElse(Or(mask, Ge(x, kZero)), z, Sub(kPi, z));
+ }
+ 
+ template <class D, class V>
+@@ -937,21 +971,22 @@ HWY_INLINE V Acosh(const D d, V x) {
+   const V kOne = Set(d, +1.0);
+   const V kTwo = Set(d, +2.0);
+ 
+-  const auto is_x_large = (x > kLarge);
+-  const auto is_x_gt_2 = (x > kTwo);
++  const auto is_x_large = Gt(x, kLarge);
++  const auto is_x_gt_2 = Gt(x, kTwo);
+ 
+-  const V x_minus_1 = (x - kOne);
+-  const V y0 = MulSub(kTwo, x, (kOne / (Sqrt(MulSub(x, x, kOne)) + x)));
++  const V x_minus_1 = Sub(x, kOne);
++  const V y0 = MulSub(kTwo, x, Div(kOne, Add(Sqrt(MulSub(x, x, kOne)), x)));
+   const V y1 =
+-      (Sqrt(MulAdd(x_minus_1, kTwo, (x_minus_1 * x_minus_1))) + x_minus_1);
++      Add(Sqrt(MulAdd(x_minus_1, kTwo, Mul(x_minus_1, x_minus_1))), x_minus_1);
+   const V y2 =
+-      IfThenElse(is_x_gt_2, IfThenElse(is_x_large, x, y0), (y1 + kOne));
++      IfThenElse(is_x_gt_2, IfThenElse(is_x_large, x, y0), Add(y1, kOne));
+   const V z = impl::Log<D, V, /*kAllowSubnormals=*/false>(d, y2);
+ 
+-  const auto is_pole = y2 == kOne;
+-  const auto divisor = IfThenZeroElse(is_pole, y2) - kOne;
+-  return IfThenElse(is_x_gt_2, z, IfThenElse(is_pole, y1, z * y1 / divisor)) +
+-         IfThenElseZero(is_x_large, kLog2);
++  const auto is_pole = Eq(y2, kOne);
++  const auto divisor = Sub(IfThenZeroElse(is_pole, y2), kOne);
++  return Add(IfThenElse(is_x_gt_2, z,
++                        IfThenElse(is_pole, y1, Div(Mul(z, y1), divisor))),
++             IfThenElseZero(is_x_large, kLog2));
+ }
+ 
+ template <class D, class V>
+@@ -959,19 +994,19 @@ HWY_INLINE V Asin(const D d, V x) {
+   using LaneType = LaneType<V>;
+ 
+   const V kHalf = Set(d, +0.5);
+-  const V kOne = Set(d, +1.0);
+   const V kTwo = Set(d, +2.0);
+   const V kPiOverTwo = Set(d, +1.57079632679489661923132169);
+ 
+   const V sign_x = And(SignBit(d), x);
+   const V abs_x = Xor(x, sign_x);
+-  const auto mask = (abs_x < kHalf);
+-  const V yy = IfThenElse(mask, (abs_x * abs_x), (kOne - abs_x) * kHalf);
++  const auto mask = Lt(abs_x, kHalf);
++  const V yy =
++      IfThenElse(mask, Mul(abs_x, abs_x), NegMulAdd(abs_x, kHalf, kHalf));
+   const V y = IfThenElse(mask, abs_x, Sqrt(yy));
+ 
+   impl::AsinImpl<LaneType> impl;
+-  const V z0 = MulAdd(impl.AsinPoly(d, yy, y), (yy * y), y);
+-  const V z1 = (kPiOverTwo - (z0 * kTwo));
++  const V z0 = MulAdd(impl.AsinPoly(d, yy, y), Mul(yy, y), y);
++  const V z1 = NegMulAdd(z0, kTwo, kPiOverTwo);
+   return Or(IfThenElse(mask, z0, z1), sign_x);
+ }
+ 
+@@ -986,23 +1021,23 @@ HWY_INLINE V Asinh(const D d, V x) {
+   const V sign_x = And(SignBit(d), x);  // Extract the sign bit
+   const V abs_x = Xor(x, sign_x);
+ 
+-  const auto is_x_large = (abs_x > kLarge);
+-  const auto is_x_lt_2 = (abs_x < kTwo);
++  const auto is_x_large = Gt(abs_x, kLarge);
++  const auto is_x_lt_2 = Lt(abs_x, kTwo);
+ 
+-  const V x2 = (x * x);
+-  const V sqrt_x2_plus_1 = Sqrt(x2 + kOne);
++  const V x2 = Mul(x, x);
++  const V sqrt_x2_plus_1 = Sqrt(Add(x2, kOne));
+ 
+-  const V y0 = MulAdd(abs_x, kTwo, (kOne / (sqrt_x2_plus_1 + abs_x)));
+-  const V y1 = ((x2 / (sqrt_x2_plus_1 + kOne)) + abs_x);
++  const V y0 = MulAdd(abs_x, kTwo, Div(kOne, Add(sqrt_x2_plus_1, abs_x)));
++  const V y1 = Add(Div(x2, Add(sqrt_x2_plus_1, kOne)), abs_x);
+   const V y2 =
+-      IfThenElse(is_x_lt_2, (y1 + kOne), IfThenElse(is_x_large, abs_x, y0));
++      IfThenElse(is_x_lt_2, Add(y1, kOne), IfThenElse(is_x_large, abs_x, y0));
+   const V z = impl::Log<D, V, /*kAllowSubnormals=*/false>(d, y2);
+ 
+-  const auto is_pole = y2 == kOne;
+-  const auto divisor = IfThenZeroElse(is_pole, y2) - kOne;
+-  const auto large = IfThenElse(is_pole, y1, z * y1 / divisor);
+-  const V y = IfThenElse(abs_x < kSmall, x, large);
+-  return Or((IfThenElse(is_x_lt_2, y, z) + IfThenElseZero(is_x_large, kLog2)),
++  const auto is_pole = Eq(y2, kOne);
++  const auto divisor = Sub(IfThenZeroElse(is_pole, y2), kOne);
++  const auto large = IfThenElse(is_pole, y1, Div(Mul(z, y1), divisor));
++  const V y = IfThenElse(Lt(abs_x, kSmall), x, large);
++  return Or(Add(IfThenElse(is_x_lt_2, y, z), IfThenElseZero(is_x_large, kLog2)),
+             sign_x);
+ }
+ 
+@@ -1015,12 +1050,12 @@ HWY_INLINE V Atan(const D d, V x) {
+ 
+   const V sign = And(SignBit(d), x);
+   const V abs_x = Xor(x, sign);
+-  const auto mask = (abs_x > kOne);
++  const auto mask = Gt(abs_x, kOne);
+ 
+   impl::AtanImpl<LaneType> impl;
+   const auto divisor = IfThenElse(mask, abs_x, kOne);
+-  const V y = impl.AtanPoly(d, IfThenElse(mask, kOne / divisor, abs_x));
+-  return Or(IfThenElse(mask, (kPiOverTwo - y), y), sign);
++  const V y = impl.AtanPoly(d, IfThenElse(mask, Div(kOne, divisor), abs_x));
++  return Or(IfThenElse(mask, Sub(kPiOverTwo, y), y), sign);
+ }
+ 
+ template <class D, class V>
+@@ -1030,7 +1065,8 @@ HWY_INLINE V Atanh(const D d, V x) {
+ 
+   const V sign = And(SignBit(d), x);  // Extract the sign bit
+   const V abs_x = Xor(x, sign);
+-  return Log1p(d, ((abs_x + abs_x) / (kOne - abs_x))) * Xor(kHalf, sign);
++  return Mul(Log1p(d, Div(Add(abs_x, abs_x), Sub(kOne, abs_x))),
++             Xor(kHalf, sign));
+ }
+ 
+ template <class D, class V>
+@@ -1049,7 +1085,7 @@ HWY_INLINE V Cos(const D d, V x) {
+   const V y = Abs(x);  // cos(x) == cos(|x|)
+ 
+   // Compute the quadrant, q = int(|x| / pi) * 2 + 1
+-  const VI32 q = (ShiftLeft<1>(impl.ToInt32(d, y * kOneOverPi)) + kOne);
++  const VI32 q = Add(ShiftLeft<1>(impl.ToInt32(d, Mul(y, kOneOverPi))), kOne);
+ 
+   // Reduce range, apply sign, and approximate.
+   return impl.Poly(
+@@ -1076,8 +1112,8 @@ HWY_INLINE V Exp(const D d, V x) {
+ 
+   // Reduce, approximate, and then reconstruct.
+   const V y = impl.LoadExpShortRange(
+-      d, (impl.ExpPoly(d, impl.ExpReduce(d, x, q)) + kOne), q);
+-  return IfThenElseZero(x >= kLowerBound, y);
++      d, Add(impl.ExpPoly(d, impl.ExpReduce(d, x, q)), kOne), q);
++  return IfThenElseZero(Ge(x, kLowerBound), y);
+ }
+ 
+ template <class D, class V>
+@@ -1102,9 +1138,9 @@ HWY_INLINE V Expm1(const D d, V x) {
+ 
+   // Reduce, approximate, and then reconstruct.
+   const V y = impl.ExpPoly(d, impl.ExpReduce(d, x, q));
+-  const V z = IfThenElse(Abs(x) < kLn2Over2, y,
+-                         impl.LoadExpShortRange(d, (y + kOne), q) - kOne);
+-  return IfThenElse(x < kLowerBound, kNegOne, z);
++  const V z = IfThenElse(Lt(Abs(x), kLn2Over2), y,
++                         Sub(impl.LoadExpShortRange(d, Add(y, kOne), q), kOne));
++  return IfThenElse(Lt(x, kLowerBound), kNegOne, z);
+ }
+ 
+ template <class D, class V>
+@@ -1114,24 +1150,24 @@ HWY_INLINE V Log(const D d, V x) {
+ 
+ template <class D, class V>
+ HWY_INLINE V Log10(const D d, V x) {
+-  return Log(d, x) * Set(d, 0.4342944819032518276511);
++  return Mul(Log(d, x), Set(d, 0.4342944819032518276511));
+ }
+ 
+ template <class D, class V>
+ HWY_INLINE V Log1p(const D d, V x) {
+   const V kOne = Set(d, +1.0);
+ 
+-  const V y = x + kOne;
+-  const auto is_pole = y == kOne;
+-  const auto divisor = IfThenZeroElse(is_pole, y) - kOne;
++  const V y = Add(x, kOne);
++  const auto is_pole = Eq(y, kOne);
++  const auto divisor = Sub(IfThenZeroElse(is_pole, y), kOne);
+   const auto non_pole =
+-      impl::Log<D, V, /*kAllowSubnormals=*/false>(d, y) * (x / divisor);
++      Mul(impl::Log<D, V, /*kAllowSubnormals=*/false>(d, y), Div(x, divisor));
+   return IfThenElse(is_pole, x, non_pole);
+ }
+ 
+ template <class D, class V>
+ HWY_INLINE V Log2(const D d, V x) {
+-  return Log(d, x) * Set(d, 1.44269504088896340735992);
++  return Mul(Log(d, x), Set(d, 1.44269504088896340735992));
+ }
+ 
+ template <class D, class V>
+@@ -1167,7 +1203,7 @@ HWY_INLINE V Sinh(const D d, V x) {
+   const V sign = And(SignBit(d), x);  // Extract the sign bit
+   const V abs_x = Xor(x, sign);
+   const V y = Expm1(d, abs_x);
+-  const V z = ((y + kTwo) / (y + kOne) * (y * kHalf));
++  const V z = Mul(Div(Add(y, kTwo), Add(y, kOne)), Mul(y, kHalf));
+   return Xor(z, sign);  // Reapply the sign bit
+ }
+ 
+@@ -1179,8 +1215,8 @@ HWY_INLINE V Tanh(const D d, V x) {
+ 
+   const V sign = And(SignBit(d), x);  // Extract the sign bit
+   const V abs_x = Xor(x, sign);
+-  const V y = Expm1(d, abs_x * kTwo);
+-  const V z = IfThenElse((abs_x > kLimit), kOne, (y / (y + kTwo)));
++  const V y = Expm1(d, Mul(abs_x, kTwo));
++  const V z = IfThenElse(Gt(abs_x, kLimit), kOne, Div(y, Add(y, kTwo)));
+   return Xor(z, sign);  // Reapply the sign bit
+ }
+ 
+diff --git a/third_party/highway/hwy/contrib/math/math_test.cc b/third_party/highway/hwy/contrib/math/math_test.cc
+index 368ecfe062f64..798f5d6d920c2 100644
+--- a/third_party/highway/hwy/contrib/math/math_test.cc
++++ b/third_party/highway/hwy/contrib/math/math_test.cc
+@@ -51,8 +51,10 @@ void TestMath(const std::string name, T (*fx1)(T), Vec<D> (*fxN)(D, Vec<D>),
+   }
+ 
+   uint64_t max_ulp = 0;
+-#if HWY_ARCH_ARM
+   // Emulation is slower, so cannot afford as many.
++#if HWY_ARCH_RVV
++  constexpr UintT kSamplesPerRange = 2500;
++#elif HWY_ARCH_ARM
+   constexpr UintT kSamplesPerRange = 25000;
+ #else
+   constexpr UintT kSamplesPerRange = 100000;
+@@ -60,9 +62,9 @@ void TestMath(const std::string name, T (*fx1)(T), Vec<D> (*fxN)(D, Vec<D>),
+   for (int range_index = 0; range_index < range_count; ++range_index) {
+     const UintT start = ranges[range_index][0];
+     const UintT stop = ranges[range_index][1];
+-    const UintT step = std::max<UintT>(1, ((stop - start) / kSamplesPerRange));
++    const UintT step = HWY_MAX(1, ((stop - start) / kSamplesPerRange));
+     for (UintT value_bits = start; value_bits <= stop; value_bits += step) {
+-      const T value = BitCast<T>(std::min(value_bits, stop));
++      const T value = BitCast<T>(HWY_MIN(value_bits, stop));
+       const T actual = GetLane(fxN(d, Set(d, value)));
+       const T expected = fx1(value);
+ 
+@@ -74,7 +76,7 @@ void TestMath(const std::string name, T (*fx1)(T), Vec<D> (*fxN)(D, Vec<D>),
+ #endif
+ 
+       const auto ulp = ComputeUlpDelta(actual, expected);
+-      max_ulp = std::max<uint64_t>(max_ulp, ulp);
++      max_ulp = HWY_MAX(max_ulp, ulp);
+       if (ulp > max_error_ulp) {
+         std::cout << name << "<" << (kIsF32 ? "F32x" : "F64x") << Lanes(d)
+                   << ">(" << value << ") expected: " << expected
+@@ -88,6 +90,25 @@ void TestMath(const std::string name, T (*fx1)(T), Vec<D> (*fxN)(D, Vec<D>),
+             << ", Max ULP: " << max_ulp << std::endl;
+ }
+ 
++// TODO(janwas): remove once RVV supports fractional LMUL
++#undef DEFINE_MATH_TEST_FUNC
++#if HWY_TARGET == HWY_RVV
++
++#define DEFINE_MATH_TEST_FUNC(NAME)                    \
++  HWY_NOINLINE void TestAll##NAME() {                  \
++    ForFloatTypes(ForShrinkableVectors<Test##NAME>()); \
++  }
++
++#else
++
++#define DEFINE_MATH_TEST_FUNC(NAME)                 \
++  HWY_NOINLINE void TestAll##NAME() {               \
++    ForFloatTypes(ForPartialVectors<Test##NAME>()); \
++  }
++
++#endif
++
++#undef DEFINE_MATH_TEST
+ #define DEFINE_MATH_TEST(NAME, F32x1, F32xN, F32_MIN, F32_MAX, F32_ERROR, \
+                          F64x1, F64xN, F64_MIN, F64_MAX, F64_ERROR)       \
+   struct Test##NAME {                                                     \
+@@ -102,9 +123,7 @@ void TestMath(const std::string name, T (*fx1)(T), Vec<D> (*fxN)(D, Vec<D>),
+       }                                                                   \
+     }                                                                     \
+   };                                                                      \
+-  HWY_NOINLINE void TestAll##NAME() {                                     \
+-    ForFloatTypes(ForPartialVectors<Test##NAME>());                       \
+-  }
++  DEFINE_MATH_TEST_FUNC(NAME)
+ 
+ // Floating point values closest to but less than 1.0
+ const float kNearOneF = BitCast<float>(0x3F7FFFFF);
+@@ -167,6 +186,7 @@ DEFINE_MATH_TEST(Tanh,
+ HWY_AFTER_NAMESPACE();
+ 
+ #if HWY_ONCE
++
+ namespace hwy {
+ HWY_BEFORE_TEST(HwyMathTest);
+ HWY_EXPORT_AND_TEST_P(HwyMathTest, TestAllAcos);
+@@ -186,4 +206,11 @@ HWY_EXPORT_AND_TEST_P(HwyMathTest, TestAllSin);
+ HWY_EXPORT_AND_TEST_P(HwyMathTest, TestAllSinh);
+ HWY_EXPORT_AND_TEST_P(HwyMathTest, TestAllTanh);
+ }  // namespace hwy
++
++// Ought not to be necessary, but without this, no tests run on RVV.
++int main(int argc, char **argv) {
++  ::testing::InitGoogleTest(&argc, argv);
++  return RUN_ALL_TESTS();
++}
++
+ #endif
+diff --git a/third_party/highway/hwy/detect_compiler_arch.h b/third_party/highway/hwy/detect_compiler_arch.h
+new file mode 100644
+index 0000000000000..8a1e3ac31e978
+--- /dev/null
++++ b/third_party/highway/hwy/detect_compiler_arch.h
+@@ -0,0 +1,188 @@
++// Copyright 2020 Google LLC
++//
++// Licensed under the Apache License, Version 2.0 (the "License");
++// you may not use this file except in compliance with the License.
++// You may obtain a copy of the License at
++//
++//      http://www.apache.org/licenses/LICENSE-2.0
++//
++// Unless required by applicable law or agreed to in writing, software
++// distributed under the License is distributed on an "AS IS" BASIS,
++// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
++// See the License for the specific language governing permissions and
++// limitations under the License.
++
++#ifndef HIGHWAY_HWY_DETECT_COMPILER_ARCH_H_
++#define HIGHWAY_HWY_DETECT_COMPILER_ARCH_H_
++
++// Detects compiler and arch from predefined macros. Zero dependencies for
++// inclusion by foreach_target.h.
++
++// Add to #if conditions to prevent IDE from graying out code.
++#if (defined __CDT_PARSER__) || (defined __INTELLISENSE__) || \
++    (defined Q_CREATOR_RUN) || (defined(__CLANGD__))
++#define HWY_IDE 1
++#else
++#define HWY_IDE 0
++#endif
++
++//------------------------------------------------------------------------------
++// Compiler
++
++// clang-cl defines _MSC_VER but doesn't behave like MSVC in other aspects like
++// used in HWY_DIAGNOSTICS(). We include a check that we are not clang for that
++// purpose.
++#if defined(_MSC_VER) && !defined(__clang__)
++#define HWY_COMPILER_MSVC _MSC_VER
++#else
++#define HWY_COMPILER_MSVC 0
++#endif
++
++#ifdef __INTEL_COMPILER
++#define HWY_COMPILER_ICC __INTEL_COMPILER
++#else
++#define HWY_COMPILER_ICC 0
++#endif
++
++#ifdef __GNUC__
++#define HWY_COMPILER_GCC (__GNUC__ * 100 + __GNUC_MINOR__)
++#else
++#define HWY_COMPILER_GCC 0
++#endif
++
++// Clang can masquerade as MSVC/GCC, in which case both are set.
++#ifdef __clang__
++#ifdef __APPLE__
++// Apple LLVM version is unrelated to the actual Clang version, which we need
++// for enabling workarounds. Use the presence of warning flags to deduce it.
++// Adapted from https://github.com/simd-everywhere/simde/ simde-detect-clang.h.
++#if __has_warning("-Wformat-insufficient-args")
++#define HWY_COMPILER_CLANG 1200
++#elif __has_warning("-Wimplicit-const-int-float-conversion")
++#define HWY_COMPILER_CLANG 1100
++#elif __has_warning("-Wmisleading-indentation")
++#define HWY_COMPILER_CLANG 1000
++#elif defined(__FILE_NAME__)
++#define HWY_COMPILER_CLANG 900
++#elif __has_warning("-Wextra-semi-stmt") || \
++    __has_builtin(__builtin_rotateleft32)
++#define HWY_COMPILER_CLANG 800
++#elif __has_warning("-Wc++98-compat-extra-semi")
++#define HWY_COMPILER_CLANG 700
++#else  // Anything older than 7.0 is not recommended for Highway.
++#define HWY_COMPILER_CLANG 600
++#endif  // __has_warning chain
++#else   // Non-Apple: normal version
++#define HWY_COMPILER_CLANG (__clang_major__ * 100 + __clang_minor__)
++#endif
++#else  // Not clang
++#define HWY_COMPILER_CLANG 0
++#endif
++
++// More than one may be nonzero, but we want at least one.
++#if !HWY_COMPILER_MSVC && !HWY_COMPILER_ICC && !HWY_COMPILER_GCC && \
++    !HWY_COMPILER_CLANG
++#error "Unsupported compiler"
++#endif
++
++#ifdef __has_builtin
++#define HWY_HAS_BUILTIN(name) __has_builtin(name)
++#else
++#define HWY_HAS_BUILTIN(name) 0
++#endif
++
++#ifdef __has_attribute
++#define HWY_HAS_ATTRIBUTE(name) __has_attribute(name)
++#else
++#define HWY_HAS_ATTRIBUTE(name) 0
++#endif
++
++//------------------------------------------------------------------------------
++// Architecture
++
++#if defined(HWY_EMULATE_SVE)
++
++#define HWY_ARCH_X86_32 0
++#define HWY_ARCH_X86_64 0
++#define HWY_ARCH_X86 0
++#define HWY_ARCH_PPC 0
++#define HWY_ARCH_ARM_A64 1
++#define HWY_ARCH_ARM_V7 0
++#define HWY_ARCH_ARM 1
++#define HWY_ARCH_WASM 0
++#define HWY_ARCH_RVV 0
++
++#else
++
++#if defined(__i386__) || defined(_M_IX86)
++#define HWY_ARCH_X86_32 1
++#else
++#define HWY_ARCH_X86_32 0
++#endif
++
++#if defined(__x86_64__) || defined(_M_X64)
++#define HWY_ARCH_X86_64 1
++#else
++#define HWY_ARCH_X86_64 0
++#endif
++
++#if HWY_ARCH_X86_32 && HWY_ARCH_X86_64
++#error "Cannot have both x86-32 and x86-64"
++#endif
++
++#if HWY_ARCH_X86_32 || HWY_ARCH_X86_64
++#define HWY_ARCH_X86 1
++#else
++#define HWY_ARCH_X86 0
++#endif
++
++#if defined(__powerpc64__) || defined(_M_PPC)
++#define HWY_ARCH_PPC 1
++#else
++#define HWY_ARCH_PPC 0
++#endif
++
++#if defined(__ARM_ARCH_ISA_A64) || defined(__aarch64__) || defined(_M_ARM64)
++#define HWY_ARCH_ARM_A64 1
++#else
++#define HWY_ARCH_ARM_A64 0
++#endif
++
++#if defined(__arm__) || defined(_M_ARM)
++#define HWY_ARCH_ARM_V7 1
++#else
++#define HWY_ARCH_ARM_V7 0
++#endif
++
++#if HWY_ARCH_ARM_A64 && HWY_ARCH_ARM_V7
++#error "Cannot have both A64 and V7"
++#endif
++
++#if HWY_ARCH_ARM_A64 || HWY_ARCH_ARM_V7
++#define HWY_ARCH_ARM 1
++#else
++#define HWY_ARCH_ARM 0
++#endif
++
++#if defined(__EMSCRIPTEN__) || defined(__wasm__) || defined(__WASM__)
++#define HWY_ARCH_WASM 1
++#else
++#define HWY_ARCH_WASM 0
++#endif
++
++#ifdef __riscv
++#define HWY_ARCH_RVV 1
++#else
++#define HWY_ARCH_RVV 0
++#endif
++
++#endif // defined(HWY_EMULATE_SVE)
++
++// It is an error to detect multiple architectures at the same time, but OK to
++// detect none of the above.
++#if (HWY_ARCH_X86 + HWY_ARCH_PPC + HWY_ARCH_ARM + HWY_ARCH_WASM + \
++     HWY_ARCH_RVV) > 1
++#error "Must not detect more than one architecture"
++#endif
++
++#endif  // HIGHWAY_HWY_DETECT_COMPILER_ARCH_H_
+diff --git a/third_party/highway/hwy/detect_targets.h b/third_party/highway/hwy/detect_targets.h
+new file mode 100644
+index 0000000000000..c3ef2e4f3ef4a
+--- /dev/null
++++ b/third_party/highway/hwy/detect_targets.h
+@@ -0,0 +1,386 @@
++// Copyright 2021 Google LLC
++//
++// Licensed under the Apache License, Version 2.0 (the "License");
++// you may not use this file except in compliance with the License.
++// You may obtain a copy of the License at
++//
++//      http://www.apache.org/licenses/LICENSE-2.0
++//
++// Unless required by applicable law or agreed to in writing, software
++// distributed under the License is distributed on an "AS IS" BASIS,
++// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
++// See the License for the specific language governing permissions and
++// limitations under the License.
++
++#ifndef HIGHWAY_HWY_DETECT_TARGETS_H_
++#define HIGHWAY_HWY_DETECT_TARGETS_H_
++
++// Defines targets and chooses which to enable.
++
++#include "hwy/detect_compiler_arch.h"
++
++//------------------------------------------------------------------------------
++// Optional configuration
++
++// See ../quick_reference.md for documentation of these macros.
++
++// Uncomment to override the default baseline determined from predefined macros:
++// #define HWY_BASELINE_TARGETS (HWY_SSE4 | HWY_SCALAR)
++
++// Uncomment to override the default blocklist:
++// #define HWY_BROKEN_TARGETS HWY_AVX3
++
++// Uncomment to definitely avoid generating those target(s):
++// #define HWY_DISABLED_TARGETS HWY_SSE4
++
++// Uncomment to avoid emitting BMI/BMI2/FMA instructions (allows generating
++// AVX2 target for VMs which support AVX2 but not the other instruction sets)
++// #define HWY_DISABLE_BMI2_FMA
++
++//------------------------------------------------------------------------------
++// Targets
++
++// Unique bit value for each target. A lower value is "better" (e.g. more lanes)
++// than a higher value within the same group/platform - see HWY_STATIC_TARGET.
++//
++// All values are unconditionally defined so we can test HWY_TARGETS without
++// first checking the HWY_ARCH_*.
++//
++// The C99 preprocessor evaluates #if expressions using intmax_t types, so we
++// can use 32-bit literals.
++
++// 1,2: reserved
++
++// Currently satisfiable by Ice Lake (VNNI, VPCLMULQDQ, VAES). Later to be
++// added: BF16 (Cooper Lake). VP2INTERSECT is only in Tiger Lake? We do not yet
++// have uses for VBMI, VBMI2, VPOPCNTDQ, BITALG, GFNI.
++#define HWY_AVX3_DL 4  // see HWY_WANT_AVX3_DL below
++#define HWY_AVX3 8
++#define HWY_AVX2 16
++// 32: reserved for AVX
++#define HWY_SSE4 64
++#define HWY_SSSE3 128
++// 0x100, 0x200: reserved for SSE3, SSE2
++
++// The highest bit in the HWY_TARGETS mask that a x86 target can have. Used for
++// dynamic dispatch. All x86 target bits must be lower or equal to
++// (1 << HWY_HIGHEST_TARGET_BIT_X86) and they can only use
++// HWY_MAX_DYNAMIC_TARGETS in total.
++#define HWY_HIGHEST_TARGET_BIT_X86 9
++
++#define HWY_SVE2 0x400
++#define HWY_SVE 0x800
++// 0x1000 reserved for Helium
++#define HWY_NEON 0x2000
++
++#define HWY_HIGHEST_TARGET_BIT_ARM 13
++
++// 0x4000, 0x8000 reserved
++#define HWY_PPC8 0x10000  // v2.07 or 3
++// 0x20000, 0x40000 reserved for prior VSX/AltiVec
++
++#define HWY_HIGHEST_TARGET_BIT_PPC 18
++
++// 0x80000 reserved
++#define HWY_WASM 0x100000
++
++#define HWY_HIGHEST_TARGET_BIT_WASM 20
++
++// 0x200000, 0x400000, 0x800000 reserved
++
++#define HWY_RVV 0x1000000
++
++#define HWY_HIGHEST_TARGET_BIT_RVV 24
++
++// 0x2000000, 0x4000000, 0x8000000, 0x10000000 reserved
++
++#define HWY_SCALAR 0x20000000
++
++#define HWY_HIGHEST_TARGET_BIT_SCALAR 29
++
++// Cannot use higher values, otherwise HWY_TARGETS computation might overflow.
++
++//------------------------------------------------------------------------------
++// Set default blocklists
++
++// Disabled means excluded from enabled at user's request. A separate config
++// macro allows disabling without deactivating the blocklist below.
++#ifndef HWY_DISABLED_TARGETS
++#define HWY_DISABLED_TARGETS 0
++#endif
++
++// Broken means excluded from enabled due to known compiler issues. Allow the
++// user to override this blocklist without any guarantee of success.
++#ifndef HWY_BROKEN_TARGETS
++
++// x86 clang-6: we saw multiple AVX2/3 compile errors and in one case invalid
++// SSE4 codegen (possibly only for msan), so disable all those targets.
++#if HWY_ARCH_X86 && (HWY_COMPILER_CLANG != 0 && HWY_COMPILER_CLANG < 700)
++#define HWY_BROKEN_TARGETS (HWY_SSE4 | HWY_AVX2 | HWY_AVX3 | HWY_AVX3_DL)
++// This entails a major speed reduction, so warn unless the user explicitly
++// opts in to scalar-only.
++#if !defined(HWY_COMPILE_ONLY_SCALAR)
++#pragma message("x86 Clang <= 6: define HWY_COMPILE_ONLY_SCALAR or upgrade.")
++#endif
++
++// 32-bit may fail to compile AVX2/3.
++#elif HWY_ARCH_X86_32
++#define HWY_BROKEN_TARGETS (HWY_AVX2 | HWY_AVX3 | HWY_AVX3_DL)
++
++// MSVC AVX3 support is buggy: https://github.com/Mysticial/Flops/issues/16
++#elif HWY_COMPILER_MSVC != 0
++#define HWY_BROKEN_TARGETS (HWY_AVX3 | HWY_AVX3_DL)
++
++// armv7be has not been tested and is not yet supported.
++#elif HWY_ARCH_ARM_V7 && (defined(__ARM_BIG_ENDIAN) || defined(__BIG_ENDIAN))
++#define HWY_BROKEN_TARGETS (HWY_NEON)
++
++// SVE[2] require recent clang or gcc versions.
++#elif (HWY_COMPILER_CLANG && HWY_COMPILER_CLANG < 1100) ||\
++(!HWY_COMPILER_CLANG && HWY_COMPILER_GCC && HWY_COMPILER_GCC < 1000)
++#define HWY_BROKEN_TARGETS (HWY_SVE | HWY_SVE2)
++
++#else
++#define HWY_BROKEN_TARGETS 0
++#endif
++
++#endif  // HWY_BROKEN_TARGETS
++
++// Enabled means not disabled nor blocklisted.
++#define HWY_ENABLED(targets) \
++  ((targets) & ~((HWY_DISABLED_TARGETS) | (HWY_BROKEN_TARGETS)))
++
++//------------------------------------------------------------------------------
++// Detect baseline targets using predefined macros
++
++// Baseline means the targets for which the compiler is allowed to generate
++// instructions, implying the target CPU would have to support them. Do not use
++// this directly because it does not take the blocklist into account. Allow the
++// user to override this without any guarantee of success.
++#ifndef HWY_BASELINE_TARGETS
++
++#if defined(HWY_EMULATE_SVE)
++#define HWY_BASELINE_TARGETS HWY_SVE  // does not support SVE2
++#define HWY_BASELINE_AVX3_DL 0
++#else
++
++// Also check HWY_ARCH to ensure that simulating unknown platforms ends up with
++// HWY_TARGET == HWY_SCALAR.
++
++#if HWY_ARCH_WASM && defined(__wasm_simd128__)
++#define HWY_BASELINE_WASM HWY_WASM
++#else
++#define HWY_BASELINE_WASM 0
++#endif
++
++// Avoid choosing the PPC target until we have an implementation.
++#if HWY_ARCH_PPC && defined(__VSX__) && 0
++#define HWY_BASELINE_PPC8 HWY_PPC8
++#else
++#define HWY_BASELINE_PPC8 0
++#endif
++
++// SVE compiles, but is not yet tested.
++#if HWY_ARCH_ARM && defined(__ARM_FEATURE_SVE2)
++#define HWY_BASELINE_SVE2 HWY_SVE2
++#else
++#define HWY_BASELINE_SVE2 0
++#endif
++
++#if HWY_ARCH_ARM && defined(__ARM_FEATURE_SVE)
++#define HWY_BASELINE_SVE HWY_SVE
++#else
++#define HWY_BASELINE_SVE 0
++#endif
++
++// GCC 4.5.4 only defines __ARM_NEON__; 5.4 defines both.
++#if HWY_ARCH_ARM && (defined(__ARM_NEON__) || defined(__ARM_NEON))
++#define HWY_BASELINE_NEON HWY_NEON
++#else
++#define HWY_BASELINE_NEON 0
++#endif
++
++// Special handling for MSVC because it has fewer predefined macros
++#if HWY_COMPILER_MSVC && !HWY_COMPILER_CLANG
++
++// We can only be sure SSSE3/SSE4 are enabled if AVX is
++// (https://stackoverflow.com/questions/18563978/)
++#if defined(__AVX__)
++#define HWY_CHECK_SSSE3 1
++#define HWY_CHECK_SSE4 1
++#else
++#define HWY_CHECK_SSSE3 0
++#define HWY_CHECK_SSE4 0
++#endif
++
++// Cannot check for PCLMUL/AES and BMI2/FMA/F16C individually; we assume
++// PCLMUL/AES are available if SSE4 is, and BMI2/FMA/F16C if AVX2 is.
++#define HWY_CHECK_PCLMUL_AES 1
++#define HWY_CHECK_BMI2_FMA 1
++#define HWY_CHECK_F16C 1
++
++#else  // non-MSVC
++
++#if defined(__SSSE3__)
++#define HWY_CHECK_SSSE3 1
++#else
++#define HWY_CHECK_SSSE3 0
++#endif
++
++#if defined(__SSE4_1__) && defined(__SSE4_2__)
++#define HWY_CHECK_SSE4 1
++#else
++#define HWY_CHECK_SSE4 0
++#endif
++
++// If these are disabled, they should not gate the availability of SSE4/AVX2.
++#if defined(HWY_DISABLE_PCLMUL_AES) || (defined(__PCLMUL__) && defined(__AES__))
++#define HWY_CHECK_PCLMUL_AES 1
++#else
++#define HWY_CHECK_PCLMUL_AES 0
++#endif
++
++#if defined(HWY_DISABLE_BMI2_FMA) || (defined(__BMI2__) && defined(__FMA__))
++#define HWY_CHECK_BMI2_FMA 1
++#else
++#define HWY_CHECK_BMI2_FMA 0
++#endif
++
++#if defined(HWY_DISABLE_F16C) || defined(__F16C__)
++#define HWY_CHECK_F16C 1
++#else
++#define HWY_CHECK_F16C 0
++#endif
++
++#endif  // non-MSVC
++
++#if HWY_ARCH_X86 && HWY_CHECK_SSSE3
++#define HWY_BASELINE_SSSE3 HWY_SSSE3
++#else
++#define HWY_BASELINE_SSSE3 0
++#endif
++
++#if HWY_ARCH_X86 && HWY_CHECK_SSE4 && HWY_CHECK_PCLMUL_AES
++#define HWY_BASELINE_SSE4 HWY_SSE4
++#else
++#define HWY_BASELINE_SSE4 0
++#endif
++
++#if HWY_BASELINE_SSE4 != 0 && HWY_CHECK_BMI2_FMA && HWY_CHECK_F16C && \
++    defined(__AVX2__)
++#define HWY_BASELINE_AVX2 HWY_AVX2
++#else
++#define HWY_BASELINE_AVX2 0
++#endif
++
++// Require everything in AVX2 plus AVX-512 flags (also set by MSVC)
++#if HWY_BASELINE_AVX2 != 0 && defined(__AVX512F__) && defined(__AVX512BW__) && \
++    defined(__AVX512DQ__) && defined(__AVX512VL__)
++#define HWY_BASELINE_AVX3 HWY_AVX3
++#else
++#define HWY_BASELINE_AVX3 0
++#endif
++
++// TODO(janwas): not yet known whether these will be set by MSVC
++#if HWY_BASELINE_AVX3 != 0 && defined(__AVXVNNI__) && defined(__VAES__) && \
++    defined(__VPCLMULQDQ__)
++#define HWY_BASELINE_AVX3_DL HWY_AVX3_DL
++#else
++#define HWY_BASELINE_AVX3_DL 0
++#endif
++
++#if HWY_ARCH_RVV && defined(__riscv_vector)
++#define HWY_BASELINE_RVV HWY_RVV
++#else
++#define HWY_BASELINE_RVV 0
++#endif
++
++#define HWY_BASELINE_TARGETS                                                \
++  (HWY_SCALAR | HWY_BASELINE_WASM | HWY_BASELINE_PPC8 | HWY_BASELINE_SVE2 | \
++   HWY_BASELINE_SVE | HWY_BASELINE_NEON | HWY_BASELINE_SSSE3 |              \
++   HWY_BASELINE_SSE4 | HWY_BASELINE_AVX2 | HWY_BASELINE_AVX3 |              \
++   HWY_BASELINE_AVX3_DL | HWY_BASELINE_RVV)
++
++#endif  // HWY_EMULATE_SVE
++
++#else
++// User already defined HWY_BASELINE_TARGETS, but we still need to define
++// HWY_BASELINE_AVX3 (matching user's definition) for HWY_CHECK_AVX3_DL.
++#define HWY_BASELINE_AVX3_DL (HWY_BASELINE_TARGETS & HWY_AVX3_DL)
++#endif  // HWY_BASELINE_TARGETS
++
++//------------------------------------------------------------------------------
++// Choose target for static dispatch
++
++#define HWY_ENABLED_BASELINE HWY_ENABLED(HWY_BASELINE_TARGETS)
++#if HWY_ENABLED_BASELINE == 0
++#error "At least one baseline target must be defined and enabled"
++#endif
++
++// Best baseline, used for static dispatch. This is the least-significant 1-bit
++// within HWY_ENABLED_BASELINE and lower bit values imply "better".
++#define HWY_STATIC_TARGET (HWY_ENABLED_BASELINE & -HWY_ENABLED_BASELINE)
++
++// Start by assuming static dispatch. If we later use dynamic dispatch, this
++// will be defined to other targets during the multiple-inclusion, and finally
++// return to the initial value. Defining this outside begin/end_target ensures
++// inl headers successfully compile by themselves (required by Bazel).
++#define HWY_TARGET HWY_STATIC_TARGET
++
++//------------------------------------------------------------------------------
++// Choose targets for dynamic dispatch according to one of four policies
++
++#if (defined(HWY_COMPILE_ONLY_SCALAR) + defined(HWY_COMPILE_ONLY_STATIC) + \
++     defined(HWY_COMPILE_ALL_ATTAINABLE)) > 1
++#error "Invalid config: can only define a single policy for targets"
++#endif
++
++// Further to checking for disabled/broken targets, we only use AVX3_DL after
++// explicit opt-in (via this macro OR baseline compiler flags) to avoid
++// generating a codepath which is only helpful if the app uses AVX3_DL features.
++#if defined(HWY_WANT_AVX3_DL)
++#define HWY_CHECK_AVX3_DL HWY_AVX3_DL
++#else
++#define HWY_CHECK_AVX3_DL HWY_BASELINE_AVX3_DL
++#endif
++
++// Attainable means enabled and the compiler allows intrinsics (even when not
++// allowed to autovectorize). Used in 3 and 4.
++#if HWY_ARCH_X86
++#define HWY_ATTAINABLE_TARGETS                                          \
++  HWY_ENABLED(HWY_SCALAR | HWY_SSSE3 | HWY_SSE4 | HWY_AVX2 | HWY_AVX3 | \
++              HWY_CHECK_AVX3_DL)
++#else
++#define HWY_ATTAINABLE_TARGETS HWY_ENABLED_BASELINE
++#endif
++
++// 1) For older compilers: disable all SIMD (could also set HWY_DISABLED_TARGETS
++// to ~HWY_SCALAR, but this is more explicit).
++#if defined(HWY_COMPILE_ONLY_SCALAR)
++#undef HWY_STATIC_TARGET
++#define HWY_STATIC_TARGET HWY_SCALAR  // override baseline
++#define HWY_TARGETS HWY_SCALAR
++
++// 2) For forcing static dispatch without code changes (removing HWY_EXPORT)
++#elif defined(HWY_COMPILE_ONLY_STATIC)
++#define HWY_TARGETS HWY_STATIC_TARGET
++
++// 3) For tests: include all attainable targets (in particular: scalar)
++#elif defined(HWY_COMPILE_ALL_ATTAINABLE) || defined(HWY_IS_TEST)
++#define HWY_TARGETS HWY_ATTAINABLE_TARGETS
++
++// 4) Default: attainable WITHOUT non-best baseline. This reduces code size by
++// excluding superseded targets, in particular scalar.
++#else
++#define HWY_TARGETS (HWY_ATTAINABLE_TARGETS & (2 * HWY_STATIC_TARGET - 1))
++
++#endif  // target policy
++
++// HWY_ONCE and the multiple-inclusion mechanism rely on HWY_STATIC_TARGET being
++// one of the dynamic targets. This also implies HWY_TARGETS != 0 and
++// (HWY_TARGETS & HWY_ENABLED_BASELINE) != 0.
++#if (HWY_TARGETS & HWY_STATIC_TARGET) == 0
++#error "Logic error: best baseline should be included in dynamic targets"
++#endif
++
++#endif  // HIGHWAY_HWY_DETECT_TARGETS_H_
+diff --git a/third_party/highway/hwy/examples/benchmark.cc b/third_party/highway/hwy/examples/benchmark.cc
+index 0debfd7db5877..f9b60ad5a2241 100644
+--- a/third_party/highway/hwy/examples/benchmark.cc
++++ b/third_party/highway/hwy/examples/benchmark.cc
+@@ -131,7 +131,7 @@ class BenchmarkDot : public TwoArray {
+         sum[i] += sum[i + power];
+       }
+     }
+-    dot_ = GetLane(SumOfLanes(sum[0]));
++    dot_ = GetLane(SumOfLanes(d, sum[0]));
+     return static_cast<FuncOutput>(dot_);
+   }
+   void Verify(size_t num_items) {
+diff --git a/third_party/highway/hwy/examples/skeleton.cc b/third_party/highway/hwy/examples/skeleton.cc
+index fc05eb371f85b..460bc8f5b436d 100644
+--- a/third_party/highway/hwy/examples/skeleton.cc
++++ b/third_party/highway/hwy/examples/skeleton.cc
+@@ -45,7 +45,7 @@ HWY_NOINLINE void OneFloorLog2(const DF df, const uint8_t* HWY_RESTRICT values,
+ 
+   const auto u8 = Load(d8, values);
+   const auto bits = BitCast(d32, ConvertTo(df, PromoteTo(d32, u8)));
+-  const auto exponent = ShiftRight<23>(bits) - Set(d32, 127);
++  const auto exponent = Sub(ShiftRight<23>(bits), Set(d32, 127));
+   Store(DemoteTo(d8, exponent), d8, log2);
+ }
+ 
+diff --git a/third_party/highway/hwy/examples/skeleton_test.cc b/third_party/highway/hwy/examples/skeleton_test.cc
+index 4a6a8769b346c..7f79b189f6d2e 100644
+--- a/third_party/highway/hwy/examples/skeleton_test.cc
++++ b/third_party/highway/hwy/examples/skeleton_test.cc
+@@ -99,9 +99,17 @@ HWY_NOINLINE void TestAllSumMulAdd() {
+ HWY_AFTER_NAMESPACE();
+ 
+ #if HWY_ONCE
++
+ namespace skeleton {
+ HWY_BEFORE_TEST(SkeletonTest);
+ HWY_EXPORT_AND_TEST_P(SkeletonTest, TestAllFloorLog2);
+ HWY_EXPORT_AND_TEST_P(SkeletonTest, TestAllSumMulAdd);
+ }  // namespace skeleton
++
++// Ought not to be necessary, but without this, no tests run on RVV.
++int main(int argc, char **argv) {
++  ::testing::InitGoogleTest(&argc, argv);
++  return RUN_ALL_TESTS();
++}
++
+ #endif
+diff --git a/third_party/highway/hwy/foreach_target.h b/third_party/highway/hwy/foreach_target.h
+index a0c4198b17c32..63a90cbbd086a 100644
+--- a/third_party/highway/hwy/foreach_target.h
++++ b/third_party/highway/hwy/foreach_target.h
+@@ -19,7 +19,7 @@
+ // targets except HWY_STATIC_TARGET. Defines unique HWY_TARGET each time so that
+ // highway.h defines the corresponding macro/namespace.
+ 
+-#include "hwy/targets.h"
++#include "hwy/detect_targets.h"
+ 
+ // *_inl.h may include other headers, which requires include guards to prevent
+ // repeated inclusion. The guards must be reset after compiling each target, so
+@@ -74,6 +74,17 @@
+ #endif
+ #endif
+ 
++#if (HWY_TARGETS & HWY_SSSE3) && (HWY_STATIC_TARGET != HWY_SSSE3)
++#undef HWY_TARGET
++#define HWY_TARGET HWY_SSSE3
++#include HWY_TARGET_INCLUDE
++#ifdef HWY_TARGET_TOGGLE
++#undef HWY_TARGET_TOGGLE
++#else
++#define HWY_TARGET_TOGGLE
++#endif
++#endif
++
+ #if (HWY_TARGETS & HWY_SSE4) && (HWY_STATIC_TARGET != HWY_SSE4)
+ #undef HWY_TARGET
+ #define HWY_TARGET HWY_SSE4
+@@ -107,6 +118,17 @@
+ #endif
+ #endif
+ 
++#if (HWY_TARGETS & HWY_AVX3_DL) && (HWY_STATIC_TARGET != HWY_AVX3_DL)
++#undef HWY_TARGET
++#define HWY_TARGET HWY_AVX3_DL
++#include HWY_TARGET_INCLUDE
++#ifdef HWY_TARGET_TOGGLE
++#undef HWY_TARGET_TOGGLE
++#else
++#define HWY_TARGET_TOGGLE
++#endif
++#endif
++
+ #if (HWY_TARGETS & HWY_WASM) && (HWY_STATIC_TARGET != HWY_WASM)
+ #undef HWY_TARGET
+ #define HWY_TARGET HWY_WASM
+diff --git a/third_party/highway/hwy/highway.h b/third_party/highway/hwy/highway.h
+index a34c60ebef486..9312b5ad36328 100644
+--- a/third_party/highway/hwy/highway.h
++++ b/third_party/highway/hwy/highway.h
+@@ -27,17 +27,12 @@ namespace hwy {
+ 
+ // API version (https://semver.org/); keep in sync with CMakeLists.txt.
+ #define HWY_MAJOR 0
+-#define HWY_MINOR 12
+-#define HWY_PATCH 1
++#define HWY_MINOR 14
++#define HWY_PATCH 0
+ 
+ //------------------------------------------------------------------------------
+ // Shorthand for descriptors (defined in shared-inl.h) used to select overloads.
+ 
+-// Because Highway functions take descriptor and/or vector arguments, ADL finds
+-// these functions without requiring users in project::HWY_NAMESPACE to
+-// qualify Highway functions with hwy::HWY_NAMESPACE. However, ADL rules for
+-// templates require `using hwy::HWY_NAMESPACE::ShiftLeft;` etc. declarations.
+-
+ // HWY_FULL(T[,LMUL=1]) is a native vector/group. LMUL is the number of
+ // registers in the group, and is ignored on targets that do not support groups.
+ #define HWY_FULL1(T) hwy::HWY_NAMESPACE::Simd<T, HWY_LANES(T)>
+@@ -81,12 +76,16 @@ namespace hwy {
+ #define HWY_STATIC_DISPATCH(FUNC_NAME) N_SVE2::FUNC_NAME
+ #elif HWY_STATIC_TARGET == HWY_PPC8
+ #define HWY_STATIC_DISPATCH(FUNC_NAME) N_PPC8::FUNC_NAME
++#elif HWY_STATIC_TARGET == HWY_SSSE3
++#define HWY_STATIC_DISPATCH(FUNC_NAME) N_SSSE3::FUNC_NAME
+ #elif HWY_STATIC_TARGET == HWY_SSE4
+ #define HWY_STATIC_DISPATCH(FUNC_NAME) N_SSE4::FUNC_NAME
+ #elif HWY_STATIC_TARGET == HWY_AVX2
+ #define HWY_STATIC_DISPATCH(FUNC_NAME) N_AVX2::FUNC_NAME
+ #elif HWY_STATIC_TARGET == HWY_AVX3
+ #define HWY_STATIC_DISPATCH(FUNC_NAME) N_AVX3::FUNC_NAME
++#elif HWY_STATIC_TARGET == HWY_AVX3_DL
++#define HWY_STATIC_DISPATCH(FUNC_NAME) N_AVX3_DL::FUNC_NAME
+ #endif
+ 
+ // Dynamic dispatch declarations.
+@@ -165,6 +164,12 @@ FunctionCache<RetType, Args...> FunctionCacheFactory(RetType (*)(Args...)) {
+ #define HWY_CHOOSE_PPC8(FUNC_NAME) nullptr
+ #endif
+ 
++#if HWY_TARGETS & HWY_SSSE3
++#define HWY_CHOOSE_SSSE3(FUNC_NAME) &N_SSSE3::FUNC_NAME
++#else
++#define HWY_CHOOSE_SSSE3(FUNC_NAME) nullptr
++#endif
++
+ #if HWY_TARGETS & HWY_SSE4
+ #define HWY_CHOOSE_SSE4(FUNC_NAME) &N_SSE4::FUNC_NAME
+ #else
+@@ -183,6 +188,12 @@ FunctionCache<RetType, Args...> FunctionCacheFactory(RetType (*)(Args...)) {
+ #define HWY_CHOOSE_AVX3(FUNC_NAME) nullptr
+ #endif
+ 
++#if HWY_TARGETS & HWY_AVX3_DL
++#define HWY_CHOOSE_AVX3_DL(FUNC_NAME) &N_AVX3_DL::FUNC_NAME
++#else
++#define HWY_CHOOSE_AVX3_DL(FUNC_NAME) nullptr
++#endif
++
+ #define HWY_DISPATCH_TABLE(FUNC_NAME) \
+   HWY_CONCAT(FUNC_NAME, HighwayDispatchTable)
+ 
+@@ -270,11 +281,11 @@ FunctionCache<RetType, Args...> FunctionCacheFactory(RetType (*)(Args...)) {
+ #endif
+ 
+ // These define ops inside namespace hwy::HWY_NAMESPACE.
+-#if HWY_TARGET == HWY_SSE4
++#if HWY_TARGET == HWY_SSSE3 || HWY_TARGET == HWY_SSE4
+ #include "hwy/ops/x86_128-inl.h"
+ #elif HWY_TARGET == HWY_AVX2
+ #include "hwy/ops/x86_256-inl.h"
+-#elif HWY_TARGET == HWY_AVX3
++#elif HWY_TARGET == HWY_AVX3 || HWY_TARGET == HWY_AVX3_DL
+ #include "hwy/ops/x86_512-inl.h"
+ #elif HWY_TARGET == HWY_PPC8
+ #error "PPC is not yet supported"
+@@ -292,65 +303,6 @@ FunctionCache<RetType, Args...> FunctionCacheFactory(RetType (*)(Args...)) {
+ #pragma message("HWY_TARGET does not match any known target")
+ #endif  // HWY_TARGET
+ 
+-// Commonly used functions/types that must come after ops are defined.
+-HWY_BEFORE_NAMESPACE();
+-namespace hwy {
+-namespace HWY_NAMESPACE {
+-
+-// The lane type of a vector type, e.g. float for Vec<Simd<float, 4>>.
+-template <class V>
+-using LaneType = decltype(GetLane(V()));
+-
+-// Vector type, e.g. Vec128<float> for Simd<float, 4>. Useful as the return type
+-// of functions that do not take a vector argument, or as an argument type if
+-// the function only has a template argument for D, or for explicit type names
+-// instead of auto. This may be a built-in type.
+-template <class D>
+-using Vec = decltype(Zero(D()));
+-
+-// Mask type. Useful as the return type of functions that do not take a mask
+-// argument, or as an argument type if the function only has a template argument
+-// for D, or for explicit type names instead of auto.
+-template <class D>
+-using Mask = decltype(MaskFromVec(Zero(D())));
+-
+-// Returns the closest value to v within [lo, hi].
+-template <class V>
+-HWY_API V Clamp(const V v, const V lo, const V hi) {
+-  return Min(Max(lo, v), hi);
+-}
+-
+-// CombineShiftRightBytes (and ..Lanes) are not available for the scalar target.
+-// TODO(janwas): implement for RVV
+-#if HWY_TARGET != HWY_SCALAR && HWY_TARGET != HWY_RVV
+-
+-template <size_t kLanes, class V>
+-HWY_API V CombineShiftRightLanes(const V hi, const V lo) {
+-  return CombineShiftRightBytes<kLanes * sizeof(LaneType<V>)>(hi, lo);
+-}
+-
+-#endif
+-
+-// Returns lanes with the most significant bit set and all other bits zero.
+-template <class D>
+-HWY_API Vec<D> SignBit(D d) {
+-  using Unsigned = MakeUnsigned<TFromD<D>>;
+-  const Unsigned bit = Unsigned(1) << (sizeof(Unsigned) * 8 - 1);
+-  return BitCast(d, Set(Rebind<Unsigned, D>(), bit));
+-}
+-
+-// Returns quiet NaN.
+-template <class D>
+-HWY_API Vec<D> NaN(D d) {
+-  const RebindToSigned<D> di;
+-  // LimitsMax sets all exponent and mantissa bits to 1. The exponent plus
+-  // mantissa MSB (to indicate quiet) would be sufficient.
+-  return BitCast(d, Set(di, LimitsMax<TFromD<decltype(di)>>()));
+-}
+-
+-// NOLINTNEXTLINE(google-readability-namespace-comments)
+-}  // namespace HWY_NAMESPACE
+-}  // namespace hwy
+-HWY_AFTER_NAMESPACE();
++#include "hwy/ops/generic_ops-inl.h"
+ 
+ #endif  // HWY_HIGHWAY_PER_TARGET
+diff --git a/third_party/highway/hwy/highway_test.cc b/third_party/highway/hwy/highway_test.cc
+index ebe57f0ed5da0..8fc57be074efe 100644
+--- a/third_party/highway/hwy/highway_test.cc
++++ b/third_party/highway/hwy/highway_test.cc
+@@ -66,9 +66,9 @@ struct TestOverflow {
+     const auto vmax = Set(d, LimitsMax<T>());
+     const auto vmin = Set(d, LimitsMin<T>());
+     // Unsigned underflow / negative -> positive
+-    HWY_ASSERT_VEC_EQ(d, vmax, vmin - v1);
++    HWY_ASSERT_VEC_EQ(d, vmax, Sub(vmin, v1));
+     // Unsigned overflow / positive -> negative
+-    HWY_ASSERT_VEC_EQ(d, vmin, vmax + v1);
++    HWY_ASSERT_VEC_EQ(d, vmin, Add(vmax, v1));
+   }
+ };
+ 
+@@ -76,6 +76,22 @@ HWY_NOINLINE void TestAllOverflow() {
+   ForIntegerTypes(ForPartialVectors<TestOverflow>());
+ }
+ 
++struct TestClamp {
++  template <class T, class D>
++  HWY_NOINLINE void operator()(T /*unused*/, D d) {
++    const auto v0 = Zero(d);
++    const auto v1 = Set(d, 1);
++    const auto v2 = Set(d, 2);
++
++    HWY_ASSERT_VEC_EQ(d, v1, Clamp(v2, v0, v1));
++    HWY_ASSERT_VEC_EQ(d, v1, Clamp(v0, v1, v2));
++  }
++};
++
++HWY_NOINLINE void TestAllClamp() {
++  ForAllTypes(ForPartialVectors<TestClamp>());
++}
++
+ struct TestSignBitInteger {
+   template <class T, class D>
+   HWY_NOINLINE void operator()(T /*unused*/, D d) {
+@@ -187,18 +203,18 @@ struct TestNaN {
+     HWY_ASSERT_NAN(d, Or(nan, v1));
+ 
+     // Comparison
+-    HWY_ASSERT(AllFalse(Eq(nan, v1)));
+-    HWY_ASSERT(AllFalse(Gt(nan, v1)));
+-    HWY_ASSERT(AllFalse(Lt(nan, v1)));
+-    HWY_ASSERT(AllFalse(Ge(nan, v1)));
+-    HWY_ASSERT(AllFalse(Le(nan, v1)));
++    HWY_ASSERT(AllFalse(d, Eq(nan, v1)));
++    HWY_ASSERT(AllFalse(d, Gt(nan, v1)));
++    HWY_ASSERT(AllFalse(d, Lt(nan, v1)));
++    HWY_ASSERT(AllFalse(d, Ge(nan, v1)));
++    HWY_ASSERT(AllFalse(d, Le(nan, v1)));
+ 
+     // Reduction
+-    HWY_ASSERT_NAN(d, SumOfLanes(nan));
++    HWY_ASSERT_NAN(d, SumOfLanes(d, nan));
+ // TODO(janwas): re-enable after QEMU is fixed
+ #if HWY_TARGET != HWY_RVV
+-    HWY_ASSERT_NAN(d, MinOfLanes(nan));
+-    HWY_ASSERT_NAN(d, MaxOfLanes(nan));
++    HWY_ASSERT_NAN(d, MinOfLanes(d, nan));
++    HWY_ASSERT_NAN(d, MaxOfLanes(d, nan));
+ #endif
+ 
+     // Min
+@@ -227,13 +243,6 @@ struct TestNaN {
+ #endif
+     HWY_ASSERT_NAN(d, Min(nan, nan));
+     HWY_ASSERT_NAN(d, Max(nan, nan));
+-
+-    // Comparison
+-    HWY_ASSERT(AllFalse(Eq(nan, v1)));
+-    HWY_ASSERT(AllFalse(Gt(nan, v1)));
+-    HWY_ASSERT(AllFalse(Lt(nan, v1)));
+-    HWY_ASSERT(AllFalse(Ge(nan, v1)));
+-    HWY_ASSERT(AllFalse(Le(nan, v1)));
+   }
+ };
+ 
+@@ -286,6 +295,19 @@ HWY_NOINLINE void TestAllGetLane() {
+   ForAllTypes(ForPartialVectors<TestGetLane>());
+ }
+ 
++struct TestDFromV {
++  template <class T, class D>
++  HWY_NOINLINE void operator()(T /*unused*/, D d) {
++    const auto v0 = Zero(d);
++    using D0 = DFromV<decltype(v0)>;         // not necessarily same as D
++    const auto v0b = And(v0, Set(D0(), 1));  // but vectors can interoperate
++    HWY_ASSERT_VEC_EQ(d, v0, v0b);
++  }
++};
++
++HWY_NOINLINE void TestAllDFromV() {
++  ForAllTypes(ForPartialVectors<TestDFromV>());
++}
+ 
+ // NOLINTNEXTLINE(google-readability-namespace-comments)
+ }  // namespace HWY_NAMESPACE
+@@ -293,13 +315,23 @@ HWY_NOINLINE void TestAllGetLane() {
+ HWY_AFTER_NAMESPACE();
+ 
+ #if HWY_ONCE
++
+ namespace hwy {
+ HWY_BEFORE_TEST(HighwayTest);
+ HWY_EXPORT_AND_TEST_P(HighwayTest, TestAllSet);
+ HWY_EXPORT_AND_TEST_P(HighwayTest, TestAllOverflow);
++HWY_EXPORT_AND_TEST_P(HighwayTest, TestAllClamp);
+ HWY_EXPORT_AND_TEST_P(HighwayTest, TestAllSignBit);
+ HWY_EXPORT_AND_TEST_P(HighwayTest, TestAllNaN);
+ HWY_EXPORT_AND_TEST_P(HighwayTest, TestAllCopyAndAssign);
+ HWY_EXPORT_AND_TEST_P(HighwayTest, TestAllGetLane);
++HWY_EXPORT_AND_TEST_P(HighwayTest, TestAllDFromV);
+ }  // namespace hwy
++
++// Ought not to be necessary, but without this, no tests run on RVV.
++int main(int argc, char** argv) {
++  ::testing::InitGoogleTest(&argc, argv);
++  return RUN_ALL_TESTS();
++}
++
+ #endif
+diff --git a/third_party/highway/hwy/nanobenchmark.cc b/third_party/highway/hwy/nanobenchmark.cc
+index a31ca1b263c05..0fb189dff8c8b 100644
+--- a/third_party/highway/hwy/nanobenchmark.cc
++++ b/third_party/highway/hwy/nanobenchmark.cc
+@@ -459,7 +459,7 @@ timer::Ticks SampleUntilStable(const double max_rel_mad, double* rel_mad,
+       static_cast<size_t>(ticks_per_second * p.seconds_per_eval);
+   size_t samples_per_eval =
+       est == 0 ? p.min_samples_per_eval : ticks_per_eval / est;
+-  samples_per_eval = std::max(samples_per_eval, p.min_samples_per_eval);
++  samples_per_eval = HWY_MAX(samples_per_eval, p.min_samples_per_eval);
+ 
+   std::vector<timer::Ticks> samples;
+   samples.reserve(1 + samples_per_eval);
+@@ -530,7 +530,7 @@ size_t NumSkip(const Func func, const uint8_t* arg, const InputVec& unique,
+     const timer::Ticks total = SampleUntilStable(
+         p.target_rel_mad, &rel_mad, p,
+         [func, arg, input]() { platform::PreventElision(func(arg, input)); });
+-    min_duration = std::min(min_duration, total - timer_resolution);
++    min_duration = HWY_MIN(min_duration, total - timer_resolution);
+   }
+ 
+   // Number of repetitions required to reach the target resolution.
+@@ -615,7 +615,7 @@ timer::Ticks TotalDuration(const Func func, const uint8_t* arg,
+           platform::PreventElision(func(arg, input));
+         }
+       });
+-  *max_rel_mad = std::max(*max_rel_mad, rel_mad);
++  *max_rel_mad = HWY_MAX(*max_rel_mad, rel_mad);
+   return duration;
+ }
+ 
+diff --git a/third_party/highway/hwy/nanobenchmark_test.cc b/third_party/highway/hwy/nanobenchmark_test.cc
+index a42caf5230aa7..b5fa7620c5d9b 100644
+--- a/third_party/highway/hwy/nanobenchmark_test.cc
++++ b/third_party/highway/hwy/nanobenchmark_test.cc
+@@ -23,6 +23,13 @@
+ namespace hwy {
+ namespace {
+ 
++// Governs duration of test; avoid timeout in debug builds.
++#ifdef NDEBUG
++constexpr size_t kMaxEvals = 4;
++#else
++constexpr size_t kMaxEvals = 3;
++#endif
++
+ FuncOutput Div(const void*, FuncInput in) {
+   // Here we're measuring the throughput because benchmark invocations are
+   // independent. Any dividend will do; the divisor is nonzero.
+@@ -34,7 +41,7 @@ void MeasureDiv(const FuncInput (&inputs)[N]) {
+   printf("Measuring integer division (output on final two lines)\n");
+   Result results[N];
+   Params params;
+-  params.max_evals = 4;  // avoid test timeout
++  params.max_evals = kMaxEvals;
+   const size_t num_results = Measure(&Div, nullptr, inputs, N, results, params);
+   for (size_t i = 0; i < num_results; ++i) {
+     printf("%5zu: %6.2f ticks; MAD=%4.2f%%\n", results[i].input,
+@@ -59,7 +66,7 @@ template <size_t N>
+ void MeasureRandom(const FuncInput (&inputs)[N]) {
+   Result results[N];
+   Params p;
+-  p.max_evals = 4;  // avoid test timeout
++  p.max_evals = kMaxEvals;
+   p.verbose = false;
+   const size_t num_results = Measure(&Random, nullptr, inputs, N, results, p);
+   for (size_t i = 0; i < num_results; ++i) {
+@@ -78,3 +85,9 @@ TEST(NanobenchmarkTest, RunAll) {
+ 
+ }  // namespace
+ }  // namespace hwy
++
++// Ought not to be necessary, but without this, no tests run on RVV.
++int main(int argc, char** argv) {
++  ::testing::InitGoogleTest(&argc, argv);
++  return RUN_ALL_TESTS();
++}
+diff --git a/third_party/highway/hwy/ops/arm_neon-inl.h b/third_party/highway/hwy/ops/arm_neon-inl.h
+index f1ed0c742d8dd..db29560af3a27 100644
+--- a/third_party/highway/hwy/ops/arm_neon-inl.h
++++ b/third_party/highway/hwy/ops/arm_neon-inl.h
+@@ -26,6 +26,9 @@ HWY_BEFORE_NAMESPACE();
+ namespace hwy {
+ namespace HWY_NAMESPACE {
+ 
++template <typename T>
++using Full128 = Simd<T, 16 / sizeof(T)>;
++
+ namespace detail {  // for code folding and Raw128
+ 
+ // Macros used to define single and double function calls for multiple types
+@@ -70,7 +73,7 @@ namespace detail {  // for code folding and Raw128
+ // parameters passed here (see HWY_NEON_BUILD_* macros defined before).
+ #define HWY_NEON_DEF_FUNCTION(type, size, name, prefix, infix, suffix, args) \
+   HWY_CONCAT(HWY_NEON_BUILD_TPL_, args)                                      \
+-  HWY_INLINE HWY_CONCAT(HWY_NEON_BUILD_RET_, args)(type, size)               \
++  HWY_API HWY_CONCAT(HWY_NEON_BUILD_RET_, args)(type, size)                  \
+       name(HWY_CONCAT(HWY_NEON_BUILD_PARAM_, args)(type, size)) {            \
+     return HWY_CONCAT(HWY_NEON_BUILD_RET_, args)(type, size)(                \
+         HWY_NEON_EVAL(prefix##infix##suffix, HWY_NEON_BUILD_ARG_##args));    \
+@@ -441,9 +444,6 @@ struct Raw128<int8_t, 1> {
+ 
+ }  // namespace detail
+ 
+-template <typename T>
+-using Full128 = Simd<T, 16 / sizeof(T)>;
+-
+ template <typename T, size_t N = 16 / sizeof(T)>
+ class Vec128 {
+   using Raw = typename detail::Raw128<T, N>::type;
+@@ -481,7 +481,7 @@ class Vec128 {
+   Raw raw;
+ };
+ 
+-// FF..FF or 0, also for floating-point - see README.
++// FF..FF or 0.
+ template <typename T, size_t N = 16 / sizeof(T)>
+ class Mask128 {
+   // ARM C Language Extensions return and expect unsigned type.
+@@ -496,6 +496,21 @@ class Mask128 {
+   Raw raw;
+ };
+ 
++namespace detail {
++
++// Deduce Simd<T, N> from Vec128<T, N>
++struct DeduceD {
++  template <typename T, size_t N>
++  Simd<T, N> operator()(Vec128<T, N>) const {
++    return Simd<T, N>();
++  }
++};
++
++}  // namespace detail
++
++template <class V>
++using DFromV = decltype(detail::DeduceD()(V()));
++
+ // ------------------------------ BitCast
+ 
+ namespace detail {
+@@ -637,8 +652,8 @@ HWY_INLINE Vec128<float16_t, N> BitCastFromByte(Simd<float16_t, N> /* tag */,
+ }  // namespace detail
+ 
+ template <typename T, size_t N, typename FromT>
+-HWY_INLINE Vec128<T, N> BitCast(
+-    Simd<T, N> d, Vec128<FromT, N * sizeof(T) / sizeof(FromT)> v) {
++HWY_API Vec128<T, N> BitCast(Simd<T, N> d,
++                             Vec128<FromT, N * sizeof(T) / sizeof(FromT)> v) {
+   return detail::BitCastFromByte(d, detail::BitCastToByte(v));
+ }
+ 
+@@ -660,13 +675,16 @@ HWY_NEON_DEF_FUNCTION_ALL_TYPES(Set, vdup, _n_, HWY_SET1)
+ 
+ // Returns an all-zero vector.
+ template <typename T, size_t N>
+-HWY_INLINE Vec128<T, N> Zero(Simd<T, N> d) {
++HWY_API Vec128<T, N> Zero(Simd<T, N> d) {
+   return Set(d, 0);
+ }
+ 
++template <class D>
++using VFromD = decltype(Zero(D()));
++
+ // Returns a vector with uninitialized elements.
+ template <typename T, size_t N>
+-HWY_INLINE Vec128<T, N> Undefined(Simd<T, N> /*d*/) {
++HWY_API Vec128<T, N> Undefined(Simd<T, N> /*d*/) {
+   HWY_DIAGNOSTICS(push)
+   HWY_DIAGNOSTICS_OFF(disable : 4701, ignored "-Wuninitialized")
+   typename detail::Raw128<T, N>::type a;
+@@ -686,81 +704,81 @@ Vec128<T, N> Iota(const Simd<T, N> d, const T2 first) {
+ 
+ // ------------------------------ GetLane
+ 
+-HWY_INLINE uint8_t GetLane(const Vec128<uint8_t, 16> v) {
++HWY_API uint8_t GetLane(const Vec128<uint8_t, 16> v) {
+   return vgetq_lane_u8(v.raw, 0);
+ }
+ template <size_t N>
+-HWY_INLINE uint8_t GetLane(const Vec128<uint8_t, N> v) {
++HWY_API uint8_t GetLane(const Vec128<uint8_t, N> v) {
+   return vget_lane_u8(v.raw, 0);
+ }
+ 
+-HWY_INLINE int8_t GetLane(const Vec128<int8_t, 16> v) {
++HWY_API int8_t GetLane(const Vec128<int8_t, 16> v) {
+   return vgetq_lane_s8(v.raw, 0);
+ }
+ template <size_t N>
+-HWY_INLINE int8_t GetLane(const Vec128<int8_t, N> v) {
++HWY_API int8_t GetLane(const Vec128<int8_t, N> v) {
+   return vget_lane_s8(v.raw, 0);
+ }
+ 
+-HWY_INLINE uint16_t GetLane(const Vec128<uint16_t, 8> v) {
++HWY_API uint16_t GetLane(const Vec128<uint16_t, 8> v) {
+   return vgetq_lane_u16(v.raw, 0);
+ }
+ template <size_t N>
+-HWY_INLINE uint16_t GetLane(const Vec128<uint16_t, N> v) {
++HWY_API uint16_t GetLane(const Vec128<uint16_t, N> v) {
+   return vget_lane_u16(v.raw, 0);
+ }
+ 
+-HWY_INLINE int16_t GetLane(const Vec128<int16_t, 8> v) {
++HWY_API int16_t GetLane(const Vec128<int16_t, 8> v) {
+   return vgetq_lane_s16(v.raw, 0);
+ }
+ template <size_t N>
+-HWY_INLINE int16_t GetLane(const Vec128<int16_t, N> v) {
++HWY_API int16_t GetLane(const Vec128<int16_t, N> v) {
+   return vget_lane_s16(v.raw, 0);
+ }
+ 
+-HWY_INLINE uint32_t GetLane(const Vec128<uint32_t, 4> v) {
++HWY_API uint32_t GetLane(const Vec128<uint32_t, 4> v) {
+   return vgetq_lane_u32(v.raw, 0);
+ }
+ template <size_t N>
+-HWY_INLINE uint32_t GetLane(const Vec128<uint32_t, N> v) {
++HWY_API uint32_t GetLane(const Vec128<uint32_t, N> v) {
+   return vget_lane_u32(v.raw, 0);
+ }
+ 
+-HWY_INLINE int32_t GetLane(const Vec128<int32_t, 4> v) {
++HWY_API int32_t GetLane(const Vec128<int32_t, 4> v) {
+   return vgetq_lane_s32(v.raw, 0);
+ }
+ template <size_t N>
+-HWY_INLINE int32_t GetLane(const Vec128<int32_t, N> v) {
++HWY_API int32_t GetLane(const Vec128<int32_t, N> v) {
+   return vget_lane_s32(v.raw, 0);
+ }
+ 
+-HWY_INLINE uint64_t GetLane(const Vec128<uint64_t, 2> v) {
++HWY_API uint64_t GetLane(const Vec128<uint64_t, 2> v) {
+   return vgetq_lane_u64(v.raw, 0);
+ }
+-HWY_INLINE uint64_t GetLane(const Vec128<uint64_t, 1> v) {
++HWY_API uint64_t GetLane(const Vec128<uint64_t, 1> v) {
+   return vget_lane_u64(v.raw, 0);
+ }
+-HWY_INLINE int64_t GetLane(const Vec128<int64_t, 2> v) {
++HWY_API int64_t GetLane(const Vec128<int64_t, 2> v) {
+   return vgetq_lane_s64(v.raw, 0);
+ }
+-HWY_INLINE int64_t GetLane(const Vec128<int64_t, 1> v) {
++HWY_API int64_t GetLane(const Vec128<int64_t, 1> v) {
+   return vget_lane_s64(v.raw, 0);
+ }
+ 
+-HWY_INLINE float GetLane(const Vec128<float, 4> v) {
++HWY_API float GetLane(const Vec128<float, 4> v) {
+   return vgetq_lane_f32(v.raw, 0);
+ }
+-HWY_INLINE float GetLane(const Vec128<float, 2> v) {
++HWY_API float GetLane(const Vec128<float, 2> v) {
+   return vget_lane_f32(v.raw, 0);
+ }
+-HWY_INLINE float GetLane(const Vec128<float, 1> v) {
++HWY_API float GetLane(const Vec128<float, 1> v) {
+   return vget_lane_f32(v.raw, 0);
+ }
+ #if HWY_ARCH_ARM_A64
+-HWY_INLINE double GetLane(const Vec128<double, 2> v) {
++HWY_API double GetLane(const Vec128<double, 2> v) {
+   return vgetq_lane_f64(v.raw, 0);
+ }
+-HWY_INLINE double GetLane(const Vec128<double, 1> v) {
++HWY_API double GetLane(const Vec128<double, 1> v) {
+   return vget_lane_f64(v.raw, 0);
+ }
+ #endif
+@@ -803,56 +821,12 @@ HWY_NEON_DEF_FUNCTION_INT_64(SaturatedSub, vqsub, _, 2)
+ HWY_NEON_DEF_FUNCTION_UINT_8(AverageRound, vrhadd, _, 2)
+ HWY_NEON_DEF_FUNCTION_UINT_16(AverageRound, vrhadd, _, 2)
+ 
+-// ------------------------------ Absolute value
+-
+-// Returns absolute value, except that LimitsMin() maps to LimitsMax() + 1.
+-HWY_INLINE Vec128<int8_t> Abs(const Vec128<int8_t> v) {
+-  return Vec128<int8_t>(vabsq_s8(v.raw));
+-}
+-HWY_INLINE Vec128<int16_t> Abs(const Vec128<int16_t> v) {
+-  return Vec128<int16_t>(vabsq_s16(v.raw));
+-}
+-HWY_INLINE Vec128<int32_t> Abs(const Vec128<int32_t> v) {
+-  return Vec128<int32_t>(vabsq_s32(v.raw));
+-}
+-// i64 is implemented after BroadcastSignBit.
+-HWY_INLINE Vec128<float> Abs(const Vec128<float> v) {
+-  return Vec128<float>(vabsq_f32(v.raw));
+-}
+-
+-template <size_t N, HWY_IF_LE64(int8_t, N)>
+-HWY_INLINE Vec128<int8_t, N> Abs(const Vec128<int8_t, N> v) {
+-  return Vec128<int8_t, N>(vabs_s8(v.raw));
+-}
+-template <size_t N, HWY_IF_LE64(int16_t, N)>
+-HWY_INLINE Vec128<int16_t, N> Abs(const Vec128<int16_t, N> v) {
+-  return Vec128<int16_t, N>(vabs_s16(v.raw));
+-}
+-template <size_t N, HWY_IF_LE64(int32_t, N)>
+-HWY_INLINE Vec128<int32_t, N> Abs(const Vec128<int32_t, N> v) {
+-  return Vec128<int32_t, N>(vabs_s32(v.raw));
+-}
+-template <size_t N, HWY_IF_LE64(float, N)>
+-HWY_INLINE Vec128<float, N> Abs(const Vec128<float, N> v) {
+-  return Vec128<float, N>(vabs_f32(v.raw));
+-}
+-
+-#if HWY_ARCH_ARM_A64
+-HWY_INLINE Vec128<double> Abs(const Vec128<double> v) {
+-  return Vec128<double>(vabsq_f64(v.raw));
+-}
+-
+-HWY_INLINE Vec128<double, 1> Abs(const Vec128<double, 1> v) {
+-  return Vec128<double, 1>(vabs_f64(v.raw));
+-}
+-#endif
+-
+ // ------------------------------ Neg
+ 
+ HWY_NEON_DEF_FUNCTION_ALL_FLOATS(Neg, vneg, _, 1)
+ HWY_NEON_DEF_FUNCTION_INT_8_16_32(Neg, vneg, _, 1)  // i64 implemented below
+ 
+-HWY_INLINE Vec128<int64_t, 1> Neg(const Vec128<int64_t, 1> v) {
++HWY_API Vec128<int64_t, 1> Neg(const Vec128<int64_t, 1> v) {
+ #if HWY_ARCH_ARM_A64
+   return Vec128<int64_t, 1>(vneg_s64(v.raw));
+ #else
+@@ -860,7 +834,7 @@ HWY_INLINE Vec128<int64_t, 1> Neg(const Vec128<int64_t, 1> v) {
+ #endif
+ }
+ 
+-HWY_INLINE Vec128<int64_t> Neg(const Vec128<int64_t> v) {
++HWY_API Vec128<int64_t> Neg(const Vec128<int64_t> v) {
+ #if HWY_ARCH_ARM_A64
+   return Vec128<int64_t>(vnegq_s64(v.raw));
+ #else
+@@ -875,7 +849,7 @@ HWY_INLINE Vec128<int64_t> Neg(const Vec128<int64_t> v) {
+ #undef HWY_NEON_DEF_FUNCTION
+ #define HWY_NEON_DEF_FUNCTION(type, size, name, prefix, infix, suffix, args)   \
+   template <int kBits>                                                         \
+-  HWY_INLINE Vec128<type, size> name(const Vec128<type, size> v) {             \
++  HWY_API Vec128<type, size> name(const Vec128<type, size> v) {                \
+     return kBits == 0 ? v                                                      \
+                       : Vec128<type, size>(HWY_NEON_EVAL(                      \
+                             prefix##infix##suffix, v.raw, HWY_MAX(1, kBits))); \
+@@ -890,230 +864,230 @@ HWY_NEON_DEF_FUNCTION_INTS(ShiftRight, vshr, _n_, HWY_SHIFT)
+ 
+ // ------------------------------ Shl
+ 
+-HWY_INLINE Vec128<uint8_t> operator<<(const Vec128<uint8_t> v,
+-                                      const Vec128<uint8_t> bits) {
++HWY_API Vec128<uint8_t> operator<<(const Vec128<uint8_t> v,
++                                   const Vec128<uint8_t> bits) {
+   return Vec128<uint8_t>(vshlq_u8(v.raw, vreinterpretq_s8_u8(bits.raw)));
+ }
+ template <size_t N, HWY_IF_LE64(uint8_t, N)>
+-HWY_INLINE Vec128<uint8_t, N> operator<<(const Vec128<uint8_t, N> v,
+-                                         const Vec128<uint8_t, N> bits) {
++HWY_API Vec128<uint8_t, N> operator<<(const Vec128<uint8_t, N> v,
++                                      const Vec128<uint8_t, N> bits) {
+   return Vec128<uint8_t, N>(vshl_u8(v.raw, vreinterpret_s8_u8(bits.raw)));
+ }
+ 
+-HWY_INLINE Vec128<uint16_t> operator<<(const Vec128<uint16_t> v,
+-                                       const Vec128<uint16_t> bits) {
++HWY_API Vec128<uint16_t> operator<<(const Vec128<uint16_t> v,
++                                    const Vec128<uint16_t> bits) {
+   return Vec128<uint16_t>(vshlq_u16(v.raw, vreinterpretq_s16_u16(bits.raw)));
+ }
+ template <size_t N, HWY_IF_LE64(uint16_t, N)>
+-HWY_INLINE Vec128<uint16_t, N> operator<<(const Vec128<uint16_t, N> v,
+-                                          const Vec128<uint16_t, N> bits) {
++HWY_API Vec128<uint16_t, N> operator<<(const Vec128<uint16_t, N> v,
++                                       const Vec128<uint16_t, N> bits) {
+   return Vec128<uint16_t, N>(vshl_u16(v.raw, vreinterpret_s16_u16(bits.raw)));
+ }
+ 
+-HWY_INLINE Vec128<uint32_t> operator<<(const Vec128<uint32_t> v,
+-                                       const Vec128<uint32_t> bits) {
++HWY_API Vec128<uint32_t> operator<<(const Vec128<uint32_t> v,
++                                    const Vec128<uint32_t> bits) {
+   return Vec128<uint32_t>(vshlq_u32(v.raw, vreinterpretq_s32_u32(bits.raw)));
+ }
+ template <size_t N, HWY_IF_LE64(uint32_t, N)>
+-HWY_INLINE Vec128<uint32_t, N> operator<<(const Vec128<uint32_t, N> v,
+-                                          const Vec128<uint32_t, N> bits) {
++HWY_API Vec128<uint32_t, N> operator<<(const Vec128<uint32_t, N> v,
++                                       const Vec128<uint32_t, N> bits) {
+   return Vec128<uint32_t, N>(vshl_u32(v.raw, vreinterpret_s32_u32(bits.raw)));
+ }
+ 
+-HWY_INLINE Vec128<uint64_t> operator<<(const Vec128<uint64_t> v,
+-                                       const Vec128<uint64_t> bits) {
++HWY_API Vec128<uint64_t> operator<<(const Vec128<uint64_t> v,
++                                    const Vec128<uint64_t> bits) {
+   return Vec128<uint64_t>(vshlq_u64(v.raw, vreinterpretq_s64_u64(bits.raw)));
+ }
+-HWY_INLINE Vec128<uint64_t, 1> operator<<(const Vec128<uint64_t, 1> v,
+-                                          const Vec128<uint64_t, 1> bits) {
++HWY_API Vec128<uint64_t, 1> operator<<(const Vec128<uint64_t, 1> v,
++                                       const Vec128<uint64_t, 1> bits) {
+   return Vec128<uint64_t, 1>(vshl_u64(v.raw, vreinterpret_s64_u64(bits.raw)));
+ }
+ 
+-HWY_INLINE Vec128<int8_t> operator<<(const Vec128<int8_t> v,
+-                                     const Vec128<int8_t> bits) {
++HWY_API Vec128<int8_t> operator<<(const Vec128<int8_t> v,
++                                  const Vec128<int8_t> bits) {
+   return Vec128<int8_t>(vshlq_s8(v.raw, bits.raw));
+ }
+ template <size_t N, HWY_IF_LE64(int8_t, N)>
+-HWY_INLINE Vec128<int8_t, N> operator<<(const Vec128<int8_t, N> v,
+-                                        const Vec128<int8_t, N> bits) {
++HWY_API Vec128<int8_t, N> operator<<(const Vec128<int8_t, N> v,
++                                     const Vec128<int8_t, N> bits) {
+   return Vec128<int8_t, N>(vshl_s8(v.raw, bits.raw));
+ }
+ 
+-HWY_INLINE Vec128<int16_t> operator<<(const Vec128<int16_t> v,
+-                                      const Vec128<int16_t> bits) {
++HWY_API Vec128<int16_t> operator<<(const Vec128<int16_t> v,
++                                   const Vec128<int16_t> bits) {
+   return Vec128<int16_t>(vshlq_s16(v.raw, bits.raw));
+ }
+ template <size_t N, HWY_IF_LE64(int16_t, N)>
+-HWY_INLINE Vec128<int16_t, N> operator<<(const Vec128<int16_t, N> v,
+-                                         const Vec128<int16_t, N> bits) {
++HWY_API Vec128<int16_t, N> operator<<(const Vec128<int16_t, N> v,
++                                      const Vec128<int16_t, N> bits) {
+   return Vec128<int16_t, N>(vshl_s16(v.raw, bits.raw));
+ }
+ 
+-HWY_INLINE Vec128<int32_t> operator<<(const Vec128<int32_t> v,
+-                                      const Vec128<int32_t> bits) {
++HWY_API Vec128<int32_t> operator<<(const Vec128<int32_t> v,
++                                   const Vec128<int32_t> bits) {
+   return Vec128<int32_t>(vshlq_s32(v.raw, bits.raw));
+ }
+ template <size_t N, HWY_IF_LE64(int32_t, N)>
+-HWY_INLINE Vec128<int32_t, N> operator<<(const Vec128<int32_t, N> v,
+-                                         const Vec128<int32_t, N> bits) {
++HWY_API Vec128<int32_t, N> operator<<(const Vec128<int32_t, N> v,
++                                      const Vec128<int32_t, N> bits) {
+   return Vec128<int32_t, N>(vshl_s32(v.raw, bits.raw));
+ }
+ 
+-HWY_INLINE Vec128<int64_t> operator<<(const Vec128<int64_t> v,
+-                                      const Vec128<int64_t> bits) {
++HWY_API Vec128<int64_t> operator<<(const Vec128<int64_t> v,
++                                   const Vec128<int64_t> bits) {
+   return Vec128<int64_t>(vshlq_s64(v.raw, bits.raw));
+ }
+-HWY_INLINE Vec128<int64_t, 1> operator<<(const Vec128<int64_t, 1> v,
+-                                         const Vec128<int64_t, 1> bits) {
++HWY_API Vec128<int64_t, 1> operator<<(const Vec128<int64_t, 1> v,
++                                      const Vec128<int64_t, 1> bits) {
+   return Vec128<int64_t, 1>(vshl_s64(v.raw, bits.raw));
+ }
+ 
+ // ------------------------------ Shr (Neg)
+ 
+-HWY_INLINE Vec128<uint8_t> operator>>(const Vec128<uint8_t> v,
+-                                      const Vec128<uint8_t> bits) {
++HWY_API Vec128<uint8_t> operator>>(const Vec128<uint8_t> v,
++                                   const Vec128<uint8_t> bits) {
+   const int8x16_t neg_bits = Neg(BitCast(Full128<int8_t>(), bits)).raw;
+   return Vec128<uint8_t>(vshlq_u8(v.raw, neg_bits));
+ }
+ template <size_t N, HWY_IF_LE64(uint8_t, N)>
+-HWY_INLINE Vec128<uint8_t, N> operator>>(const Vec128<uint8_t, N> v,
+-                                         const Vec128<uint8_t, N> bits) {
++HWY_API Vec128<uint8_t, N> operator>>(const Vec128<uint8_t, N> v,
++                                      const Vec128<uint8_t, N> bits) {
+   const int8x8_t neg_bits = Neg(BitCast(Simd<int8_t, N>(), bits)).raw;
+   return Vec128<uint8_t, N>(vshl_u8(v.raw, neg_bits));
+ }
+ 
+-HWY_INLINE Vec128<uint16_t> operator>>(const Vec128<uint16_t> v,
+-                                       const Vec128<uint16_t> bits) {
++HWY_API Vec128<uint16_t> operator>>(const Vec128<uint16_t> v,
++                                    const Vec128<uint16_t> bits) {
+   const int16x8_t neg_bits = Neg(BitCast(Full128<int16_t>(), bits)).raw;
+   return Vec128<uint16_t>(vshlq_u16(v.raw, neg_bits));
+ }
+ template <size_t N, HWY_IF_LE64(uint16_t, N)>
+-HWY_INLINE Vec128<uint16_t, N> operator>>(const Vec128<uint16_t, N> v,
+-                                          const Vec128<uint16_t, N> bits) {
++HWY_API Vec128<uint16_t, N> operator>>(const Vec128<uint16_t, N> v,
++                                       const Vec128<uint16_t, N> bits) {
+   const int16x4_t neg_bits = Neg(BitCast(Simd<int16_t, N>(), bits)).raw;
+   return Vec128<uint16_t, N>(vshl_u16(v.raw, neg_bits));
+ }
+ 
+-HWY_INLINE Vec128<uint32_t> operator>>(const Vec128<uint32_t> v,
+-                                       const Vec128<uint32_t> bits) {
++HWY_API Vec128<uint32_t> operator>>(const Vec128<uint32_t> v,
++                                    const Vec128<uint32_t> bits) {
+   const int32x4_t neg_bits = Neg(BitCast(Full128<int32_t>(), bits)).raw;
+   return Vec128<uint32_t>(vshlq_u32(v.raw, neg_bits));
+ }
+ template <size_t N, HWY_IF_LE64(uint32_t, N)>
+-HWY_INLINE Vec128<uint32_t, N> operator>>(const Vec128<uint32_t, N> v,
+-                                          const Vec128<uint32_t, N> bits) {
++HWY_API Vec128<uint32_t, N> operator>>(const Vec128<uint32_t, N> v,
++                                       const Vec128<uint32_t, N> bits) {
+   const int32x2_t neg_bits = Neg(BitCast(Simd<int32_t, N>(), bits)).raw;
+   return Vec128<uint32_t, N>(vshl_u32(v.raw, neg_bits));
+ }
+ 
+-HWY_INLINE Vec128<uint64_t> operator>>(const Vec128<uint64_t> v,
+-                                       const Vec128<uint64_t> bits) {
++HWY_API Vec128<uint64_t> operator>>(const Vec128<uint64_t> v,
++                                    const Vec128<uint64_t> bits) {
+   const int64x2_t neg_bits = Neg(BitCast(Full128<int64_t>(), bits)).raw;
+   return Vec128<uint64_t>(vshlq_u64(v.raw, neg_bits));
+ }
+-HWY_INLINE Vec128<uint64_t, 1> operator>>(const Vec128<uint64_t, 1> v,
+-                                          const Vec128<uint64_t, 1> bits) {
++HWY_API Vec128<uint64_t, 1> operator>>(const Vec128<uint64_t, 1> v,
++                                       const Vec128<uint64_t, 1> bits) {
+   const int64x1_t neg_bits = Neg(BitCast(Simd<int64_t, 1>(), bits)).raw;
+   return Vec128<uint64_t, 1>(vshl_u64(v.raw, neg_bits));
+ }
+ 
+-HWY_INLINE Vec128<int8_t> operator>>(const Vec128<int8_t> v,
+-                                     const Vec128<int8_t> bits) {
++HWY_API Vec128<int8_t> operator>>(const Vec128<int8_t> v,
++                                  const Vec128<int8_t> bits) {
+   return Vec128<int8_t>(vshlq_s8(v.raw, Neg(bits).raw));
+ }
+ template <size_t N, HWY_IF_LE64(int8_t, N)>
+-HWY_INLINE Vec128<int8_t, N> operator>>(const Vec128<int8_t, N> v,
+-                                        const Vec128<int8_t, N> bits) {
++HWY_API Vec128<int8_t, N> operator>>(const Vec128<int8_t, N> v,
++                                     const Vec128<int8_t, N> bits) {
+   return Vec128<int8_t, N>(vshl_s8(v.raw, Neg(bits).raw));
+ }
+ 
+-HWY_INLINE Vec128<int16_t> operator>>(const Vec128<int16_t> v,
+-                                      const Vec128<int16_t> bits) {
++HWY_API Vec128<int16_t> operator>>(const Vec128<int16_t> v,
++                                   const Vec128<int16_t> bits) {
+   return Vec128<int16_t>(vshlq_s16(v.raw, Neg(bits).raw));
+ }
+ template <size_t N, HWY_IF_LE64(int16_t, N)>
+-HWY_INLINE Vec128<int16_t, N> operator>>(const Vec128<int16_t, N> v,
+-                                         const Vec128<int16_t, N> bits) {
++HWY_API Vec128<int16_t, N> operator>>(const Vec128<int16_t, N> v,
++                                      const Vec128<int16_t, N> bits) {
+   return Vec128<int16_t, N>(vshl_s16(v.raw, Neg(bits).raw));
+ }
+ 
+-HWY_INLINE Vec128<int32_t> operator>>(const Vec128<int32_t> v,
+-                                      const Vec128<int32_t> bits) {
++HWY_API Vec128<int32_t> operator>>(const Vec128<int32_t> v,
++                                   const Vec128<int32_t> bits) {
+   return Vec128<int32_t>(vshlq_s32(v.raw, Neg(bits).raw));
+ }
+ template <size_t N, HWY_IF_LE64(int32_t, N)>
+-HWY_INLINE Vec128<int32_t, N> operator>>(const Vec128<int32_t, N> v,
+-                                         const Vec128<int32_t, N> bits) {
++HWY_API Vec128<int32_t, N> operator>>(const Vec128<int32_t, N> v,
++                                      const Vec128<int32_t, N> bits) {
+   return Vec128<int32_t, N>(vshl_s32(v.raw, Neg(bits).raw));
+ }
+ 
+-HWY_INLINE Vec128<int64_t> operator>>(const Vec128<int64_t> v,
+-                                      const Vec128<int64_t> bits) {
++HWY_API Vec128<int64_t> operator>>(const Vec128<int64_t> v,
++                                   const Vec128<int64_t> bits) {
+   return Vec128<int64_t>(vshlq_s64(v.raw, Neg(bits).raw));
+ }
+-HWY_INLINE Vec128<int64_t, 1> operator>>(const Vec128<int64_t, 1> v,
+-                                         const Vec128<int64_t, 1> bits) {
++HWY_API Vec128<int64_t, 1> operator>>(const Vec128<int64_t, 1> v,
++                                      const Vec128<int64_t, 1> bits) {
+   return Vec128<int64_t, 1>(vshl_s64(v.raw, Neg(bits).raw));
+ }
+ 
+ // ------------------------------ ShiftLeftSame (Shl)
+ 
+ template <typename T, size_t N>
+-HWY_INLINE Vec128<T, N> ShiftLeftSame(const Vec128<T, N> v, int bits) {
++HWY_API Vec128<T, N> ShiftLeftSame(const Vec128<T, N> v, int bits) {
+   return v << Set(Simd<T, N>(), bits);
+ }
+ template <typename T, size_t N>
+-HWY_INLINE Vec128<T, N> ShiftRightSame(const Vec128<T, N> v, int bits) {
++HWY_API Vec128<T, N> ShiftRightSame(const Vec128<T, N> v, int bits) {
+   return v >> Set(Simd<T, N>(), bits);
+ }
+ 
+ // ------------------------------ Integer multiplication
+ 
+ // Unsigned
+-HWY_INLINE Vec128<uint16_t> operator*(const Vec128<uint16_t> a,
+-                                      const Vec128<uint16_t> b) {
++HWY_API Vec128<uint16_t> operator*(const Vec128<uint16_t> a,
++                                   const Vec128<uint16_t> b) {
+   return Vec128<uint16_t>(vmulq_u16(a.raw, b.raw));
+ }
+-HWY_INLINE Vec128<uint32_t> operator*(const Vec128<uint32_t> a,
+-                                      const Vec128<uint32_t> b) {
++HWY_API Vec128<uint32_t> operator*(const Vec128<uint32_t> a,
++                                   const Vec128<uint32_t> b) {
+   return Vec128<uint32_t>(vmulq_u32(a.raw, b.raw));
+ }
+ 
+ template <size_t N, HWY_IF_LE64(uint16_t, N)>
+-HWY_INLINE Vec128<uint16_t, N> operator*(const Vec128<uint16_t, N> a,
+-                                         const Vec128<uint16_t, N> b) {
++HWY_API Vec128<uint16_t, N> operator*(const Vec128<uint16_t, N> a,
++                                      const Vec128<uint16_t, N> b) {
+   return Vec128<uint16_t, N>(vmul_u16(a.raw, b.raw));
+ }
+ template <size_t N, HWY_IF_LE64(uint32_t, N)>
+-HWY_INLINE Vec128<uint32_t, N> operator*(const Vec128<uint32_t, N> a,
+-                                         const Vec128<uint32_t, N> b) {
++HWY_API Vec128<uint32_t, N> operator*(const Vec128<uint32_t, N> a,
++                                      const Vec128<uint32_t, N> b) {
+   return Vec128<uint32_t, N>(vmul_u32(a.raw, b.raw));
+ }
+ 
+ // Signed
+-HWY_INLINE Vec128<int16_t> operator*(const Vec128<int16_t> a,
+-                                     const Vec128<int16_t> b) {
++HWY_API Vec128<int16_t> operator*(const Vec128<int16_t> a,
++                                  const Vec128<int16_t> b) {
+   return Vec128<int16_t>(vmulq_s16(a.raw, b.raw));
+ }
+-HWY_INLINE Vec128<int32_t> operator*(const Vec128<int32_t> a,
+-                                     const Vec128<int32_t> b) {
++HWY_API Vec128<int32_t> operator*(const Vec128<int32_t> a,
++                                  const Vec128<int32_t> b) {
+   return Vec128<int32_t>(vmulq_s32(a.raw, b.raw));
+ }
+ 
+ template <size_t N, HWY_IF_LE64(uint16_t, N)>
+-HWY_INLINE Vec128<int16_t, N> operator*(const Vec128<int16_t, N> a,
+-                                        const Vec128<int16_t, N> b) {
++HWY_API Vec128<int16_t, N> operator*(const Vec128<int16_t, N> a,
++                                     const Vec128<int16_t, N> b) {
+   return Vec128<int16_t, N>(vmul_s16(a.raw, b.raw));
+ }
+ template <size_t N, HWY_IF_LE64(int32_t, N)>
+-HWY_INLINE Vec128<int32_t, N> operator*(const Vec128<int32_t, N> a,
+-                                        const Vec128<int32_t, N> b) {
++HWY_API Vec128<int32_t, N> operator*(const Vec128<int32_t, N> a,
++                                     const Vec128<int32_t, N> b) {
+   return Vec128<int32_t, N>(vmul_s32(a.raw, b.raw));
+ }
+ 
+ // Returns the upper 16 bits of a * b in each lane.
+-HWY_INLINE Vec128<int16_t> MulHigh(const Vec128<int16_t> a,
+-                                   const Vec128<int16_t> b) {
++HWY_API Vec128<int16_t> MulHigh(const Vec128<int16_t> a,
++                                const Vec128<int16_t> b) {
+   int32x4_t rlo = vmull_s16(vget_low_s16(a.raw), vget_low_s16(b.raw));
+ #if HWY_ARCH_ARM_A64
+   int32x4_t rhi = vmull_high_s16(a.raw, b.raw);
+@@ -1123,8 +1097,8 @@ HWY_INLINE Vec128<int16_t> MulHigh(const Vec128<int16_t> a,
+   return Vec128<int16_t>(
+       vuzp2q_s16(vreinterpretq_s16_s32(rlo), vreinterpretq_s16_s32(rhi)));
+ }
+-HWY_INLINE Vec128<uint16_t> MulHigh(const Vec128<uint16_t> a,
+-                                    const Vec128<uint16_t> b) {
++HWY_API Vec128<uint16_t> MulHigh(const Vec128<uint16_t> a,
++                                 const Vec128<uint16_t> b) {
+   uint32x4_t rlo = vmull_u16(vget_low_u16(a.raw), vget_low_u16(b.raw));
+ #if HWY_ARCH_ARM_A64
+   uint32x4_t rhi = vmull_high_u16(a.raw, b.raw);
+@@ -1136,29 +1110,29 @@ HWY_INLINE Vec128<uint16_t> MulHigh(const Vec128<uint16_t> a,
+ }
+ 
+ template <size_t N, HWY_IF_LE64(int16_t, N)>
+-HWY_INLINE Vec128<int16_t, N> MulHigh(const Vec128<int16_t, N> a,
+-                                      const Vec128<int16_t, N> b) {
++HWY_API Vec128<int16_t, N> MulHigh(const Vec128<int16_t, N> a,
++                                   const Vec128<int16_t, N> b) {
+   int16x8_t hi_lo = vreinterpretq_s16_s32(vmull_s16(a.raw, b.raw));
+   return Vec128<int16_t, N>(vget_low_s16(vuzp2q_s16(hi_lo, hi_lo)));
+ }
+ template <size_t N, HWY_IF_LE64(uint16_t, N)>
+-HWY_INLINE Vec128<uint16_t, N> MulHigh(const Vec128<uint16_t, N> a,
+-                                       const Vec128<uint16_t, N> b) {
++HWY_API Vec128<uint16_t, N> MulHigh(const Vec128<uint16_t, N> a,
++                                    const Vec128<uint16_t, N> b) {
+   uint16x8_t hi_lo = vreinterpretq_u16_u32(vmull_u16(a.raw, b.raw));
+   return Vec128<uint16_t, N>(vget_low_u16(vuzp2q_u16(hi_lo, hi_lo)));
+ }
+ 
+ // Multiplies even lanes (0, 2 ..) and places the double-wide result into
+ // even and the upper half into its odd neighbor lane.
+-HWY_INLINE Vec128<int64_t> MulEven(const Vec128<int32_t> a,
+-                                   const Vec128<int32_t> b) {
++HWY_API Vec128<int64_t> MulEven(const Vec128<int32_t> a,
++                                const Vec128<int32_t> b) {
+   int32x4_t a_packed = vuzp1q_s32(a.raw, a.raw);
+   int32x4_t b_packed = vuzp1q_s32(b.raw, b.raw);
+   return Vec128<int64_t>(
+       vmull_s32(vget_low_s32(a_packed), vget_low_s32(b_packed)));
+ }
+-HWY_INLINE Vec128<uint64_t> MulEven(const Vec128<uint32_t> a,
+-                                    const Vec128<uint32_t> b) {
++HWY_API Vec128<uint64_t> MulEven(const Vec128<uint32_t> a,
++                                 const Vec128<uint32_t> b) {
+   uint32x4_t a_packed = vuzp1q_u32(a.raw, a.raw);
+   uint32x4_t b_packed = vuzp1q_u32(b.raw, b.raw);
+   return Vec128<uint64_t>(
+@@ -1166,32 +1140,46 @@ HWY_INLINE Vec128<uint64_t> MulEven(const Vec128<uint32_t> a,
+ }
+ 
+ template <size_t N>
+-HWY_INLINE Vec128<int64_t, (N + 1) / 2> MulEven(const Vec128<int32_t, N> a,
+-                                                const Vec128<int32_t, N> b) {
++HWY_API Vec128<int64_t, (N + 1) / 2> MulEven(const Vec128<int32_t, N> a,
++                                             const Vec128<int32_t, N> b) {
+   int32x2_t a_packed = vuzp1_s32(a.raw, a.raw);
+   int32x2_t b_packed = vuzp1_s32(b.raw, b.raw);
+   return Vec128<int64_t, (N + 1) / 2>(
+       vget_low_s64(vmull_s32(a_packed, b_packed)));
+ }
+ template <size_t N>
+-HWY_INLINE Vec128<uint64_t, (N + 1) / 2> MulEven(const Vec128<uint32_t, N> a,
+-                                                 const Vec128<uint32_t, N> b) {
++HWY_API Vec128<uint64_t, (N + 1) / 2> MulEven(const Vec128<uint32_t, N> a,
++                                              const Vec128<uint32_t, N> b) {
+   uint32x2_t a_packed = vuzp1_u32(a.raw, a.raw);
+   uint32x2_t b_packed = vuzp1_u32(b.raw, b.raw);
+   return Vec128<uint64_t, (N + 1) / 2>(
+       vget_low_u64(vmull_u32(a_packed, b_packed)));
+ }
+ 
++HWY_INLINE Vec128<uint64_t> MulEven(const Vec128<uint64_t> a,
++                                    const Vec128<uint64_t> b) {
++  uint64_t hi;
++  uint64_t lo = Mul128(vgetq_lane_u64(a.raw, 0), vgetq_lane_u64(b.raw, 0), &hi);
++  return Vec128<uint64_t>(vsetq_lane_u64(hi, vdupq_n_u64(lo), 1));
++}
++
++HWY_INLINE Vec128<uint64_t> MulOdd(const Vec128<uint64_t> a,
++                                   const Vec128<uint64_t> b) {
++  uint64_t hi;
++  uint64_t lo = Mul128(vgetq_lane_u64(a.raw, 1), vgetq_lane_u64(b.raw, 1), &hi);
++  return Vec128<uint64_t>(vsetq_lane_u64(hi, vdupq_n_u64(lo), 1));
++}
++
+ // ------------------------------ Floating-point mul / div
+ 
+ HWY_NEON_DEF_FUNCTION_ALL_FLOATS(operator*, vmul, _, 2)
+ 
+ // Approximate reciprocal
+-HWY_INLINE Vec128<float> ApproximateReciprocal(const Vec128<float> v) {
++HWY_API Vec128<float> ApproximateReciprocal(const Vec128<float> v) {
+   return Vec128<float>(vrecpeq_f32(v.raw));
+ }
+ template <size_t N>
+-HWY_INLINE Vec128<float, N> ApproximateReciprocal(const Vec128<float, N> v) {
++HWY_API Vec128<float, N> ApproximateReciprocal(const Vec128<float, N> v) {
+   return Vec128<float, N>(vrecpe_f32(v.raw));
+ }
+ 
+@@ -1214,8 +1202,8 @@ HWY_INLINE Vec128<float, N> ReciprocalNewtonRaphsonStep(
+ }  // namespace detail
+ 
+ template <size_t N>
+-HWY_INLINE Vec128<float, N> operator/(const Vec128<float, N> a,
+-                                      const Vec128<float, N> b) {
++HWY_API Vec128<float, N> operator/(const Vec128<float, N> a,
++                                   const Vec128<float, N> b) {
+   auto x = ApproximateReciprocal(b);
+   x *= detail::ReciprocalNewtonRaphsonStep(x, b);
+   x *= detail::ReciprocalNewtonRaphsonStep(x, b);
+@@ -1226,12 +1214,12 @@ HWY_INLINE Vec128<float, N> operator/(const Vec128<float, N> a,
+ 
+ // ------------------------------ Absolute value of difference.
+ 
+-HWY_INLINE Vec128<float> AbsDiff(const Vec128<float> a, const Vec128<float> b) {
++HWY_API Vec128<float> AbsDiff(const Vec128<float> a, const Vec128<float> b) {
+   return Vec128<float>(vabdq_f32(a.raw, b.raw));
+ }
+ template <size_t N, HWY_IF_LE64(float, N)>
+-HWY_INLINE Vec128<float, N> AbsDiff(const Vec128<float, N> a,
+-                                    const Vec128<float, N> b) {
++HWY_API Vec128<float, N> AbsDiff(const Vec128<float, N> a,
++                                 const Vec128<float, N> b) {
+   return Vec128<float, N>(vabd_f32(a.raw, b.raw));
+ }
+ 
+@@ -1240,34 +1228,33 @@ HWY_INLINE Vec128<float, N> AbsDiff(const Vec128<float, N> a,
+ // Returns add + mul * x
+ #if defined(__ARM_VFPV4__) || HWY_ARCH_ARM_A64
+ template <size_t N, HWY_IF_LE64(float, N)>
+-HWY_INLINE Vec128<float, N> MulAdd(const Vec128<float, N> mul,
+-                                   const Vec128<float, N> x,
+-                                   const Vec128<float, N> add) {
++HWY_API Vec128<float, N> MulAdd(const Vec128<float, N> mul,
++                                const Vec128<float, N> x,
++                                const Vec128<float, N> add) {
+   return Vec128<float, N>(vfma_f32(add.raw, mul.raw, x.raw));
+ }
+-HWY_INLINE Vec128<float> MulAdd(const Vec128<float> mul, const Vec128<float> x,
+-                                const Vec128<float> add) {
++HWY_API Vec128<float> MulAdd(const Vec128<float> mul, const Vec128<float> x,
++                             const Vec128<float> add) {
+   return Vec128<float>(vfmaq_f32(add.raw, mul.raw, x.raw));
+ }
+ #else
+ // Emulate FMA for floats.
+ template <size_t N>
+-HWY_INLINE Vec128<float, N> MulAdd(const Vec128<float, N> mul,
+-                                   const Vec128<float, N> x,
+-                                   const Vec128<float, N> add) {
++HWY_API Vec128<float, N> MulAdd(const Vec128<float, N> mul,
++                                const Vec128<float, N> x,
++                                const Vec128<float, N> add) {
+   return mul * x + add;
+ }
+ #endif
+ 
+ #if HWY_ARCH_ARM_A64
+-HWY_INLINE Vec128<double, 1> MulAdd(const Vec128<double, 1> mul,
+-                                    const Vec128<double, 1> x,
+-                                    const Vec128<double, 1> add) {
++HWY_API Vec128<double, 1> MulAdd(const Vec128<double, 1> mul,
++                                 const Vec128<double, 1> x,
++                                 const Vec128<double, 1> add) {
+   return Vec128<double, 1>(vfma_f64(add.raw, mul.raw, x.raw));
+ }
+-HWY_INLINE Vec128<double> MulAdd(const Vec128<double> mul,
+-                                 const Vec128<double> x,
+-                                 const Vec128<double> add) {
++HWY_API Vec128<double> MulAdd(const Vec128<double> mul, const Vec128<double> x,
++                              const Vec128<double> add) {
+   return Vec128<double>(vfmaq_f64(add.raw, mul.raw, x.raw));
+ }
+ #endif
+@@ -1275,66 +1262,65 @@ HWY_INLINE Vec128<double> MulAdd(const Vec128<double> mul,
+ // Returns add - mul * x
+ #if defined(__ARM_VFPV4__) || HWY_ARCH_ARM_A64
+ template <size_t N, HWY_IF_LE64(float, N)>
+-HWY_INLINE Vec128<float, N> NegMulAdd(const Vec128<float, N> mul,
+-                                      const Vec128<float, N> x,
+-                                      const Vec128<float, N> add) {
++HWY_API Vec128<float, N> NegMulAdd(const Vec128<float, N> mul,
++                                   const Vec128<float, N> x,
++                                   const Vec128<float, N> add) {
+   return Vec128<float, N>(vfms_f32(add.raw, mul.raw, x.raw));
+ }
+-HWY_INLINE Vec128<float> NegMulAdd(const Vec128<float> mul,
+-                                   const Vec128<float> x,
+-                                   const Vec128<float> add) {
++HWY_API Vec128<float> NegMulAdd(const Vec128<float> mul, const Vec128<float> x,
++                                const Vec128<float> add) {
+   return Vec128<float>(vfmsq_f32(add.raw, mul.raw, x.raw));
+ }
+ #else
+ // Emulate FMA for floats.
+ template <size_t N>
+-HWY_INLINE Vec128<float, N> NegMulAdd(const Vec128<float, N> mul,
+-                                      const Vec128<float, N> x,
+-                                      const Vec128<float, N> add) {
++HWY_API Vec128<float, N> NegMulAdd(const Vec128<float, N> mul,
++                                   const Vec128<float, N> x,
++                                   const Vec128<float, N> add) {
+   return add - mul * x;
+ }
+ #endif
+ 
+ #if HWY_ARCH_ARM_A64
+-HWY_INLINE Vec128<double, 1> NegMulAdd(const Vec128<double, 1> mul,
+-                                       const Vec128<double, 1> x,
+-                                       const Vec128<double, 1> add) {
++HWY_API Vec128<double, 1> NegMulAdd(const Vec128<double, 1> mul,
++                                    const Vec128<double, 1> x,
++                                    const Vec128<double, 1> add) {
+   return Vec128<double, 1>(vfms_f64(add.raw, mul.raw, x.raw));
+ }
+-HWY_INLINE Vec128<double> NegMulAdd(const Vec128<double> mul,
+-                                    const Vec128<double> x,
+-                                    const Vec128<double> add) {
++HWY_API Vec128<double> NegMulAdd(const Vec128<double> mul,
++                                 const Vec128<double> x,
++                                 const Vec128<double> add) {
+   return Vec128<double>(vfmsq_f64(add.raw, mul.raw, x.raw));
+ }
+ #endif
+ 
+ // Returns mul * x - sub
+ template <size_t N>
+-HWY_INLINE Vec128<float, N> MulSub(const Vec128<float, N> mul,
+-                                   const Vec128<float, N> x,
+-                                   const Vec128<float, N> sub) {
++HWY_API Vec128<float, N> MulSub(const Vec128<float, N> mul,
++                                const Vec128<float, N> x,
++                                const Vec128<float, N> sub) {
+   return MulAdd(mul, x, Neg(sub));
+ }
+ 
+ // Returns -mul * x - sub
+ template <size_t N>
+-HWY_INLINE Vec128<float, N> NegMulSub(const Vec128<float, N> mul,
+-                                      const Vec128<float, N> x,
+-                                      const Vec128<float, N> sub) {
++HWY_API Vec128<float, N> NegMulSub(const Vec128<float, N> mul,
++                                   const Vec128<float, N> x,
++                                   const Vec128<float, N> sub) {
+   return Neg(MulAdd(mul, x, sub));
+ }
+ 
+ #if HWY_ARCH_ARM_A64
+ template <size_t N>
+-HWY_INLINE Vec128<double, N> MulSub(const Vec128<double, N> mul,
+-                                    const Vec128<double, N> x,
+-                                    const Vec128<double, N> sub) {
++HWY_API Vec128<double, N> MulSub(const Vec128<double, N> mul,
++                                 const Vec128<double, N> x,
++                                 const Vec128<double, N> sub) {
+   return MulAdd(mul, x, Neg(sub));
+ }
+ template <size_t N>
+-HWY_INLINE Vec128<double, N> NegMulSub(const Vec128<double, N> mul,
+-                                       const Vec128<double, N> x,
+-                                       const Vec128<double, N> sub) {
++HWY_API Vec128<double, N> NegMulSub(const Vec128<double, N> mul,
++                                    const Vec128<double, N> x,
++                                    const Vec128<double, N> sub) {
+   return Neg(MulAdd(mul, x, sub));
+ }
+ #endif
+@@ -1342,12 +1328,11 @@ HWY_INLINE Vec128<double, N> NegMulSub(const Vec128<double, N> mul,
+ // ------------------------------ Floating-point square root (IfThenZeroElse)
+ 
+ // Approximate reciprocal square root
+-HWY_INLINE Vec128<float> ApproximateReciprocalSqrt(const Vec128<float> v) {
++HWY_API Vec128<float> ApproximateReciprocalSqrt(const Vec128<float> v) {
+   return Vec128<float>(vrsqrteq_f32(v.raw));
+ }
+ template <size_t N>
+-HWY_INLINE Vec128<float, N> ApproximateReciprocalSqrt(
+-    const Vec128<float, N> v) {
++HWY_API Vec128<float, N> ApproximateReciprocalSqrt(const Vec128<float, N> v) {
+   return Vec128<float, N>(vrsqrte_f32(v.raw));
+ }
+ 
+@@ -1371,7 +1356,7 @@ HWY_INLINE Vec128<float, N> ReciprocalSqrtStep(const Vec128<float, N> root,
+ 
+ // Not defined on armv7: approximate
+ template <size_t N>
+-HWY_INLINE Vec128<float, N> Sqrt(const Vec128<float, N> v) {
++HWY_API Vec128<float, N> Sqrt(const Vec128<float, N> v) {
+   auto recip = ApproximateReciprocalSqrt(v);
+ 
+   recip *= detail::ReciprocalSqrtStep(v * recip, recip);
+@@ -1389,13 +1374,13 @@ HWY_INLINE Vec128<float, N> Sqrt(const Vec128<float, N> v) {
+ 
+ // There is no 64-bit vmvn, so cast instead of using HWY_NEON_DEF_FUNCTION.
+ template <typename T>
+-HWY_INLINE Vec128<T> Not(const Vec128<T> v) {
++HWY_API Vec128<T> Not(const Vec128<T> v) {
+   const Full128<T> d;
+   const Repartition<uint8_t, decltype(d)> d8;
+   return BitCast(d, Vec128<uint8_t>(vmvnq_u8(BitCast(d8, v).raw)));
+ }
+ template <typename T, size_t N, HWY_IF_LE64(T, N)>
+-HWY_INLINE Vec128<T, N> Not(const Vec128<T, N> v) {
++HWY_API Vec128<T, N> Not(const Vec128<T, N> v) {
+   const Simd<T, N> d;
+   const Repartition<uint8_t, decltype(d)> d8;
+   using V8 = decltype(Zero(d8));
+@@ -1407,7 +1392,7 @@ HWY_NEON_DEF_FUNCTION_INTS_UINTS(And, vand, _, 2)
+ 
+ // Uses the u32/64 defined above.
+ template <typename T, size_t N, HWY_IF_FLOAT(T)>
+-HWY_INLINE Vec128<T, N> And(const Vec128<T, N> a, const Vec128<T, N> b) {
++HWY_API Vec128<T, N> And(const Vec128<T, N> a, const Vec128<T, N> b) {
+   const Simd<MakeUnsigned<T>, N> d;
+   return BitCast(Simd<T, N>(), BitCast(d, a) & BitCast(d, b));
+ }
+@@ -1421,15 +1406,15 @@ HWY_NEON_DEF_FUNCTION_INTS_UINTS(reversed_andnot, vbic, _, 2)
+ 
+ // Returns ~not_mask & mask.
+ template <typename T, size_t N, HWY_IF_NOT_FLOAT(T)>
+-HWY_INLINE Vec128<T, N> AndNot(const Vec128<T, N> not_mask,
+-                               const Vec128<T, N> mask) {
++HWY_API Vec128<T, N> AndNot(const Vec128<T, N> not_mask,
++                            const Vec128<T, N> mask) {
+   return internal::reversed_andnot(mask, not_mask);
+ }
+ 
+ // Uses the u32/64 defined above.
+ template <typename T, size_t N, HWY_IF_FLOAT(T)>
+-HWY_INLINE Vec128<T, N> AndNot(const Vec128<T, N> not_mask,
+-                               const Vec128<T, N> mask) {
++HWY_API Vec128<T, N> AndNot(const Vec128<T, N> not_mask,
++                            const Vec128<T, N> mask) {
+   const Simd<MakeUnsigned<T>, N> du;
+   Vec128<MakeUnsigned<T>, N> ret =
+       internal::reversed_andnot(BitCast(du, mask), BitCast(du, not_mask));
+@@ -1442,7 +1427,7 @@ HWY_NEON_DEF_FUNCTION_INTS_UINTS(Or, vorr, _, 2)
+ 
+ // Uses the u32/64 defined above.
+ template <typename T, size_t N, HWY_IF_FLOAT(T)>
+-HWY_INLINE Vec128<T, N> Or(const Vec128<T, N> a, const Vec128<T, N> b) {
++HWY_API Vec128<T, N> Or(const Vec128<T, N> a, const Vec128<T, N> b) {
+   const Simd<MakeUnsigned<T>, N> d;
+   return BitCast(Simd<T, N>(), BitCast(d, a) | BitCast(d, b));
+ }
+@@ -1453,7 +1438,7 @@ HWY_NEON_DEF_FUNCTION_INTS_UINTS(Xor, veor, _, 2)
+ 
+ // Uses the u32/64 defined above.
+ template <typename T, size_t N, HWY_IF_FLOAT(T)>
+-HWY_INLINE Vec128<T, N> Xor(const Vec128<T, N> a, const Vec128<T, N> b) {
++HWY_API Vec128<T, N> Xor(const Vec128<T, N> a, const Vec128<T, N> b) {
+   const Simd<MakeUnsigned<T>, N> d;
+   return BitCast(Simd<T, N>(), BitCast(d, a) ^ BitCast(d, b));
+ }
+@@ -1461,20 +1446,138 @@ HWY_INLINE Vec128<T, N> Xor(const Vec128<T, N> a, const Vec128<T, N> b) {
+ // ------------------------------ Operator overloads (internal-only if float)
+ 
+ template <typename T, size_t N>
+-HWY_INLINE Vec128<T, N> operator&(const Vec128<T, N> a, const Vec128<T, N> b) {
++HWY_API Vec128<T, N> operator&(const Vec128<T, N> a, const Vec128<T, N> b) {
+   return And(a, b);
+ }
+ 
+ template <typename T, size_t N>
+-HWY_INLINE Vec128<T, N> operator|(const Vec128<T, N> a, const Vec128<T, N> b) {
++HWY_API Vec128<T, N> operator|(const Vec128<T, N> a, const Vec128<T, N> b) {
+   return Or(a, b);
+ }
+ 
+ template <typename T, size_t N>
+-HWY_INLINE Vec128<T, N> operator^(const Vec128<T, N> a, const Vec128<T, N> b) {
++HWY_API Vec128<T, N> operator^(const Vec128<T, N> a, const Vec128<T, N> b) {
+   return Xor(a, b);
+ }
+ 
++// ------------------------------ PopulationCount
++
++#ifdef HWY_NATIVE_POPCNT
++#undef HWY_NATIVE_POPCNT
++#else
++#define HWY_NATIVE_POPCNT
++#endif
++
++namespace detail {
++
++template <typename T>
++HWY_INLINE Vec128<T> PopulationCount(hwy::SizeTag<1> /* tag */, Vec128<T> v) {
++  const Full128<uint8_t> d8;
++  return Vec128<T>(vcntq_u8(BitCast(d8, v).raw));
++}
++template <typename T, size_t N, HWY_IF_LE64(T, N)>
++HWY_INLINE Vec128<T, N> PopulationCount(hwy::SizeTag<1> /* tag */,
++                                        Vec128<T, N> v) {
++  const Simd<uint8_t, N> d8;
++  return Vec128<T, N>(vcnt_u8(BitCast(d8, v).raw));
++}
++
++// ARM lacks popcount for lane sizes > 1, so take pairwise sums of the bytes.
++template <typename T>
++HWY_INLINE Vec128<T> PopulationCount(hwy::SizeTag<2> /* tag */, Vec128<T> v) {
++  const Full128<uint8_t> d8;
++  const uint8x16_t bytes = vcntq_u8(BitCast(d8, v).raw);
++  return Vec128<T>(vpaddlq_u8(bytes));
++}
++template <typename T, size_t N, HWY_IF_LE64(T, N)>
++HWY_INLINE Vec128<T, N> PopulationCount(hwy::SizeTag<2> /* tag */,
++                                        Vec128<T, N> v) {
++  const Repartition<uint8_t, Simd<T, N>> d8;
++  const uint8x8_t bytes = vcnt_u8(BitCast(d8, v).raw);
++  return Vec128<T, N>(vpaddl_u8(bytes));
++}
++
++template <typename T>
++HWY_INLINE Vec128<T> PopulationCount(hwy::SizeTag<4> /* tag */, Vec128<T> v) {
++  const Full128<uint8_t> d8;
++  const uint8x16_t bytes = vcntq_u8(BitCast(d8, v).raw);
++  return Vec128<T>(vpaddlq_u16(vpaddlq_u8(bytes)));
++}
++template <typename T, size_t N, HWY_IF_LE64(T, N)>
++HWY_INLINE Vec128<T, N> PopulationCount(hwy::SizeTag<4> /* tag */,
++                                        Vec128<T, N> v) {
++  const Repartition<uint8_t, Simd<T, N>> d8;
++  const uint8x8_t bytes = vcnt_u8(BitCast(d8, v).raw);
++  return Vec128<T, N>(vpaddl_u16(vpaddl_u8(bytes)));
++}
++
++template <typename T>
++HWY_INLINE Vec128<T> PopulationCount(hwy::SizeTag<8> /* tag */, Vec128<T> v) {
++  const Full128<uint8_t> d8;
++  const uint8x16_t bytes = vcntq_u8(BitCast(d8, v).raw);
++  return Vec128<T>(vpaddlq_u32(vpaddlq_u16(vpaddlq_u8(bytes))));
++}
++template <typename T, size_t N, HWY_IF_LE64(T, N)>
++HWY_INLINE Vec128<T, N> PopulationCount(hwy::SizeTag<8> /* tag */,
++                                        Vec128<T, N> v) {
++  const Repartition<uint8_t, Simd<T, N>> d8;
++  const uint8x8_t bytes = vcnt_u8(BitCast(d8, v).raw);
++  return Vec128<T, N>(vpaddl_u32(vpaddl_u16(vpaddl_u8(bytes))));
++}
++
++}  // namespace detail
++
++template <typename T, size_t N, HWY_IF_NOT_FLOAT(T)>
++HWY_API Vec128<T, N> PopulationCount(Vec128<T, N> v) {
++  return detail::PopulationCount(hwy::SizeTag<sizeof(T)>(), v);
++}
++
++// ================================================== SIGN
++
++// ------------------------------ Abs
++
++// Returns absolute value, except that LimitsMin() maps to LimitsMax() + 1.
++HWY_API Vec128<int8_t> Abs(const Vec128<int8_t> v) {
++  return Vec128<int8_t>(vabsq_s8(v.raw));
++}
++HWY_API Vec128<int16_t> Abs(const Vec128<int16_t> v) {
++  return Vec128<int16_t>(vabsq_s16(v.raw));
++}
++HWY_API Vec128<int32_t> Abs(const Vec128<int32_t> v) {
++  return Vec128<int32_t>(vabsq_s32(v.raw));
++}
++// i64 is implemented after BroadcastSignBit.
++HWY_API Vec128<float> Abs(const Vec128<float> v) {
++  return Vec128<float>(vabsq_f32(v.raw));
++}
++
++template <size_t N, HWY_IF_LE64(int8_t, N)>
++HWY_API Vec128<int8_t, N> Abs(const Vec128<int8_t, N> v) {
++  return Vec128<int8_t, N>(vabs_s8(v.raw));
++}
++template <size_t N, HWY_IF_LE64(int16_t, N)>
++HWY_API Vec128<int16_t, N> Abs(const Vec128<int16_t, N> v) {
++  return Vec128<int16_t, N>(vabs_s16(v.raw));
++}
++template <size_t N, HWY_IF_LE64(int32_t, N)>
++HWY_API Vec128<int32_t, N> Abs(const Vec128<int32_t, N> v) {
++  return Vec128<int32_t, N>(vabs_s32(v.raw));
++}
++template <size_t N, HWY_IF_LE64(float, N)>
++HWY_API Vec128<float, N> Abs(const Vec128<float, N> v) {
++  return Vec128<float, N>(vabs_f32(v.raw));
++}
++
++#if HWY_ARCH_ARM_A64
++HWY_API Vec128<double> Abs(const Vec128<double> v) {
++  return Vec128<double>(vabsq_f64(v.raw));
++}
++
++HWY_API Vec128<double, 1> Abs(const Vec128<double, 1> v) {
++  return Vec128<double, 1>(vabs_f64(v.raw));
++}
++#endif
++
+ // ------------------------------ CopySign
+ 
+ template <typename T, size_t N>
+@@ -1505,19 +1608,19 @@ HWY_API Vec128<T, N> BroadcastSignBit(const Vec128<T, N> v) {
+ 
+ // Mask and Vec have the same representation (true = FF..FF).
+ template <typename T, size_t N>
+-HWY_INLINE Mask128<T, N> MaskFromVec(const Vec128<T, N> v) {
++HWY_API Mask128<T, N> MaskFromVec(const Vec128<T, N> v) {
+   const Simd<MakeUnsigned<T>, N> du;
+   return Mask128<T, N>(BitCast(du, v).raw);
+ }
+ 
+ // DEPRECATED
+ template <typename T, size_t N>
+-HWY_INLINE Vec128<T, N> VecFromMask(const Mask128<T, N> v) {
++HWY_API Vec128<T, N> VecFromMask(const Mask128<T, N> v) {
+   return BitCast(Simd<T, N>(), Vec128<MakeUnsigned<T>, N>(v.raw));
+ }
+ 
+ template <typename T, size_t N>
+-HWY_INLINE Vec128<T, N> VecFromMask(Simd<T, N> d, const Mask128<T, N> v) {
++HWY_API Vec128<T, N> VecFromMask(Simd<T, N> d, const Mask128<T, N> v) {
+   return BitCast(d, Vec128<MakeUnsigned<T>, N>(v.raw));
+ }
+ 
+@@ -1547,20 +1650,20 @@ HWY_NEON_DEF_FUNCTION_ALL_TYPES(IfThenElse, vbsl, _, HWY_IF)
+ 
+ // mask ? yes : 0
+ template <typename T, size_t N>
+-HWY_INLINE Vec128<T, N> IfThenElseZero(const Mask128<T, N> mask,
+-                                       const Vec128<T, N> yes) {
++HWY_API Vec128<T, N> IfThenElseZero(const Mask128<T, N> mask,
++                                    const Vec128<T, N> yes) {
+   return yes & VecFromMask(Simd<T, N>(), mask);
+ }
+ 
+ // mask ? 0 : no
+ template <typename T, size_t N>
+-HWY_INLINE Vec128<T, N> IfThenZeroElse(const Mask128<T, N> mask,
+-                                       const Vec128<T, N> no) {
++HWY_API Vec128<T, N> IfThenZeroElse(const Mask128<T, N> mask,
++                                    const Vec128<T, N> no) {
+   return AndNot(VecFromMask(Simd<T, N>(), mask), no);
+ }
+ 
+ template <typename T, size_t N>
+-HWY_INLINE Vec128<T, N> ZeroIfNegative(Vec128<T, N> v) {
++HWY_API Vec128<T, N> ZeroIfNegative(Vec128<T, N> v) {
+   const auto zero = Zero(Simd<T, N>());
+   return Max(zero, v);
+ }
+@@ -1569,8 +1672,7 @@ HWY_INLINE Vec128<T, N> ZeroIfNegative(Vec128<T, N> v) {
+ 
+ template <typename T, size_t N>
+ HWY_API Mask128<T, N> Not(const Mask128<T, N> m) {
+-  const Simd<T, N> d;
+-  return MaskFromVec(Not(VecFromMask(d, m)));
++  return MaskFromVec(Not(VecFromMask(Simd<T, N>(), m)));
+ }
+ 
+ template <typename T, size_t N>
+@@ -1604,22 +1706,22 @@ HWY_API Mask128<T, N> Xor(const Mask128<T, N> a, Mask128<T, N> b) {
+ // ------------------------------ Shuffle2301 (for i64 compares)
+ 
+ // Swap 32-bit halves in 64-bits
+-HWY_INLINE Vec128<uint32_t, 2> Shuffle2301(const Vec128<uint32_t, 2> v) {
++HWY_API Vec128<uint32_t, 2> Shuffle2301(const Vec128<uint32_t, 2> v) {
+   return Vec128<uint32_t, 2>(vrev64_u32(v.raw));
+ }
+-HWY_INLINE Vec128<int32_t, 2> Shuffle2301(const Vec128<int32_t, 2> v) {
++HWY_API Vec128<int32_t, 2> Shuffle2301(const Vec128<int32_t, 2> v) {
+   return Vec128<int32_t, 2>(vrev64_s32(v.raw));
+ }
+-HWY_INLINE Vec128<float, 2> Shuffle2301(const Vec128<float, 2> v) {
++HWY_API Vec128<float, 2> Shuffle2301(const Vec128<float, 2> v) {
+   return Vec128<float, 2>(vrev64_f32(v.raw));
+ }
+-HWY_INLINE Vec128<uint32_t> Shuffle2301(const Vec128<uint32_t> v) {
++HWY_API Vec128<uint32_t> Shuffle2301(const Vec128<uint32_t> v) {
+   return Vec128<uint32_t>(vrev64q_u32(v.raw));
+ }
+-HWY_INLINE Vec128<int32_t> Shuffle2301(const Vec128<int32_t> v) {
++HWY_API Vec128<int32_t> Shuffle2301(const Vec128<int32_t> v) {
+   return Vec128<int32_t>(vrev64q_s32(v.raw));
+ }
+-HWY_INLINE Vec128<float> Shuffle2301(const Vec128<float> v) {
++HWY_API Vec128<float> Shuffle2301(const Vec128<float> v) {
+   return Vec128<float>(vrev64q_f32(v.raw));
+ }
+ 
+@@ -1639,6 +1741,12 @@ HWY_NEON_DEF_FUNCTION_INT_8_16_32(operator==, vceq, _, HWY_COMPARE)
+ HWY_NEON_DEF_FUNCTION_UINT_8_16_32(operator==, vceq, _, HWY_COMPARE)
+ #endif
+ 
++// ------------------------------ Inequality
++template <typename T, size_t N>
++HWY_API Mask128<T, N> operator!=(const Vec128<T, N> a, const Vec128<T, N> b) {
++  return Not(a == b);
++}
++
+ // ------------------------------ Strict inequality (signed, float)
+ #if HWY_ARCH_ARM_A64
+ HWY_NEON_DEF_FUNCTION_INTS(operator<, vclt, _, HWY_COMPARE)
+@@ -1660,8 +1768,8 @@ HWY_NEON_DEF_FUNCTION_ALL_FLOATS(operator<=, vcle, _, HWY_COMPARE)
+ #if HWY_ARCH_ARM_V7
+ 
+ template <size_t N>
+-HWY_INLINE Mask128<int64_t, N> operator==(const Vec128<int64_t, N> a,
+-                                          const Vec128<int64_t, N> b) {
++HWY_API Mask128<int64_t, N> operator==(const Vec128<int64_t, N> a,
++                                       const Vec128<int64_t, N> b) {
+   const Simd<int32_t, N * 2> d32;
+   const Simd<int64_t, N> d64;
+   const auto cmp32 = VecFromMask(d32, Eq(BitCast(d32, a), BitCast(d32, b)));
+@@ -1670,8 +1778,8 @@ HWY_INLINE Mask128<int64_t, N> operator==(const Vec128<int64_t, N> a,
+ }
+ 
+ template <size_t N>
+-HWY_INLINE Mask128<uint64_t, N> operator==(const Vec128<uint64_t, N> a,
+-                                           const Vec128<uint64_t, N> b) {
++HWY_API Mask128<uint64_t, N> operator==(const Vec128<uint64_t, N> a,
++                                        const Vec128<uint64_t, N> b) {
+   const Simd<uint32_t, N * 2> d32;
+   const Simd<uint64_t, N> d64;
+   const auto cmp32 = VecFromMask(d32, Eq(BitCast(d32, a), BitCast(d32, b)));
+@@ -1679,13 +1787,13 @@ HWY_INLINE Mask128<uint64_t, N> operator==(const Vec128<uint64_t, N> a,
+   return MaskFromVec(BitCast(d64, cmp64));
+ }
+ 
+-HWY_INLINE Mask128<int64_t> operator<(const Vec128<int64_t> a,
+-                                      const Vec128<int64_t> b) {
++HWY_API Mask128<int64_t> operator<(const Vec128<int64_t> a,
++                                   const Vec128<int64_t> b) {
+   const int64x2_t sub = vqsubq_s64(a.raw, b.raw);
+   return MaskFromVec(BroadcastSignBit(Vec128<int64_t>(sub)));
+ }
+-HWY_INLINE Mask128<int64_t, 1> operator<(const Vec128<int64_t, 1> a,
+-                                         const Vec128<int64_t, 1> b) {
++HWY_API Mask128<int64_t, 1> operator<(const Vec128<int64_t, 1> a,
++                                      const Vec128<int64_t, 1> b) {
+   const int64x1_t sub = vqsub_s64(a.raw, b.raw);
+   return MaskFromVec(BroadcastSignBit(Vec128<int64_t, 1>(sub)));
+ }
+@@ -1727,13 +1835,13 @@ HWY_NEON_DEF_FUNCTION_UINT_8_16_32(TestBit, vtst, _, HWY_TESTBIT)
+ HWY_NEON_DEF_FUNCTION_INT_8_16_32(TestBit, vtst, _, HWY_TESTBIT)
+ 
+ template <size_t N>
+-HWY_INLINE Mask128<uint64_t, N> TestBit(Vec128<uint64_t, N> v,
+-                                        Vec128<uint64_t, N> bit) {
++HWY_API Mask128<uint64_t, N> TestBit(Vec128<uint64_t, N> v,
++                                     Vec128<uint64_t, N> bit) {
+   return (v & bit) == bit;
+ }
+ template <size_t N>
+-HWY_INLINE Mask128<int64_t, N> TestBit(Vec128<int64_t, N> v,
+-                                       Vec128<int64_t, N> bit) {
++HWY_API Mask128<int64_t, N> TestBit(Vec128<int64_t, N> v,
++                                    Vec128<int64_t, N> bit) {
+   return (v & bit) == bit;
+ }
+ 
+@@ -1744,7 +1852,7 @@ HWY_INLINE Mask128<int64_t, N> TestBit(Vec128<int64_t, N> v,
+ #undef HWY_NEON_BUILD_ARG_HWY_TESTBIT
+ 
+ // ------------------------------ Abs i64 (IfThenElse, BroadcastSignBit)
+-HWY_INLINE Vec128<int64_t> Abs(const Vec128<int64_t> v) {
++HWY_API Vec128<int64_t> Abs(const Vec128<int64_t> v) {
+ #if HWY_ARCH_ARM_A64
+   return Vec128<int64_t>(vabsq_s64(v.raw));
+ #else
+@@ -1752,7 +1860,7 @@ HWY_INLINE Vec128<int64_t> Abs(const Vec128<int64_t> v) {
+   return IfThenElse(MaskFromVec(BroadcastSignBit(v)), zero - v, v);
+ #endif
+ }
+-HWY_INLINE Vec128<int64_t, 1> Abs(const Vec128<int64_t, 1> v) {
++HWY_API Vec128<int64_t, 1> Abs(const Vec128<int64_t, 1> v) {
+ #if HWY_ARCH_ARM_A64
+   return Vec128<int64_t, 1>(vabs_s64(v.raw));
+ #else
+@@ -1765,11 +1873,11 @@ HWY_INLINE Vec128<int64_t, 1> Abs(const Vec128<int64_t, 1> v) {
+ 
+ #if HWY_ARCH_ARM_A64
+ 
+-HWY_INLINE Mask128<uint64_t> operator<(Vec128<uint64_t> a, Vec128<uint64_t> b) {
++HWY_API Mask128<uint64_t> operator<(Vec128<uint64_t> a, Vec128<uint64_t> b) {
+   return Mask128<uint64_t>(vcltq_u64(a.raw, b.raw));
+ }
+-HWY_INLINE Mask128<uint64_t, 1> operator<(Vec128<uint64_t, 1> a,
+-                                          Vec128<uint64_t, 1> b) {
++HWY_API Mask128<uint64_t, 1> operator<(Vec128<uint64_t, 1> a,
++                                       Vec128<uint64_t, 1> b) {
+   return Mask128<uint64_t, 1>(vclt_u64(a.raw, b.raw));
+ }
+ 
+@@ -1779,8 +1887,8 @@ HWY_INLINE Mask128<uint64_t, 1> operator<(Vec128<uint64_t, 1> a,
+ HWY_NEON_DEF_FUNCTION_UINT_8_16_32(Min, vmin, _, 2)
+ 
+ template <size_t N>
+-HWY_INLINE Vec128<uint64_t, N> Min(const Vec128<uint64_t, N> a,
+-                                   const Vec128<uint64_t, N> b) {
++HWY_API Vec128<uint64_t, N> Min(const Vec128<uint64_t, N> a,
++                                const Vec128<uint64_t, N> b) {
+ #if HWY_ARCH_ARM_A64
+   return IfThenElse(b < a, b, a);
+ #else
+@@ -1794,8 +1902,8 @@ HWY_INLINE Vec128<uint64_t, N> Min(const Vec128<uint64_t, N> a,
+ HWY_NEON_DEF_FUNCTION_INT_8_16_32(Min, vmin, _, 2)
+ 
+ template <size_t N>
+-HWY_INLINE Vec128<int64_t, N> Min(const Vec128<int64_t, N> a,
+-                                  const Vec128<int64_t, N> b) {
++HWY_API Vec128<int64_t, N> Min(const Vec128<int64_t, N> a,
++                               const Vec128<int64_t, N> b) {
+ #if HWY_ARCH_ARM_A64
+   return IfThenElse(b < a, b, a);
+ #else
+@@ -1817,8 +1925,8 @@ HWY_NEON_DEF_FUNCTION_ALL_FLOATS(Min, vmin, _, 2)
+ HWY_NEON_DEF_FUNCTION_UINT_8_16_32(Max, vmax, _, 2)
+ 
+ template <size_t N>
+-HWY_INLINE Vec128<uint64_t, N> Max(const Vec128<uint64_t, N> a,
+-                                   const Vec128<uint64_t, N> b) {
++HWY_API Vec128<uint64_t, N> Max(const Vec128<uint64_t, N> a,
++                                const Vec128<uint64_t, N> b) {
+ #if HWY_ARCH_ARM_A64
+   return IfThenElse(b < a, a, b);
+ #else
+@@ -1832,8 +1940,8 @@ HWY_INLINE Vec128<uint64_t, N> Max(const Vec128<uint64_t, N> a,
+ HWY_NEON_DEF_FUNCTION_INT_8_16_32(Max, vmax, _, 2)
+ 
+ template <size_t N>
+-HWY_INLINE Vec128<int64_t, N> Max(const Vec128<int64_t, N> a,
+-                                  const Vec128<int64_t, N> b) {
++HWY_API Vec128<int64_t, N> Max(const Vec128<int64_t, N> a,
++                               const Vec128<int64_t, N> b) {
+ #if HWY_ARCH_ARM_A64
+   return IfThenElse(b < a, a, b);
+ #else
+@@ -1853,90 +1961,90 @@ HWY_NEON_DEF_FUNCTION_ALL_FLOATS(Max, vmax, _, 2)
+ 
+ // ------------------------------ Load 128
+ 
+-HWY_INLINE Vec128<uint8_t> LoadU(Full128<uint8_t> /* tag */,
+-                                 const uint8_t* HWY_RESTRICT aligned) {
++HWY_API Vec128<uint8_t> LoadU(Full128<uint8_t> /* tag */,
++                              const uint8_t* HWY_RESTRICT aligned) {
+   return Vec128<uint8_t>(vld1q_u8(aligned));
+ }
+-HWY_INLINE Vec128<uint16_t> LoadU(Full128<uint16_t> /* tag */,
+-                                  const uint16_t* HWY_RESTRICT aligned) {
++HWY_API Vec128<uint16_t> LoadU(Full128<uint16_t> /* tag */,
++                               const uint16_t* HWY_RESTRICT aligned) {
+   return Vec128<uint16_t>(vld1q_u16(aligned));
+ }
+-HWY_INLINE Vec128<uint32_t> LoadU(Full128<uint32_t> /* tag */,
+-                                  const uint32_t* HWY_RESTRICT aligned) {
++HWY_API Vec128<uint32_t> LoadU(Full128<uint32_t> /* tag */,
++                               const uint32_t* HWY_RESTRICT aligned) {
+   return Vec128<uint32_t>(vld1q_u32(aligned));
+ }
+-HWY_INLINE Vec128<uint64_t> LoadU(Full128<uint64_t> /* tag */,
+-                                  const uint64_t* HWY_RESTRICT aligned) {
++HWY_API Vec128<uint64_t> LoadU(Full128<uint64_t> /* tag */,
++                               const uint64_t* HWY_RESTRICT aligned) {
+   return Vec128<uint64_t>(vld1q_u64(aligned));
+ }
+-HWY_INLINE Vec128<int8_t> LoadU(Full128<int8_t> /* tag */,
+-                                const int8_t* HWY_RESTRICT aligned) {
++HWY_API Vec128<int8_t> LoadU(Full128<int8_t> /* tag */,
++                             const int8_t* HWY_RESTRICT aligned) {
+   return Vec128<int8_t>(vld1q_s8(aligned));
+ }
+-HWY_INLINE Vec128<int16_t> LoadU(Full128<int16_t> /* tag */,
+-                                 const int16_t* HWY_RESTRICT aligned) {
++HWY_API Vec128<int16_t> LoadU(Full128<int16_t> /* tag */,
++                              const int16_t* HWY_RESTRICT aligned) {
+   return Vec128<int16_t>(vld1q_s16(aligned));
+ }
+-HWY_INLINE Vec128<int32_t> LoadU(Full128<int32_t> /* tag */,
+-                                 const int32_t* HWY_RESTRICT aligned) {
++HWY_API Vec128<int32_t> LoadU(Full128<int32_t> /* tag */,
++                              const int32_t* HWY_RESTRICT aligned) {
+   return Vec128<int32_t>(vld1q_s32(aligned));
+ }
+-HWY_INLINE Vec128<int64_t> LoadU(Full128<int64_t> /* tag */,
+-                                 const int64_t* HWY_RESTRICT aligned) {
++HWY_API Vec128<int64_t> LoadU(Full128<int64_t> /* tag */,
++                              const int64_t* HWY_RESTRICT aligned) {
+   return Vec128<int64_t>(vld1q_s64(aligned));
+ }
+-HWY_INLINE Vec128<float> LoadU(Full128<float> /* tag */,
+-                               const float* HWY_RESTRICT aligned) {
++HWY_API Vec128<float> LoadU(Full128<float> /* tag */,
++                            const float* HWY_RESTRICT aligned) {
+   return Vec128<float>(vld1q_f32(aligned));
+ }
+ #if HWY_ARCH_ARM_A64
+-HWY_INLINE Vec128<double> LoadU(Full128<double> /* tag */,
+-                                const double* HWY_RESTRICT aligned) {
++HWY_API Vec128<double> LoadU(Full128<double> /* tag */,
++                             const double* HWY_RESTRICT aligned) {
+   return Vec128<double>(vld1q_f64(aligned));
+ }
+ #endif
+ 
+ // ------------------------------ Load 64
+ 
+-HWY_INLINE Vec128<uint8_t, 8> LoadU(Simd<uint8_t, 8> /* tag */,
+-                                    const uint8_t* HWY_RESTRICT p) {
++HWY_API Vec128<uint8_t, 8> LoadU(Simd<uint8_t, 8> /* tag */,
++                                 const uint8_t* HWY_RESTRICT p) {
+   return Vec128<uint8_t, 8>(vld1_u8(p));
+ }
+-HWY_INLINE Vec128<uint16_t, 4> LoadU(Simd<uint16_t, 4> /* tag */,
+-                                     const uint16_t* HWY_RESTRICT p) {
++HWY_API Vec128<uint16_t, 4> LoadU(Simd<uint16_t, 4> /* tag */,
++                                  const uint16_t* HWY_RESTRICT p) {
+   return Vec128<uint16_t, 4>(vld1_u16(p));
+ }
+-HWY_INLINE Vec128<uint32_t, 2> LoadU(Simd<uint32_t, 2> /* tag */,
+-                                     const uint32_t* HWY_RESTRICT p) {
++HWY_API Vec128<uint32_t, 2> LoadU(Simd<uint32_t, 2> /* tag */,
++                                  const uint32_t* HWY_RESTRICT p) {
+   return Vec128<uint32_t, 2>(vld1_u32(p));
+ }
+-HWY_INLINE Vec128<uint64_t, 1> LoadU(Simd<uint64_t, 1> /* tag */,
+-                                     const uint64_t* HWY_RESTRICT p) {
++HWY_API Vec128<uint64_t, 1> LoadU(Simd<uint64_t, 1> /* tag */,
++                                  const uint64_t* HWY_RESTRICT p) {
+   return Vec128<uint64_t, 1>(vld1_u64(p));
+ }
+-HWY_INLINE Vec128<int8_t, 8> LoadU(Simd<int8_t, 8> /* tag */,
+-                                   const int8_t* HWY_RESTRICT p) {
++HWY_API Vec128<int8_t, 8> LoadU(Simd<int8_t, 8> /* tag */,
++                                const int8_t* HWY_RESTRICT p) {
+   return Vec128<int8_t, 8>(vld1_s8(p));
+ }
+-HWY_INLINE Vec128<int16_t, 4> LoadU(Simd<int16_t, 4> /* tag */,
+-                                    const int16_t* HWY_RESTRICT p) {
++HWY_API Vec128<int16_t, 4> LoadU(Simd<int16_t, 4> /* tag */,
++                                 const int16_t* HWY_RESTRICT p) {
+   return Vec128<int16_t, 4>(vld1_s16(p));
+ }
+-HWY_INLINE Vec128<int32_t, 2> LoadU(Simd<int32_t, 2> /* tag */,
+-                                    const int32_t* HWY_RESTRICT p) {
++HWY_API Vec128<int32_t, 2> LoadU(Simd<int32_t, 2> /* tag */,
++                                 const int32_t* HWY_RESTRICT p) {
+   return Vec128<int32_t, 2>(vld1_s32(p));
+ }
+-HWY_INLINE Vec128<int64_t, 1> LoadU(Simd<int64_t, 1> /* tag */,
+-                                    const int64_t* HWY_RESTRICT p) {
++HWY_API Vec128<int64_t, 1> LoadU(Simd<int64_t, 1> /* tag */,
++                                 const int64_t* HWY_RESTRICT p) {
+   return Vec128<int64_t, 1>(vld1_s64(p));
+ }
+-HWY_INLINE Vec128<float, 2> LoadU(Simd<float, 2> /* tag */,
+-                                  const float* HWY_RESTRICT p) {
++HWY_API Vec128<float, 2> LoadU(Simd<float, 2> /* tag */,
++                               const float* HWY_RESTRICT p) {
+   return Vec128<float, 2>(vld1_f32(p));
+ }
+ #if HWY_ARCH_ARM_A64
+-HWY_INLINE Vec128<double, 1> LoadU(Simd<double, 1> /* tag */,
+-                                   const double* HWY_RESTRICT p) {
++HWY_API Vec128<double, 1> LoadU(Simd<double, 1> /* tag */,
++                                const double* HWY_RESTRICT p) {
+   return Vec128<double, 1>(vld1_f64(p));
+ }
+ #endif
+@@ -1948,44 +2056,44 @@ HWY_INLINE Vec128<double, 1> LoadU(Simd<double, 1> /* tag */,
+ // we don't actually care what is in it, and we don't want
+ // to introduce extra overhead by initializing it to something.
+ 
+-HWY_INLINE Vec128<uint8_t, 4> LoadU(Simd<uint8_t, 4> /*tag*/,
+-                                    const uint8_t* HWY_RESTRICT p) {
++HWY_API Vec128<uint8_t, 4> LoadU(Simd<uint8_t, 4> /*tag*/,
++                                 const uint8_t* HWY_RESTRICT p) {
+   uint32x2_t a = Undefined(Simd<uint32_t, 2>()).raw;
+   uint32x2_t b = vld1_lane_u32(reinterpret_cast<const uint32_t*>(p), a, 0);
+   return Vec128<uint8_t, 4>(vreinterpret_u8_u32(b));
+ }
+-HWY_INLINE Vec128<uint16_t, 2> LoadU(Simd<uint16_t, 2> /*tag*/,
+-                                     const uint16_t* HWY_RESTRICT p) {
++HWY_API Vec128<uint16_t, 2> LoadU(Simd<uint16_t, 2> /*tag*/,
++                                  const uint16_t* HWY_RESTRICT p) {
+   uint32x2_t a = Undefined(Simd<uint32_t, 2>()).raw;
+   uint32x2_t b = vld1_lane_u32(reinterpret_cast<const uint32_t*>(p), a, 0);
+   return Vec128<uint16_t, 2>(vreinterpret_u16_u32(b));
+ }
+-HWY_INLINE Vec128<uint32_t, 1> LoadU(Simd<uint32_t, 1> /*tag*/,
+-                                     const uint32_t* HWY_RESTRICT p) {
++HWY_API Vec128<uint32_t, 1> LoadU(Simd<uint32_t, 1> /*tag*/,
++                                  const uint32_t* HWY_RESTRICT p) {
+   uint32x2_t a = Undefined(Simd<uint32_t, 2>()).raw;
+   uint32x2_t b = vld1_lane_u32(p, a, 0);
+   return Vec128<uint32_t, 1>(b);
+ }
+-HWY_INLINE Vec128<int8_t, 4> LoadU(Simd<int8_t, 4> /*tag*/,
+-                                   const int8_t* HWY_RESTRICT p) {
++HWY_API Vec128<int8_t, 4> LoadU(Simd<int8_t, 4> /*tag*/,
++                                const int8_t* HWY_RESTRICT p) {
+   int32x2_t a = Undefined(Simd<int32_t, 2>()).raw;
+   int32x2_t b = vld1_lane_s32(reinterpret_cast<const int32_t*>(p), a, 0);
+   return Vec128<int8_t, 4>(vreinterpret_s8_s32(b));
+ }
+-HWY_INLINE Vec128<int16_t, 2> LoadU(Simd<int16_t, 2> /*tag*/,
+-                                    const int16_t* HWY_RESTRICT p) {
++HWY_API Vec128<int16_t, 2> LoadU(Simd<int16_t, 2> /*tag*/,
++                                 const int16_t* HWY_RESTRICT p) {
+   int32x2_t a = Undefined(Simd<int32_t, 2>()).raw;
+   int32x2_t b = vld1_lane_s32(reinterpret_cast<const int32_t*>(p), a, 0);
+   return Vec128<int16_t, 2>(vreinterpret_s16_s32(b));
+ }
+-HWY_INLINE Vec128<int32_t, 1> LoadU(Simd<int32_t, 1> /*tag*/,
+-                                    const int32_t* HWY_RESTRICT p) {
++HWY_API Vec128<int32_t, 1> LoadU(Simd<int32_t, 1> /*tag*/,
++                                 const int32_t* HWY_RESTRICT p) {
+   int32x2_t a = Undefined(Simd<int32_t, 2>()).raw;
+   int32x2_t b = vld1_lane_s32(p, a, 0);
+   return Vec128<int32_t, 1>(b);
+ }
+-HWY_INLINE Vec128<float, 1> LoadU(Simd<float, 1> /*tag*/,
+-                                  const float* HWY_RESTRICT p) {
++HWY_API Vec128<float, 1> LoadU(Simd<float, 1> /*tag*/,
++                               const float* HWY_RESTRICT p) {
+   float32x2_t a = Undefined(Simd<float, 2>()).raw;
+   float32x2_t b = vld1_lane_f32(p, a, 0);
+   return Vec128<float, 1>(b);
+@@ -1993,26 +2101,26 @@ HWY_INLINE Vec128<float, 1> LoadU(Simd<float, 1> /*tag*/,
+ 
+ // ------------------------------ Load 16
+ 
+-HWY_INLINE Vec128<uint8_t, 2> LoadU(Simd<uint8_t, 2> /*tag*/,
+-                                    const uint8_t* HWY_RESTRICT p) {
++HWY_API Vec128<uint8_t, 2> LoadU(Simd<uint8_t, 2> /*tag*/,
++                                 const uint8_t* HWY_RESTRICT p) {
+   uint16x4_t a = Undefined(Simd<uint16_t, 4>()).raw;
+   uint16x4_t b = vld1_lane_u16(reinterpret_cast<const uint16_t*>(p), a, 0);
+   return Vec128<uint8_t, 2>(vreinterpret_u8_u16(b));
+ }
+-HWY_INLINE Vec128<uint16_t, 1> LoadU(Simd<uint16_t, 1> /*tag*/,
+-                                     const uint16_t* HWY_RESTRICT p) {
++HWY_API Vec128<uint16_t, 1> LoadU(Simd<uint16_t, 1> /*tag*/,
++                                  const uint16_t* HWY_RESTRICT p) {
+   uint16x4_t a = Undefined(Simd<uint16_t, 4>()).raw;
+   uint16x4_t b = vld1_lane_u16(p, a, 0);
+   return Vec128<uint16_t, 1>(b);
+ }
+-HWY_INLINE Vec128<int8_t, 2> LoadU(Simd<int8_t, 2> /*tag*/,
+-                                   const int8_t* HWY_RESTRICT p) {
++HWY_API Vec128<int8_t, 2> LoadU(Simd<int8_t, 2> /*tag*/,
++                                const int8_t* HWY_RESTRICT p) {
+   int16x4_t a = Undefined(Simd<int16_t, 4>()).raw;
+   int16x4_t b = vld1_lane_s16(reinterpret_cast<const int16_t*>(p), a, 0);
+   return Vec128<int8_t, 2>(vreinterpret_s8_s16(b));
+ }
+-HWY_INLINE Vec128<int16_t, 1> LoadU(Simd<int16_t, 1> /*tag*/,
+-                                    const int16_t* HWY_RESTRICT p) {
++HWY_API Vec128<int16_t, 1> LoadU(Simd<int16_t, 1> /*tag*/,
++                                 const int16_t* HWY_RESTRICT p) {
+   int16x4_t a = Undefined(Simd<int16_t, 4>()).raw;
+   int16x4_t b = vld1_lane_s16(p, a, 0);
+   return Vec128<int16_t, 1>(b);
+@@ -2020,15 +2128,15 @@ HWY_INLINE Vec128<int16_t, 1> LoadU(Simd<int16_t, 1> /*tag*/,
+ 
+ // ------------------------------ Load 8
+ 
+-HWY_INLINE Vec128<uint8_t, 1> LoadU(Simd<uint8_t, 1> d,
+-                                    const uint8_t* HWY_RESTRICT p) {
++HWY_API Vec128<uint8_t, 1> LoadU(Simd<uint8_t, 1> d,
++                                 const uint8_t* HWY_RESTRICT p) {
+   uint8x8_t a = Undefined(d).raw;
+   uint8x8_t b = vld1_lane_u8(p, a, 0);
+   return Vec128<uint8_t, 1>(b);
+ }
+ 
+-HWY_INLINE Vec128<int8_t, 1> LoadU(Simd<int8_t, 1> d,
+-                                   const int8_t* HWY_RESTRICT p) {
++HWY_API Vec128<int8_t, 1> LoadU(Simd<int8_t, 1> d,
++                                const int8_t* HWY_RESTRICT p) {
+   int8x8_t a = Undefined(d).raw;
+   int8x8_t b = vld1_lane_s8(p, a, 0);
+   return Vec128<int8_t, 1>(b);
+@@ -2036,8 +2144,8 @@ HWY_INLINE Vec128<int8_t, 1> LoadU(Simd<int8_t, 1> d,
+ 
+ // float16_t uses the same Raw as uint16_t, so forward to that.
+ template <size_t N>
+-HWY_INLINE Vec128<float16_t, N> LoadU(Simd<float16_t, N> /*d*/,
+-                                      const float16_t* HWY_RESTRICT p) {
++HWY_API Vec128<float16_t, N> LoadU(Simd<float16_t, N> /*d*/,
++                                   const float16_t* HWY_RESTRICT p) {
+   const Simd<uint16_t, N> du16;
+   const auto pu16 = reinterpret_cast<const uint16_t*>(p);
+   return Vec128<float16_t, N>(LoadU(du16, pu16).raw);
+@@ -2045,171 +2153,170 @@ HWY_INLINE Vec128<float16_t, N> LoadU(Simd<float16_t, N> /*d*/,
+ 
+ // On ARM, Load is the same as LoadU.
+ template <typename T, size_t N>
+-HWY_INLINE Vec128<T, N> Load(Simd<T, N> d, const T* HWY_RESTRICT p) {
++HWY_API Vec128<T, N> Load(Simd<T, N> d, const T* HWY_RESTRICT p) {
+   return LoadU(d, p);
+ }
+ 
+ // 128-bit SIMD => nothing to duplicate, same as an unaligned load.
+ template <typename T, size_t N, HWY_IF_LE128(T, N)>
+-HWY_INLINE Vec128<T, N> LoadDup128(Simd<T, N> d,
+-                                   const T* const HWY_RESTRICT p) {
++HWY_API Vec128<T, N> LoadDup128(Simd<T, N> d, const T* const HWY_RESTRICT p) {
+   return LoadU(d, p);
+ }
+ 
+ // ------------------------------ Store 128
+ 
+-HWY_INLINE void StoreU(const Vec128<uint8_t> v, Full128<uint8_t> /* tag */,
+-                       uint8_t* HWY_RESTRICT aligned) {
++HWY_API void StoreU(const Vec128<uint8_t> v, Full128<uint8_t> /* tag */,
++                    uint8_t* HWY_RESTRICT aligned) {
+   vst1q_u8(aligned, v.raw);
+ }
+-HWY_INLINE void StoreU(const Vec128<uint16_t> v, Full128<uint16_t> /* tag */,
+-                       uint16_t* HWY_RESTRICT aligned) {
++HWY_API void StoreU(const Vec128<uint16_t> v, Full128<uint16_t> /* tag */,
++                    uint16_t* HWY_RESTRICT aligned) {
+   vst1q_u16(aligned, v.raw);
+ }
+-HWY_INLINE void StoreU(const Vec128<uint32_t> v, Full128<uint32_t> /* tag */,
+-                       uint32_t* HWY_RESTRICT aligned) {
++HWY_API void StoreU(const Vec128<uint32_t> v, Full128<uint32_t> /* tag */,
++                    uint32_t* HWY_RESTRICT aligned) {
+   vst1q_u32(aligned, v.raw);
+ }
+-HWY_INLINE void StoreU(const Vec128<uint64_t> v, Full128<uint64_t> /* tag */,
+-                       uint64_t* HWY_RESTRICT aligned) {
++HWY_API void StoreU(const Vec128<uint64_t> v, Full128<uint64_t> /* tag */,
++                    uint64_t* HWY_RESTRICT aligned) {
+   vst1q_u64(aligned, v.raw);
+ }
+-HWY_INLINE void StoreU(const Vec128<int8_t> v, Full128<int8_t> /* tag */,
+-                       int8_t* HWY_RESTRICT aligned) {
++HWY_API void StoreU(const Vec128<int8_t> v, Full128<int8_t> /* tag */,
++                    int8_t* HWY_RESTRICT aligned) {
+   vst1q_s8(aligned, v.raw);
+ }
+-HWY_INLINE void StoreU(const Vec128<int16_t> v, Full128<int16_t> /* tag */,
+-                       int16_t* HWY_RESTRICT aligned) {
++HWY_API void StoreU(const Vec128<int16_t> v, Full128<int16_t> /* tag */,
++                    int16_t* HWY_RESTRICT aligned) {
+   vst1q_s16(aligned, v.raw);
+ }
+-HWY_INLINE void StoreU(const Vec128<int32_t> v, Full128<int32_t> /* tag */,
+-                       int32_t* HWY_RESTRICT aligned) {
++HWY_API void StoreU(const Vec128<int32_t> v, Full128<int32_t> /* tag */,
++                    int32_t* HWY_RESTRICT aligned) {
+   vst1q_s32(aligned, v.raw);
+ }
+-HWY_INLINE void StoreU(const Vec128<int64_t> v, Full128<int64_t> /* tag */,
+-                       int64_t* HWY_RESTRICT aligned) {
++HWY_API void StoreU(const Vec128<int64_t> v, Full128<int64_t> /* tag */,
++                    int64_t* HWY_RESTRICT aligned) {
+   vst1q_s64(aligned, v.raw);
+ }
+-HWY_INLINE void StoreU(const Vec128<float> v, Full128<float> /* tag */,
+-                       float* HWY_RESTRICT aligned) {
++HWY_API void StoreU(const Vec128<float> v, Full128<float> /* tag */,
++                    float* HWY_RESTRICT aligned) {
+   vst1q_f32(aligned, v.raw);
+ }
+ #if HWY_ARCH_ARM_A64
+-HWY_INLINE void StoreU(const Vec128<double> v, Full128<double> /* tag */,
+-                       double* HWY_RESTRICT aligned) {
++HWY_API void StoreU(const Vec128<double> v, Full128<double> /* tag */,
++                    double* HWY_RESTRICT aligned) {
+   vst1q_f64(aligned, v.raw);
+ }
+ #endif
+ 
+ // ------------------------------ Store 64
+ 
+-HWY_INLINE void StoreU(const Vec128<uint8_t, 8> v, Simd<uint8_t, 8> /* tag */,
+-                       uint8_t* HWY_RESTRICT p) {
++HWY_API void StoreU(const Vec128<uint8_t, 8> v, Simd<uint8_t, 8> /* tag */,
++                    uint8_t* HWY_RESTRICT p) {
+   vst1_u8(p, v.raw);
+ }
+-HWY_INLINE void StoreU(const Vec128<uint16_t, 4> v, Simd<uint16_t, 4> /* tag */,
+-                       uint16_t* HWY_RESTRICT p) {
++HWY_API void StoreU(const Vec128<uint16_t, 4> v, Simd<uint16_t, 4> /* tag */,
++                    uint16_t* HWY_RESTRICT p) {
+   vst1_u16(p, v.raw);
+ }
+-HWY_INLINE void StoreU(const Vec128<uint32_t, 2> v, Simd<uint32_t, 2> /* tag */,
+-                       uint32_t* HWY_RESTRICT p) {
++HWY_API void StoreU(const Vec128<uint32_t, 2> v, Simd<uint32_t, 2> /* tag */,
++                    uint32_t* HWY_RESTRICT p) {
+   vst1_u32(p, v.raw);
+ }
+-HWY_INLINE void StoreU(const Vec128<uint64_t, 1> v, Simd<uint64_t, 1> /* tag */,
+-                       uint64_t* HWY_RESTRICT p) {
++HWY_API void StoreU(const Vec128<uint64_t, 1> v, Simd<uint64_t, 1> /* tag */,
++                    uint64_t* HWY_RESTRICT p) {
+   vst1_u64(p, v.raw);
+ }
+-HWY_INLINE void StoreU(const Vec128<int8_t, 8> v, Simd<int8_t, 8> /* tag */,
+-                       int8_t* HWY_RESTRICT p) {
++HWY_API void StoreU(const Vec128<int8_t, 8> v, Simd<int8_t, 8> /* tag */,
++                    int8_t* HWY_RESTRICT p) {
+   vst1_s8(p, v.raw);
+ }
+-HWY_INLINE void StoreU(const Vec128<int16_t, 4> v, Simd<int16_t, 4> /* tag */,
+-                       int16_t* HWY_RESTRICT p) {
++HWY_API void StoreU(const Vec128<int16_t, 4> v, Simd<int16_t, 4> /* tag */,
++                    int16_t* HWY_RESTRICT p) {
+   vst1_s16(p, v.raw);
+ }
+-HWY_INLINE void StoreU(const Vec128<int32_t, 2> v, Simd<int32_t, 2> /* tag */,
+-                       int32_t* HWY_RESTRICT p) {
++HWY_API void StoreU(const Vec128<int32_t, 2> v, Simd<int32_t, 2> /* tag */,
++                    int32_t* HWY_RESTRICT p) {
+   vst1_s32(p, v.raw);
+ }
+-HWY_INLINE void StoreU(const Vec128<int64_t, 1> v, Simd<int64_t, 1> /* tag */,
+-                       int64_t* HWY_RESTRICT p) {
++HWY_API void StoreU(const Vec128<int64_t, 1> v, Simd<int64_t, 1> /* tag */,
++                    int64_t* HWY_RESTRICT p) {
+   vst1_s64(p, v.raw);
+ }
+-HWY_INLINE void StoreU(const Vec128<float, 2> v, Simd<float, 2> /* tag */,
+-                       float* HWY_RESTRICT p) {
++HWY_API void StoreU(const Vec128<float, 2> v, Simd<float, 2> /* tag */,
++                    float* HWY_RESTRICT p) {
+   vst1_f32(p, v.raw);
+ }
+ #if HWY_ARCH_ARM_A64
+-HWY_INLINE void StoreU(const Vec128<double, 1> v, Simd<double, 1> /* tag */,
+-                       double* HWY_RESTRICT p) {
++HWY_API void StoreU(const Vec128<double, 1> v, Simd<double, 1> /* tag */,
++                    double* HWY_RESTRICT p) {
+   vst1_f64(p, v.raw);
+ }
+ #endif
+ 
+ // ------------------------------ Store 32
+ 
+-HWY_INLINE void StoreU(const Vec128<uint8_t, 4> v, Simd<uint8_t, 4>,
+-                       uint8_t* HWY_RESTRICT p) {
++HWY_API void StoreU(const Vec128<uint8_t, 4> v, Simd<uint8_t, 4>,
++                    uint8_t* HWY_RESTRICT p) {
+   uint32x2_t a = vreinterpret_u32_u8(v.raw);
+   vst1_lane_u32(reinterpret_cast<uint32_t*>(p), a, 0);
+ }
+-HWY_INLINE void StoreU(const Vec128<uint16_t, 2> v, Simd<uint16_t, 2>,
+-                       uint16_t* HWY_RESTRICT p) {
++HWY_API void StoreU(const Vec128<uint16_t, 2> v, Simd<uint16_t, 2>,
++                    uint16_t* HWY_RESTRICT p) {
+   uint32x2_t a = vreinterpret_u32_u16(v.raw);
+   vst1_lane_u32(reinterpret_cast<uint32_t*>(p), a, 0);
+ }
+-HWY_INLINE void StoreU(const Vec128<uint32_t, 1> v, Simd<uint32_t, 1>,
+-                       uint32_t* HWY_RESTRICT p) {
++HWY_API void StoreU(const Vec128<uint32_t, 1> v, Simd<uint32_t, 1>,
++                    uint32_t* HWY_RESTRICT p) {
+   vst1_lane_u32(p, v.raw, 0);
+ }
+-HWY_INLINE void StoreU(const Vec128<int8_t, 4> v, Simd<int8_t, 4>,
+-                       int8_t* HWY_RESTRICT p) {
++HWY_API void StoreU(const Vec128<int8_t, 4> v, Simd<int8_t, 4>,
++                    int8_t* HWY_RESTRICT p) {
+   int32x2_t a = vreinterpret_s32_s8(v.raw);
+   vst1_lane_s32(reinterpret_cast<int32_t*>(p), a, 0);
+ }
+-HWY_INLINE void StoreU(const Vec128<int16_t, 2> v, Simd<int16_t, 2>,
+-                       int16_t* HWY_RESTRICT p) {
++HWY_API void StoreU(const Vec128<int16_t, 2> v, Simd<int16_t, 2>,
++                    int16_t* HWY_RESTRICT p) {
+   int32x2_t a = vreinterpret_s32_s16(v.raw);
+   vst1_lane_s32(reinterpret_cast<int32_t*>(p), a, 0);
+ }
+-HWY_INLINE void StoreU(const Vec128<int32_t, 1> v, Simd<int32_t, 1>,
+-                       int32_t* HWY_RESTRICT p) {
++HWY_API void StoreU(const Vec128<int32_t, 1> v, Simd<int32_t, 1>,
++                    int32_t* HWY_RESTRICT p) {
+   vst1_lane_s32(p, v.raw, 0);
+ }
+-HWY_INLINE void StoreU(const Vec128<float, 1> v, Simd<float, 1>,
+-                       float* HWY_RESTRICT p) {
++HWY_API void StoreU(const Vec128<float, 1> v, Simd<float, 1>,
++                    float* HWY_RESTRICT p) {
+   vst1_lane_f32(p, v.raw, 0);
+ }
+ 
+ // ------------------------------ Store 16
+ 
+-HWY_INLINE void StoreU(const Vec128<uint8_t, 2> v, Simd<uint8_t, 2>,
+-                       uint8_t* HWY_RESTRICT p) {
++HWY_API void StoreU(const Vec128<uint8_t, 2> v, Simd<uint8_t, 2>,
++                    uint8_t* HWY_RESTRICT p) {
+   uint16x4_t a = vreinterpret_u16_u8(v.raw);
+   vst1_lane_u16(reinterpret_cast<uint16_t*>(p), a, 0);
+ }
+-HWY_INLINE void StoreU(const Vec128<uint16_t, 1> v, Simd<uint16_t, 1>,
+-                       uint16_t* HWY_RESTRICT p) {
++HWY_API void StoreU(const Vec128<uint16_t, 1> v, Simd<uint16_t, 1>,
++                    uint16_t* HWY_RESTRICT p) {
+   vst1_lane_u16(p, v.raw, 0);
+ }
+-HWY_INLINE void StoreU(const Vec128<int8_t, 2> v, Simd<int8_t, 2>,
+-                       int8_t* HWY_RESTRICT p) {
++HWY_API void StoreU(const Vec128<int8_t, 2> v, Simd<int8_t, 2>,
++                    int8_t* HWY_RESTRICT p) {
+   int16x4_t a = vreinterpret_s16_s8(v.raw);
+   vst1_lane_s16(reinterpret_cast<int16_t*>(p), a, 0);
+ }
+-HWY_INLINE void StoreU(const Vec128<int16_t, 1> v, Simd<int16_t, 1>,
+-                       int16_t* HWY_RESTRICT p) {
++HWY_API void StoreU(const Vec128<int16_t, 1> v, Simd<int16_t, 1>,
++                    int16_t* HWY_RESTRICT p) {
+   vst1_lane_s16(p, v.raw, 0);
+ }
+ 
+ // ------------------------------ Store 8
+ 
+-HWY_INLINE void StoreU(const Vec128<uint8_t, 1> v, Simd<uint8_t, 1>,
+-                       uint8_t* HWY_RESTRICT p) {
++HWY_API void StoreU(const Vec128<uint8_t, 1> v, Simd<uint8_t, 1>,
++                    uint8_t* HWY_RESTRICT p) {
+   vst1_lane_u8(p, v.raw, 0);
+ }
+-HWY_INLINE void StoreU(const Vec128<int8_t, 1> v, Simd<int8_t, 1>,
+-                       int8_t* HWY_RESTRICT p) {
++HWY_API void StoreU(const Vec128<int8_t, 1> v, Simd<int8_t, 1>,
++                    int8_t* HWY_RESTRICT p) {
+   vst1_lane_s8(p, v.raw, 0);
+ }
+ 
+@@ -2224,7 +2331,7 @@ HWY_API void StoreU(Vec128<float16_t, N> v, Simd<float16_t, N> /* tag */,
+ 
+ // On ARM, Store is the same as StoreU.
+ template <typename T, size_t N>
+-HWY_INLINE void Store(Vec128<T, N> v, Simd<T, N> d, T* HWY_RESTRICT p) {
++HWY_API void Store(Vec128<T, N> v, Simd<T, N> d, T* HWY_RESTRICT p) {
+   StoreU(v, d, p);
+ }
+ 
+@@ -2233,8 +2340,8 @@ HWY_INLINE void Store(Vec128<T, N> v, Simd<T, N> d, T* HWY_RESTRICT p) {
+ // Same as aligned stores on non-x86.
+ 
+ template <typename T, size_t N>
+-HWY_INLINE void Stream(const Vec128<T, N> v, Simd<T, N> d,
+-                       T* HWY_RESTRICT aligned) {
++HWY_API void Stream(const Vec128<T, N> v, Simd<T, N> d,
++                    T* HWY_RESTRICT aligned) {
+   Store(v, d, aligned);
+ }
+ 
+@@ -2243,131 +2350,131 @@ HWY_INLINE void Stream(const Vec128<T, N> v, Simd<T, N> d,
+ // ------------------------------ Promotions (part w/ narrow lanes -> full)
+ 
+ // Unsigned: zero-extend to full vector.
+-HWY_INLINE Vec128<uint16_t> PromoteTo(Full128<uint16_t> /* tag */,
+-                                      const Vec128<uint8_t, 8> v) {
++HWY_API Vec128<uint16_t> PromoteTo(Full128<uint16_t> /* tag */,
++                                   const Vec128<uint8_t, 8> v) {
+   return Vec128<uint16_t>(vmovl_u8(v.raw));
+ }
+-HWY_INLINE Vec128<uint32_t> PromoteTo(Full128<uint32_t> /* tag */,
+-                                      const Vec128<uint8_t, 4> v) {
++HWY_API Vec128<uint32_t> PromoteTo(Full128<uint32_t> /* tag */,
++                                   const Vec128<uint8_t, 4> v) {
+   uint16x8_t a = vmovl_u8(v.raw);
+   return Vec128<uint32_t>(vmovl_u16(vget_low_u16(a)));
+ }
+-HWY_INLINE Vec128<uint32_t> PromoteTo(Full128<uint32_t> /* tag */,
+-                                      const Vec128<uint16_t, 4> v) {
++HWY_API Vec128<uint32_t> PromoteTo(Full128<uint32_t> /* tag */,
++                                   const Vec128<uint16_t, 4> v) {
+   return Vec128<uint32_t>(vmovl_u16(v.raw));
+ }
+-HWY_INLINE Vec128<uint64_t> PromoteTo(Full128<uint64_t> /* tag */,
+-                                      const Vec128<uint32_t, 2> v) {
++HWY_API Vec128<uint64_t> PromoteTo(Full128<uint64_t> /* tag */,
++                                   const Vec128<uint32_t, 2> v) {
+   return Vec128<uint64_t>(vmovl_u32(v.raw));
+ }
+-HWY_INLINE Vec128<int16_t> PromoteTo(Full128<int16_t> d,
+-                                     const Vec128<uint8_t, 8> v) {
++HWY_API Vec128<int16_t> PromoteTo(Full128<int16_t> d,
++                                  const Vec128<uint8_t, 8> v) {
+   return BitCast(d, Vec128<uint16_t>(vmovl_u8(v.raw)));
+ }
+-HWY_INLINE Vec128<int32_t> PromoteTo(Full128<int32_t> d,
+-                                     const Vec128<uint8_t, 4> v) {
++HWY_API Vec128<int32_t> PromoteTo(Full128<int32_t> d,
++                                  const Vec128<uint8_t, 4> v) {
+   uint16x8_t a = vmovl_u8(v.raw);
+   return BitCast(d, Vec128<uint32_t>(vmovl_u16(vget_low_u16(a))));
+ }
+-HWY_INLINE Vec128<int32_t> PromoteTo(Full128<int32_t> d,
+-                                     const Vec128<uint16_t, 4> v) {
++HWY_API Vec128<int32_t> PromoteTo(Full128<int32_t> d,
++                                  const Vec128<uint16_t, 4> v) {
+   return BitCast(d, Vec128<uint32_t>(vmovl_u16(v.raw)));
+ }
+ 
+ // Unsigned: zero-extend to half vector.
+ template <size_t N, HWY_IF_LE64(uint16_t, N)>
+-HWY_INLINE Vec128<uint16_t, N> PromoteTo(Simd<uint16_t, N> /* tag */,
+-                                         const Vec128<uint8_t, N> v) {
++HWY_API Vec128<uint16_t, N> PromoteTo(Simd<uint16_t, N> /* tag */,
++                                      const Vec128<uint8_t, N> v) {
+   return Vec128<uint16_t, N>(vget_low_u16(vmovl_u8(v.raw)));
+ }
+ template <size_t N, HWY_IF_LE64(uint32_t, N)>
+-HWY_INLINE Vec128<uint32_t, N> PromoteTo(Simd<uint32_t, N> /* tag */,
+-                                         const Vec128<uint8_t, N> v) {
++HWY_API Vec128<uint32_t, N> PromoteTo(Simd<uint32_t, N> /* tag */,
++                                      const Vec128<uint8_t, N> v) {
+   uint16x8_t a = vmovl_u8(v.raw);
+   return Vec128<uint32_t, N>(vget_low_u32(vmovl_u16(vget_low_u16(a))));
+ }
+ template <size_t N>
+-HWY_INLINE Vec128<uint32_t, N> PromoteTo(Simd<uint32_t, N> /* tag */,
+-                                         const Vec128<uint16_t, N> v) {
++HWY_API Vec128<uint32_t, N> PromoteTo(Simd<uint32_t, N> /* tag */,
++                                      const Vec128<uint16_t, N> v) {
+   return Vec128<uint32_t, N>(vget_low_u32(vmovl_u16(v.raw)));
+ }
+ template <size_t N, HWY_IF_LE64(uint64_t, N)>
+-HWY_INLINE Vec128<uint64_t, N> PromoteTo(Simd<uint64_t, N> /* tag */,
+-                                         const Vec128<uint32_t, N> v) {
++HWY_API Vec128<uint64_t, N> PromoteTo(Simd<uint64_t, N> /* tag */,
++                                      const Vec128<uint32_t, N> v) {
+   return Vec128<uint64_t, N>(vget_low_u64(vmovl_u32(v.raw)));
+ }
+ template <size_t N, HWY_IF_LE64(int16_t, N)>
+-HWY_INLINE Vec128<int16_t, N> PromoteTo(Simd<int16_t, N> d,
+-                                        const Vec128<uint8_t, N> v) {
++HWY_API Vec128<int16_t, N> PromoteTo(Simd<int16_t, N> d,
++                                     const Vec128<uint8_t, N> v) {
+   return BitCast(d, Vec128<uint16_t, N>(vget_low_u16(vmovl_u8(v.raw))));
+ }
+ template <size_t N, HWY_IF_LE64(int32_t, N)>
+-HWY_INLINE Vec128<int32_t, N> PromoteTo(Simd<int32_t, N> /* tag */,
+-                                        const Vec128<uint8_t, N> v) {
++HWY_API Vec128<int32_t, N> PromoteTo(Simd<int32_t, N> /* tag */,
++                                     const Vec128<uint8_t, N> v) {
+   uint16x8_t a = vmovl_u8(v.raw);
+   uint32x4_t b = vmovl_u16(vget_low_u16(a));
+   return Vec128<int32_t, N>(vget_low_s32(vreinterpretq_s32_u32(b)));
+ }
+ template <size_t N, HWY_IF_LE64(int32_t, N)>
+-HWY_INLINE Vec128<int32_t, N> PromoteTo(Simd<int32_t, N> /* tag */,
+-                                        const Vec128<uint16_t, N> v) {
++HWY_API Vec128<int32_t, N> PromoteTo(Simd<int32_t, N> /* tag */,
++                                     const Vec128<uint16_t, N> v) {
+   uint32x4_t a = vmovl_u16(v.raw);
+   return Vec128<int32_t, N>(vget_low_s32(vreinterpretq_s32_u32(a)));
+ }
+ 
+ // Signed: replicate sign bit to full vector.
+-HWY_INLINE Vec128<int16_t> PromoteTo(Full128<int16_t> /* tag */,
+-                                     const Vec128<int8_t, 8> v) {
++HWY_API Vec128<int16_t> PromoteTo(Full128<int16_t> /* tag */,
++                                  const Vec128<int8_t, 8> v) {
+   return Vec128<int16_t>(vmovl_s8(v.raw));
+ }
+-HWY_INLINE Vec128<int32_t> PromoteTo(Full128<int32_t> /* tag */,
+-                                     const Vec128<int8_t, 4> v) {
++HWY_API Vec128<int32_t> PromoteTo(Full128<int32_t> /* tag */,
++                                  const Vec128<int8_t, 4> v) {
+   int16x8_t a = vmovl_s8(v.raw);
+   return Vec128<int32_t>(vmovl_s16(vget_low_s16(a)));
+ }
+-HWY_INLINE Vec128<int32_t> PromoteTo(Full128<int32_t> /* tag */,
+-                                     const Vec128<int16_t, 4> v) {
++HWY_API Vec128<int32_t> PromoteTo(Full128<int32_t> /* tag */,
++                                  const Vec128<int16_t, 4> v) {
+   return Vec128<int32_t>(vmovl_s16(v.raw));
+ }
+-HWY_INLINE Vec128<int64_t> PromoteTo(Full128<int64_t> /* tag */,
+-                                     const Vec128<int32_t, 2> v) {
++HWY_API Vec128<int64_t> PromoteTo(Full128<int64_t> /* tag */,
++                                  const Vec128<int32_t, 2> v) {
+   return Vec128<int64_t>(vmovl_s32(v.raw));
+ }
+ 
+ // Signed: replicate sign bit to half vector.
+ template <size_t N>
+-HWY_INLINE Vec128<int16_t, N> PromoteTo(Simd<int16_t, N> /* tag */,
+-                                        const Vec128<int8_t, N> v) {
++HWY_API Vec128<int16_t, N> PromoteTo(Simd<int16_t, N> /* tag */,
++                                     const Vec128<int8_t, N> v) {
+   return Vec128<int16_t, N>(vget_low_s16(vmovl_s8(v.raw)));
+ }
+ template <size_t N>
+-HWY_INLINE Vec128<int32_t, N> PromoteTo(Simd<int32_t, N> /* tag */,
+-                                        const Vec128<int8_t, N> v) {
++HWY_API Vec128<int32_t, N> PromoteTo(Simd<int32_t, N> /* tag */,
++                                     const Vec128<int8_t, N> v) {
+   int16x8_t a = vmovl_s8(v.raw);
+   int32x4_t b = vmovl_s16(vget_low_s16(a));
+   return Vec128<int32_t, N>(vget_low_s32(b));
+ }
+ template <size_t N>
+-HWY_INLINE Vec128<int32_t, N> PromoteTo(Simd<int32_t, N> /* tag */,
+-                                        const Vec128<int16_t, N> v) {
++HWY_API Vec128<int32_t, N> PromoteTo(Simd<int32_t, N> /* tag */,
++                                     const Vec128<int16_t, N> v) {
+   return Vec128<int32_t, N>(vget_low_s32(vmovl_s16(v.raw)));
+ }
+ template <size_t N>
+-HWY_INLINE Vec128<int64_t, N> PromoteTo(Simd<int64_t, N> /* tag */,
+-                                        const Vec128<int32_t, N> v) {
++HWY_API Vec128<int64_t, N> PromoteTo(Simd<int64_t, N> /* tag */,
++                                     const Vec128<int32_t, N> v) {
+   return Vec128<int64_t, N>(vget_low_s64(vmovl_s32(v.raw)));
+ }
+ 
+ #if __ARM_FP & 2
+ 
+-HWY_INLINE Vec128<float> PromoteTo(Full128<float> /* tag */,
+-                                   const Vec128<float16_t, 4> v) {
++HWY_API Vec128<float> PromoteTo(Full128<float> /* tag */,
++                                const Vec128<float16_t, 4> v) {
+   const float32x4_t f32 = vcvt_f32_f16(vreinterpret_f16_u16(v.raw));
+   return Vec128<float>(f32);
+ }
+ template <size_t N>
+-HWY_INLINE Vec128<float, N> PromoteTo(Simd<float, N> /* tag */,
+-                                      const Vec128<float16_t, N> v) {
++HWY_API Vec128<float, N> PromoteTo(Simd<float, N> /* tag */,
++                                   const Vec128<float16_t, N> v) {
+   const float32x4_t f32 = vcvt_f32_f16(vreinterpret_f16_u16(v.raw));
+   return Vec128<float, N>(vget_low_f32(f32));
+ }
+@@ -2375,8 +2482,8 @@ HWY_INLINE Vec128<float, N> PromoteTo(Simd<float, N> /* tag */,
+ #else
+ 
+ template <size_t N>
+-HWY_INLINE Vec128<float, N> PromoteTo(Simd<float, N> /* tag */,
+-                                      const Vec128<float16_t, N> v) {
++HWY_API Vec128<float, N> PromoteTo(Simd<float, N> /* tag */,
++                                   const Vec128<float16_t, N> v) {
+   const Simd<int32_t, N> di32;
+   const Simd<uint32_t, N> du32;
+   const Simd<float, N> df32;
+@@ -2400,24 +2507,24 @@ HWY_INLINE Vec128<float, N> PromoteTo(Simd<float, N> /* tag */,
+ 
+ #if HWY_ARCH_ARM_A64
+ 
+-HWY_INLINE Vec128<double> PromoteTo(Full128<double> /* tag */,
+-                                    const Vec128<float, 2> v) {
++HWY_API Vec128<double> PromoteTo(Full128<double> /* tag */,
++                                 const Vec128<float, 2> v) {
+   return Vec128<double>(vcvt_f64_f32(v.raw));
+ }
+ 
+-HWY_INLINE Vec128<double, 1> PromoteTo(Simd<double, 1> /* tag */,
+-                                       const Vec128<float, 1> v) {
++HWY_API Vec128<double, 1> PromoteTo(Simd<double, 1> /* tag */,
++                                    const Vec128<float, 1> v) {
+   return Vec128<double, 1>(vget_low_f64(vcvt_f64_f32(v.raw)));
+ }
+ 
+-HWY_INLINE Vec128<double> PromoteTo(Full128<double> /* tag */,
+-                                    const Vec128<int32_t, 2> v) {
++HWY_API Vec128<double> PromoteTo(Full128<double> /* tag */,
++                                 const Vec128<int32_t, 2> v) {
+   const int64x2_t i64 = vmovl_s32(v.raw);
+   return Vec128<double>(vcvtq_f64_s64(i64));
+ }
+ 
+-HWY_INLINE Vec128<double, 1> PromoteTo(Simd<double, 1> /* tag */,
+-                                       const Vec128<int32_t, 1> v) {
++HWY_API Vec128<double, 1> PromoteTo(Simd<double, 1> /* tag */,
++                                    const Vec128<int32_t, 1> v) {
+   const int64x1_t i64 = vget_low_s64(vmovl_s32(v.raw));
+   return Vec128<double, 1>(vcvt_f64_s64(i64));
+ }
+@@ -2427,76 +2534,76 @@ HWY_INLINE Vec128<double, 1> PromoteTo(Simd<double, 1> /* tag */,
+ // ------------------------------ Demotions (full -> part w/ narrow lanes)
+ 
+ // From full vector to half or quarter
+-HWY_INLINE Vec128<uint16_t, 4> DemoteTo(Simd<uint16_t, 4> /* tag */,
+-                                        const Vec128<int32_t> v) {
++HWY_API Vec128<uint16_t, 4> DemoteTo(Simd<uint16_t, 4> /* tag */,
++                                     const Vec128<int32_t> v) {
+   return Vec128<uint16_t, 4>(vqmovun_s32(v.raw));
+ }
+-HWY_INLINE Vec128<int16_t, 4> DemoteTo(Simd<int16_t, 4> /* tag */,
+-                                       const Vec128<int32_t> v) {
++HWY_API Vec128<int16_t, 4> DemoteTo(Simd<int16_t, 4> /* tag */,
++                                    const Vec128<int32_t> v) {
+   return Vec128<int16_t, 4>(vqmovn_s32(v.raw));
+ }
+-HWY_INLINE Vec128<uint8_t, 4> DemoteTo(Simd<uint8_t, 4> /* tag */,
+-                                       const Vec128<int32_t> v) {
++HWY_API Vec128<uint8_t, 4> DemoteTo(Simd<uint8_t, 4> /* tag */,
++                                    const Vec128<int32_t> v) {
+   const uint16x4_t a = vqmovun_s32(v.raw);
+   return Vec128<uint8_t, 4>(vqmovn_u16(vcombine_u16(a, a)));
+ }
+-HWY_INLINE Vec128<uint8_t, 8> DemoteTo(Simd<uint8_t, 8> /* tag */,
+-                                       const Vec128<int16_t> v) {
++HWY_API Vec128<uint8_t, 8> DemoteTo(Simd<uint8_t, 8> /* tag */,
++                                    const Vec128<int16_t> v) {
+   return Vec128<uint8_t, 8>(vqmovun_s16(v.raw));
+ }
+-HWY_INLINE Vec128<int8_t, 4> DemoteTo(Simd<int8_t, 4> /* tag */,
+-                                      const Vec128<int32_t> v) {
++HWY_API Vec128<int8_t, 4> DemoteTo(Simd<int8_t, 4> /* tag */,
++                                   const Vec128<int32_t> v) {
+   const int16x4_t a = vqmovn_s32(v.raw);
+   return Vec128<int8_t, 4>(vqmovn_s16(vcombine_s16(a, a)));
+ }
+-HWY_INLINE Vec128<int8_t, 8> DemoteTo(Simd<int8_t, 8> /* tag */,
+-                                      const Vec128<int16_t> v) {
++HWY_API Vec128<int8_t, 8> DemoteTo(Simd<int8_t, 8> /* tag */,
++                                   const Vec128<int16_t> v) {
+   return Vec128<int8_t, 8>(vqmovn_s16(v.raw));
+ }
+ 
+ // From half vector to partial half
+ template <size_t N, HWY_IF_LE64(int32_t, N)>
+-HWY_INLINE Vec128<uint16_t, N> DemoteTo(Simd<uint16_t, N> /* tag */,
+-                                        const Vec128<int32_t, N> v) {
++HWY_API Vec128<uint16_t, N> DemoteTo(Simd<uint16_t, N> /* tag */,
++                                     const Vec128<int32_t, N> v) {
+   return Vec128<uint16_t, N>(vqmovun_s32(vcombine_s32(v.raw, v.raw)));
+ }
+ template <size_t N, HWY_IF_LE64(int32_t, N)>
+-HWY_INLINE Vec128<int16_t, N> DemoteTo(Simd<int16_t, N> /* tag */,
+-                                       const Vec128<int32_t, N> v) {
++HWY_API Vec128<int16_t, N> DemoteTo(Simd<int16_t, N> /* tag */,
++                                    const Vec128<int32_t, N> v) {
+   return Vec128<int16_t, N>(vqmovn_s32(vcombine_s32(v.raw, v.raw)));
+ }
+ template <size_t N, HWY_IF_LE64(int32_t, N)>
+-HWY_INLINE Vec128<uint8_t, N> DemoteTo(Simd<uint8_t, N> /* tag */,
+-                                       const Vec128<int32_t, N> v) {
++HWY_API Vec128<uint8_t, N> DemoteTo(Simd<uint8_t, N> /* tag */,
++                                    const Vec128<int32_t, N> v) {
+   const uint16x4_t a = vqmovun_s32(vcombine_s32(v.raw, v.raw));
+   return Vec128<uint8_t, N>(vqmovn_u16(vcombine_u16(a, a)));
+ }
+ template <size_t N, HWY_IF_LE64(int16_t, N)>
+-HWY_INLINE Vec128<uint8_t, N> DemoteTo(Simd<uint8_t, N> /* tag */,
+-                                       const Vec128<int16_t, N> v) {
++HWY_API Vec128<uint8_t, N> DemoteTo(Simd<uint8_t, N> /* tag */,
++                                    const Vec128<int16_t, N> v) {
+   return Vec128<uint8_t, N>(vqmovun_s16(vcombine_s16(v.raw, v.raw)));
+ }
+ template <size_t N, HWY_IF_LE64(int32_t, N)>
+-HWY_INLINE Vec128<int8_t, N> DemoteTo(Simd<int8_t, N> /* tag */,
+-                                      const Vec128<int32_t, N> v) {
++HWY_API Vec128<int8_t, N> DemoteTo(Simd<int8_t, N> /* tag */,
++                                   const Vec128<int32_t, N> v) {
+   const int16x4_t a = vqmovn_s32(vcombine_s32(v.raw, v.raw));
+   return Vec128<int8_t, N>(vqmovn_s16(vcombine_s16(a, a)));
+ }
+ template <size_t N, HWY_IF_LE64(int16_t, N)>
+-HWY_INLINE Vec128<int8_t, N> DemoteTo(Simd<int8_t, N> /* tag */,
+-                                      const Vec128<int16_t, N> v) {
++HWY_API Vec128<int8_t, N> DemoteTo(Simd<int8_t, N> /* tag */,
++                                   const Vec128<int16_t, N> v) {
+   return Vec128<int8_t, N>(vqmovn_s16(vcombine_s16(v.raw, v.raw)));
+ }
+ 
+ #if __ARM_FP & 2
+ 
+-HWY_INLINE Vec128<float16_t, 4> DemoteTo(Simd<float16_t, 4> /* tag */,
+-                                         const Vec128<float> v) {
++HWY_API Vec128<float16_t, 4> DemoteTo(Simd<float16_t, 4> /* tag */,
++                                      const Vec128<float> v) {
+   return Vec128<float16_t, 4>{vreinterpret_u16_f16(vcvt_f16_f32(v.raw))};
+ }
+ template <size_t N>
+-HWY_INLINE Vec128<float16_t, N> DemoteTo(Simd<float16_t, N> /* tag */,
+-                                         const Vec128<float, N> v) {
++HWY_API Vec128<float16_t, N> DemoteTo(Simd<float16_t, N> /* tag */,
++                                      const Vec128<float, N> v) {
+   const float16x4_t f16 = vcvt_f16_f32(vcombine_f32(v.raw, v.raw));
+   return Vec128<float16_t, N>(vreinterpret_u16_f16(f16));
+ }
+@@ -2504,8 +2611,8 @@ HWY_INLINE Vec128<float16_t, N> DemoteTo(Simd<float16_t, N> /* tag */,
+ #else
+ 
+ template <size_t N>
+-HWY_INLINE Vec128<float16_t, N> DemoteTo(Simd<float16_t, N> /* tag */,
+-                                         const Vec128<float, N> v) {
++HWY_API Vec128<float16_t, N> DemoteTo(Simd<float16_t, N> /* tag */,
++                                      const Vec128<float, N> v) {
+   const Simd<int32_t, N> di;
+   const Simd<uint32_t, N> du;
+   const Simd<uint16_t, N> du16;
+@@ -2536,22 +2643,22 @@ HWY_INLINE Vec128<float16_t, N> DemoteTo(Simd<float16_t, N> /* tag */,
+ #endif
+ #if HWY_ARCH_ARM_A64
+ 
+-HWY_INLINE Vec128<float, 2> DemoteTo(Simd<float, 2> /* tag */,
+-                                     const Vec128<double> v) {
++HWY_API Vec128<float, 2> DemoteTo(Simd<float, 2> /* tag */,
++                                  const Vec128<double> v) {
+   return Vec128<float, 2>(vcvt_f32_f64(v.raw));
+ }
+-HWY_INLINE Vec128<float, 1> DemoteTo(Simd<float, 1> /* tag */,
+-                                     const Vec128<double, 1> v) {
++HWY_API Vec128<float, 1> DemoteTo(Simd<float, 1> /* tag */,
++                                  const Vec128<double, 1> v) {
+   return Vec128<float, 1>(vcvt_f32_f64(vcombine_f64(v.raw, v.raw)));
+ }
+ 
+-HWY_INLINE Vec128<int32_t, 2> DemoteTo(Simd<int32_t, 2> /* tag */,
+-                                       const Vec128<double> v) {
++HWY_API Vec128<int32_t, 2> DemoteTo(Simd<int32_t, 2> /* tag */,
++                                    const Vec128<double> v) {
+   const int64x2_t i64 = vcvtq_s64_f64(v.raw);
+   return Vec128<int32_t, 2>(vqmovn_s64(i64));
+ }
+-HWY_INLINE Vec128<int32_t, 1> DemoteTo(Simd<int32_t, 1> /* tag */,
+-                                       const Vec128<double, 1> v) {
++HWY_API Vec128<int32_t, 1> DemoteTo(Simd<int32_t, 1> /* tag */,
++                                    const Vec128<double, 1> v) {
+   const int64x1_t i64 = vcvt_s64_f64(v.raw);
+   // There is no i64x1 -> i32x1 narrow, so expand to int64x2_t first.
+   const int64x2_t i64x2 = vcombine_s64(i64, i64);
+@@ -2579,8 +2686,8 @@ HWY_DIAGNOSTICS(push)
+ HWY_DIAGNOSTICS_OFF(disable : 4701, ignored "-Wuninitialized")
+ 
+ template <size_t N>
+-HWY_INLINE Vec128<uint8_t, N> DemoteTo(Simd<uint8_t, N> /* tag */,
+-                                       const Vec128<int32_t> v) {
++HWY_API Vec128<uint8_t, N> DemoteTo(Simd<uint8_t, N> /* tag */,
++                                    const Vec128<int32_t> v) {
+   Vec128<uint16_t, N> a = DemoteTo(Simd<uint16_t, N>(), v);
+   Vec128<uint16_t, N> b;
+   uint16x8_t c = vcombine_u16(a.raw, b.raw);
+@@ -2588,8 +2695,8 @@ HWY_INLINE Vec128<uint8_t, N> DemoteTo(Simd<uint8_t, N> /* tag */,
+ }
+ 
+ template <size_t N>
+-HWY_INLINE Vec128<int8_t, N> DemoteTo(Simd<int8_t, N> /* tag */,
+-                                      const Vec128<int32_t> v) {
++HWY_API Vec128<int8_t, N> DemoteTo(Simd<int8_t, N> /* tag */,
++                                   const Vec128<int32_t> v) {
+   Vec128<int16_t, N> a = DemoteTo(Simd<int16_t, N>(), v);
+   Vec128<int16_t, N> b;
+   int16x8_t c = vcombine_s16(a.raw, b.raw);
+@@ -2600,45 +2707,45 @@ HWY_DIAGNOSTICS(pop)
+ 
+ // ------------------------------ Convert integer <=> floating-point
+ 
+-HWY_INLINE Vec128<float> ConvertTo(Full128<float> /* tag */,
+-                                   const Vec128<int32_t> v) {
++HWY_API Vec128<float> ConvertTo(Full128<float> /* tag */,
++                                const Vec128<int32_t> v) {
+   return Vec128<float>(vcvtq_f32_s32(v.raw));
+ }
+ template <size_t N, HWY_IF_LE64(int32_t, N)>
+-HWY_INLINE Vec128<float, N> ConvertTo(Simd<float, N> /* tag */,
+-                                      const Vec128<int32_t, N> v) {
++HWY_API Vec128<float, N> ConvertTo(Simd<float, N> /* tag */,
++                                   const Vec128<int32_t, N> v) {
+   return Vec128<float, N>(vcvt_f32_s32(v.raw));
+ }
+ 
+ // Truncates (rounds toward zero).
+-HWY_INLINE Vec128<int32_t> ConvertTo(Full128<int32_t> /* tag */,
+-                                     const Vec128<float> v) {
++HWY_API Vec128<int32_t> ConvertTo(Full128<int32_t> /* tag */,
++                                  const Vec128<float> v) {
+   return Vec128<int32_t>(vcvtq_s32_f32(v.raw));
+ }
+ template <size_t N, HWY_IF_LE64(float, N)>
+-HWY_INLINE Vec128<int32_t, N> ConvertTo(Simd<int32_t, N> /* tag */,
+-                                        const Vec128<float, N> v) {
++HWY_API Vec128<int32_t, N> ConvertTo(Simd<int32_t, N> /* tag */,
++                                     const Vec128<float, N> v) {
+   return Vec128<int32_t, N>(vcvt_s32_f32(v.raw));
+ }
+ 
+ #if HWY_ARCH_ARM_A64
+ 
+-HWY_INLINE Vec128<double> ConvertTo(Full128<double> /* tag */,
+-                                    const Vec128<int64_t> v) {
++HWY_API Vec128<double> ConvertTo(Full128<double> /* tag */,
++                                 const Vec128<int64_t> v) {
+   return Vec128<double>(vcvtq_f64_s64(v.raw));
+ }
+-HWY_INLINE Vec128<double, 1> ConvertTo(Simd<double, 1> /* tag */,
+-                                       const Vec128<int64_t, 1> v) {
++HWY_API Vec128<double, 1> ConvertTo(Simd<double, 1> /* tag */,
++                                    const Vec128<int64_t, 1> v) {
+   return Vec128<double, 1>(vcvt_f64_s64(v.raw));
+ }
+ 
+ // Truncates (rounds toward zero).
+-HWY_INLINE Vec128<int64_t> ConvertTo(Full128<int64_t> /* tag */,
+-                                     const Vec128<double> v) {
++HWY_API Vec128<int64_t> ConvertTo(Full128<int64_t> /* tag */,
++                                  const Vec128<double> v) {
+   return Vec128<int64_t>(vcvtq_s64_f64(v.raw));
+ }
+-HWY_INLINE Vec128<int64_t, 1> ConvertTo(Simd<int64_t, 1> /* tag */,
+-                                        const Vec128<double, 1> v) {
++HWY_API Vec128<int64_t, 1> ConvertTo(Simd<int64_t, 1> /* tag */,
++                                     const Vec128<double, 1> v) {
+   return Vec128<int64_t, 1>(vcvt_s64_f64(v.raw));
+ }
+ 
+@@ -2672,14 +2779,14 @@ namespace detail {
+ // The original value is already the desired result if NaN or the magnitude is
+ // large (i.e. the value is already an integer).
+ template <size_t N>
+-HWY_API Mask128<float, N> UseInt(const Vec128<float, N> v) {
++HWY_INLINE Mask128<float, N> UseInt(const Vec128<float, N> v) {
+   return Abs(v) < Set(Simd<float, N>(), MantissaEnd<float>());
+ }
+ 
+ }  // namespace detail
+ 
+ template <size_t N>
+-HWY_INLINE Vec128<float, N> Trunc(const Vec128<float, N> v) {
++HWY_API Vec128<float, N> Trunc(const Vec128<float, N> v) {
+   const Simd<float, N> df;
+   const RebindToSigned<decltype(df)> di;
+ 
+@@ -2690,7 +2797,7 @@ HWY_INLINE Vec128<float, N> Trunc(const Vec128<float, N> v) {
+ }
+ 
+ template <size_t N>
+-HWY_INLINE Vec128<float, N> Round(const Vec128<float, N> v) {
++HWY_API Vec128<float, N> Round(const Vec128<float, N> v) {
+   const Simd<float, N> df;
+ 
+   // ARMv7 also lacks a native NearestInt, but we can instead rely on rounding
+@@ -2707,7 +2814,7 @@ HWY_INLINE Vec128<float, N> Round(const Vec128<float, N> v) {
+ }
+ 
+ template <size_t N>
+-HWY_INLINE Vec128<float, N> Ceil(const Vec128<float, N> v) {
++HWY_API Vec128<float, N> Ceil(const Vec128<float, N> v) {
+   const Simd<float, N> df;
+   const RebindToSigned<decltype(df)> di;
+ 
+@@ -2721,7 +2828,7 @@ HWY_INLINE Vec128<float, N> Ceil(const Vec128<float, N> v) {
+ }
+ 
+ template <size_t N>
+-HWY_INLINE Vec128<float, N> Floor(const Vec128<float, N> v) {
++HWY_API Vec128<float, N> Floor(const Vec128<float, N> v) {
+   const Simd<float, N> df;
+   const Simd<int32_t, N> di;
+ 
+@@ -2740,18 +2847,18 @@ HWY_INLINE Vec128<float, N> Floor(const Vec128<float, N> v) {
+ 
+ #if HWY_ARCH_ARM_A64
+ 
+-HWY_INLINE Vec128<int32_t> NearestInt(const Vec128<float> v) {
++HWY_API Vec128<int32_t> NearestInt(const Vec128<float> v) {
+   return Vec128<int32_t>(vcvtnq_s32_f32(v.raw));
+ }
+ template <size_t N, HWY_IF_LE64(float, N)>
+-HWY_INLINE Vec128<int32_t, N> NearestInt(const Vec128<float, N> v) {
++HWY_API Vec128<int32_t, N> NearestInt(const Vec128<float, N> v) {
+   return Vec128<int32_t, N>(vcvtn_s32_f32(v.raw));
+ }
+ 
+ #else
+ 
+ template <size_t N>
+-HWY_INLINE Vec128<int32_t, N> NearestInt(const Vec128<float, N> v) {
++HWY_API Vec128<int32_t, N> NearestInt(const Vec128<float, N> v) {
+   const Simd<int32_t, N> di;
+   return ConvertTo(di, Round(v));
+ }
+@@ -2760,104 +2867,98 @@ HWY_INLINE Vec128<int32_t, N> NearestInt(const Vec128<float, N> v) {
+ 
+ // ================================================== SWIZZLE
+ 
+-// ------------------------------ Extract half
++// ------------------------------ LowerHalf
+ 
+ // <= 64 bit: just return different type
+ template <typename T, size_t N, HWY_IF_LE64(uint8_t, N)>
+-HWY_INLINE Vec128<T, N / 2> LowerHalf(const Vec128<T, N> v) {
++HWY_API Vec128<T, N / 2> LowerHalf(const Vec128<T, N> v) {
+   return Vec128<T, N / 2>(v.raw);
+ }
+ 
+-HWY_INLINE Vec128<uint8_t, 8> LowerHalf(const Vec128<uint8_t> v) {
++HWY_API Vec128<uint8_t, 8> LowerHalf(const Vec128<uint8_t> v) {
+   return Vec128<uint8_t, 8>(vget_low_u8(v.raw));
+ }
+-HWY_INLINE Vec128<uint16_t, 4> LowerHalf(const Vec128<uint16_t> v) {
++HWY_API Vec128<uint16_t, 4> LowerHalf(const Vec128<uint16_t> v) {
+   return Vec128<uint16_t, 4>(vget_low_u16(v.raw));
+ }
+-HWY_INLINE Vec128<uint32_t, 2> LowerHalf(const Vec128<uint32_t> v) {
++HWY_API Vec128<uint32_t, 2> LowerHalf(const Vec128<uint32_t> v) {
+   return Vec128<uint32_t, 2>(vget_low_u32(v.raw));
+ }
+-HWY_INLINE Vec128<uint64_t, 1> LowerHalf(const Vec128<uint64_t> v) {
++HWY_API Vec128<uint64_t, 1> LowerHalf(const Vec128<uint64_t> v) {
+   return Vec128<uint64_t, 1>(vget_low_u64(v.raw));
+ }
+-HWY_INLINE Vec128<int8_t, 8> LowerHalf(const Vec128<int8_t> v) {
++HWY_API Vec128<int8_t, 8> LowerHalf(const Vec128<int8_t> v) {
+   return Vec128<int8_t, 8>(vget_low_s8(v.raw));
+ }
+-HWY_INLINE Vec128<int16_t, 4> LowerHalf(const Vec128<int16_t> v) {
++HWY_API Vec128<int16_t, 4> LowerHalf(const Vec128<int16_t> v) {
+   return Vec128<int16_t, 4>(vget_low_s16(v.raw));
+ }
+-HWY_INLINE Vec128<int32_t, 2> LowerHalf(const Vec128<int32_t> v) {
++HWY_API Vec128<int32_t, 2> LowerHalf(const Vec128<int32_t> v) {
+   return Vec128<int32_t, 2>(vget_low_s32(v.raw));
+ }
+-HWY_INLINE Vec128<int64_t, 1> LowerHalf(const Vec128<int64_t> v) {
++HWY_API Vec128<int64_t, 1> LowerHalf(const Vec128<int64_t> v) {
+   return Vec128<int64_t, 1>(vget_low_s64(v.raw));
+ }
+-HWY_INLINE Vec128<float, 2> LowerHalf(const Vec128<float> v) {
++HWY_API Vec128<float, 2> LowerHalf(const Vec128<float> v) {
+   return Vec128<float, 2>(vget_low_f32(v.raw));
+ }
+ #if HWY_ARCH_ARM_A64
+-HWY_INLINE Vec128<double, 1> LowerHalf(const Vec128<double> v) {
++HWY_API Vec128<double, 1> LowerHalf(const Vec128<double> v) {
+   return Vec128<double, 1>(vget_low_f64(v.raw));
+ }
+ #endif
+ 
+-HWY_INLINE Vec128<uint8_t, 8> UpperHalf(const Vec128<uint8_t> v) {
+-  return Vec128<uint8_t, 8>(vget_high_u8(v.raw));
+-}
+-HWY_INLINE Vec128<uint16_t, 4> UpperHalf(const Vec128<uint16_t> v) {
+-  return Vec128<uint16_t, 4>(vget_high_u16(v.raw));
+-}
+-HWY_INLINE Vec128<uint32_t, 2> UpperHalf(const Vec128<uint32_t> v) {
+-  return Vec128<uint32_t, 2>(vget_high_u32(v.raw));
+-}
+-HWY_INLINE Vec128<uint64_t, 1> UpperHalf(const Vec128<uint64_t> v) {
+-  return Vec128<uint64_t, 1>(vget_high_u64(v.raw));
+-}
+-HWY_INLINE Vec128<int8_t, 8> UpperHalf(const Vec128<int8_t> v) {
+-  return Vec128<int8_t, 8>(vget_high_s8(v.raw));
+-}
+-HWY_INLINE Vec128<int16_t, 4> UpperHalf(const Vec128<int16_t> v) {
+-  return Vec128<int16_t, 4>(vget_high_s16(v.raw));
+-}
+-HWY_INLINE Vec128<int32_t, 2> UpperHalf(const Vec128<int32_t> v) {
+-  return Vec128<int32_t, 2>(vget_high_s32(v.raw));
+-}
+-HWY_INLINE Vec128<int64_t, 1> UpperHalf(const Vec128<int64_t> v) {
+-  return Vec128<int64_t, 1>(vget_high_s64(v.raw));
+-}
+-HWY_INLINE Vec128<float, 2> UpperHalf(const Vec128<float> v) {
+-  return Vec128<float, 2>(vget_high_f32(v.raw));
+-}
+-#if HWY_ARCH_ARM_A64
+-HWY_INLINE Vec128<double, 1> UpperHalf(const Vec128<double> v) {
+-  return Vec128<double, 1>(vget_high_f64(v.raw));
++template <typename T, size_t N>
++HWY_API Vec128<T, N / 2> LowerHalf(Simd<T, N / 2> /* tag */, Vec128<T, N> v) {
++  return LowerHalf(v);
+ }
+-#endif
+ 
+-// ------------------------------ Extract from 2x 128-bit at constant offset
++// ------------------------------ CombineShiftRightBytes
+ 
+-// Extracts 128 bits from <hi, lo> by skipping the least-significant kBytes.
+-template <int kBytes, typename T>
+-HWY_INLINE Vec128<T> CombineShiftRightBytes(const Vec128<T> hi,
+-                                            const Vec128<T> lo) {
++// 128-bit
++template <int kBytes, typename T, class V128 = Vec128<T>>
++HWY_API V128 CombineShiftRightBytes(Full128<T> d, V128 hi, V128 lo) {
+   static_assert(0 < kBytes && kBytes < 16, "kBytes must be in [1, 15]");
+-  const Full128<uint8_t> d8;
+-  return BitCast(Full128<T>(),
+-                 Vec128<uint8_t>(vextq_u8(BitCast(d8, lo).raw,
+-                                          BitCast(d8, hi).raw, kBytes)));
++  const Repartition<uint8_t, decltype(d)> d8;
++  uint8x16_t v8 = vextq_u8(BitCast(d8, lo).raw, BitCast(d8, hi).raw, kBytes);
++  return BitCast(d, Vec128<uint8_t>(v8));
++}
++
++// 64-bit
++template <int kBytes, typename T, class V64 = Vec128<T, 8 / sizeof(T)>>
++HWY_API V64 CombineShiftRightBytes(Simd<T, 8 / sizeof(T)> d, V64 hi, V64 lo) {
++  static_assert(0 < kBytes && kBytes < 8, "kBytes must be in [1, 7]");
++  const Repartition<uint8_t, decltype(d)> d8;
++  uint8x8_t v8 = vext_u8(BitCast(d8, lo).raw, BitCast(d8, hi).raw, kBytes);
++  return BitCast(d, VFromD<decltype(d8)>(v8));
+ }
+ 
++// <= 32-bit defined after ShiftLeftBytes.
++
+ // ------------------------------ Shift vector by constant #bytes
+ 
+ namespace detail {
+ 
+-// Need to partially specialize because CombineShiftRightBytes<16> and <0> are
+-// compile errors.
++// Partially specialize because kBytes = 0 and >= size are compile errors;
++// callers replace the latter with 0xFF for easier specialization.
+ template <int kBytes>
+ struct ShiftLeftBytesT {
+-  template <class T, size_t N>
++  // Full
++  template <class T>
++  HWY_INLINE Vec128<T> operator()(const Vec128<T> v) {
++    const Full128<T> d;
++    return CombineShiftRightBytes<16 - kBytes>(d, v, Zero(d));
++  }
++
++  // Partial
++  template <class T, size_t N, HWY_IF_LE64(T, N)>
+   HWY_INLINE Vec128<T, N> operator()(const Vec128<T, N> v) {
+-    return CombineShiftRightBytes<16 - kBytes>(v, Zero(Full128<T>()));
++    // Expand to 64-bit so we only use the native EXT instruction.
++    const Simd<T, 8 / sizeof(T)> d64;
++    const auto zero64 = Zero(d64);
++    const decltype(zero64) v64(v.raw);
++    return Vec128<T, N>(
++        CombineShiftRightBytes<8 - kBytes>(d64, v64, zero64).raw);
+   }
+ };
+ template <>
+@@ -2867,12 +2968,27 @@ struct ShiftLeftBytesT<0> {
+     return v;
+   }
+ };
++template <>
++struct ShiftLeftBytesT<0xFF> {
++  template <class T, size_t N>
++  HWY_INLINE Vec128<T, N> operator()(const Vec128<T, N> /* v */) {
++    return Zero(Simd<T, N>());
++  }
++};
+ 
+ template <int kBytes>
+ struct ShiftRightBytesT {
+   template <class T, size_t N>
+-  HWY_INLINE Vec128<T, N> operator()(const Vec128<T, N> v) {
+-    return CombineShiftRightBytes<kBytes>(Zero(Full128<T>()), v);
++  HWY_INLINE Vec128<T, N> operator()(Vec128<T, N> v) {
++    const Simd<T, N> d;
++    // For < 64-bit vectors, zero undefined lanes so we shift in zeros.
++    if (N * sizeof(T) < 8) {
++      constexpr size_t kReg = N * sizeof(T) == 16 ? 16 : 8;
++      const Simd<T, kReg / sizeof(T)> dreg;
++      v = Vec128<T, N>(
++          IfThenElseZero(FirstN(dreg, N), VFromD<decltype(dreg)>(v.raw)).raw);
++    }
++    return CombineShiftRightBytes<kBytes>(d, Zero(d), v);
+   }
+ };
+ template <>
+@@ -2882,61 +2998,151 @@ struct ShiftRightBytesT<0> {
+     return v;
+   }
+ };
++template <>
++struct ShiftRightBytesT<0xFF> {
++  template <class T, size_t N>
++  HWY_INLINE Vec128<T, N> operator()(const Vec128<T, N> /* v */) {
++    return Zero(Simd<T, N>());
++  }
++};
+ 
+ }  // namespace detail
+ 
+-// 0x01..0F, kBytes = 1 => 0x02..0F00
+ template <int kBytes, typename T, size_t N>
+-HWY_INLINE Vec128<T, N> ShiftLeftBytes(const Vec128<T, N> v) {
+-  return detail::ShiftLeftBytesT<kBytes>()(v);
++HWY_API Vec128<T, N> ShiftLeftBytes(Simd<T, N> /* tag */, Vec128<T, N> v) {
++  return detail::ShiftLeftBytesT < kBytes >= N * sizeof(T) ? 0xFF
++                                                           : kBytes > ()(v);
++}
++
++template <int kBytes, typename T, size_t N>
++HWY_API Vec128<T, N> ShiftLeftBytes(const Vec128<T, N> v) {
++  return ShiftLeftBytes<kBytes>(Simd<T, N>(), v);
+ }
+ 
+ template <int kLanes, typename T, size_t N>
+-HWY_INLINE Vec128<T, N> ShiftLeftLanes(const Vec128<T, N> v) {
+-  const Simd<uint8_t, N * sizeof(T)> d8;
+-  const Simd<T, N> d;
++HWY_API Vec128<T, N> ShiftLeftLanes(Simd<T, N> d, const Vec128<T, N> v) {
++  const Repartition<uint8_t, decltype(d)> d8;
+   return BitCast(d, ShiftLeftBytes<kLanes * sizeof(T)>(BitCast(d8, v)));
+ }
+ 
++template <int kLanes, typename T, size_t N>
++HWY_API Vec128<T, N> ShiftLeftLanes(const Vec128<T, N> v) {
++  return ShiftLeftLanes<kLanes>(Simd<T, N>(), v);
++}
++
+ // 0x01..0F, kBytes = 1 => 0x0001..0E
+ template <int kBytes, typename T, size_t N>
+-HWY_INLINE Vec128<T, N> ShiftRightBytes(const Vec128<T, N> v) {
+-  return detail::ShiftRightBytesT<kBytes>()(v);
++HWY_API Vec128<T, N> ShiftRightBytes(Simd<T, N> /* tag */, Vec128<T, N> v) {
++  return detail::ShiftRightBytesT < kBytes >= N * sizeof(T) ? 0xFF
++                                                            : kBytes > ()(v);
+ }
+ 
+ template <int kLanes, typename T, size_t N>
+-HWY_INLINE Vec128<T, N> ShiftRightLanes(const Vec128<T, N> v) {
+-  const Simd<uint8_t, N * sizeof(T)> d8;
+-  const Simd<T, N> d;
++HWY_API Vec128<T, N> ShiftRightLanes(Simd<T, N> d, const Vec128<T, N> v) {
++  const Repartition<uint8_t, decltype(d)> d8;
+   return BitCast(d, ShiftRightBytes<kLanes * sizeof(T)>(BitCast(d8, v)));
+ }
+ 
++// Calls ShiftLeftBytes
++template <int kBytes, typename T, size_t N, HWY_IF_LE32(T, N)>
++HWY_API Vec128<T, N> CombineShiftRightBytes(Simd<T, N> d, Vec128<T, N> hi,
++                                            Vec128<T, N> lo) {
++  constexpr size_t kSize = N * sizeof(T);
++  static_assert(0 < kBytes && kBytes < kSize, "kBytes invalid");
++  const Repartition<uint8_t, decltype(d)> d8;
++  const Simd<uint8_t, 8> d_full8;
++  const Repartition<T, decltype(d_full8)> d_full;
++  using V64 = VFromD<decltype(d_full8)>;
++  const V64 hi64(BitCast(d8, hi).raw);
++  // Move into most-significant bytes
++  const V64 lo64 = ShiftLeftBytes<8 - kSize>(V64(BitCast(d8, lo).raw));
++  const V64 r = CombineShiftRightBytes<8 - kSize + kBytes>(d_full8, hi64, lo64);
++  // After casting to full 64-bit vector of correct type, shrink to 32-bit
++  return Vec128<T, N>(BitCast(d_full, r).raw);
++}
++
++// ------------------------------ UpperHalf (ShiftRightBytes)
++
++// Full input
++HWY_API Vec128<uint8_t, 8> UpperHalf(Simd<uint8_t, 8> /* tag */,
++                                     const Vec128<uint8_t> v) {
++  return Vec128<uint8_t, 8>(vget_high_u8(v.raw));
++}
++HWY_API Vec128<uint16_t, 4> UpperHalf(Simd<uint16_t, 4> /* tag */,
++                                      const Vec128<uint16_t> v) {
++  return Vec128<uint16_t, 4>(vget_high_u16(v.raw));
++}
++HWY_API Vec128<uint32_t, 2> UpperHalf(Simd<uint32_t, 2> /* tag */,
++                                      const Vec128<uint32_t> v) {
++  return Vec128<uint32_t, 2>(vget_high_u32(v.raw));
++}
++HWY_API Vec128<uint64_t, 1> UpperHalf(Simd<uint64_t, 1> /* tag */,
++                                      const Vec128<uint64_t> v) {
++  return Vec128<uint64_t, 1>(vget_high_u64(v.raw));
++}
++HWY_API Vec128<int8_t, 8> UpperHalf(Simd<int8_t, 8> /* tag */,
++                                    const Vec128<int8_t> v) {
++  return Vec128<int8_t, 8>(vget_high_s8(v.raw));
++}
++HWY_API Vec128<int16_t, 4> UpperHalf(Simd<int16_t, 4> /* tag */,
++                                     const Vec128<int16_t> v) {
++  return Vec128<int16_t, 4>(vget_high_s16(v.raw));
++}
++HWY_API Vec128<int32_t, 2> UpperHalf(Simd<int32_t, 2> /* tag */,
++                                     const Vec128<int32_t> v) {
++  return Vec128<int32_t, 2>(vget_high_s32(v.raw));
++}
++HWY_API Vec128<int64_t, 1> UpperHalf(Simd<int64_t, 1> /* tag */,
++                                     const Vec128<int64_t> v) {
++  return Vec128<int64_t, 1>(vget_high_s64(v.raw));
++}
++HWY_API Vec128<float, 2> UpperHalf(Simd<float, 2> /* tag */,
++                                   const Vec128<float> v) {
++  return Vec128<float, 2>(vget_high_f32(v.raw));
++}
++#if HWY_ARCH_ARM_A64
++HWY_API Vec128<double, 1> UpperHalf(Simd<double, 1> /* tag */,
++                                    const Vec128<double> v) {
++  return Vec128<double, 1>(vget_high_f64(v.raw));
++}
++#endif
++
++// Partial
++template <typename T, size_t N, HWY_IF_LE64(T, N)>
++HWY_API Vec128<T, (N + 1) / 2> UpperHalf(Half<Simd<T, N>> /* tag */,
++                                         Vec128<T, N> v) {
++  const Simd<T, N> d;
++  const auto vu = BitCast(RebindToUnsigned<decltype(d)>(), v);
++  const auto upper = BitCast(d, ShiftRightBytes<N * sizeof(T) / 2>(vu));
++  return Vec128<T, (N + 1) / 2>(upper.raw);
++}
++
+ // ------------------------------ Broadcast/splat any lane
+ 
+ #if HWY_ARCH_ARM_A64
+ // Unsigned
+ template <int kLane>
+-HWY_INLINE Vec128<uint16_t> Broadcast(const Vec128<uint16_t> v) {
++HWY_API Vec128<uint16_t> Broadcast(const Vec128<uint16_t> v) {
+   static_assert(0 <= kLane && kLane < 8, "Invalid lane");
+   return Vec128<uint16_t>(vdupq_laneq_u16(v.raw, kLane));
+ }
+ template <int kLane, size_t N, HWY_IF_LE64(uint16_t, N)>
+-HWY_INLINE Vec128<uint16_t, N> Broadcast(const Vec128<uint16_t, N> v) {
++HWY_API Vec128<uint16_t, N> Broadcast(const Vec128<uint16_t, N> v) {
+   static_assert(0 <= kLane && kLane < N, "Invalid lane");
+   return Vec128<uint16_t, N>(vdup_lane_u16(v.raw, kLane));
+ }
+ template <int kLane>
+-HWY_INLINE Vec128<uint32_t> Broadcast(const Vec128<uint32_t> v) {
++HWY_API Vec128<uint32_t> Broadcast(const Vec128<uint32_t> v) {
+   static_assert(0 <= kLane && kLane < 4, "Invalid lane");
+   return Vec128<uint32_t>(vdupq_laneq_u32(v.raw, kLane));
+ }
+ template <int kLane, size_t N, HWY_IF_LE64(uint32_t, N)>
+-HWY_INLINE Vec128<uint32_t, N> Broadcast(const Vec128<uint32_t, N> v) {
++HWY_API Vec128<uint32_t, N> Broadcast(const Vec128<uint32_t, N> v) {
+   static_assert(0 <= kLane && kLane < N, "Invalid lane");
+   return Vec128<uint32_t, N>(vdup_lane_u32(v.raw, kLane));
+ }
+ template <int kLane>
+-HWY_INLINE Vec128<uint64_t> Broadcast(const Vec128<uint64_t> v) {
++HWY_API Vec128<uint64_t> Broadcast(const Vec128<uint64_t> v) {
+   static_assert(0 <= kLane && kLane < 2, "Invalid lane");
+   return Vec128<uint64_t>(vdupq_laneq_u64(v.raw, kLane));
+ }
+@@ -2944,27 +3150,27 @@ HWY_INLINE Vec128<uint64_t> Broadcast(const Vec128<uint64_t> v) {
+ 
+ // Signed
+ template <int kLane>
+-HWY_INLINE Vec128<int16_t> Broadcast(const Vec128<int16_t> v) {
++HWY_API Vec128<int16_t> Broadcast(const Vec128<int16_t> v) {
+   static_assert(0 <= kLane && kLane < 8, "Invalid lane");
+   return Vec128<int16_t>(vdupq_laneq_s16(v.raw, kLane));
+ }
+ template <int kLane, size_t N, HWY_IF_LE64(int16_t, N)>
+-HWY_INLINE Vec128<int16_t, N> Broadcast(const Vec128<int16_t, N> v) {
++HWY_API Vec128<int16_t, N> Broadcast(const Vec128<int16_t, N> v) {
+   static_assert(0 <= kLane && kLane < N, "Invalid lane");
+   return Vec128<int16_t, N>(vdup_lane_s16(v.raw, kLane));
+ }
+ template <int kLane>
+-HWY_INLINE Vec128<int32_t> Broadcast(const Vec128<int32_t> v) {
++HWY_API Vec128<int32_t> Broadcast(const Vec128<int32_t> v) {
+   static_assert(0 <= kLane && kLane < 4, "Invalid lane");
+   return Vec128<int32_t>(vdupq_laneq_s32(v.raw, kLane));
+ }
+ template <int kLane, size_t N, HWY_IF_LE64(int32_t, N)>
+-HWY_INLINE Vec128<int32_t, N> Broadcast(const Vec128<int32_t, N> v) {
++HWY_API Vec128<int32_t, N> Broadcast(const Vec128<int32_t, N> v) {
+   static_assert(0 <= kLane && kLane < N, "Invalid lane");
+   return Vec128<int32_t, N>(vdup_lane_s32(v.raw, kLane));
+ }
+ template <int kLane>
+-HWY_INLINE Vec128<int64_t> Broadcast(const Vec128<int64_t> v) {
++HWY_API Vec128<int64_t> Broadcast(const Vec128<int64_t> v) {
+   static_assert(0 <= kLane && kLane < 2, "Invalid lane");
+   return Vec128<int64_t>(vdupq_laneq_s64(v.raw, kLane));
+ }
+@@ -2972,22 +3178,22 @@ HWY_INLINE Vec128<int64_t> Broadcast(const Vec128<int64_t> v) {
+ 
+ // Float
+ template <int kLane>
+-HWY_INLINE Vec128<float> Broadcast(const Vec128<float> v) {
++HWY_API Vec128<float> Broadcast(const Vec128<float> v) {
+   static_assert(0 <= kLane && kLane < 4, "Invalid lane");
+   return Vec128<float>(vdupq_laneq_f32(v.raw, kLane));
+ }
+ template <int kLane, size_t N, HWY_IF_LE64(float, N)>
+-HWY_INLINE Vec128<float, N> Broadcast(const Vec128<float, N> v) {
++HWY_API Vec128<float, N> Broadcast(const Vec128<float, N> v) {
+   static_assert(0 <= kLane && kLane < N, "Invalid lane");
+   return Vec128<float, N>(vdup_lane_f32(v.raw, kLane));
+ }
+ template <int kLane>
+-HWY_INLINE Vec128<double> Broadcast(const Vec128<double> v) {
++HWY_API Vec128<double> Broadcast(const Vec128<double> v) {
+   static_assert(0 <= kLane && kLane < 2, "Invalid lane");
+   return Vec128<double>(vdupq_laneq_f64(v.raw, kLane));
+ }
+ template <int kLane>
+-HWY_INLINE Vec128<double, 1> Broadcast(const Vec128<double, 1> v) {
++HWY_API Vec128<double, 1> Broadcast(const Vec128<double, 1> v) {
+   static_assert(0 <= kLane && kLane < 1, "Invalid lane");
+   return v;
+ }
+@@ -2997,27 +3203,27 @@ HWY_INLINE Vec128<double, 1> Broadcast(const Vec128<double, 1> v) {
+ 
+ // Unsigned
+ template <int kLane>
+-HWY_INLINE Vec128<uint16_t> Broadcast(const Vec128<uint16_t> v) {
++HWY_API Vec128<uint16_t> Broadcast(const Vec128<uint16_t> v) {
+   static_assert(0 <= kLane && kLane < 8, "Invalid lane");
+   return Vec128<uint16_t>(vdupq_n_u16(vgetq_lane_u16(v.raw, kLane)));
+ }
+ template <int kLane, size_t N, HWY_IF_LE64(uint16_t, N)>
+-HWY_INLINE Vec128<uint16_t, N> Broadcast(const Vec128<uint16_t, N> v) {
++HWY_API Vec128<uint16_t, N> Broadcast(const Vec128<uint16_t, N> v) {
+   static_assert(0 <= kLane && kLane < N, "Invalid lane");
+   return Vec128<uint16_t, N>(vdup_lane_u16(v.raw, kLane));
+ }
+ template <int kLane>
+-HWY_INLINE Vec128<uint32_t> Broadcast(const Vec128<uint32_t> v) {
++HWY_API Vec128<uint32_t> Broadcast(const Vec128<uint32_t> v) {
+   static_assert(0 <= kLane && kLane < 4, "Invalid lane");
+   return Vec128<uint32_t>(vdupq_n_u32(vgetq_lane_u32(v.raw, kLane)));
+ }
+ template <int kLane, size_t N, HWY_IF_LE64(uint32_t, N)>
+-HWY_INLINE Vec128<uint32_t, N> Broadcast(const Vec128<uint32_t, N> v) {
++HWY_API Vec128<uint32_t, N> Broadcast(const Vec128<uint32_t, N> v) {
+   static_assert(0 <= kLane && kLane < N, "Invalid lane");
+   return Vec128<uint32_t, N>(vdup_lane_u32(v.raw, kLane));
+ }
+ template <int kLane>
+-HWY_INLINE Vec128<uint64_t> Broadcast(const Vec128<uint64_t> v) {
++HWY_API Vec128<uint64_t> Broadcast(const Vec128<uint64_t> v) {
+   static_assert(0 <= kLane && kLane < 2, "Invalid lane");
+   return Vec128<uint64_t>(vdupq_n_u64(vgetq_lane_u64(v.raw, kLane)));
+ }
+@@ -3025,27 +3231,27 @@ HWY_INLINE Vec128<uint64_t> Broadcast(const Vec128<uint64_t> v) {
+ 
+ // Signed
+ template <int kLane>
+-HWY_INLINE Vec128<int16_t> Broadcast(const Vec128<int16_t> v) {
++HWY_API Vec128<int16_t> Broadcast(const Vec128<int16_t> v) {
+   static_assert(0 <= kLane && kLane < 8, "Invalid lane");
+   return Vec128<int16_t>(vdupq_n_s16(vgetq_lane_s16(v.raw, kLane)));
+ }
+ template <int kLane, size_t N, HWY_IF_LE64(int16_t, N)>
+-HWY_INLINE Vec128<int16_t, N> Broadcast(const Vec128<int16_t, N> v) {
++HWY_API Vec128<int16_t, N> Broadcast(const Vec128<int16_t, N> v) {
+   static_assert(0 <= kLane && kLane < N, "Invalid lane");
+   return Vec128<int16_t, N>(vdup_lane_s16(v.raw, kLane));
+ }
+ template <int kLane>
+-HWY_INLINE Vec128<int32_t> Broadcast(const Vec128<int32_t> v) {
++HWY_API Vec128<int32_t> Broadcast(const Vec128<int32_t> v) {
+   static_assert(0 <= kLane && kLane < 4, "Invalid lane");
+   return Vec128<int32_t>(vdupq_n_s32(vgetq_lane_s32(v.raw, kLane)));
+ }
+ template <int kLane, size_t N, HWY_IF_LE64(int32_t, N)>
+-HWY_INLINE Vec128<int32_t, N> Broadcast(const Vec128<int32_t, N> v) {
++HWY_API Vec128<int32_t, N> Broadcast(const Vec128<int32_t, N> v) {
+   static_assert(0 <= kLane && kLane < N, "Invalid lane");
+   return Vec128<int32_t, N>(vdup_lane_s32(v.raw, kLane));
+ }
+ template <int kLane>
+-HWY_INLINE Vec128<int64_t> Broadcast(const Vec128<int64_t> v) {
++HWY_API Vec128<int64_t> Broadcast(const Vec128<int64_t> v) {
+   static_assert(0 <= kLane && kLane < 2, "Invalid lane");
+   return Vec128<int64_t>(vdupq_n_s64(vgetq_lane_s64(v.raw, kLane)));
+ }
+@@ -3053,12 +3259,12 @@ HWY_INLINE Vec128<int64_t> Broadcast(const Vec128<int64_t> v) {
+ 
+ // Float
+ template <int kLane>
+-HWY_INLINE Vec128<float> Broadcast(const Vec128<float> v) {
++HWY_API Vec128<float> Broadcast(const Vec128<float> v) {
+   static_assert(0 <= kLane && kLane < 4, "Invalid lane");
+   return Vec128<float>(vdupq_n_f32(vgetq_lane_f32(v.raw, kLane)));
+ }
+ template <int kLane, size_t N, HWY_IF_LE64(float, N)>
+-HWY_INLINE Vec128<float, N> Broadcast(const Vec128<float, N> v) {
++HWY_API Vec128<float, N> Broadcast(const Vec128<float, N> v) {
+   static_assert(0 <= kLane && kLane < N, "Invalid lane");
+   return Vec128<float, N>(vdup_lane_f32(v.raw, kLane));
+ }
+@@ -3066,50 +3272,16 @@ HWY_INLINE Vec128<float, N> Broadcast(const Vec128<float, N> v) {
+ #endif
+ 
+ template <int kLane>
+-HWY_INLINE Vec128<uint64_t, 1> Broadcast(const Vec128<uint64_t, 1> v) {
++HWY_API Vec128<uint64_t, 1> Broadcast(const Vec128<uint64_t, 1> v) {
+   static_assert(0 <= kLane && kLane < 1, "Invalid lane");
+   return v;
+ }
+ template <int kLane>
+-HWY_INLINE Vec128<int64_t, 1> Broadcast(const Vec128<int64_t, 1> v) {
++HWY_API Vec128<int64_t, 1> Broadcast(const Vec128<int64_t, 1> v) {
+   static_assert(0 <= kLane && kLane < 1, "Invalid lane");
+   return v;
+ }
+ 
+-// ------------------------------ Shuffle bytes with variable indices
+-
+-// Returns vector of bytes[from[i]]. "from" is also interpreted as bytes, i.e.
+-// lane indices in [0, 16).
+-template <typename T>
+-HWY_API Vec128<T> TableLookupBytes(const Vec128<T> bytes,
+-                                   const Vec128<T> from) {
+-  const Full128<T> d;
+-  const Repartition<uint8_t, decltype(d)> d8;
+-#if HWY_ARCH_ARM_A64
+-  return BitCast(d, Vec128<uint8_t>(vqtbl1q_u8(BitCast(d8, bytes).raw,
+-                                               BitCast(d8, from).raw)));
+-#else
+-  uint8x16_t table0 = BitCast(d8, bytes).raw;
+-  uint8x8x2_t table;
+-  table.val[0] = vget_low_u8(table0);
+-  table.val[1] = vget_high_u8(table0);
+-  uint8x16_t idx = BitCast(d8, from).raw;
+-  uint8x8_t low = vtbl2_u8(table, vget_low_u8(idx));
+-  uint8x8_t hi = vtbl2_u8(table, vget_high_u8(idx));
+-  return BitCast(d, Vec128<uint8_t>(vcombine_u8(low, hi)));
+-#endif
+-}
+-
+-template <typename T, size_t N, typename TI, HWY_IF_LE64(T, N)>
+-HWY_INLINE Vec128<T, N> TableLookupBytes(
+-    const Vec128<T, N> bytes,
+-    const Vec128<TI, N * sizeof(T) / sizeof(TI)> from) {
+-  const Simd<T, N> d;
+-  const Repartition<uint8_t, decltype(d)> d8;
+-  return BitCast(d, decltype(Zero(d8))(vtbl1_u8(BitCast(d8, bytes).raw,
+-                                                BitCast(d8, from).raw)));
+-}
+-
+ // ------------------------------ TableLookupLanes
+ 
+ // Returned by SetTableIndices for use by TableLookupLanes.
+@@ -3119,7 +3291,7 @@ struct Indices128 {
+ };
+ 
+ template <typename T, size_t N, HWY_IF_LE128(T, N)>
+-HWY_INLINE Indices128<T, N> SetTableIndices(Simd<T, N> d, const int32_t* idx) {
++HWY_API Indices128<T, N> SetTableIndices(Simd<T, N> d, const int32_t* idx) {
+ #if !defined(NDEBUG) || defined(ADDRESS_SANITIZER)
+   for (size_t i = 0; i < N; ++i) {
+     HWY_DASSERT(0 <= idx[i] && idx[i] < static_cast<int32_t>(N));
+@@ -3138,18 +3310,18 @@ HWY_INLINE Indices128<T, N> SetTableIndices(Simd<T, N> d, const int32_t* idx) {
+ }
+ 
+ template <size_t N>
+-HWY_INLINE Vec128<uint32_t, N> TableLookupLanes(
++HWY_API Vec128<uint32_t, N> TableLookupLanes(
+     const Vec128<uint32_t, N> v, const Indices128<uint32_t, N> idx) {
+   return TableLookupBytes(v, Vec128<uint32_t, N>{idx.raw});
+ }
+ template <size_t N>
+-HWY_INLINE Vec128<int32_t, N> TableLookupLanes(
+-    const Vec128<int32_t, N> v, const Indices128<int32_t, N> idx) {
++HWY_API Vec128<int32_t, N> TableLookupLanes(const Vec128<int32_t, N> v,
++                                            const Indices128<int32_t, N> idx) {
+   return TableLookupBytes(v, Vec128<int32_t, N>{idx.raw});
+ }
+ template <size_t N>
+-HWY_INLINE Vec128<float, N> TableLookupLanes(const Vec128<float, N> v,
+-                                             const Indices128<float, N> idx) {
++HWY_API Vec128<float, N> TableLookupLanes(const Vec128<float, N> v,
++                                          const Indices128<float, N> idx) {
+   const Simd<int32_t, N> di;
+   const auto idx_i = BitCast(di, Vec128<float, N>{idx.raw});
+   return BitCast(Simd<float, N>(), TableLookupBytes(BitCast(di, v), idx_i));
+@@ -3164,42 +3336,33 @@ HWY_INLINE Vec128<float, N> TableLookupLanes(const Vec128<float, N> v,
+ 
+ // Swap 64-bit halves
+ template <typename T>
+-HWY_INLINE Vec128<T> Shuffle1032(const Vec128<T> v) {
+-  return CombineShiftRightBytes<8>(v, v);
++HWY_API Vec128<T> Shuffle1032(const Vec128<T> v) {
++  return CombineShiftRightBytes<8>(Full128<T>(), v, v);
+ }
+ template <typename T>
+-HWY_INLINE Vec128<T> Shuffle01(const Vec128<T> v) {
+-  return CombineShiftRightBytes<8>(v, v);
++HWY_API Vec128<T> Shuffle01(const Vec128<T> v) {
++  return CombineShiftRightBytes<8>(Full128<T>(), v, v);
+ }
+ 
+ // Rotate right 32 bits
+ template <typename T>
+-HWY_INLINE Vec128<T> Shuffle0321(const Vec128<T> v) {
+-  return CombineShiftRightBytes<4>(v, v);
++HWY_API Vec128<T> Shuffle0321(const Vec128<T> v) {
++  return CombineShiftRightBytes<4>(Full128<T>(), v, v);
+ }
+ 
+ // Rotate left 32 bits
+ template <typename T>
+-HWY_INLINE Vec128<T> Shuffle2103(const Vec128<T> v) {
+-  return CombineShiftRightBytes<12>(v, v);
++HWY_API Vec128<T> Shuffle2103(const Vec128<T> v) {
++  return CombineShiftRightBytes<12>(Full128<T>(), v, v);
+ }
+ 
+ // Reverse
+ template <typename T>
+-HWY_INLINE Vec128<T> Shuffle0123(const Vec128<T> v) {
+-  static_assert(sizeof(T) == 4,
+-                "Shuffle0123 should only be applied to 32-bit types");
+-  // TODO(janwas): more efficient implementation?,
+-  // It is possible to use two instructions (vrev64q_u32 and vcombine_u32 of the
+-  // high/low parts) instead of the extra memory and load.
+-  static constexpr uint8_t bytes[16] = {12, 13, 14, 15, 8, 9, 10, 11,
+-                                        4,  5,  6,  7,  0, 1, 2,  3};
+-  const Full128<uint8_t> d8;
+-  const Full128<T> d;
+-  return TableLookupBytes(v, BitCast(d, Load(d8, bytes)));
++HWY_API Vec128<T> Shuffle0123(const Vec128<T> v) {
++  return Shuffle2301(Shuffle1032(v));
+ }
+ 
+-// ------------------------------ Interleave lanes
++// ------------------------------ InterleaveLower
+ 
+ // Interleaves lanes from halves of the 128-bit blocks of "a" (which provides
+ // the least-significant lane) and "b". To concatenate two half-width integers
+@@ -3207,244 +3370,331 @@ HWY_INLINE Vec128<T> Shuffle0123(const Vec128<T> v) {
+ HWY_NEON_DEF_FUNCTION_INT_8_16_32(InterleaveLower, vzip1, _, 2)
+ HWY_NEON_DEF_FUNCTION_UINT_8_16_32(InterleaveLower, vzip1, _, 2)
+ 
+-HWY_NEON_DEF_FUNCTION_INT_8_16_32(InterleaveUpper, vzip2, _, 2)
+-HWY_NEON_DEF_FUNCTION_UINT_8_16_32(InterleaveUpper, vzip2, _, 2)
+-
+ #if HWY_ARCH_ARM_A64
+-// For 64 bit types, we only have the "q" version of the function defined as
+-// interleaving 64-wide registers with 64-wide types in them makes no sense.
+-HWY_INLINE Vec128<uint64_t> InterleaveLower(const Vec128<uint64_t> a,
+-                                            const Vec128<uint64_t> b) {
++// N=1 makes no sense (in that case, there would be no upper/lower).
++HWY_API Vec128<uint64_t> InterleaveLower(const Vec128<uint64_t> a,
++                                         const Vec128<uint64_t> b) {
+   return Vec128<uint64_t>(vzip1q_u64(a.raw, b.raw));
+ }
+-HWY_INLINE Vec128<int64_t> InterleaveLower(const Vec128<int64_t> a,
+-                                           const Vec128<int64_t> b) {
++HWY_API Vec128<int64_t> InterleaveLower(const Vec128<int64_t> a,
++                                        const Vec128<int64_t> b) {
+   return Vec128<int64_t>(vzip1q_s64(a.raw, b.raw));
+ }
+-
+-HWY_INLINE Vec128<uint64_t> InterleaveUpper(const Vec128<uint64_t> a,
+-                                            const Vec128<uint64_t> b) {
+-  return Vec128<uint64_t>(vzip2q_u64(a.raw, b.raw));
+-}
+-HWY_INLINE Vec128<int64_t> InterleaveUpper(const Vec128<int64_t> a,
+-                                           const Vec128<int64_t> b) {
+-  return Vec128<int64_t>(vzip2q_s64(a.raw, b.raw));
++HWY_API Vec128<double> InterleaveLower(const Vec128<double> a,
++                                       const Vec128<double> b) {
++  return Vec128<double>(vzip1q_f64(a.raw, b.raw));
+ }
+ #else
+ // ARMv7 emulation.
+-HWY_INLINE Vec128<uint64_t> InterleaveLower(const Vec128<uint64_t> a,
+-                                            const Vec128<uint64_t> b) {
+-  auto flip = CombineShiftRightBytes<8>(a, a);
+-  return CombineShiftRightBytes<8>(b, flip);
+-}
+-HWY_INLINE Vec128<int64_t> InterleaveLower(const Vec128<int64_t> a,
+-                                           const Vec128<int64_t> b) {
+-  auto flip = CombineShiftRightBytes<8>(a, a);
+-  return CombineShiftRightBytes<8>(b, flip);
++HWY_API Vec128<uint64_t> InterleaveLower(const Vec128<uint64_t> a,
++                                         const Vec128<uint64_t> b) {
++  return CombineShiftRightBytes<8>(Full128<uint64_t>(), b, Shuffle01(a));
+ }
+-
+-HWY_INLINE Vec128<uint64_t> InterleaveUpper(const Vec128<uint64_t> a,
+-                                            const Vec128<uint64_t> b) {
+-  auto flip = CombineShiftRightBytes<8>(b, b);
+-  return CombineShiftRightBytes<8>(flip, a);
+-}
+-HWY_INLINE Vec128<int64_t> InterleaveUpper(const Vec128<int64_t> a,
+-                                           const Vec128<int64_t> b) {
+-  auto flip = CombineShiftRightBytes<8>(b, b);
+-  return CombineShiftRightBytes<8>(flip, a);
++HWY_API Vec128<int64_t> InterleaveLower(const Vec128<int64_t> a,
++                                        const Vec128<int64_t> b) {
++  return CombineShiftRightBytes<8>(Full128<int64_t>(), b, Shuffle01(a));
+ }
+ #endif
+ 
+ // Floats
+-HWY_INLINE Vec128<float> InterleaveLower(const Vec128<float> a,
+-                                         const Vec128<float> b) {
++HWY_API Vec128<float> InterleaveLower(const Vec128<float> a,
++                                      const Vec128<float> b) {
+   return Vec128<float>(vzip1q_f32(a.raw, b.raw));
+ }
+-#if HWY_ARCH_ARM_A64
+-HWY_INLINE Vec128<double> InterleaveLower(const Vec128<double> a,
+-                                          const Vec128<double> b) {
+-  return Vec128<double>(vzip1q_f64(a.raw, b.raw));
++template <size_t N, HWY_IF_LE64(float, N)>
++HWY_API Vec128<float, N> InterleaveLower(const Vec128<float, N> a,
++                                         const Vec128<float, N> b) {
++  return Vec128<float, N>(vzip1_f32(a.raw, b.raw));
+ }
+-#endif
+ 
+-HWY_INLINE Vec128<float> InterleaveUpper(const Vec128<float> a,
+-                                         const Vec128<float> b) {
+-  return Vec128<float>(vzip2q_f32(a.raw, b.raw));
++// < 64 bit parts
++template <typename T, size_t N, HWY_IF_LE32(T, N)>
++HWY_API Vec128<T, N> InterleaveLower(Vec128<T, N> a, Vec128<T, N> b) {
++  using V64 = Vec128<T, 8 / sizeof(T)>;
++  return Vec128<T, N>(InterleaveLower(V64(a.raw), V64(b.raw)).raw);
+ }
++
++// Additional overload for the optional Simd<> tag.
++template <typename T, size_t N, class V = Vec128<T, N>>
++HWY_API V InterleaveLower(Simd<T, N> /* tag */, V a, V b) {
++  return InterleaveLower(a, b);
++}
++
++// ------------------------------ InterleaveUpper (UpperHalf)
++
++// All functions inside detail lack the required D parameter.
++namespace detail {
++HWY_NEON_DEF_FUNCTION_INT_8_16_32(InterleaveUpper, vzip2, _, 2)
++HWY_NEON_DEF_FUNCTION_UINT_8_16_32(InterleaveUpper, vzip2, _, 2)
++
+ #if HWY_ARCH_ARM_A64
+-HWY_INLINE Vec128<double> InterleaveUpper(const Vec128<double> a,
+-                                          const Vec128<double> b) {
++// N=1 makes no sense (in that case, there would be no upper/lower).
++HWY_API Vec128<uint64_t> InterleaveUpper(const Vec128<uint64_t> a,
++                                         const Vec128<uint64_t> b) {
++  return Vec128<uint64_t>(vzip2q_u64(a.raw, b.raw));
++}
++HWY_API Vec128<int64_t> InterleaveUpper(Vec128<int64_t> a, Vec128<int64_t> b) {
++  return Vec128<int64_t>(vzip2q_s64(a.raw, b.raw));
++}
++HWY_API Vec128<double> InterleaveUpper(Vec128<double> a, Vec128<double> b) {
+   return Vec128<double>(vzip2q_f64(a.raw, b.raw));
+ }
++#else
++// ARMv7 emulation.
++HWY_API Vec128<uint64_t> InterleaveUpper(const Vec128<uint64_t> a,
++                                         const Vec128<uint64_t> b) {
++  return CombineShiftRightBytes<8>(Full128<uint64_t>(), Shuffle01(b), a);
++}
++HWY_API Vec128<int64_t> InterleaveUpper(Vec128<int64_t> a, Vec128<int64_t> b) {
++  return CombineShiftRightBytes<8>(Full128<int64_t>(), Shuffle01(b), a);
++}
+ #endif
+ 
+-// ------------------------------ Zip lanes
++HWY_API Vec128<float> InterleaveUpper(Vec128<float> a, Vec128<float> b) {
++  return Vec128<float>(vzip2q_f32(a.raw, b.raw));
++}
++HWY_API Vec128<float, 2> InterleaveUpper(const Vec128<float, 2> a,
++                                         const Vec128<float, 2> b) {
++  return Vec128<float, 2>(vzip2_f32(a.raw, b.raw));
++}
++
++}  // namespace detail
++
++// Full register
++template <typename T, size_t N, HWY_IF_GE64(T, N), class V = Vec128<T, N>>
++HWY_API V InterleaveUpper(Simd<T, N> /* tag */, V a, V b) {
++  return detail::InterleaveUpper(a, b);
++}
++
++// Partial
++template <typename T, size_t N, HWY_IF_LE32(T, N), class V = Vec128<T, N>>
++HWY_API V InterleaveUpper(Simd<T, N> d, V a, V b) {
++  const Half<decltype(d)> d2;
++  return InterleaveLower(d, V(UpperHalf(d2, a).raw), V(UpperHalf(d2, b).raw));
++}
++
++// ------------------------------ ZipLower/ZipUpper (InterleaveLower)
+ 
+ // Same as Interleave*, except that the return lanes are double-width integers;
+ // this is necessary because the single-lane scalar cannot return two values.
+-
+-// Full vectors
+-HWY_INLINE Vec128<uint16_t> ZipLower(const Vec128<uint8_t> a,
+-                                     const Vec128<uint8_t> b) {
+-  return Vec128<uint16_t>(vreinterpretq_u16_u8(vzip1q_u8(a.raw, b.raw)));
++template <typename T, size_t N, class DW = RepartitionToWide<Simd<T, N>>>
++HWY_API VFromD<DW> ZipLower(Vec128<T, N> a, Vec128<T, N> b) {
++  return BitCast(DW(), InterleaveLower(a, b));
+ }
+-HWY_INLINE Vec128<uint32_t> ZipLower(const Vec128<uint16_t> a,
+-                                     const Vec128<uint16_t> b) {
+-  return Vec128<uint32_t>(vreinterpretq_u32_u16(vzip1q_u16(a.raw, b.raw)));
++template <typename T, size_t N, class D = Simd<T, N>,
++          class DW = RepartitionToWide<D>>
++HWY_API VFromD<DW> ZipLower(DW dw, Vec128<T, N> a, Vec128<T, N> b) {
++  return BitCast(dw, InterleaveLower(D(), a, b));
+ }
+-HWY_INLINE Vec128<uint64_t> ZipLower(const Vec128<uint32_t> a,
+-                                     const Vec128<uint32_t> b) {
+-  return Vec128<uint64_t>(vreinterpretq_u64_u32(vzip1q_u32(a.raw, b.raw)));
++
++template <typename T, size_t N, class D = Simd<T, N>,
++          class DW = RepartitionToWide<D>>
++HWY_API VFromD<DW> ZipUpper(DW dw, Vec128<T, N> a, Vec128<T, N> b) {
++  return BitCast(dw, InterleaveUpper(D(), a, b));
+ }
+ 
+-HWY_INLINE Vec128<int16_t> ZipLower(const Vec128<int8_t> a,
+-                                    const Vec128<int8_t> b) {
+-  return Vec128<int16_t>(vreinterpretq_s16_s8(vzip1q_s8(a.raw, b.raw)));
++// ================================================== COMBINE
++
++// ------------------------------ Combine (InterleaveLower)
++
++// Full result
++HWY_API Vec128<uint8_t> Combine(Full128<uint8_t> /* tag */,
++                                Vec128<uint8_t, 8> hi, Vec128<uint8_t, 8> lo) {
++  return Vec128<uint8_t>(vcombine_u8(lo.raw, hi.raw));
+ }
+-HWY_INLINE Vec128<int32_t> ZipLower(const Vec128<int16_t> a,
+-                                    const Vec128<int16_t> b) {
+-  return Vec128<int32_t>(vreinterpretq_s32_s16(vzip1q_s16(a.raw, b.raw)));
++HWY_API Vec128<uint16_t> Combine(Full128<uint16_t> /* tag */,
++                                 Vec128<uint16_t, 4> hi,
++                                 Vec128<uint16_t, 4> lo) {
++  return Vec128<uint16_t>(vcombine_u16(lo.raw, hi.raw));
+ }
+-HWY_INLINE Vec128<int64_t> ZipLower(const Vec128<int32_t> a,
+-                                    const Vec128<int32_t> b) {
+-  return Vec128<int64_t>(vreinterpretq_s64_s32(vzip1q_s32(a.raw, b.raw)));
++HWY_API Vec128<uint32_t> Combine(Full128<uint32_t> /* tag */,
++                                 Vec128<uint32_t, 2> hi,
++                                 Vec128<uint32_t, 2> lo) {
++  return Vec128<uint32_t>(vcombine_u32(lo.raw, hi.raw));
++}
++HWY_API Vec128<uint64_t> Combine(Full128<uint64_t> /* tag */,
++                                 Vec128<uint64_t, 1> hi,
++                                 Vec128<uint64_t, 1> lo) {
++  return Vec128<uint64_t>(vcombine_u64(lo.raw, hi.raw));
+ }
+ 
+-HWY_INLINE Vec128<uint16_t> ZipUpper(const Vec128<uint8_t> a,
+-                                     const Vec128<uint8_t> b) {
+-  return Vec128<uint16_t>(vreinterpretq_u16_u8(vzip2q_u8(a.raw, b.raw)));
++HWY_API Vec128<int8_t> Combine(Full128<int8_t> /* tag */, Vec128<int8_t, 8> hi,
++                               Vec128<int8_t, 8> lo) {
++  return Vec128<int8_t>(vcombine_s8(lo.raw, hi.raw));
+ }
+-HWY_INLINE Vec128<uint32_t> ZipUpper(const Vec128<uint16_t> a,
+-                                     const Vec128<uint16_t> b) {
+-  return Vec128<uint32_t>(vreinterpretq_u32_u16(vzip2q_u16(a.raw, b.raw)));
++HWY_API Vec128<int16_t> Combine(Full128<int16_t> /* tag */,
++                                Vec128<int16_t, 4> hi, Vec128<int16_t, 4> lo) {
++  return Vec128<int16_t>(vcombine_s16(lo.raw, hi.raw));
+ }
+-HWY_INLINE Vec128<uint64_t> ZipUpper(const Vec128<uint32_t> a,
+-                                     const Vec128<uint32_t> b) {
+-  return Vec128<uint64_t>(vreinterpretq_u64_u32(vzip2q_u32(a.raw, b.raw)));
++HWY_API Vec128<int32_t> Combine(Full128<int32_t> /* tag */,
++                                Vec128<int32_t, 2> hi, Vec128<int32_t, 2> lo) {
++  return Vec128<int32_t>(vcombine_s32(lo.raw, hi.raw));
+ }
+-
+-HWY_INLINE Vec128<int16_t> ZipUpper(const Vec128<int8_t> a,
+-                                    const Vec128<int8_t> b) {
+-  return Vec128<int16_t>(vreinterpretq_s16_s8(vzip2q_s8(a.raw, b.raw)));
++HWY_API Vec128<int64_t> Combine(Full128<int64_t> /* tag */,
++                                Vec128<int64_t, 1> hi, Vec128<int64_t, 1> lo) {
++  return Vec128<int64_t>(vcombine_s64(lo.raw, hi.raw));
+ }
+-HWY_INLINE Vec128<int32_t> ZipUpper(const Vec128<int16_t> a,
+-                                    const Vec128<int16_t> b) {
+-  return Vec128<int32_t>(vreinterpretq_s32_s16(vzip2q_s16(a.raw, b.raw)));
++
++HWY_API Vec128<float> Combine(Full128<float> /* tag */, Vec128<float, 2> hi,
++                              Vec128<float, 2> lo) {
++  return Vec128<float>(vcombine_f32(lo.raw, hi.raw));
+ }
+-HWY_INLINE Vec128<int64_t> ZipUpper(const Vec128<int32_t> a,
+-                                    const Vec128<int32_t> b) {
+-  return Vec128<int64_t>(vreinterpretq_s64_s32(vzip2q_s32(a.raw, b.raw)));
++#if HWY_ARCH_ARM_A64
++HWY_API Vec128<double> Combine(Full128<double> /* tag */, Vec128<double, 1> hi,
++                               Vec128<double, 1> lo) {
++  return Vec128<double>(vcombine_f64(lo.raw, hi.raw));
+ }
++#endif
+ 
+-// Half vectors or less
+-template <size_t N, HWY_IF_LE64(uint8_t, N)>
+-HWY_INLINE Vec128<uint16_t, (N + 1) / 2> ZipLower(const Vec128<uint8_t, N> a,
+-                                                  const Vec128<uint8_t, N> b) {
+-  return Vec128<uint16_t, (N + 1) / 2>(
+-      vreinterpret_u16_u8(vzip1_u8(a.raw, b.raw)));
+-}
+-template <size_t N, HWY_IF_LE64(uint16_t, N)>
+-HWY_INLINE Vec128<uint32_t, (N + 1) / 2> ZipLower(const Vec128<uint16_t, N> a,
+-                                                  const Vec128<uint16_t, N> b) {
+-  return Vec128<uint32_t, (N + 1) / 2>(
+-      vreinterpret_u32_u16(vzip1_u16(a.raw, b.raw)));
++// < 64bit input, <= 64 bit result
++template <typename T, size_t N, HWY_IF_LE64(T, N)>
++HWY_API Vec128<T, N> Combine(Simd<T, N> d, Vec128<T, N / 2> hi,
++                             Vec128<T, N / 2> lo) {
++  // First double N (only lower halves will be used).
++  const Vec128<T, N> hi2(hi.raw);
++  const Vec128<T, N> lo2(lo.raw);
++  // Repartition to two unsigned lanes (each the size of the valid input).
++  const Simd<UnsignedFromSize<N * sizeof(T) / 2>, 2> du;
++  return BitCast(d, InterleaveLower(BitCast(du, lo2), BitCast(du, hi2)));
+ }
+-template <size_t N, HWY_IF_LE64(uint32_t, N)>
+-HWY_INLINE Vec128<uint64_t, (N + 1) / 2> ZipLower(const Vec128<uint32_t, N> a,
+-                                                  const Vec128<uint32_t, N> b) {
+-  return Vec128<uint64_t, (N + 1) / 2>(
+-      vreinterpret_u64_u32(vzip1_u32(a.raw, b.raw)));
++
++// ------------------------------ ZeroExtendVector (Combine)
++
++template <typename T, size_t N>
++HWY_API Vec128<T, N> ZeroExtendVector(Simd<T, N> d, Vec128<T, N / 2> lo) {
++  return Combine(d, Zero(Half<decltype(d)>()), lo);
+ }
+ 
+-template <size_t N, HWY_IF_LE64(int8_t, N)>
+-HWY_INLINE Vec128<int16_t, (N + 1) / 2> ZipLower(const Vec128<int8_t, N> a,
+-                                                 const Vec128<int8_t, N> b) {
+-  return Vec128<int16_t, (N + 1) / 2>(
+-      vreinterpret_s16_s8(vzip1_s8(a.raw, b.raw)));
++// ------------------------------ ConcatLowerLower
++
++// 64 or 128-bit input: just interleave
++template <typename T, size_t N, HWY_IF_GE64(T, N)>
++HWY_API Vec128<T, N> ConcatLowerLower(const Simd<T, N> d, Vec128<T, N> hi,
++                                      Vec128<T, N> lo) {
++  // Treat half-width input as a single lane and interleave them.
++  const Repartition<UnsignedFromSize<N * sizeof(T) / 2>, decltype(d)> du;
++  return BitCast(d, InterleaveLower(BitCast(du, lo), BitCast(du, hi)));
+ }
+-template <size_t N, HWY_IF_LE64(int16_t, N)>
+-HWY_INLINE Vec128<int32_t, (N + 1) / 2> ZipLower(const Vec128<int16_t, N> a,
+-                                                 const Vec128<int16_t, N> b) {
+-  return Vec128<int32_t, (N + 1) / 2>(
+-      vreinterpret_s32_s16(vzip1_s16(a.raw, b.raw)));
++
++#if HWY_ARCH_ARM_A64
++namespace detail {
++
++HWY_INLINE Vec128<uint8_t, 2> ConcatEven(Vec128<uint8_t, 2> hi,
++                                         Vec128<uint8_t, 2> lo) {
++  return Vec128<uint8_t, 2>(vtrn1_u8(lo.raw, hi.raw));
+ }
+-template <size_t N, HWY_IF_LE64(int32_t, N)>
+-HWY_INLINE Vec128<int64_t, (N + 1) / 2> ZipLower(const Vec128<int32_t, N> a,
+-                                                 const Vec128<int32_t, N> b) {
+-  return Vec128<int64_t, (N + 1) / 2>(
+-      vreinterpret_s64_s32(vzip1_s32(a.raw, b.raw)));
++HWY_INLINE Vec128<uint16_t, 2> ConcatEven(Vec128<uint16_t, 2> hi,
++                                          Vec128<uint16_t, 2> lo) {
++  return Vec128<uint16_t, 2>(vtrn1_u16(lo.raw, hi.raw));
+ }
+ 
+-template <size_t N, HWY_IF_LE64(uint8_t, N)>
+-HWY_INLINE Vec128<uint16_t, N / 2> ZipUpper(const Vec128<uint8_t, N> a,
+-                                            const Vec128<uint8_t, N> b) {
+-  return Vec128<uint16_t, N / 2>(vreinterpret_u16_u8(vzip2_u8(a.raw, b.raw)));
++}  // namespace detail
++
++// <= 32-bit input/output
++template <typename T, size_t N, HWY_IF_LE32(T, N)>
++HWY_API Vec128<T, N> ConcatLowerLower(const Simd<T, N> d, Vec128<T, N> hi,
++                                      Vec128<T, N> lo) {
++  // Treat half-width input as two lanes and take every second one.
++  const Repartition<UnsignedFromSize<N * sizeof(T) / 2>, decltype(d)> du;
++  return BitCast(d, detail::ConcatEven(BitCast(du, hi), BitCast(du, lo)));
+ }
+-template <size_t N, HWY_IF_LE64(uint16_t, N)>
+-HWY_INLINE Vec128<uint32_t, N / 2> ZipUpper(const Vec128<uint16_t, N> a,
+-                                            const Vec128<uint16_t, N> b) {
+-  return Vec128<uint32_t, N / 2>(vreinterpret_u32_u16(vzip2_u16(a.raw, b.raw)));
++
++#else
++
++template <typename T, size_t N, HWY_IF_LE32(T, N)>
++HWY_API Vec128<T, N> ConcatLowerLower(const Simd<T, N> d, Vec128<T, N> hi,
++                                      Vec128<T, N> lo) {
++  const Half<decltype(d)> d2;
++  return Combine(LowerHalf(d2, hi), LowerHalf(d2, lo));
+ }
+-template <size_t N, HWY_IF_LE64(uint32_t, N)>
+-HWY_INLINE Vec128<uint64_t, N / 2> ZipUpper(const Vec128<uint32_t, N> a,
+-                                            const Vec128<uint32_t, N> b) {
+-  return Vec128<uint64_t, N / 2>(vreinterpret_u64_u32(vzip2_u32(a.raw, b.raw)));
++#endif  // HWY_ARCH_ARM_A64
++
++// ------------------------------ ConcatUpperUpper
++
++// 64 or 128-bit input: just interleave
++template <typename T, size_t N, HWY_IF_GE64(T, N)>
++HWY_API Vec128<T, N> ConcatUpperUpper(const Simd<T, N> d, Vec128<T, N> hi,
++                                      Vec128<T, N> lo) {
++  // Treat half-width input as a single lane and interleave them.
++  const Repartition<UnsignedFromSize<N * sizeof(T) / 2>, decltype(d)> du;
++  return BitCast(d, InterleaveUpper(du, BitCast(du, lo), BitCast(du, hi)));
+ }
+ 
+-template <size_t N, HWY_IF_LE64(int8_t, N)>
+-HWY_INLINE Vec128<int16_t, N / 2> ZipUpper(const Vec128<int8_t, N> a,
+-                                           const Vec128<int8_t, N> b) {
+-  return Vec128<int16_t, N / 2>(vreinterpret_s16_s8(vzip2_s8(a.raw, b.raw)));
++#if HWY_ARCH_ARM_A64
++namespace detail {
++
++HWY_INLINE Vec128<uint8_t, 2> ConcatOdd(Vec128<uint8_t, 2> hi,
++                                        Vec128<uint8_t, 2> lo) {
++  return Vec128<uint8_t, 2>(vtrn2_u8(lo.raw, hi.raw));
+ }
+-template <size_t N, HWY_IF_LE64(int16_t, N)>
+-HWY_INLINE Vec128<int32_t, N / 2> ZipUpper(const Vec128<int16_t, N> a,
+-                                           const Vec128<int16_t, N> b) {
+-  return Vec128<int32_t, N / 2>(vreinterpret_s32_s16(vzip2_s16(a.raw, b.raw)));
++HWY_INLINE Vec128<uint16_t, 2> ConcatOdd(Vec128<uint16_t, 2> hi,
++                                         Vec128<uint16_t, 2> lo) {
++  return Vec128<uint16_t, 2>(vtrn2_u16(lo.raw, hi.raw));
+ }
+-template <size_t N, HWY_IF_LE64(int32_t, N)>
+-HWY_INLINE Vec128<int64_t, N / 2> ZipUpper(const Vec128<int32_t, N> a,
+-                                           const Vec128<int32_t, N> b) {
+-  return Vec128<int64_t, N / 2>(vreinterpret_s64_s32(vzip2_s32(a.raw, b.raw)));
++
++}  // namespace detail
++
++// <= 32-bit input/output
++template <typename T, size_t N, HWY_IF_LE32(T, N)>
++HWY_API Vec128<T, N> ConcatUpperUpper(const Simd<T, N> d, Vec128<T, N> hi,
++                                      Vec128<T, N> lo) {
++  // Treat half-width input as two lanes and take every second one.
++  const Repartition<UnsignedFromSize<N * sizeof(T) / 2>, decltype(d)> du;
++  return BitCast(d, detail::ConcatOdd(BitCast(du, hi), BitCast(du, lo)));
+ }
+ 
+-// ------------------------------ Blocks
++#else
+ 
+-// hiH,hiL loH,loL |-> hiL,loL (= lower halves)
+-template <typename T>
+-HWY_INLINE Vec128<T> ConcatLowerLower(const Vec128<T> hi, const Vec128<T> lo) {
+-  const Full128<uint64_t> d64;
+-  return BitCast(Full128<T>(),
+-                 InterleaveLower(BitCast(d64, lo), BitCast(d64, hi)));
++template <typename T, size_t N, HWY_IF_LE32(T, N)>
++HWY_API Vec128<T, N> ConcatUpperUpper(const Simd<T, N> d, Vec128<T, N> hi,
++                                      Vec128<T, N> lo) {
++  const Half<decltype(d)> d2;
++  return Combine(UpperHalf(d2, hi), UpperHalf(d2, lo));
+ }
+ 
+-// hiH,hiL loH,loL |-> hiH,loH (= upper halves)
+-template <typename T>
+-HWY_INLINE Vec128<T> ConcatUpperUpper(const Vec128<T> hi, const Vec128<T> lo) {
+-  const Full128<uint64_t> d64;
+-  return BitCast(Full128<T>(),
+-                 InterleaveUpper(BitCast(d64, lo), BitCast(d64, hi)));
++#endif  // HWY_ARCH_ARM_A64
++
++// ------------------------------ ConcatLowerUpper (ShiftLeftBytes)
++
++// 64 or 128-bit input: extract from concatenated
++template <typename T, size_t N, HWY_IF_GE64(T, N)>
++HWY_API Vec128<T, N> ConcatLowerUpper(const Simd<T, N> d, Vec128<T, N> hi,
++                                      Vec128<T, N> lo) {
++  return CombineShiftRightBytes<N * sizeof(T) / 2>(d, hi, lo);
+ }
+ 
+-// hiH,hiL loH,loL |-> hiL,loH (= inner halves)
+-template <typename T>
+-HWY_INLINE Vec128<T> ConcatLowerUpper(const Vec128<T> hi, const Vec128<T> lo) {
+-  return CombineShiftRightBytes<8>(hi, lo);
++// <= 32-bit input/output
++template <typename T, size_t N, HWY_IF_LE32(T, N)>
++HWY_API Vec128<T, N> ConcatLowerUpper(const Simd<T, N> d, Vec128<T, N> hi,
++                                      Vec128<T, N> lo) {
++  constexpr size_t kSize = N * sizeof(T);
++  const Repartition<uint8_t, decltype(d)> d8;
++  const Simd<uint8_t, 8> d8x8;
++  const Simd<T, 8 / sizeof(T)> d64;
++  using V8x8 = VFromD<decltype(d8x8)>;
++  const V8x8 hi8x8(BitCast(d8, hi).raw);
++  // Move into most-significant bytes
++  const V8x8 lo8x8 = ShiftLeftBytes<8 - kSize>(V8x8(BitCast(d8, lo).raw));
++  const V8x8 r = CombineShiftRightBytes<8 - kSize / 2>(d8x8, hi8x8, lo8x8);
++  // Back to original lane type, then shrink N.
++  return Vec128<T, N>(BitCast(d64, r).raw);
+ }
+ 
+-// hiH,hiL loH,loL |-> hiH,loL (= outer halves)
+-template <typename T>
+-HWY_INLINE Vec128<T> ConcatUpperLower(const Vec128<T> hi, const Vec128<T> lo) {
+-  // TODO(janwas): more efficient implementation?
+-  alignas(16) const uint8_t kBytes[16] = {
+-      0xFF, 0xFF, 0xFF, 0xFF, 0xFF, 0xFF, 0xFF, 0xFF, 0, 0, 0, 0, 0, 0, 0, 0};
+-  const auto vec = BitCast(Full128<T>(), Load(Full128<uint8_t>(), kBytes));
+-  return IfThenElse(MaskFromVec(vec), lo, hi);
++// ------------------------------ ConcatUpperLower
++
++// Works for all N.
++template <typename T, size_t N>
++HWY_API Vec128<T, N> ConcatUpperLower(Simd<T, N> d, Vec128<T, N> hi,
++                                      Vec128<T, N> lo) {
++  return IfThenElse(FirstN(d, Lanes(d) / 2), lo, hi);
+ }
+ 
+-// ------------------------------ Odd/even lanes
++// ------------------------------ OddEven (IfThenElse)
+ 
+-template <typename T>
+-HWY_INLINE Vec128<T> OddEven(const Vec128<T> a, const Vec128<T> b) {
++template <typename T, size_t N>
++HWY_API Vec128<T, N> OddEven(const Vec128<T, N> a, const Vec128<T, N> b) {
++  const Simd<T, N> d;
++  const Repartition<uint8_t, decltype(d)> d8;
+   alignas(16) constexpr uint8_t kBytes[16] = {
+       ((0 / sizeof(T)) & 1) ? 0 : 0xFF,  ((1 / sizeof(T)) & 1) ? 0 : 0xFF,
+       ((2 / sizeof(T)) & 1) ? 0 : 0xFF,  ((3 / sizeof(T)) & 1) ? 0 : 0xFF,
+@@ -3455,12 +3705,107 @@ HWY_INLINE Vec128<T> OddEven(const Vec128<T> a, const Vec128<T> b) {
+       ((12 / sizeof(T)) & 1) ? 0 : 0xFF, ((13 / sizeof(T)) & 1) ? 0 : 0xFF,
+       ((14 / sizeof(T)) & 1) ? 0 : 0xFF, ((15 / sizeof(T)) & 1) ? 0 : 0xFF,
+   };
+-  const auto vec = BitCast(Full128<T>(), Load(Full128<uint8_t>(), kBytes));
++  const auto vec = BitCast(d, Load(d8, kBytes));
+   return IfThenElse(MaskFromVec(vec), b, a);
+ }
+ 
++// ================================================== CRYPTO
++
++#if defined(__ARM_FEATURE_AES)
++
++// Per-target flag to prevent generic_ops-inl.h from defining AESRound.
++#ifdef HWY_NATIVE_AES
++#undef HWY_NATIVE_AES
++#else
++#define HWY_NATIVE_AES
++#endif
++
++HWY_API Vec128<uint8_t> AESRound(Vec128<uint8_t> state,
++                                 Vec128<uint8_t> round_key) {
++  // NOTE: it is important that AESE and AESMC be consecutive instructions so
++  // they can be fused. AESE includes AddRoundKey, which is a different ordering
++  // than the AES-NI semantics we adopted, so XOR by 0 and later with the actual
++  // round key (the compiler will hopefully optimize this for multiple rounds).
++  return Vec128<uint8_t>(vaesmcq_u8(vaeseq_u8(state.raw, vdupq_n_u8(0)))) ^
++         round_key;
++}
++
++HWY_API Vec128<uint64_t> CLMulLower(Vec128<uint64_t> a, Vec128<uint64_t> b) {
++  return Vec128<uint64_t>((uint64x2_t)vmull_p64(GetLane(a), GetLane(b)));
++}
++
++HWY_API Vec128<uint64_t> CLMulUpper(Vec128<uint64_t> a, Vec128<uint64_t> b) {
++  return Vec128<uint64_t>(
++      (uint64x2_t)vmull_high_p64((poly64x2_t)a.raw, (poly64x2_t)b.raw));
++}
++
++#endif  // __ARM_FEATURE_AES
++
+ // ================================================== MISC
+ 
++// ------------------------------ TableLookupBytes (Combine, LowerHalf)
++
++// Both full
++template <typename T>
++HWY_API Vec128<T> TableLookupBytes(const Vec128<T> bytes,
++                                   const Vec128<T> from) {
++  const Full128<T> d;
++  const Repartition<uint8_t, decltype(d)> d8;
++#if HWY_ARCH_ARM_A64
++  return BitCast(d, Vec128<uint8_t>(vqtbl1q_u8(BitCast(d8, bytes).raw,
++                                               BitCast(d8, from).raw)));
++#else
++  uint8x16_t table0 = BitCast(d8, bytes).raw;
++  uint8x8x2_t table;
++  table.val[0] = vget_low_u8(table0);
++  table.val[1] = vget_high_u8(table0);
++  uint8x16_t idx = BitCast(d8, from).raw;
++  uint8x8_t low = vtbl2_u8(table, vget_low_u8(idx));
++  uint8x8_t hi = vtbl2_u8(table, vget_high_u8(idx));
++  return BitCast(d, Vec128<uint8_t>(vcombine_u8(low, hi)));
++#endif
++}
++
++// Partial index vector
++template <typename T, size_t N, HWY_IF_LE64(T, N)>
++HWY_API Vec128<T, N> TableLookupBytes(const Vec128<T> bytes,
++                                      const Vec128<T, N> from) {
++  const Full128<T> d_full;
++  const Vec128<T, 8 / sizeof(T)> from64(from.raw);
++  const auto idx_full = Combine(d_full, from64, from64);
++  const auto out_full = TableLookupBytes(bytes, idx_full);
++  return Vec128<T, N>(LowerHalf(Half<decltype(d_full)>(), out_full).raw);
++}
++
++// Partial table vector
++template <typename T, size_t N, HWY_IF_LE64(T, N)>
++HWY_API Vec128<T> TableLookupBytes(const Vec128<T, N> bytes,
++                                   const Vec128<T> from) {
++  const Full128<T> d_full;
++  return TableLookupBytes(Combine(d_full, bytes, bytes), from);
++}
++
++// Partial both
++template <typename T, size_t N, typename TI, size_t NI, HWY_IF_LE64(T, N),
++          HWY_IF_LE64(TI, NI)>
++HWY_API VFromD<Repartition<T, Simd<TI, NI>>> TableLookupBytes(
++    Vec128<T, N> bytes, Vec128<TI, NI> from) {
++  const Simd<T, N> d;
++  const Simd<TI, NI> d_idx;
++  const Repartition<uint8_t, decltype(d_idx)> d_idx8;
++  // uint8x8
++  const auto bytes8 = BitCast(Repartition<uint8_t, decltype(d)>(), bytes);
++  const auto from8 = BitCast(d_idx8, from);
++  const VFromD<decltype(d_idx8)> v8(vtbl1_u8(bytes8.raw, from8.raw));
++  return BitCast(d_idx, v8);
++}
++
++// For all vector widths; ARM anyway zeroes if >= 0x10.
++template <class V, class VI>
++HWY_API VI TableLookupBytesOr0(const V bytes, const VI from) {
++  return TableLookupBytes(bytes, from);
++}
++
+ // ------------------------------ Scatter (Store)
+ 
+ template <typename T, size_t N, typename Offset, HWY_IF_LE128(T, N)>
+@@ -3536,33 +3881,33 @@ namespace detail {
+ 
+ // N=1 for any T: no-op
+ template <typename T>
+-HWY_API Vec128<T, 1> SumOfLanes(const Vec128<T, 1> v) {
++HWY_INLINE Vec128<T, 1> SumOfLanes(const Vec128<T, 1> v) {
+   return v;
+ }
+ template <typename T>
+-HWY_API Vec128<T, 1> MinOfLanes(hwy::SizeTag<sizeof(T)> /* tag */,
+-                                const Vec128<T, 1> v) {
++HWY_INLINE Vec128<T, 1> MinOfLanes(hwy::SizeTag<sizeof(T)> /* tag */,
++                                   const Vec128<T, 1> v) {
+   return v;
+ }
+ template <typename T>
+-HWY_API Vec128<T, 1> MaxOfLanes(hwy::SizeTag<sizeof(T)> /* tag */,
+-                                const Vec128<T, 1> v) {
++HWY_INLINE Vec128<T, 1> MaxOfLanes(hwy::SizeTag<sizeof(T)> /* tag */,
++                                   const Vec128<T, 1> v) {
+   return v;
+ }
+ 
+ // u32/i32/f32: N=2
+ template <typename T, HWY_IF_LANE_SIZE(T, 4)>
+-HWY_API Vec128<T, 2> SumOfLanes(const Vec128<T, 2> v10) {
++HWY_INLINE Vec128<T, 2> SumOfLanes(const Vec128<T, 2> v10) {
+   return v10 + Shuffle2301(v10);
+ }
+ template <typename T>
+-HWY_API Vec128<T, 2> MinOfLanes(hwy::SizeTag<4> /* tag */,
+-                                const Vec128<T, 2> v10) {
++HWY_INLINE Vec128<T, 2> MinOfLanes(hwy::SizeTag<4> /* tag */,
++                                   const Vec128<T, 2> v10) {
+   return Min(v10, Shuffle2301(v10));
+ }
+ template <typename T>
+-HWY_API Vec128<T, 2> MaxOfLanes(hwy::SizeTag<4> /* tag */,
+-                                const Vec128<T, 2> v10) {
++HWY_INLINE Vec128<T, 2> MaxOfLanes(hwy::SizeTag<4> /* tag */,
++                                   const Vec128<T, 2> v10) {
+   return Max(v10, Shuffle2301(v10));
+ }
+ 
+@@ -3607,22 +3952,24 @@ HWY_INLINE Vec128<float> SumOfLanes(const Vec128<float> v) {
+   return Vec128<float>(vaddq_f32(v1.val[0], v1.val[1]));
+ }
+ HWY_INLINE Vec128<uint64_t> SumOfLanes(const Vec128<uint64_t> v) {
+-  return v + CombineShiftRightBytes<8>(v, v);
++  return v + Shuffle01(v);
+ }
+ HWY_INLINE Vec128<int64_t> SumOfLanes(const Vec128<int64_t> v) {
+-  return v + CombineShiftRightBytes<8>(v, v);
++  return v + Shuffle01(v);
+ }
+ #endif
+ 
+ template <typename T>
+-HWY_API Vec128<T> MinOfLanes(hwy::SizeTag<4> /* tag */, const Vec128<T> v3210) {
++HWY_INLINE Vec128<T> MinOfLanes(hwy::SizeTag<4> /* tag */,
++                                const Vec128<T> v3210) {
+   const Vec128<T> v1032 = Shuffle1032(v3210);
+   const Vec128<T> v31_20_31_20 = Min(v3210, v1032);
+   const Vec128<T> v20_31_20_31 = Shuffle0321(v31_20_31_20);
+   return Min(v20_31_20_31, v31_20_31_20);
+ }
+ template <typename T>
+-HWY_API Vec128<T> MaxOfLanes(hwy::SizeTag<4> /* tag */, const Vec128<T> v3210) {
++HWY_INLINE Vec128<T> MaxOfLanes(hwy::SizeTag<4> /* tag */,
++                                const Vec128<T> v3210) {
+   const Vec128<T> v1032 = Shuffle1032(v3210);
+   const Vec128<T> v31_20_31_20 = Max(v3210, v1032);
+   const Vec128<T> v20_31_20_31 = Shuffle0321(v31_20_31_20);
+@@ -3631,12 +3978,14 @@ HWY_API Vec128<T> MaxOfLanes(hwy::SizeTag<4> /* tag */, const Vec128<T> v3210) {
+ 
+ // For u64/i64[/f64].
+ template <typename T>
+-HWY_API Vec128<T> MinOfLanes(hwy::SizeTag<8> /* tag */, const Vec128<T> v10) {
++HWY_INLINE Vec128<T> MinOfLanes(hwy::SizeTag<8> /* tag */,
++                                const Vec128<T> v10) {
+   const Vec128<T> v01 = Shuffle01(v10);
+   return Min(v10, v01);
+ }
+ template <typename T>
+-HWY_API Vec128<T> MaxOfLanes(hwy::SizeTag<8> /* tag */, const Vec128<T> v10) {
++HWY_INLINE Vec128<T> MaxOfLanes(hwy::SizeTag<8> /* tag */,
++                                const Vec128<T> v10) {
+   const Vec128<T> v01 = Shuffle01(v10);
+   return Max(v10, v01);
+ }
+@@ -3644,15 +3993,15 @@ HWY_API Vec128<T> MaxOfLanes(hwy::SizeTag<8> /* tag */, const Vec128<T> v10) {
+ }  // namespace detail
+ 
+ template <typename T, size_t N>
+-HWY_API Vec128<T, N> SumOfLanes(const Vec128<T, N> v) {
++HWY_API Vec128<T, N> SumOfLanes(Simd<T, N> /* tag */, const Vec128<T, N> v) {
+   return detail::SumOfLanes(v);
+ }
+ template <typename T, size_t N>
+-HWY_API Vec128<T, N> MinOfLanes(const Vec128<T, N> v) {
++HWY_API Vec128<T, N> MinOfLanes(Simd<T, N> /* tag */, const Vec128<T, N> v) {
+   return detail::MinOfLanes(hwy::SizeTag<sizeof(T)>(), v);
+ }
+ template <typename T, size_t N>
+-HWY_API Vec128<T, N> MaxOfLanes(const Vec128<T, N> v) {
++HWY_API Vec128<T, N> MaxOfLanes(Simd<T, N> /* tag */, const Vec128<T, N> v) {
+   return detail::MaxOfLanes(hwy::SizeTag<sizeof(T)>(), v);
+ }
+ 
+@@ -3882,18 +4231,26 @@ HWY_INLINE size_t CountTrue(hwy::SizeTag<8> /*tag*/, const Mask128<T> mask) {
+ 
+ // Full
+ template <typename T>
+-HWY_INLINE size_t CountTrue(const Mask128<T> mask) {
++HWY_API size_t CountTrue(Full128<T> /* tag */, const Mask128<T> mask) {
+   return detail::CountTrue(hwy::SizeTag<sizeof(T)>(), mask);
+ }
+ 
+ // Partial
+ template <typename T, size_t N, HWY_IF_LE64(T, N)>
+-HWY_INLINE size_t CountTrue(const Mask128<T, N> mask) {
++HWY_API size_t CountTrue(Simd<T, N> /* tag */, const Mask128<T, N> mask) {
+   return PopCount(detail::BitsFromMask(mask));
+ }
+ 
+ template <typename T, size_t N>
+-HWY_INLINE size_t StoreMaskBits(const Mask128<T, N> mask, uint8_t* p) {
++HWY_API intptr_t FindFirstTrue(const Simd<T, N> /* tag */,
++                              const Mask128<T, N> mask) {
++  const uint64_t bits = detail::BitsFromMask(mask);
++  return bits ? Num0BitsBelowLS1Bit_Nonzero64(bits) : -1;
++}
++
++template <typename T, size_t N>
++HWY_API size_t StoreMaskBits(Simd<T, N> /* tag */, const Mask128<T, N> mask,
++                             uint8_t* p) {
+   const uint64_t bits = detail::BitsFromMask(mask);
+   const size_t kNumBytes = (N + 7) / 8;
+   CopyBytes<kNumBytes>(&bits, p);
+@@ -3902,13 +4259,13 @@ HWY_INLINE size_t StoreMaskBits(const Mask128<T, N> mask, uint8_t* p) {
+ 
+ // Full
+ template <typename T>
+-HWY_INLINE bool AllFalse(const Mask128<T> m) {
++HWY_API bool AllFalse(const Full128<T> d, const Mask128<T> m) {
+ #if HWY_ARCH_ARM_A64
+   const Full128<uint32_t> d32;
+-  const auto m32 = MaskFromVec(BitCast(d32, VecFromMask(Full128<T>(), m)));
++  const auto m32 = MaskFromVec(BitCast(d32, VecFromMask(d, m)));
+   return (vmaxvq_u32(m32.raw) == 0);
+ #else
+-  const auto v64 = BitCast(Full128<uint64_t>(), VecFromMask(Full128<T>(), m));
++  const auto v64 = BitCast(Full128<uint64_t>(), VecFromMask(d, m));
+   uint32x2_t a = vqmovn_u64(v64.raw);
+   return vget_lane_u64(vreinterpret_u64_u32(a), 0) == 0;
+ #endif
+@@ -3916,13 +4273,12 @@ HWY_INLINE bool AllFalse(const Mask128<T> m) {
+ 
+ // Partial
+ template <typename T, size_t N, HWY_IF_LE64(T, N)>
+-HWY_INLINE bool AllFalse(const Mask128<T, N> m) {
++HWY_API bool AllFalse(const Simd<T, N> /* tag */, const Mask128<T, N> m) {
+   return detail::BitsFromMask(m) == 0;
+ }
+ 
+ template <typename T, size_t N>
+-HWY_INLINE bool AllTrue(const Mask128<T, N> m) {
+-  const Simd<T, N> d;
++HWY_API bool AllTrue(const Simd<T, N> d, const Mask128<T, N> m) {
+   return AllFalse(VecFromMask(d, m) == Zero(d));
+ }
+ 
+@@ -4134,7 +4490,7 @@ HWY_INLINE Vec128<T, N> IdxFromBits(hwy::SizeTag<8> /*tag*/,
+ // Helper function called by both Compress and CompressStore - avoids a
+ // redundant BitsFromMask in the latter.
+ template <typename T, size_t N>
+-HWY_API Vec128<T, N> Compress(Vec128<T, N> v, const uint64_t mask_bits) {
++HWY_INLINE Vec128<T, N> Compress(Vec128<T, N> v, const uint64_t mask_bits) {
+   const auto idx =
+       detail::IdxFromBits<T, N>(hwy::SizeTag<sizeof(T)>(), mask_bits);
+   using D = Simd<T, N>;
+@@ -4232,6 +4588,102 @@ HWY_API void StoreInterleaved4(const Vec128<uint8_t, N> v0,
+   CopyBytes<N * 4>(buf, unaligned);
+ }
+ 
++// ================================================== DEPRECATED
++
++template <typename T, size_t N>
++HWY_API size_t StoreMaskBits(const Mask128<T, N> mask, uint8_t* p) {
++  return StoreMaskBits(Simd<T, N>(), mask, p);
++}
++
++template <typename T, size_t N>
++HWY_API bool AllTrue(const Mask128<T, N> mask) {
++  return AllTrue(Simd<T, N>(), mask);
++}
++
++template <typename T, size_t N>
++HWY_API bool AllFalse(const Mask128<T, N> mask) {
++  return AllFalse(Simd<T, N>(), mask);
++}
++
++template <typename T, size_t N>
++HWY_API size_t CountTrue(const Mask128<T, N> mask) {
++  return CountTrue(Simd<T, N>(), mask);
++}
++
++template <typename T, size_t N>
++HWY_API Vec128<T, N> SumOfLanes(const Vec128<T, N> v) {
++  return SumOfLanes(Simd<T, N>(), v);
++}
++template <typename T, size_t N>
++HWY_API Vec128<T, N> MinOfLanes(const Vec128<T, N> v) {
++  return MinOfLanes(Simd<T, N>(), v);
++}
++template <typename T, size_t N>
++HWY_API Vec128<T, N> MaxOfLanes(const Vec128<T, N> v) {
++  return MaxOfLanes(Simd<T, N>(), v);
++}
++
++template <typename T, size_t N>
++HWY_API Vec128<T, (N + 1) / 2> UpperHalf(Vec128<T, N> v) {
++  return UpperHalf(Half<Simd<T, N>>(), v);
++}
++
++template <int kBytes, typename T, size_t N>
++HWY_API Vec128<T, N> ShiftRightBytes(const Vec128<T, N> v) {
++  return ShiftRightBytes<kBytes>(Simd<T, N>(), v);
++}
++
++template <int kLanes, typename T, size_t N>
++HWY_API Vec128<T, N> ShiftRightLanes(const Vec128<T, N> v) {
++  return ShiftRightLanes<kLanes>(Simd<T, N>(), v);
++}
++
++template <size_t kBytes, typename T, size_t N>
++HWY_API Vec128<T, N> CombineShiftRightBytes(Vec128<T, N> hi, Vec128<T, N> lo) {
++  return CombineShiftRightBytes<kBytes>(Simd<T, N>(), hi, lo);
++}
++
++template <typename T, size_t N>
++HWY_API Vec128<T, N> InterleaveUpper(Vec128<T, N> a, Vec128<T, N> b) {
++  return InterleaveUpper(Simd<T, N>(), a, b);
++}
++
++template <typename T, size_t N, class D = Simd<T, N>>
++HWY_API VFromD<RepartitionToWide<D>> ZipUpper(Vec128<T, N> a, Vec128<T, N> b) {
++  return InterleaveUpper(RepartitionToWide<D>(), a, b);
++}
++
++template <typename T, size_t N2>
++HWY_API Vec128<T, N2 * 2> Combine(Vec128<T, N2> hi2, Vec128<T, N2> lo2) {
++  return Combine(Simd<T, N2 * 2>(), hi2, lo2);
++}
++
++template <typename T, size_t N2, HWY_IF_LE64(T, N2)>
++HWY_API Vec128<T, N2 * 2> ZeroExtendVector(Vec128<T, N2> lo) {
++  return ZeroExtendVector(Simd<T, N2 * 2>(), lo);
++}
++
++template <typename T, size_t N>
++HWY_API Vec128<T, N> ConcatLowerLower(Vec128<T, N> hi, Vec128<T, N> lo) {
++  return ConcatLowerLower(Simd<T, N>(), hi, lo);
++}
++
++template <typename T, size_t N>
++HWY_API Vec128<T, N> ConcatUpperUpper(Vec128<T, N> hi, Vec128<T, N> lo) {
++  return ConcatUpperUpper(Simd<T, N>(), hi, lo);
++}
++
++template <typename T, size_t N>
++HWY_API Vec128<T, N> ConcatLowerUpper(const Vec128<T, N> hi,
++                                      const Vec128<T, N> lo) {
++  return ConcatLowerUpper(Simd<T, N>(), hi, lo);
++}
++
++template <typename T, size_t N>
++HWY_API Vec128<T, N> ConcatUpperLower(Vec128<T, N> hi, Vec128<T, N> lo) {
++  return ConcatUpperLower(Simd<T, N>(), hi, lo);
++}
++
+ // ================================================== Operator wrapper
+ 
+ // These apply to all x86_*-inl.h because there are no restrictions on V.
+@@ -4268,6 +4720,10 @@ HWY_API auto Eq(V a, V b) -> decltype(a == b) {
+   return a == b;
+ }
+ template <class V>
++HWY_API auto Ne(V a, V b) -> decltype(a == b) {
++  return a != b;
++}
++template <class V>
+ HWY_API auto Lt(V a, V b) -> decltype(a == b) {
+   return a < b;
+ }
+diff --git a/third_party/highway/hwy/ops/arm_sve-inl.h b/third_party/highway/hwy/ops/arm_sve-inl.h
+index 3d57d3586689d..81db96dbd6626 100644
+--- a/third_party/highway/hwy/ops/arm_sve-inl.h
++++ b/third_party/highway/hwy/ops/arm_sve-inl.h
+@@ -15,10 +15,15 @@
+ // ARM SVE[2] vectors (length not known at compile time).
+ // External include guard in highway.h - see comment there.
+ 
+-#include <arm_sve.h>
+ #include <stddef.h>
+ #include <stdint.h>
+ 
++#if defined(HWY_EMULATE_SVE)
++#include "third_party/farm_sve/farm_sve.h"
++#else
++#include <arm_sve.h>
++#endif
++
+ #include "hwy/base.h"
+ #include "hwy/ops/shared-inl.h"
+ 
+@@ -26,18 +31,22 @@ HWY_BEFORE_NAMESPACE();
+ namespace hwy {
+ namespace HWY_NAMESPACE {
+ 
++// SVE only supports fractions, not LMUL > 1.
++template <typename T, int kShift = 0>
++using Full = Simd<T, (kShift <= 0) ? (HWY_LANES(T) >> (-kShift)) : 0>;
++
+ template <class V>
+ struct DFromV_t {};  // specialized in macros
+ template <class V>
+-using DFromV = typename DFromV_t<V>::type;
++using DFromV = typename DFromV_t<RemoveConst<V>>::type;
+ 
+ template <class V>
+ using TFromV = TFromD<DFromV<V>>;
+ 
+-#define HWY_IF_UNSIGNED_V(V) hwy::EnableIf<!IsSigned<TFromV<V>>()>* = nullptr
+-#define HWY_IF_SIGNED_V(V) \
+-  hwy::EnableIf<IsSigned<TFromV<V>>() && !IsFloat<TFromV<V>>()>* = nullptr
+-#define HWY_IF_FLOAT_V(V) hwy::EnableIf<IsFloat<TFromV<V>>()>* = nullptr
++#define HWY_IF_UNSIGNED_V(V) HWY_IF_UNSIGNED(TFromV<V>)
++#define HWY_IF_SIGNED_V(V) HWY_IF_SIGNED(TFromV<V>)
++#define HWY_IF_FLOAT_V(V) HWY_IF_FLOAT(TFromV<V>)
++#define HWY_IF_LANE_SIZE_V(V, bytes) HWY_IF_LANE_SIZE(TFromV<V>, bytes)
+ 
+ // ================================================== MACROS
+ 
+@@ -53,10 +62,10 @@ namespace detail {  // for code folding
+ #define HWY_SVE_FOREACH_U64(X_MACRO, NAME, OP) X_MACRO(uint, u, 64, NAME, OP)
+ 
+ // Signed:
+-#define HWY_SVE_FOREACH_I08(X_MACRO, NAME, OP) X_MACRO(int, i, 8, NAME, OP)
+-#define HWY_SVE_FOREACH_I16(X_MACRO, NAME, OP) X_MACRO(int, i, 16, NAME, OP)
+-#define HWY_SVE_FOREACH_I32(X_MACRO, NAME, OP) X_MACRO(int, i, 32, NAME, OP)
+-#define HWY_SVE_FOREACH_I64(X_MACRO, NAME, OP) X_MACRO(int, i, 64, NAME, OP)
++#define HWY_SVE_FOREACH_I08(X_MACRO, NAME, OP) X_MACRO(int, s, 8, NAME, OP)
++#define HWY_SVE_FOREACH_I16(X_MACRO, NAME, OP) X_MACRO(int, s, 16, NAME, OP)
++#define HWY_SVE_FOREACH_I32(X_MACRO, NAME, OP) X_MACRO(int, s, 32, NAME, OP)
++#define HWY_SVE_FOREACH_I64(X_MACRO, NAME, OP) X_MACRO(int, s, 64, NAME, OP)
+ 
+ // Float:
+ #define HWY_SVE_FOREACH_F16(X_MACRO, NAME, OP) X_MACRO(float, f, 16, NAME, OP)
+@@ -82,6 +91,10 @@ namespace detail {  // for code folding
+   HWY_SVE_FOREACH_F64(X_MACRO, NAME, OP)
+ 
+ // Commonly used type categories for a given element size:
++#define HWY_SVE_FOREACH_UI08(X_MACRO, NAME, OP) \
++  HWY_SVE_FOREACH_U08(X_MACRO, NAME, OP)        \
++  HWY_SVE_FOREACH_I08(X_MACRO, NAME, OP)
++
+ #define HWY_SVE_FOREACH_UI16(X_MACRO, NAME, OP) \
+   HWY_SVE_FOREACH_U16(X_MACRO, NAME, OP)        \
+   HWY_SVE_FOREACH_I16(X_MACRO, NAME, OP)
+@@ -94,11 +107,21 @@ namespace detail {  // for code folding
+   HWY_SVE_FOREACH_U64(X_MACRO, NAME, OP)        \
+   HWY_SVE_FOREACH_I64(X_MACRO, NAME, OP)
+ 
++#define HWY_SVE_FOREACH_UIF3264(X_MACRO, NAME, OP) \
++  HWY_SVE_FOREACH_UI32(X_MACRO, NAME, OP)          \
++  HWY_SVE_FOREACH_UI64(X_MACRO, NAME, OP)          \
++  HWY_SVE_FOREACH_F32(X_MACRO, NAME, OP)           \
++  HWY_SVE_FOREACH_F64(X_MACRO, NAME, OP)
++
+ // Commonly used type categories:
+ #define HWY_SVE_FOREACH_UI(X_MACRO, NAME, OP) \
+   HWY_SVE_FOREACH_U(X_MACRO, NAME, OP)        \
+   HWY_SVE_FOREACH_I(X_MACRO, NAME, OP)
+ 
++#define HWY_SVE_FOREACH_IF(X_MACRO, NAME, OP) \
++  HWY_SVE_FOREACH_I(X_MACRO, NAME, OP)        \
++  HWY_SVE_FOREACH_F(X_MACRO, NAME, OP)
++
+ #define HWY_SVE_FOREACH(X_MACRO, NAME, OP) \
+   HWY_SVE_FOREACH_U(X_MACRO, NAME, OP)     \
+   HWY_SVE_FOREACH_I(X_MACRO, NAME, OP)     \
+@@ -106,135 +129,213 @@ namespace detail {  // for code folding
+ 
+ // Assemble types for use in x-macros
+ #define HWY_SVE_T(BASE, BITS) BASE##BITS##_t
+-#define HWY_SVE_D(CHAR, BITS) D##CHAR##BITS
++#define HWY_SVE_D(BASE, BITS, N) Simd<HWY_SVE_T(BASE, BITS), N>
+ #define HWY_SVE_V(BASE, BITS) sv##BASE##BITS##_t
+ 
+ }  // namespace detail
+ 
+-// TODO(janwas): remove typedefs and only use HWY_SVE_V etc. directly
+-
+-#define HWY_SPECIALIZE(BASE, CHAR, BITS, NAME, OP)                   \
+-  using HWY_SVE_D(CHAR, BITS) =                                      \
+-      Simd<HWY_SVE_T(BASE, BITS), HWY_LANES(HWY_SVE_T(BASE, BITS))>; \
+-  using V##CHAR##BITS = HWY_SVE_V(BASE, BITS);                       \
+-  template <>                                                        \
+-  struct DFromV_t<HWY_SVE_V(BASE, BITS)> {                           \
+-    using Lane = HWY_SVE_T(BASE, BITS);                              \
+-    using type = Simd<Lane, HWY_LANES(Lane)>;                        \
++#define HWY_SPECIALIZE(BASE, CHAR, BITS, NAME, OP)                        \
++  template <>                                                             \
++  struct DFromV_t<HWY_SVE_V(BASE, BITS)> {                                \
++    using type = HWY_SVE_D(BASE, BITS, HWY_LANES(HWY_SVE_T(BASE, BITS))); \
+   };
+-using Vf16 = svfloat16_t;
+-using Df16 = Simd<float16_t, HWY_LANES(float16_t)>;
+ 
+ HWY_SVE_FOREACH(HWY_SPECIALIZE, _, _)
+ #undef HWY_SPECIALIZE
+ 
+-// vector = f(d), e.g. Zero
+-#define HWY_SVE_RETV_ARGD(BASE, CHAR, BITS, NAME, OP)           \
+-  HWY_API HWY_SVE_V(BASE, BITS) NAME(HWY_SVE_D(CHAR, BITS) d) { \
+-    (void)Lanes(d);                                             \
+-    return v##OP##_##CHAR##BITS();                              \
++// vector = f(d), e.g. Undefined
++#define HWY_SVE_RETV_ARGD(BASE, CHAR, BITS, NAME, OP)              \
++  template <size_t N>                                              \
++  HWY_API HWY_SVE_V(BASE, BITS) NAME(HWY_SVE_D(BASE, BITS, N) d) { \
++    return sv##OP##_##CHAR##BITS();                                \
+   }
+ 
++// Note: _x (don't-care value for inactive lanes) avoids additional MOVPRFX
++// instructions, and we anyway only use it when the predicate is ptrue.
++
+ // vector = f(vector), e.g. Not
++#define HWY_SVE_RETV_ARGPV(BASE, CHAR, BITS, NAME, OP)          \
++  HWY_API HWY_SVE_V(BASE, BITS) NAME(HWY_SVE_V(BASE, BITS) v) { \
++    return sv##OP##_##CHAR##BITS##_x(HWY_SVE_PTRUE(BITS), v);   \
++  }
+ #define HWY_SVE_RETV_ARGV(BASE, CHAR, BITS, NAME, OP)           \
+   HWY_API HWY_SVE_V(BASE, BITS) NAME(HWY_SVE_V(BASE, BITS) v) { \
+-    return v##OP##_v_##CHAR##BITS(v);                           \
++    return sv##OP##_##CHAR##BITS(v);                            \
+   }
+ 
+-// vector = f(vector, scalar), e.g. detail::Add
+-#define HWY_SVE_RETV_ARGVS(BASE, CHAR, BITS, NAME, OP)         \
++// vector = f(vector, scalar), e.g. detail::AddK
++#define HWY_SVE_RETV_ARGPVN(BASE, CHAR, BITS, NAME, OP)          \
++  HWY_API HWY_SVE_V(BASE, BITS)                                  \
++      NAME(HWY_SVE_V(BASE, BITS) a, HWY_SVE_T(BASE, BITS) b) {   \
++    return sv##OP##_##CHAR##BITS##_x(HWY_SVE_PTRUE(BITS), a, b); \
++  }
++#define HWY_SVE_RETV_ARGVN(BASE, CHAR, BITS, NAME, OP)         \
+   HWY_API HWY_SVE_V(BASE, BITS)                                \
+       NAME(HWY_SVE_V(BASE, BITS) a, HWY_SVE_T(BASE, BITS) b) { \
+-    return v##OP##_##CHAR##BITS(a, b);                         \
++    return sv##OP##_##CHAR##BITS(a, b);                        \
+   }
+ 
+ // vector = f(vector, vector), e.g. Add
++#define HWY_SVE_RETV_ARGPVV(BASE, CHAR, BITS, NAME, OP)          \
++  HWY_API HWY_SVE_V(BASE, BITS)                                  \
++      NAME(HWY_SVE_V(BASE, BITS) a, HWY_SVE_V(BASE, BITS) b) {   \
++    return sv##OP##_##CHAR##BITS##_x(HWY_SVE_PTRUE(BITS), a, b); \
++  }
+ #define HWY_SVE_RETV_ARGVV(BASE, CHAR, BITS, NAME, OP)         \
+   HWY_API HWY_SVE_V(BASE, BITS)                                \
+       NAME(HWY_SVE_V(BASE, BITS) a, HWY_SVE_V(BASE, BITS) b) { \
+-    return v##OP##_vv_##CHAR##BITS(a, b);                      \
++    return sv##OP##_##CHAR##BITS(a, b);                        \
+   }
+ 
+-// ================================================== INIT
+-
+ // ------------------------------ Lanes
+ 
+-// WARNING: we want to query VLMAX/sizeof(T), but this actually changes VL!
+-// vlenb is not exposed through intrinsics and vreadvl is not VLMAX.
+-#define HWY_SVE_LANES(BASE, CHAR, BITS, NAME, OP) \
+-  HWY_API size_t NAME(HWY_SVE_D(CHAR, BITS) /* d */) { return v##OP##BITS(); }
++namespace detail {
++
++// Returns actual lanes of a hardware vector, rounded down to a power of two.
++HWY_INLINE size_t HardwareLanes(hwy::SizeTag<1> /* tag */) {
++  return svcntb_pat(SV_POW2);
++}
++HWY_INLINE size_t HardwareLanes(hwy::SizeTag<2> /* tag */) {
++  return svcnth_pat(SV_POW2);
++}
++HWY_INLINE size_t HardwareLanes(hwy::SizeTag<8> /* tag */) {
++  return svcntd_pat(SV_POW2);
++}
++HWY_INLINE size_t HardwareLanes(hwy::SizeTag<4> /* tag */) {
++  return svcntw_pat(SV_POW2);
++}
+ 
+-HWY_SVE_FOREACH(HWY_SVE_LANES, Lanes, setvlmax_e)
+-#undef HWY_SVE_LANES
++}  // namespace detail
+ 
+-// ------------------------------ Zero
++// Capped to <= 128-bit: SVE is at least that large, so no need to query actual.
++template <typename T, size_t N, HWY_IF_LE128(T, N)>
++HWY_API constexpr size_t Lanes(Simd<T, N> /* tag */) {
++  return N;
++}
+ 
+-HWY_SVE_FOREACH(HWY_SVE_RETV_ARGD, Zero, zero)
++// Returns actual number of lanes after dividing by div={1,2,4,8}.
++// May return 0 if div > 16/sizeof(T): there is no "1/8th" of a u32x4, but it
++// would be valid for u32x8 (i.e. hardware vectors >= 256 bits).
++template <typename T, size_t N, HWY_IF_GT128(T, N)>
++HWY_API size_t Lanes(Simd<T, N> /* tag */) {
++  static_assert(N <= HWY_LANES(T), "N cannot exceed a full vector");
+ 
+-template <class D>
+-using VFromD = decltype(Zero(D()));
++  const size_t actual = detail::HardwareLanes(hwy::SizeTag<sizeof(T)>());
++  const size_t div = HWY_LANES(T) / N;
++  static_assert(div <= 8, "Invalid N - must be <=128 bit, or >=1/8th");
++  return actual / div;
++}
++
++// ================================================== MASK INIT
++
++// One mask bit per byte; only the one belonging to the lowest byte is valid.
++
++// ------------------------------ FirstN
++#define HWY_SVE_FIRSTN(BASE, CHAR, BITS, NAME, OP)                       \
++  template <size_t KN>                                                   \
++  HWY_API svbool_t NAME(HWY_SVE_D(BASE, BITS, KN) /* d */, uint32_t N) { \
++    return sv##OP##_b##BITS##_u32(uint32_t(0), N);                       \
++  }
++HWY_SVE_FOREACH(HWY_SVE_FIRSTN, FirstN, whilelt)
++#undef HWY_SVE_FIRSTN
++
++namespace detail {
++
++// All-true mask from a macro
++#define HWY_SVE_PTRUE(BITS) svptrue_pat_b##BITS(SV_POW2)
++
++#define HWY_SVE_WRAP_PTRUE(BASE, CHAR, BITS, NAME, OP) \
++  template <size_t N>                                  \
++  HWY_API svbool_t NAME(HWY_SVE_D(BASE, BITS, N) d) {  \
++    return HWY_SVE_PTRUE(BITS);                        \
++  }
++
++HWY_SVE_FOREACH(HWY_SVE_WRAP_PTRUE, PTrue, ptrue)  // return all-true
++#undef HWY_SVE_WRAP_PTRUE
++
++HWY_API svbool_t PFalse() { return svpfalse_b(); }
++
++// Returns all-true if d is HWY_FULL or FirstN(N) after capping N.
++//
++// This is used in functions that load/store memory; other functions (e.g.
++// arithmetic on partial vectors) can ignore d and use PTrue instead.
++template <typename T, size_t N>
++svbool_t Mask(Simd<T, N> d) {
++  return N == HWY_LANES(T) ? PTrue(d) : FirstN(d, Lanes(d));
++}
++
++}  // namespace detail
++
++// ================================================== INIT
+ 
+ // ------------------------------ Set
+ // vector = f(d, scalar), e.g. Set
+-#define HWY_SVE_SET(BASE, CHAR, BITS, NAME, OP)                  \
+-  HWY_API HWY_SVE_V(BASE, BITS)                                  \
+-      NAME(HWY_SVE_D(CHAR, BITS) d, HWY_SVE_T(BASE, BITS) arg) { \
+-    (void)Lanes(d);                                              \
+-    return v##OP##_##CHAR##BITS(arg);                            \
++#define HWY_SVE_SET(BASE, CHAR, BITS, NAME, OP)                     \
++  template <size_t N>                                               \
++  HWY_API HWY_SVE_V(BASE, BITS)                                     \
++      NAME(HWY_SVE_D(BASE, BITS, N) d, HWY_SVE_T(BASE, BITS) arg) { \
++    return sv##OP##_##CHAR##BITS(arg);                              \
+   }
+ 
+-HWY_SVE_FOREACH_UI(HWY_SVE_SET, Set, mv_v_x)
+-HWY_SVE_FOREACH_F(HWY_SVE_SET, Set, fmv_v_f)
++HWY_SVE_FOREACH(HWY_SVE_SET, Set, dup_n)
+ #undef HWY_SVE_SET
+ 
++template <class D>
++using VFromD = decltype(Set(D(), 0));
++
++// ------------------------------ Zero
++
++template <class D>
++VFromD<D> Zero(D d) {
++  return Set(d, 0);
++}
++
+ // ------------------------------ Undefined
+ 
+-HWY_SVE_FOREACH(HWY_SVE_RETV_ARGD, Undefined, undefined)
++#if defined(HWY_EMULATE_SVE)
++template <class D>
++VFromD<D> Undefined(D d) {
++  return Zero(d);
++}
++#else
++HWY_SVE_FOREACH(HWY_SVE_RETV_ARGD, Undefined, undef)
++#endif
+ 
+ // ------------------------------ BitCast
+ 
+ namespace detail {
+ 
+ // u8: no change
+-#define HWY_SVE_CAST_NOP(BASE, CHAR, BITS, NAME, OP)                           \
+-  HWY_API HWY_SVE_V(BASE, BITS) BitCastToByte(HWY_SVE_V(BASE, BITS) v) {       \
+-    return v;                                                                  \
+-  }                                                                            \
+-  HWY_API HWY_SVE_V(BASE, BITS) BitCastFromByte(HWY_SVE_D(CHAR, BITS) /* d */, \
+-                                                HWY_SVE_V(BASE, BITS) v) {     \
+-    return v;                                                                  \
+-  }
+-
+-// Other integers
+-#define HWY_SVE_CAST_UI(BASE, CHAR, BITS, NAME, OP)                   \
+-  HWY_API vuint8m##_t BitCastToByte(HWY_SVE_V(BASE, BITS) v) {        \
+-    return v##OP##_v_##CHAR##BITS##_u8m(v);                           \
+-  }                                                                   \
+-  HWY_API HWY_SVE_V(BASE, BITS)                                       \
+-      BitCastFromByte(HWY_SVE_D(CHAR, BITS) /* d */, vuint8m##_t v) { \
+-    return v##OP##_v_u8m##_##CHAR##BITS(v);                           \
++#define HWY_SVE_CAST_NOP(BASE, CHAR, BITS, NAME, OP)                     \
++  HWY_API HWY_SVE_V(BASE, BITS) BitCastToByte(HWY_SVE_V(BASE, BITS) v) { \
++    return v;                                                            \
++  }                                                                      \
++  template <size_t N>                                                    \
++  HWY_API HWY_SVE_V(BASE, BITS) BitCastFromByte(                         \
++      HWY_SVE_D(BASE, BITS, N) /* d */, HWY_SVE_V(BASE, BITS) v) {       \
++    return v;                                                            \
+   }
+ 
+-// Float: first cast to/from unsigned
+-#define HWY_SVE_CAST_F(BASE, CHAR, BITS, NAME, OP)                    \
+-  HWY_API vuint8m##_t BitCastToByte(HWY_SVE_V(BASE, BITS) v) {        \
+-    return v##OP##_v_u##BITS##_u8m(v##OP##_v_f##BITS##_u##BITS(v));   \
+-  }                                                                   \
+-  HWY_API HWY_SVE_V(BASE, BITS)                                       \
+-      BitCastFromByte(HWY_SVE_D(CHAR, BITS) /* d */, vuint8m##_t v) { \
+-    return v##OP##_v_u##BITS##_f##BITS(v##OP##_v_u8m##_u##BITS(v));   \
++// All other types
++#define HWY_SVE_CAST(BASE, CHAR, BITS, NAME, OP)                       \
++  HWY_INLINE svuint8_t BitCastToByte(HWY_SVE_V(BASE, BITS) v) {        \
++    return sv##OP##_u8_##CHAR##BITS(v);                                \
++  }                                                                    \
++  template <size_t N>                                                  \
++  HWY_INLINE HWY_SVE_V(BASE, BITS)                                     \
++      BitCastFromByte(HWY_SVE_D(BASE, BITS, N) /* d */, svuint8_t v) { \
++    return sv##OP##_##CHAR##BITS##_u8(v);                              \
+   }
+ 
+ HWY_SVE_FOREACH_U08(HWY_SVE_CAST_NOP, _, _)
+-HWY_SVE_FOREACH_I08(HWY_SVE_CAST_UI, _, reinterpret)
+-HWY_SVE_FOREACH_UI16(HWY_SVE_CAST_UI, _, reinterpret)
+-HWY_SVE_FOREACH_UI32(HWY_SVE_CAST_UI, _, reinterpret)
+-HWY_SVE_FOREACH_UI64(HWY_SVE_CAST_UI, _, reinterpret)
+-HWY_SVE_FOREACH_F(HWY_SVE_CAST_F, _, reinterpret)
++HWY_SVE_FOREACH_I08(HWY_SVE_CAST, _, reinterpret)
++HWY_SVE_FOREACH_UI16(HWY_SVE_CAST, _, reinterpret)
++HWY_SVE_FOREACH_UI32(HWY_SVE_CAST, _, reinterpret)
++HWY_SVE_FOREACH_UI64(HWY_SVE_CAST, _, reinterpret)
++HWY_SVE_FOREACH_F(HWY_SVE_CAST, _, reinterpret)
+ 
+ #undef HWY_SVE_CAST_NOP
+-#undef HWY_SVE_CAST_UI
+-#undef HWY_SVE_CAST_F
++#undef HWY_SVE_CAST
+ 
+ }  // namespace detail
+ 
+@@ -243,114 +344,120 @@ HWY_API VFromD<D> BitCast(D d, FromV v) {
+   return detail::BitCastFromByte(d, detail::BitCastToByte(v));
+ }
+ 
+-namespace detail {
++// ================================================== LOGICAL
+ 
+-template <class V, class DU = RebindToUnsigned<DFromV<V>>>
+-HWY_API VFromD<DU> BitCastToUnsigned(V v) {
+-  return BitCast(DU(), v);
+-}
++// detail::*N() functions accept a scalar argument to avoid extra Set().
+ 
+-}  // namespace detail
++// ------------------------------ Not
+ 
+-// ------------------------------ Iota
++HWY_SVE_FOREACH_UI(HWY_SVE_RETV_ARGPV, Not, not )
++
++// ------------------------------ And
+ 
+ namespace detail {
++HWY_SVE_FOREACH_UI(HWY_SVE_RETV_ARGPVN, AndN, and_n)
++}  // namespace detail
+ 
+-HWY_SVE_FOREACH_U(HWY_SVE_RETV_ARGD, Iota0, id_v)
++HWY_SVE_FOREACH_UI(HWY_SVE_RETV_ARGPVV, And, and)
+ 
+-template <class D, class DU = RebindToUnsigned<D>>
+-HWY_API VFromD<DU> Iota0(const D /*d*/) {
+-  Lanes(DU());
+-  return BitCastToUnsigned(Iota0(DU()));
++template <class V, HWY_IF_FLOAT_V(V)>
++HWY_API V And(const V a, const V b) {
++  const DFromV<V> df;
++  const RebindToUnsigned<decltype(df)> du;
++  return BitCast(df, And(BitCast(du, a), BitCast(du, b)));
+ }
+ 
+-}  // namespace detail
+-
+-// ================================================== LOGICAL
+-
+-// ------------------------------ Not
++// ------------------------------ Or
+ 
+-HWY_SVE_FOREACH_UI(HWY_SVE_RETV_ARGV, Not, not )
++HWY_SVE_FOREACH_UI(HWY_SVE_RETV_ARGPVV, Or, orr)
+ 
+ template <class V, HWY_IF_FLOAT_V(V)>
+-HWY_API V Not(const V v) {
+-  using DF = DFromV<V>;
+-  using DU = RebindToUnsigned<DF>;
+-  return BitCast(DF(), Not(BitCast(DU(), v)));
++HWY_API V Or(const V a, const V b) {
++  const DFromV<V> df;
++  const RebindToUnsigned<decltype(df)> du;
++  return BitCast(df, Or(BitCast(du, a), BitCast(du, b)));
+ }
+ 
+-// ------------------------------ And
++// ------------------------------ Xor
+ 
+-// Non-vector version (ideally immediate) for use with Iota0
+ namespace detail {
+-HWY_SVE_FOREACH_UI(HWY_SVE_RETV_ARGVS, And, and_vx)
++HWY_SVE_FOREACH_UI(HWY_SVE_RETV_ARGPVN, XorN, eor_n)
+ }  // namespace detail
+ 
+-HWY_SVE_FOREACH_UI(HWY_SVE_RETV_ARGVV, And, and)
++HWY_SVE_FOREACH_UI(HWY_SVE_RETV_ARGPVV, Xor, eor)
+ 
+ template <class V, HWY_IF_FLOAT_V(V)>
+-HWY_API V And(const V a, const V b) {
+-  using DF = DFromV<V>;
+-  using DU = RebindToUnsigned<DF>;
+-  return BitCast(DF(), And(BitCast(DU(), a), BitCast(DU(), b)));
++HWY_API V Xor(const V a, const V b) {
++  const DFromV<V> df;
++  const RebindToUnsigned<decltype(df)> du;
++  return BitCast(df, Xor(BitCast(du, a), BitCast(du, b)));
+ }
+ 
+-// ------------------------------ Or
++// ------------------------------ AndNot
+ 
+-// Scalar argument plus mask. Used by VecFromMask.
+-#define HWY_SVE_OR_MASK(BASE, CHAR, BITS, NAME, OP)                 \
+-  HWY_API HWY_SVE_V(BASE, BITS)                                     \
+-      NAME(HWY_SVE_V(BASE, BITS) v, HWY_SVE_T(BASE, BITS) imm,      \
+-           HWY_SVE_M(MLEN) mask, HWY_SVE_V(BASE, BITS) maskedoff) { \
+-    return v##OP##_##CHAR##BITS##_m(mask, maskedoff, v, imm);       \
++namespace detail {
++#define HWY_SVE_RETV_ARGPVN_SWAP(BASE, CHAR, BITS, NAME, OP)     \
++  HWY_API HWY_SVE_V(BASE, BITS)                                  \
++      NAME(HWY_SVE_T(BASE, BITS) a, HWY_SVE_V(BASE, BITS) b) {   \
++    return sv##OP##_##CHAR##BITS##_x(HWY_SVE_PTRUE(BITS), b, a); \
+   }
+ 
+-namespace detail {
+-HWY_SVE_FOREACH_UI(HWY_SVE_OR_MASK, Or, or_vx)
++HWY_SVE_FOREACH_UI(HWY_SVE_RETV_ARGPVN_SWAP, AndNotN, bic_n)
++#undef HWY_SVE_RETV_ARGPVN_SWAP
+ }  // namespace detail
+ 
+-#undef HWY_SVE_OR_MASK
+-
+-HWY_SVE_FOREACH_UI(HWY_SVE_RETV_ARGVV, Or, or)
++#define HWY_SVE_RETV_ARGPVV_SWAP(BASE, CHAR, BITS, NAME, OP)     \
++  HWY_API HWY_SVE_V(BASE, BITS)                                  \
++      NAME(HWY_SVE_V(BASE, BITS) a, HWY_SVE_V(BASE, BITS) b) {   \
++    return sv##OP##_##CHAR##BITS##_x(HWY_SVE_PTRUE(BITS), b, a); \
++  }
++HWY_SVE_FOREACH_UI(HWY_SVE_RETV_ARGPVV_SWAP, AndNot, bic)
++#undef HWY_SVE_RETV_ARGPVV_SWAP
+ 
+ template <class V, HWY_IF_FLOAT_V(V)>
+-HWY_API V Or(const V a, const V b) {
+-  using DF = DFromV<V>;
+-  using DU = RebindToUnsigned<DF>;
+-  return BitCast(DF(), Or(BitCast(DU(), a), BitCast(DU(), b)));
++HWY_API V AndNot(const V a, const V b) {
++  const DFromV<V> df;
++  const RebindToUnsigned<decltype(df)> du;
++  return BitCast(df, AndNot(BitCast(du, a), BitCast(du, b)));
+ }
+ 
+-// ------------------------------ Xor
++// ------------------------------ PopulationCount
+ 
+-// Non-vector version (ideally immediate) for use with Iota0
+-namespace detail {
+-HWY_SVE_FOREACH_UI(HWY_SVE_RETV_ARGVS, Xor, xor_vx)
+-}  // namespace detail
++#ifdef HWY_NATIVE_POPCNT
++#undef HWY_NATIVE_POPCNT
++#else
++#define HWY_NATIVE_POPCNT
++#endif
+ 
+-HWY_SVE_FOREACH_UI(HWY_SVE_RETV_ARGVV, Xor, xor)
++// Need to return original type instead of unsigned.
++#define HWY_SVE_POPCNT(BASE, CHAR, BITS, NAME, OP)                     \
++  HWY_API HWY_SVE_V(BASE, BITS) NAME(HWY_SVE_V(BASE, BITS) v) {        \
++    return BitCast(DFromV<decltype(v)>(),                              \
++                   sv##OP##_##CHAR##BITS##_x(HWY_SVE_PTRUE(BITS), v)); \
++  }
++HWY_SVE_FOREACH_UI(HWY_SVE_POPCNT, PopulationCount, cnt)
++#undef HWY_SVE_POPCNT
+ 
+-template <class V, HWY_IF_FLOAT_V(V)>
+-HWY_API V Xor(const V a, const V b) {
+-  using DF = DFromV<V>;
+-  using DU = RebindToUnsigned<DF>;
+-  return BitCast(DF(), Xor(BitCast(DU(), a), BitCast(DU(), b)));
+-}
++// ================================================== SIGN
+ 
+-// ------------------------------ AndNot
++// ------------------------------ Neg
++HWY_SVE_FOREACH_IF(HWY_SVE_RETV_ARGPV, Neg, neg)
+ 
+-template <class V>
+-HWY_API V AndNot(const V not_a, const V b) {
+-  return And(Not(not_a), b);
+-}
++// ------------------------------ Abs
++HWY_SVE_FOREACH_IF(HWY_SVE_RETV_ARGPV, Abs, abs)
+ 
+-// ------------------------------ CopySign
++// ------------------------------ CopySign[ToAbs]
+ 
+-HWY_SVE_FOREACH_F(HWY_SVE_RETV_ARGVV, CopySign, fsgnj)
++template <class V>
++HWY_API V CopySign(const V magn, const V sign) {
++  const auto msb = SignBit(DFromV<V>());
++  return Or(AndNot(msb, magn), And(msb, sign));
++}
+ 
+ template <class V>
+ HWY_API V CopySignToAbs(const V abs, const V sign) {
+-  // TODO(janwas): separate handling for abs < 0 or same?
+-  return CopySign(abs, sign);
++  const auto msb = SignBit(DFromV<V>());
++  return Or(abs, And(msb, sign));
+ }
+ 
+ // ================================================== ARITHMETIC
+@@ -358,1312 +465,1352 @@ HWY_API V CopySignToAbs(const V abs, const V sign) {
+ // ------------------------------ Add
+ 
+ namespace detail {
+-HWY_SVE_FOREACH_UI(HWY_SVE_RETV_ARGVS, Add, add_vx)
+-HWY_SVE_FOREACH_F(HWY_SVE_RETV_ARGVS, Add, fadd_vf)
++HWY_SVE_FOREACH(HWY_SVE_RETV_ARGPVN, AddN, add_n)
+ }  // namespace detail
+ 
+-HWY_SVE_FOREACH_UI(HWY_SVE_RETV_ARGVV, Add, add)
+-HWY_SVE_FOREACH_F(HWY_SVE_RETV_ARGVV, Add, fadd)
++HWY_SVE_FOREACH(HWY_SVE_RETV_ARGPVV, Add, add)
+ 
+ // ------------------------------ Sub
+-HWY_SVE_FOREACH_UI(HWY_SVE_RETV_ARGVV, Sub, sub)
+-HWY_SVE_FOREACH_F(HWY_SVE_RETV_ARGVV, Sub, fsub)
+ 
+-// ------------------------------ SaturatedAdd
++namespace detail {
++// Can't use HWY_SVE_RETV_ARGPVN because caller wants to specify pg.
++#define HWY_SVE_RETV_ARGPVN_MASK(BASE, CHAR, BITS, NAME, OP)                \
++  HWY_API HWY_SVE_V(BASE, BITS)                                             \
++      NAME(svbool_t pg, HWY_SVE_V(BASE, BITS) a, HWY_SVE_T(BASE, BITS) b) { \
++    return sv##OP##_##CHAR##BITS##_z(pg, a, b);                             \
++  }
+ 
+-HWY_SVE_FOREACH_U08(HWY_SVE_RETV_ARGVV, SaturatedAdd, saddu)
+-HWY_SVE_FOREACH_U16(HWY_SVE_RETV_ARGVV, SaturatedAdd, saddu)
++HWY_SVE_FOREACH(HWY_SVE_RETV_ARGPVN_MASK, SubN, sub_n)
++#undef HWY_SVE_RETV_ARGPVN_MASK
++}  // namespace detail
+ 
+-HWY_SVE_FOREACH_I08(HWY_SVE_RETV_ARGVV, SaturatedAdd, sadd)
+-HWY_SVE_FOREACH_I16(HWY_SVE_RETV_ARGVV, SaturatedAdd, sadd)
++HWY_SVE_FOREACH(HWY_SVE_RETV_ARGPVV, Sub, sub)
+ 
+-// ------------------------------ SaturatedSub
++// ------------------------------ SaturatedAdd
+ 
+-HWY_SVE_FOREACH_U08(HWY_SVE_RETV_ARGVV, SaturatedSub, ssubu)
+-HWY_SVE_FOREACH_U16(HWY_SVE_RETV_ARGVV, SaturatedSub, ssubu)
++HWY_SVE_FOREACH_UI08(HWY_SVE_RETV_ARGVV, SaturatedAdd, qadd)
++HWY_SVE_FOREACH_UI16(HWY_SVE_RETV_ARGVV, SaturatedAdd, qadd)
+ 
+-HWY_SVE_FOREACH_I08(HWY_SVE_RETV_ARGVV, SaturatedSub, ssub)
+-HWY_SVE_FOREACH_I16(HWY_SVE_RETV_ARGVV, SaturatedSub, ssub)
++// ------------------------------ SaturatedSub
+ 
+-// ------------------------------ AverageRound
++HWY_SVE_FOREACH_UI08(HWY_SVE_RETV_ARGVV, SaturatedSub, qsub)
++HWY_SVE_FOREACH_UI16(HWY_SVE_RETV_ARGVV, SaturatedSub, qsub)
+ 
+-// TODO(janwas): check vxrm rounding mode
+-HWY_SVE_FOREACH_U08(HWY_SVE_RETV_ARGVV, AverageRound, aaddu)
+-HWY_SVE_FOREACH_U16(HWY_SVE_RETV_ARGVV, AverageRound, aaddu)
++// ------------------------------ AbsDiff
++HWY_SVE_FOREACH_IF(HWY_SVE_RETV_ARGPVV, AbsDiff, abd)
+ 
+ // ------------------------------ ShiftLeft[Same]
+ 
+-// Intrinsics do not define .vi forms, so use .vx instead.
+-#define HWY_SVE_SHIFT(BASE, CHAR, BITS, NAME, OP)                  \
+-  template <int kBits>                                             \
+-  HWY_API HWY_SVE_V(BASE, BITS) NAME(HWY_SVE_V(BASE, BITS) v) {    \
+-    return v##OP##_vx_##CHAR##BITS(v, kBits);                      \
+-  }                                                                \
+-  HWY_API HWY_SVE_V(BASE, BITS)                                    \
+-      NAME##Same(HWY_SVE_V(BASE, BITS) v, int bits) {              \
+-    return v##OP##_vx_##CHAR##BITS(v, static_cast<uint8_t>(bits)); \
++#define HWY_SVE_SHIFT_N(BASE, CHAR, BITS, NAME, OP)                     \
++  template <int kBits>                                                  \
++  HWY_API HWY_SVE_V(BASE, BITS) NAME(HWY_SVE_V(BASE, BITS) v) {         \
++    return sv##OP##_##CHAR##BITS##_x(HWY_SVE_PTRUE(BITS), v, kBits);    \
++  }                                                                     \
++  HWY_API HWY_SVE_V(BASE, BITS)                                         \
++      NAME##Same(HWY_SVE_V(BASE, BITS) v, HWY_SVE_T(uint, BITS) bits) { \
++    return sv##OP##_##CHAR##BITS##_x(HWY_SVE_PTRUE(BITS), v, bits);     \
+   }
+ 
+-HWY_SVE_FOREACH_UI(HWY_SVE_SHIFT, ShiftLeft, sll)
++HWY_SVE_FOREACH_UI(HWY_SVE_SHIFT_N, ShiftLeft, lsl_n)
+ 
+ // ------------------------------ ShiftRight[Same]
+ 
+-HWY_SVE_FOREACH_U(HWY_SVE_SHIFT, ShiftRight, srl)
+-HWY_SVE_FOREACH_I(HWY_SVE_SHIFT, ShiftRight, sra)
+-
+-#undef HWY_SVE_SHIFT
++HWY_SVE_FOREACH_U(HWY_SVE_SHIFT_N, ShiftRight, lsr_n)
++HWY_SVE_FOREACH_I(HWY_SVE_SHIFT_N, ShiftRight, asr_n)
+ 
+-// ------------------------------ Shl
+-#define HWY_SVE_SHIFT_VV(BASE, CHAR, BITS, NAME, OP)              \
+-  HWY_API HWY_SVE_V(BASE, BITS)                                   \
+-      NAME(HWY_SVE_V(BASE, BITS) v, HWY_SVE_V(BASE, BITS) bits) { \
+-    return v##OP##_vv_##CHAR##BITS(v, bits);                      \
+-  }
++#undef HWY_SVE_SHIFT_N
+ 
+-HWY_SVE_FOREACH_U(HWY_SVE_SHIFT_VV, Shl, sll)
++// ------------------------------ Shl/r
+ 
+-#define HWY_SVE_SHIFT_II(BASE, CHAR, BITS, NAME, OP)                    \
+-  HWY_API HWY_SVE_V(BASE, BITS)                                         \
+-      NAME(HWY_SVE_V(BASE, BITS) v, HWY_SVE_V(BASE, BITS) bits) {       \
+-    return v##OP##_vv_##CHAR##BITS(v, detail::BitCastToUnsigned(bits)); \
++#define HWY_SVE_SHIFT(BASE, CHAR, BITS, NAME, OP)                          \
++  HWY_API HWY_SVE_V(BASE, BITS)                                            \
++      NAME(HWY_SVE_V(BASE, BITS) v, HWY_SVE_V(BASE, BITS) bits) {          \
++    using TU = HWY_SVE_T(uint, BITS);                                      \
++    return sv##OP##_##CHAR##BITS##_x(                                      \
++        HWY_SVE_PTRUE(BITS), v, BitCast(Simd<TU, HWY_LANES(TU)>(), bits)); \
+   }
+ 
+-HWY_SVE_FOREACH_I(HWY_SVE_SHIFT_II, Shl, sll)
+-
+-// ------------------------------ Shr
++HWY_SVE_FOREACH_UI(HWY_SVE_SHIFT, Shl, lsl)
+ 
+-HWY_SVE_FOREACH_U(HWY_SVE_SHIFT_VV, Shr, srl)
+-HWY_SVE_FOREACH_I(HWY_SVE_SHIFT_II, Shr, sra)
++HWY_SVE_FOREACH_U(HWY_SVE_SHIFT, Shr, lsr)
++HWY_SVE_FOREACH_I(HWY_SVE_SHIFT, Shr, asr)
+ 
+-#undef HWY_SVE_SHIFT_II
+-#undef HWY_SVE_SHIFT_VV
+-
+-// ------------------------------ Min
++#undef HWY_SVE_SHIFT
+ 
+-HWY_SVE_FOREACH_U(HWY_SVE_RETV_ARGVV, Min, minu)
+-HWY_SVE_FOREACH_I(HWY_SVE_RETV_ARGVV, Min, min)
+-HWY_SVE_FOREACH_F(HWY_SVE_RETV_ARGVV, Min, fmin)
++// ------------------------------ Min/Max
+ 
+-// ------------------------------ Max
++HWY_SVE_FOREACH_UI(HWY_SVE_RETV_ARGPVV, Min, min)
++HWY_SVE_FOREACH_UI(HWY_SVE_RETV_ARGPVV, Max, max)
++HWY_SVE_FOREACH_F(HWY_SVE_RETV_ARGPVV, Min, minnm)
++HWY_SVE_FOREACH_F(HWY_SVE_RETV_ARGPVV, Max, maxnm)
+ 
+ namespace detail {
+-
+-HWY_SVE_FOREACH_U(HWY_SVE_RETV_ARGVS, Max, maxu_vx)
+-HWY_SVE_FOREACH_I(HWY_SVE_RETV_ARGVS, Max, max_vx)
+-HWY_SVE_FOREACH_F(HWY_SVE_RETV_ARGVS, Max, fmax_vf)
+-
++HWY_SVE_FOREACH_UI(HWY_SVE_RETV_ARGPVN, MinN, min_n)
++HWY_SVE_FOREACH_UI(HWY_SVE_RETV_ARGPVN, MaxN, max_n)
+ }  // namespace detail
+ 
+-HWY_SVE_FOREACH_U(HWY_SVE_RETV_ARGVV, Max, maxu)
+-HWY_SVE_FOREACH_I(HWY_SVE_RETV_ARGVV, Max, max)
+-HWY_SVE_FOREACH_F(HWY_SVE_RETV_ARGVV, Max, fmax)
+-
+ // ------------------------------ Mul
+-
+-HWY_SVE_FOREACH_UI16(HWY_SVE_RETV_ARGVV, Mul, mul)
+-HWY_SVE_FOREACH_UI32(HWY_SVE_RETV_ARGVV, Mul, mul)
+-HWY_SVE_FOREACH_F(HWY_SVE_RETV_ARGVV, Mul, fmul)
++HWY_SVE_FOREACH_UI16(HWY_SVE_RETV_ARGPVV, Mul, mul)
++HWY_SVE_FOREACH_UIF3264(HWY_SVE_RETV_ARGPVV, Mul, mul)
+ 
+ // ------------------------------ MulHigh
+-
+-HWY_SVE_FOREACH_U16(HWY_SVE_RETV_ARGVV, MulHigh, mulhu)
+-HWY_SVE_FOREACH_I16(HWY_SVE_RETV_ARGVV, MulHigh, mulh)
++HWY_SVE_FOREACH_UI16(HWY_SVE_RETV_ARGPVV, MulHigh, mulh)
++namespace detail {
++HWY_SVE_FOREACH_UI32(HWY_SVE_RETV_ARGPVV, MulHigh, mulh)
++HWY_SVE_FOREACH_U64(HWY_SVE_RETV_ARGPVV, MulHigh, mulh)
++}  // namespace detail
+ 
+ // ------------------------------ Div
+-
+-HWY_SVE_FOREACH_F(HWY_SVE_RETV_ARGVV, Div, fdiv)
++HWY_SVE_FOREACH_F(HWY_SVE_RETV_ARGPVV, Div, div)
+ 
+ // ------------------------------ ApproximateReciprocal
+-
+-// TODO(janwas): not yet supported in intrinsics
+-template <class V>
+-HWY_API V ApproximateReciprocal(const V v) {
+-  return Set(DFromV<V>(), 1) / v;
+-}
+-// HWY_SVE_FOREACH_F32(HWY_SVE_RETV_ARGV, ApproximateReciprocal, frece7)
++HWY_SVE_FOREACH_F32(HWY_SVE_RETV_ARGV, ApproximateReciprocal, recpe)
+ 
+ // ------------------------------ Sqrt
+-HWY_SVE_FOREACH_F(HWY_SVE_RETV_ARGV, Sqrt, fsqrt)
++HWY_SVE_FOREACH_F(HWY_SVE_RETV_ARGPV, Sqrt, sqrt)
+ 
+ // ------------------------------ ApproximateReciprocalSqrt
+-
+-// TODO(janwas): not yet supported in intrinsics
+-template <class V>
+-HWY_API V ApproximateReciprocalSqrt(const V v) {
+-  return ApproximateReciprocal(Sqrt(v));
+-}
+-// HWY_SVE_FOREACH_F32(HWY_SVE_RETV_ARGV, ApproximateReciprocalSqrt, frsqrte7)
++HWY_SVE_FOREACH_F32(HWY_SVE_RETV_ARGV, ApproximateReciprocalSqrt, rsqrte)
+ 
+ // ------------------------------ MulAdd
+-// Note: op is still named vv, not vvv.
+-#define HWY_SVE_FMA(BASE, CHAR, BITS, NAME, OP)                \
+-  HWY_API HWY_SVE_V(BASE, BITS)                                \
+-      NAME(HWY_SVE_V(BASE, BITS) mul, HWY_SVE_V(BASE, BITS) x, \
+-           HWY_SVE_V(BASE, BITS) add) {                        \
+-    return v##OP##_vv_##CHAR##BITS(add, mul, x);               \
++#define HWY_SVE_FMA(BASE, CHAR, BITS, NAME, OP)                         \
++  HWY_API HWY_SVE_V(BASE, BITS)                                         \
++      NAME(HWY_SVE_V(BASE, BITS) mul, HWY_SVE_V(BASE, BITS) x,          \
++           HWY_SVE_V(BASE, BITS) add) {                                 \
++    return sv##OP##_##CHAR##BITS##_x(HWY_SVE_PTRUE(BITS), x, mul, add); \
+   }
+ 
+-HWY_SVE_FOREACH_F(HWY_SVE_FMA, MulAdd, fmacc)
++HWY_SVE_FOREACH_F(HWY_SVE_FMA, MulAdd, mad)
+ 
+ // ------------------------------ NegMulAdd
+-HWY_SVE_FOREACH_F(HWY_SVE_FMA, NegMulAdd, fnmsac)
++HWY_SVE_FOREACH_F(HWY_SVE_FMA, NegMulAdd, msb)
+ 
+ // ------------------------------ MulSub
+-HWY_SVE_FOREACH_F(HWY_SVE_FMA, MulSub, fmsac)
++HWY_SVE_FOREACH_F(HWY_SVE_FMA, MulSub, nmsb)
+ 
+ // ------------------------------ NegMulSub
+-HWY_SVE_FOREACH_F(HWY_SVE_FMA, NegMulSub, fnmacc)
++HWY_SVE_FOREACH_F(HWY_SVE_FMA, NegMulSub, nmad)
+ 
+ #undef HWY_SVE_FMA
+ 
+-// ================================================== COMPARE
+-
+-// Comparisons set a mask bit to 1 if the condition is true, else 0. The XX in
+-// vboolXX_t is a power of two divisor for vector bits. SLEN 8 / LMUL 1 = 1/8th
+-// of all bits; SLEN 8 / LMUL 4 = half of all bits.
+-
+-// mask = f(vector, vector)
+-#define HWY_SVE_RETM_ARGVV(BASE, CHAR, BITS, NAME, OP)         \
+-  HWY_API HWY_SVE_M(MLEN)                                      \
+-      NAME(HWY_SVE_V(BASE, BITS) a, HWY_SVE_V(BASE, BITS) b) { \
+-    (void)Lanes(DFromV<decltype(a)>());                        \
+-    return v##OP##_vv_##CHAR##BITS##_b##MLEN(a, b);            \
+-  }
+-
+-// ------------------------------ Eq
+-HWY_SVE_FOREACH_UI(HWY_SVE_RETM_ARGVV, Eq, mseq)
+-HWY_SVE_FOREACH_F(HWY_SVE_RETM_ARGVV, Eq, mfeq)
+-
+-// ------------------------------ Ne
+-HWY_SVE_FOREACH_UI(HWY_SVE_RETM_ARGVV, Ne, msne)
+-HWY_SVE_FOREACH_F(HWY_SVE_RETM_ARGVV, Ne, mfne)
++// ------------------------------ Round etc.
+ 
+-// ------------------------------ Lt
+-HWY_SVE_FOREACH_I(HWY_SVE_RETM_ARGVV, Lt, mslt)
+-HWY_SVE_FOREACH_F(HWY_SVE_RETM_ARGVV, Lt, mflt)
++HWY_SVE_FOREACH_F(HWY_SVE_RETV_ARGPV, Round, rintn)
++HWY_SVE_FOREACH_F(HWY_SVE_RETV_ARGPV, Floor, rintm)
++HWY_SVE_FOREACH_F(HWY_SVE_RETV_ARGPV, Ceil, rintp)
++HWY_SVE_FOREACH_F(HWY_SVE_RETV_ARGPV, Trunc, rintz)
+ 
+-// ------------------------------ Gt
++// ================================================== MASK
+ 
+-template <class V>
+-HWY_API auto Gt(const V a, const V b) -> decltype(Lt(a, b)) {
+-  return Lt(b, a);
++// ------------------------------ RebindMask
++template <class D, typename MFrom>
++HWY_API svbool_t RebindMask(const D /*d*/, const MFrom mask) {
++  return mask;
+ }
+ 
+-// ------------------------------ Le
+-HWY_SVE_FOREACH_F(HWY_SVE_RETM_ARGVV, Le, mfle)
+-
+-#undef HWY_SVE_RETM_ARGVV
+-
+-// ------------------------------ Ge
++// ------------------------------ Mask logical
+ 
+-template <class V>
+-HWY_API auto Ge(const V a, const V b) -> decltype(Le(a, b)) {
+-  return Le(b, a);
++HWY_API svbool_t Not(svbool_t m) {
++  // We don't know the lane type, so assume 8-bit. For larger types, this will
++  // de-canonicalize the predicate, i.e. set bits to 1 even though they do not
++  // correspond to the lowest byte in the lane. Per ARM, such bits are ignored.
++  return svnot_b_z(HWY_SVE_PTRUE(8), m);
+ }
+-
+-// ------------------------------ TestBit
+-
+-template <class V>
+-HWY_API auto TestBit(const V a, const V bit) -> decltype(Eq(a, bit)) {
+-  return Ne(And(a, bit), Zero(DFromV<V>()));
++HWY_API svbool_t And(svbool_t a, svbool_t b) {
++  return svand_b_z(b, b, a);  // same order as AndNot for consistency
++}
++HWY_API svbool_t AndNot(svbool_t a, svbool_t b) {
++  return svbic_b_z(b, b, a);  // reversed order like NEON
++}
++HWY_API svbool_t Or(svbool_t a, svbool_t b) {
++  return svsel_b(a, a, b);  // a ? true : b
++}
++HWY_API svbool_t Xor(svbool_t a, svbool_t b) {
++  return svsel_b(a, svnand_b_z(a, a, b), b);  // a ? !(a & b) : b.
+ }
+ 
+-// ------------------------------ Not
++// ------------------------------ CountTrue
+ 
+-// mask = f(mask)
+-#define HWY_SVE_RETM_ARGM(MLEN, NAME, OP)           \
+-  HWY_API HWY_SVE_M(MLEN) NAME(HWY_SVE_M(MLEN) m) { \
+-    return vm##OP##_m_b##MLEN(m);                   \
++#define HWY_SVE_COUNT_TRUE(BASE, CHAR, BITS, NAME, OP)          \
++  template <size_t N>                                           \
++  HWY_API size_t NAME(HWY_SVE_D(BASE, BITS, N) d, svbool_t m) { \
++    return sv##OP##_b##BITS(detail::Mask(d), m);                \
+   }
+ 
+-HWY_SVE_FOREACH_B(HWY_SVE_RETM_ARGM, Not, not )
+-
+-#undef HWY_SVE_RETM_ARGM
++HWY_SVE_FOREACH(HWY_SVE_COUNT_TRUE, CountTrue, cntp)
++#undef HWY_SVE_COUNT_TRUE
+ 
+-// ------------------------------ And
++// For 16-bit Compress: full vector, not limited to SV_POW2.
++namespace detail {
+ 
+-// mask = f(mask_a, mask_b) (note arg2,arg1 order!)
+-#define HWY_SVE_RETM_ARGMM(MLEN, NAME, OP)                             \
+-  HWY_API HWY_SVE_M(MLEN) NAME(HWY_SVE_M(MLEN) a, HWY_SVE_M(MLEN) b) { \
+-    return vm##OP##_mm_b##MLEN(b, a);                                  \
++#define HWY_SVE_COUNT_TRUE_FULL(BASE, CHAR, BITS, NAME, OP)     \
++  template <size_t N>                                           \
++  HWY_API size_t NAME(HWY_SVE_D(BASE, BITS, N) d, svbool_t m) { \
++    return sv##OP##_b##BITS(svptrue_b##BITS(), m);              \
+   }
+ 
+-HWY_SVE_FOREACH_B(HWY_SVE_RETM_ARGMM, And, and)
++HWY_SVE_FOREACH(HWY_SVE_COUNT_TRUE_FULL, CountTrueFull, cntp)
++#undef HWY_SVE_COUNT_TRUE_FULL
+ 
+-// ------------------------------ AndNot
+-HWY_SVE_FOREACH_B(HWY_SVE_RETM_ARGMM, AndNot, andnot)
++}  // namespace detail
+ 
+-// ------------------------------ Or
+-HWY_SVE_FOREACH_B(HWY_SVE_RETM_ARGMM, Or, or)
++// ------------------------------ AllFalse
++template <typename T, size_t N>
++HWY_API bool AllFalse(Simd<T, N> d, svbool_t m) {
++  return !svptest_any(detail::Mask(d), m);
++}
+ 
+-// ------------------------------ Xor
+-HWY_SVE_FOREACH_B(HWY_SVE_RETM_ARGMM, Xor, xor)
++// ------------------------------ AllTrue
++template <typename T, size_t N>
++HWY_API bool AllTrue(Simd<T, N> d, svbool_t m) {
++  return CountTrue(d, m) == Lanes(d);
++}
+ 
+-#undef HWY_SVE_RETM_ARGMM
++// ------------------------------ FindFirstTrue
++template <typename T, size_t N>
++HWY_API intptr_t FindFirstTrue(Simd<T, N> d, svbool_t m) {
++  return AllFalse(d, m) ? -1 : CountTrue(d, svbrkb_b_z(detail::Mask(d), m));
++}
+ 
+ // ------------------------------ IfThenElse
+-#define HWY_SVE_IF_THEN_ELSE(BASE, CHAR, BITS, NAME, OP) \
+-  HWY_API HWY_SVE_V(BASE, BITS)                          \
+-      NAME(HWY_SVE_M(MLEN) m, HWY_SVE_V(BASE, BITS) yes, \
+-           HWY_SVE_V(BASE, BITS) no) {                   \
+-    return v##OP##_vvm_##CHAR##BITS(m, no, yes);         \
++#define HWY_SVE_IF_THEN_ELSE(BASE, CHAR, BITS, NAME, OP)                      \
++  HWY_API HWY_SVE_V(BASE, BITS)                                               \
++      NAME(svbool_t m, HWY_SVE_V(BASE, BITS) yes, HWY_SVE_V(BASE, BITS) no) { \
++    return sv##OP##_##CHAR##BITS(m, yes, no);                                 \
+   }
+ 
+-HWY_SVE_FOREACH(HWY_SVE_IF_THEN_ELSE, IfThenElse, merge)
+-
++HWY_SVE_FOREACH(HWY_SVE_IF_THEN_ELSE, IfThenElse, sel)
+ #undef HWY_SVE_IF_THEN_ELSE
+-// ------------------------------ IfThenElseZero
+ 
++// ------------------------------ IfThenElseZero
+ template <class M, class V>
+ HWY_API V IfThenElseZero(const M mask, const V yes) {
+   return IfThenElse(mask, yes, Zero(DFromV<V>()));
+ }
+ 
+ // ------------------------------ IfThenZeroElse
+-
+ template <class M, class V>
+ HWY_API V IfThenZeroElse(const M mask, const V no) {
+   return IfThenElse(mask, Zero(DFromV<V>()), no);
+ }
+ 
+-// ------------------------------ MaskFromVec
++// ================================================== COMPARE
++
++// mask = f(vector, vector)
++#define HWY_SVE_COMPARE(BASE, CHAR, BITS, NAME, OP)                         \
++  HWY_API svbool_t NAME(HWY_SVE_V(BASE, BITS) a, HWY_SVE_V(BASE, BITS) b) { \
++    return sv##OP##_##CHAR##BITS(HWY_SVE_PTRUE(BITS), a, b);                \
++  }
++#define HWY_SVE_COMPARE_N(BASE, CHAR, BITS, NAME, OP)                       \
++  HWY_API svbool_t NAME(HWY_SVE_V(BASE, BITS) a, HWY_SVE_T(BASE, BITS) b) { \
++    return sv##OP##_##CHAR##BITS(HWY_SVE_PTRUE(BITS), a, b);                \
++  }
++
++// ------------------------------ Eq
++HWY_SVE_FOREACH(HWY_SVE_COMPARE, Eq, cmpeq)
++
++// ------------------------------ Ne
++HWY_SVE_FOREACH(HWY_SVE_COMPARE, Ne, cmpne)
++
++// ------------------------------ Lt
++HWY_SVE_FOREACH_IF(HWY_SVE_COMPARE, Lt, cmplt)
++namespace detail {
++HWY_SVE_FOREACH_IF(HWY_SVE_COMPARE_N, LtN, cmplt_n)
++}  // namespace detail
++
++// ------------------------------ Le
++HWY_SVE_FOREACH_F(HWY_SVE_COMPARE, Le, cmple)
++
++#undef HWY_SVE_COMPARE
++#undef HWY_SVE_COMPARE_N
++
++// ------------------------------ Gt/Ge (swapped order)
+ 
+ template <class V>
+-HWY_API auto MaskFromVec(const V v) -> decltype(Eq(v, v)) {
+-  return Ne(v, Zero(DFromV<V>()));
++HWY_API svbool_t Gt(const V a, const V b) {
++  return Lt(b, a);
++}
++template <class V>
++HWY_API svbool_t Ge(const V a, const V b) {
++  return Le(b, a);
+ }
+ 
+-template <class D>
+-using MFromD = decltype(MaskFromVec(Zero(D())));
++// ------------------------------ TestBit
++template <class V>
++HWY_API svbool_t TestBit(const V a, const V bit) {
++  return Ne(And(a, bit), Zero(DFromV<V>()));
++}
+ 
+-template <class D, typename MFrom>
+-HWY_API MFromD<D> RebindMask(const D /*d*/, const MFrom mask) {
+-  // No need to check lane size/LMUL are the same: if not, casting MFrom to
+-  // MFromD<D> would fail.
+-  return mask;
++// ------------------------------ MaskFromVec (Ne)
++template <class V>
++HWY_API svbool_t MaskFromVec(const V v) {
++  return Ne(v, Zero(DFromV<V>()));
+ }
+ 
+ // ------------------------------ VecFromMask
+ 
+ template <class D, HWY_IF_NOT_FLOAT_D(D)>
+-HWY_API VFromD<D> VecFromMask(const D d, MFromD<D> mask) {
+-  const auto v0 = Zero(d);
+-  return detail::Or(v0, -1, mask, v0);
++HWY_API VFromD<D> VecFromMask(const D d, svbool_t mask) {
++  const auto v0 = Zero(RebindToSigned<decltype(d)>());
++  return BitCast(d, detail::SubN(mask, v0, 1));
+ }
+ 
+ template <class D, HWY_IF_FLOAT_D(D)>
+-HWY_API VFromD<D> VecFromMask(const D d, MFromD<D> mask) {
++HWY_API VFromD<D> VecFromMask(const D d, svbool_t mask) {
+   return BitCast(d, VecFromMask(RebindToUnsigned<D>(), mask));
+ }
+ 
+-// ------------------------------ ZeroIfNegative
+-
+-template <class V>
+-HWY_API V ZeroIfNegative(const V v) {
+-  const auto v0 = Zero(DFromV<V>());
+-  // We already have a zero constant, so avoid IfThenZeroElse.
+-  return IfThenElse(Lt(v, v0), v0, v);
+-}
+-
+-// ------------------------------ BroadcastSignBit
+-
+-template <class V>
+-HWY_API V BroadcastSignBit(const V v) {
+-  return ShiftRight<sizeof(TFromV<V>) * 8 - 1>(v);
+-}
++// ================================================== MEMORY
+ 
+-// ------------------------------ AllFalse
++// ------------------------------ Load/Store/Stream
+ 
+-#define HWY_SVE_ALL_FALSE(MLEN, NAME, OP)          \
+-  HWY_API bool AllFalse(const HWY_SVE_M(MLEN) m) { \
+-    return vfirst_m_b##MLEN(m) < 0;                \
++#define HWY_SVE_LOAD(BASE, CHAR, BITS, NAME, OP)           \
++  template <size_t N>                                      \
++  HWY_API HWY_SVE_V(BASE, BITS)                            \
++      NAME(HWY_SVE_D(BASE, BITS, N) d,                     \
++           const HWY_SVE_T(BASE, BITS) * HWY_RESTRICT p) { \
++    return sv##OP##_##CHAR##BITS(detail::Mask(d), p);      \
+   }
+-HWY_SVE_FOREACH_B(HWY_SVE_ALL_FALSE, _, _)
+-#undef HWY_SVE_ALL_FALSE
+-
+-// ------------------------------ AllTrue
+ 
+-#define HWY_SVE_ALL_TRUE(MLEN, NAME, OP)    \
+-  HWY_API bool AllTrue(HWY_SVE_M(MLEN) m) { \
+-    return AllFalse(vmnot_m_b##MLEN(m));    \
++#define HWY_SVE_LOAD_DUP128(BASE, CHAR, BITS, NAME, OP)    \
++  template <size_t N>                                      \
++  HWY_API HWY_SVE_V(BASE, BITS)                            \
++      NAME(HWY_SVE_D(BASE, BITS, N) d,                     \
++           const HWY_SVE_T(BASE, BITS) * HWY_RESTRICT p) { \
++    /* All-true predicate to load all 128 bits. */         \
++    return sv##OP##_##CHAR##BITS(HWY_SVE_PTRUE(8), p);     \
+   }
+-HWY_SVE_FOREACH_B(HWY_SVE_ALL_TRUE, _, _)
+-#undef HWY_SVE_ALL_TRUE
+-
+-// ------------------------------ CountTrue
+-
+-#define HWY_SVE_COUNT_TRUE(MLEN, NAME, OP) \
+-  HWY_API size_t CountTrue(HWY_SVE_M(MLEN) m) { return vpopc_m_b##MLEN(m); }
+-HWY_SVE_FOREACH_B(HWY_SVE_COUNT_TRUE, _, _)
+-#undef HWY_SVE_COUNT_TRUE
+ 
+-// ================================================== MEMORY
++#define HWY_SVE_STORE(BASE, CHAR, BITS, NAME, OP)                        \
++  template <size_t N>                                                    \
++  HWY_API void NAME(HWY_SVE_V(BASE, BITS) v, HWY_SVE_D(BASE, BITS, N) d, \
++                    HWY_SVE_T(BASE, BITS) * HWY_RESTRICT p) {            \
++    sv##OP##_##CHAR##BITS(detail::Mask(d), p, v);                        \
++  }
+ 
+-// ------------------------------ Load
++HWY_SVE_FOREACH(HWY_SVE_LOAD, Load, ld1)
++HWY_SVE_FOREACH(HWY_SVE_LOAD_DUP128, LoadDup128, ld1rq)
++HWY_SVE_FOREACH(HWY_SVE_STORE, Store, st1)
++HWY_SVE_FOREACH(HWY_SVE_STORE, Stream, stnt1)
+ 
+-#define HWY_SVE_LOAD(BASE, CHAR, BITS, NAME, OP)                               \
+-  HWY_API HWY_SVE_V(BASE, BITS) NAME(                                          \
+-      HWY_SVE_D(CHAR, BITS) d, const HWY_SVE_T(BASE, BITS) * HWY_RESTRICT p) { \
+-    (void)Lanes(d);                                                            \
+-    return v##OP##BITS##_v_##CHAR##BITS(p);                                    \
+-  }
+-HWY_SVE_FOREACH(HWY_SVE_LOAD, Load, le)
+ #undef HWY_SVE_LOAD
++#undef HWY_SVE_LOAD_DUP128
++#undef HWY_SVE_STORE
+ 
+-// Partial load
+-template <typename T, size_t N, HWY_IF_LE128(T, N)>
+-HWY_API VFromD<Simd<T, N>> Load(Simd<T, N> d, const T* HWY_RESTRICT p) {
+-  return Load(d, p);
+-}
+-
+-// ------------------------------ LoadU
++// ------------------------------ Load/StoreU
+ 
+-// SVE only requires lane alignment, not natural alignment of the entire vector.
++// SVE only requires lane alignment, not natural alignment of the entire
++// vector.
+ template <class D>
+ HWY_API VFromD<D> LoadU(D d, const TFromD<D>* HWY_RESTRICT p) {
+   return Load(d, p);
+ }
+ 
+-// ------------------------------ Store
+-
+-#define HWY_SVE_RET_ARGVDP(BASE, CHAR, BITS, NAME, OP)                \
+-  HWY_API void NAME(HWY_SVE_V(BASE, BITS) v, HWY_SVE_D(CHAR, BITS) d, \
+-                    HWY_SVE_T(BASE, BITS) * HWY_RESTRICT p) {         \
+-    (void)Lanes(d);                                                   \
+-    return v##OP##BITS##_v_##CHAR##BITS(p, v);                        \
+-  }
+-HWY_SVE_FOREACH(HWY_SVE_RET_ARGVDP, Store, se)
+-#undef HWY_SVE_RET_ARGVDP
+-
+-// ------------------------------ StoreU
+-
+-// SVE only requires lane alignment, not natural alignment of the entire vector.
+ template <class V, class D>
+ HWY_API void StoreU(const V v, D d, TFromD<D>* HWY_RESTRICT p) {
+   Store(v, d, p);
+ }
+ 
+-// ------------------------------ Stream
+-
+-template <class V, class D, typename T>
+-HWY_API void Stream(const V v, D d, T* HWY_RESTRICT aligned) {
+-  Store(v, d, aligned);
+-}
+-
+-// ------------------------------ ScatterOffset
++// ------------------------------ ScatterOffset/Index
+ 
+-#define HWY_SVE_SCATTER(BASE, CHAR, BITS, NAME, OP)                         \
+-  HWY_API void NAME(HWY_SVE_V(BASE, BITS) v, HWY_SVE_D(CHAR, BITS) /* d */, \
+-                    HWY_SVE_T(BASE, BITS) * HWY_RESTRICT base,              \
+-                    HWY_SVE_V(int, BITS) offset) {                          \
+-    return v##OP##ei##BITS##_v_##CHAR##BITS(                                \
+-        base, detail::BitCastToUnsigned(offset), v);                        \
++#define HWY_SVE_SCATTER_OFFSET(BASE, CHAR, BITS, NAME, OP)                   \
++  template <size_t N>                                                        \
++  HWY_API void NAME(HWY_SVE_V(BASE, BITS) v, HWY_SVE_D(BASE, BITS, N) d,     \
++                    HWY_SVE_T(BASE, BITS) * HWY_RESTRICT base,               \
++                    HWY_SVE_V(int, BITS) offset) {                           \
++    sv##OP##_s##BITS##offset_##CHAR##BITS(detail::Mask(d), base, offset, v); \
+   }
+-HWY_SVE_FOREACH(HWY_SVE_SCATTER, ScatterOffset, sx)
+-#undef HWY_SVE_SCATTER
+ 
+-// ------------------------------ ScatterIndex
+-
+-template <class D, HWY_IF_LANE_SIZE_D(D, 4)>
+-HWY_API void ScatterIndex(VFromD<D> v, D d, TFromD<D>* HWY_RESTRICT base,
+-                          const VFromD<RebindToSigned<D>> index) {
+-  return ScatterOffset(v, d, base, ShiftLeft<2>(index));
+-}
++#define HWY_SVE_SCATTER_INDEX(BASE, CHAR, BITS, NAME, OP)                  \
++  template <size_t N>                                                      \
++  HWY_API void NAME(HWY_SVE_V(BASE, BITS) v, HWY_SVE_D(BASE, BITS, N) d,   \
++                    HWY_SVE_T(BASE, BITS) * HWY_RESTRICT base,             \
++                    HWY_SVE_V(int, BITS) index) {                          \
++    sv##OP##_s##BITS##index_##CHAR##BITS(detail::Mask(d), base, index, v); \
++  }
+ 
+-template <class D, HWY_IF_LANE_SIZE_D(D, 8)>
+-HWY_API void ScatterIndex(VFromD<D> v, D d, TFromD<D>* HWY_RESTRICT base,
+-                          const VFromD<RebindToSigned<D>> index) {
+-  return ScatterOffset(v, d, base, ShiftLeft<3>(index));
+-}
++HWY_SVE_FOREACH_UIF3264(HWY_SVE_SCATTER_OFFSET, ScatterOffset, st1_scatter)
++HWY_SVE_FOREACH_UIF3264(HWY_SVE_SCATTER_INDEX, ScatterIndex, st1_scatter)
++#undef HWY_SVE_SCATTER_OFFSET
++#undef HWY_SVE_SCATTER_INDEX
+ 
+-// ------------------------------ GatherOffset
++// ------------------------------ GatherOffset/Index
+ 
+-#define HWY_SVE_GATHER(BASE, CHAR, BITS, NAME, OP)          \
+-  HWY_API HWY_SVE_V(BASE, BITS)                             \
+-      NAME(HWY_SVE_D(CHAR, BITS) /* d */,                   \
+-           const HWY_SVE_T(BASE, BITS) * HWY_RESTRICT base, \
+-           HWY_SVE_V(int, BITS) offset) {                   \
+-    return v##OP##ei##BITS##_v_##CHAR##BITS(                \
+-        base, detail::BitCastToUnsigned(offset));           \
++#define HWY_SVE_GATHER_OFFSET(BASE, CHAR, BITS, NAME, OP)               \
++  template <size_t N>                                                   \
++  HWY_API HWY_SVE_V(BASE, BITS)                                         \
++      NAME(HWY_SVE_D(BASE, BITS, N) d,                                  \
++           const HWY_SVE_T(BASE, BITS) * HWY_RESTRICT base,             \
++           HWY_SVE_V(int, BITS) offset) {                               \
++    return sv##OP##_s##BITS##offset_##CHAR##BITS(detail::Mask(d), base, \
++                                                 offset);               \
++  }
++#define HWY_SVE_GATHER_INDEX(BASE, CHAR, BITS, NAME, OP)                       \
++  template <size_t N>                                                          \
++  HWY_API HWY_SVE_V(BASE, BITS)                                                \
++      NAME(HWY_SVE_D(BASE, BITS, N) d,                                         \
++           const HWY_SVE_T(BASE, BITS) * HWY_RESTRICT base,                    \
++           HWY_SVE_V(int, BITS) index) {                                       \
++    return sv##OP##_s##BITS##index_##CHAR##BITS(detail::Mask(d), base, index); \
+   }
+-HWY_SVE_FOREACH(HWY_SVE_GATHER, GatherOffset, lx)
+-#undef HWY_SVE_GATHER
+-
+-// ------------------------------ GatherIndex
+-
+-template <class D, HWY_IF_LANE_SIZE_D(D, 4)>
+-HWY_API VFromD<D> GatherIndex(D d, const TFromD<D>* HWY_RESTRICT base,
+-                              const VFromD<RebindToSigned<D>> index) {
+-  return GatherOffset(d, base, ShiftLeft<2>(index));
+-}
+ 
+-template <class D, HWY_IF_LANE_SIZE_D(D, 8)>
+-HWY_API VFromD<D> GatherIndex(D d, const TFromD<D>* HWY_RESTRICT base,
+-                              const VFromD<RebindToSigned<D>> index) {
+-  return GatherOffset(d, base, ShiftLeft<3>(index));
+-}
++HWY_SVE_FOREACH_UIF3264(HWY_SVE_GATHER_OFFSET, GatherOffset, ld1_gather)
++HWY_SVE_FOREACH_UIF3264(HWY_SVE_GATHER_INDEX, GatherIndex, ld1_gather)
++#undef HWY_SVE_GATHER_OFFSET
++#undef HWY_SVE_GATHER_INDEX
+ 
+ // ------------------------------ StoreInterleaved3
+ 
+-#define HWY_SVE_STORE3(BASE, CHAR, BITS, NAME, OP)                          \
+-  HWY_API void NAME(HWY_SVE_V(BASE, BITS) a, HWY_SVE_V(BASE, BITS) b,       \
+-                    HWY_SVE_V(BASE, BITS) c, HWY_SVE_D(CHAR, BITS) /* d */, \
+-                    HWY_SVE_T(BASE, BITS) * HWY_RESTRICT unaligned) {       \
+-    const v##BASE##BITS##x3_t triple = vcreate_##CHAR##BITS##x3(a, b, c);   \
+-    return v##OP##e8_v_##CHAR##BITS##x3(unaligned, triple);                 \
++#define HWY_SVE_STORE3(BASE, CHAR, BITS, NAME, OP)                            \
++  template <size_t N>                                                         \
++  HWY_API void NAME(HWY_SVE_V(BASE, BITS) v0, HWY_SVE_V(BASE, BITS) v1,       \
++                    HWY_SVE_V(BASE, BITS) v2, HWY_SVE_D(BASE, BITS, N) d,     \
++                    HWY_SVE_T(BASE, BITS) * HWY_RESTRICT unaligned) {         \
++    const sv##BASE##BITS##x3_t triple = svcreate3##_##CHAR##BITS(v0, v1, v2); \
++    sv##OP##_##CHAR##BITS(detail::Mask(d), unaligned, triple);                \
+   }
+-// Segments are limited to 8 registers, so we can only go up to LMUL=2.
+-HWY_SVE_STORE3(uint, u, 8, 1, 8, StoreInterleaved3, sseg3)
+-HWY_SVE_STORE3(uint, u, 8, 2, 4, StoreInterleaved3, sseg3)
++HWY_SVE_FOREACH_U08(HWY_SVE_STORE3, StoreInterleaved3, st3)
+ 
+ #undef HWY_SVE_STORE3
+ 
+ // ------------------------------ StoreInterleaved4
+ 
+-#define HWY_SVE_STORE4(BASE, CHAR, BITS, NAME, OP)                             \
+-  HWY_API void NAME(HWY_SVE_V(BASE, BITS) v0, HWY_SVE_V(BASE, BITS) v1,        \
+-                    HWY_SVE_V(BASE, BITS) v2, HWY_SVE_V(BASE, BITS) v3,        \
+-                    HWY_SVE_D(CHAR, BITS) /* d */,                             \
+-                    HWY_SVE_T(BASE, BITS) * HWY_RESTRICT aligned) {            \
+-    const v##BASE##BITS##x4_t quad = vcreate_##CHAR##BITS##x4(v0, v1, v2, v3); \
+-    return v##OP##e8_v_##CHAR##BITS##x4(aligned, quad);                        \
++#define HWY_SVE_STORE4(BASE, CHAR, BITS, NAME, OP)                      \
++  template <size_t N>                                                   \
++  HWY_API void NAME(HWY_SVE_V(BASE, BITS) v0, HWY_SVE_V(BASE, BITS) v1, \
++                    HWY_SVE_V(BASE, BITS) v2, HWY_SVE_V(BASE, BITS) v3, \
++                    HWY_SVE_D(BASE, BITS, N) d,                         \
++                    HWY_SVE_T(BASE, BITS) * HWY_RESTRICT unaligned) {   \
++    const sv##BASE##BITS##x4_t quad =                                   \
++        svcreate4##_##CHAR##BITS(v0, v1, v2, v3);                       \
++    sv##OP##_##CHAR##BITS(detail::Mask(d), unaligned, quad);            \
+   }
+-// Segments are limited to 8 registers, so we can only go up to LMUL=2.
+-HWY_SVE_STORE4(uint, u, 8, 1, 8, StoreInterleaved4, sseg4)
+-HWY_SVE_STORE4(uint, u, 8, 2, 4, StoreInterleaved4, sseg4)
++HWY_SVE_FOREACH_U08(HWY_SVE_STORE4, StoreInterleaved4, st4)
+ 
+ #undef HWY_SVE_STORE4
+ 
+ // ================================================== CONVERT
+ 
+-// ------------------------------ PromoteTo U
+-
+-HWY_API Vu16m2 PromoteTo(Du16m2 /* d */, Vu8m1 v) { return vzext_vf2_u16m2(v); }
+-HWY_API Vu16m4 PromoteTo(Du16m4 /* d */, Vu8m2 v) { return vzext_vf2_u16m4(v); }
+-HWY_API Vu16m8 PromoteTo(Du16m8 /* d */, Vu8m4 v) { return vzext_vf2_u16m8(v); }
++// ------------------------------ PromoteTo
++
++// Same sign
++#define HWY_SVE_PROMOTE_TO(BASE, CHAR, BITS, NAME, OP)        \
++  template <size_t N>                                         \
++  HWY_API HWY_SVE_V(BASE, BITS)                               \
++      NAME(HWY_SVE_D(BASE, BITS, N) /* tag */,                \
++           VFromD<Simd<MakeNarrow<HWY_SVE_T(BASE, BITS)>,     \
++                       HWY_LANES(HWY_SVE_T(BASE, BITS)) * 2>> \
++               v) {                                           \
++    return sv##OP##_##CHAR##BITS(v);                          \
++  }
+ 
+-HWY_API Vu32m4 PromoteTo(Du32m4 /* d */, Vu8m1 v) { return vzext_vf4_u32m4(v); }
+-HWY_API Vu32m8 PromoteTo(Du32m8 /* d */, Vu8m2 v) { return vzext_vf4_u32m8(v); }
++HWY_SVE_FOREACH_UI16(HWY_SVE_PROMOTE_TO, PromoteTo, unpklo)
++HWY_SVE_FOREACH_UI32(HWY_SVE_PROMOTE_TO, PromoteTo, unpklo)
++HWY_SVE_FOREACH_UI64(HWY_SVE_PROMOTE_TO, PromoteTo, unpklo)
+ 
+-HWY_API Vu32m2 PromoteTo(Du32m2 /* d */, const Vu16m1 v) {
+-  return vzext_vf2_u32m2(v);
++// 2x
++template <size_t N>
++HWY_API svuint32_t PromoteTo(Simd<uint32_t, N> dto, svuint8_t vfrom) {
++  const RepartitionToWide<DFromV<decltype(vfrom)>> d2;
++  return PromoteTo(dto, PromoteTo(d2, vfrom));
+ }
+-HWY_API Vu32m4 PromoteTo(Du32m4 /* d */, const Vu16m2 v) {
+-  return vzext_vf2_u32m4(v);
++template <size_t N>
++HWY_API svint32_t PromoteTo(Simd<int32_t, N> dto, svint8_t vfrom) {
++  const RepartitionToWide<DFromV<decltype(vfrom)>> d2;
++  return PromoteTo(dto, PromoteTo(d2, vfrom));
+ }
+-HWY_API Vu32m8 PromoteTo(Du32m8 /* d */, const Vu16m4 v) {
+-  return vzext_vf2_u32m8(v);
++template <size_t N>
++HWY_API svuint32_t U32FromU8(svuint8_t v) {
++  return PromoteTo(Simd<uint32_t, N>(), v);
+ }
+ 
+-HWY_API Vu64m2 PromoteTo(Du64m2 /* d */, const Vu32m1 v) {
+-  return vzext_vf2_u64m2(v);
++// Sign change
++template <size_t N>
++HWY_API svint16_t PromoteTo(Simd<int16_t, N> dto, svuint8_t vfrom) {
++  const RebindToUnsigned<decltype(dto)> du;
++  return BitCast(dto, PromoteTo(du, vfrom));
+ }
+-HWY_API Vu64m4 PromoteTo(Du64m4 /* d */, const Vu32m2 v) {
+-  return vzext_vf2_u64m4(v);
++template <size_t N>
++HWY_API svint32_t PromoteTo(Simd<int32_t, N> dto, svuint16_t vfrom) {
++  const RebindToUnsigned<decltype(dto)> du;
++  return BitCast(dto, PromoteTo(du, vfrom));
+ }
+-HWY_API Vu64m8 PromoteTo(Du64m8 /* d */, const Vu32m4 v) {
+-  return vzext_vf2_u64m8(v);
++template <size_t N>
++HWY_API svint32_t PromoteTo(Simd<int32_t, N> dto, svuint8_t vfrom) {
++  const Repartition<uint16_t, DFromV<decltype(vfrom)>> du16;
++  const Repartition<int16_t, decltype(du16)> di16;
++  return PromoteTo(dto, BitCast(di16, PromoteTo(du16, vfrom)));
+ }
+ 
++// ------------------------------ PromoteTo F
++
+ template <size_t N>
+-HWY_API VFromD<Simd<int16_t, N>> PromoteTo(Simd<int16_t, N> d,
+-                                           VFromD<Simd<uint8_t, N>> v) {
+-  return BitCast(d, PromoteTo(Simd<uint16_t, N>(), v));
++HWY_API svfloat32_t PromoteTo(Simd<float32_t, N> /* d */, const svfloat16_t v) {
++  return svcvt_f32_f16_x(detail::PTrue(Simd<float16_t, N>()), v);
+ }
+ 
+ template <size_t N>
+-HWY_API VFromD<Simd<int32_t, N>> PromoteTo(Simd<int32_t, N> d,
+-                                           VFromD<Simd<uint8_t, N>> v) {
+-  return BitCast(d, PromoteTo(Simd<uint32_t, N>(), v));
++HWY_API svfloat64_t PromoteTo(Simd<float64_t, N> /* d */, const svfloat32_t v) {
++  return svcvt_f64_f32_x(detail::PTrue(Simd<float32_t, N>()), v);
+ }
+ 
+ template <size_t N>
+-HWY_API VFromD<Simd<int32_t, N>> PromoteTo(Simd<int32_t, N> d,
+-                                           VFromD<Simd<uint16_t, N>> v) {
+-  return BitCast(d, PromoteTo(Simd<uint32_t, N>(), v));
++HWY_API svfloat64_t PromoteTo(Simd<float64_t, N> /* d */, const svint32_t v) {
++  return svcvt_f64_s32_x(detail::PTrue(Simd<int32_t, N>()), v);
+ }
+ 
+-// ------------------------------ PromoteTo I
+-
+-HWY_API Vi16m2 PromoteTo(Di16m2 /* d */, Vi8m1 v) { return vsext_vf2_i16m2(v); }
+-HWY_API Vi16m4 PromoteTo(Di16m4 /* d */, Vi8m2 v) { return vsext_vf2_i16m4(v); }
+-HWY_API Vi16m8 PromoteTo(Di16m8 /* d */, Vi8m4 v) { return vsext_vf2_i16m8(v); }
+-
+-HWY_API Vi32m4 PromoteTo(Di32m4 /* d */, Vi8m1 v) { return vsext_vf4_i32m4(v); }
+-HWY_API Vi32m8 PromoteTo(Di32m8 /* d */, Vi8m2 v) { return vsext_vf4_i32m8(v); }
++// For 16-bit Compress
++namespace detail {
++HWY_SVE_FOREACH_UI32(HWY_SVE_PROMOTE_TO, PromoteUpperTo, unpkhi)
++#undef HWY_SVE_PROMOTE_TO
+ 
+-HWY_API Vi32m2 PromoteTo(Di32m2 /* d */, const Vi16m1 v) {
+-  return vsext_vf2_i32m2(v);
+-}
+-HWY_API Vi32m4 PromoteTo(Di32m4 /* d */, const Vi16m2 v) {
+-  return vsext_vf2_i32m4(v);
+-}
+-HWY_API Vi32m8 PromoteTo(Di32m8 /* d */, const Vi16m4 v) {
+-  return vsext_vf2_i32m8(v);
++template <size_t N>
++HWY_API svfloat32_t PromoteUpperTo(Simd<float, N> df, const svfloat16_t v) {
++  const RebindToUnsigned<decltype(df)> du;
++  const RepartitionToNarrow<decltype(du)> dn;
++  return BitCast(df, PromoteUpperTo(du, BitCast(dn, v)));
+ }
+ 
+-HWY_API Vi64m2 PromoteTo(Di64m2 /* d */, const Vi32m1 v) {
+-  return vsext_vf2_i64m2(v);
+-}
+-HWY_API Vi64m4 PromoteTo(Di64m4 /* d */, const Vi32m2 v) {
+-  return vsext_vf2_i64m4(v);
+-}
+-HWY_API Vi64m8 PromoteTo(Di64m8 /* d */, const Vi32m4 v) {
+-  return vsext_vf2_i64m8(v);
+-}
++}  // namespace detail
+ 
+-// ------------------------------ PromoteTo F
++// ------------------------------ DemoteTo U
+ 
+-HWY_API Vf32m2 PromoteTo(Df32m2 /* d */, const Vf16m1 v) {
+-  return vfwcvt_f_f_v_f32m2(v);
+-}
+-HWY_API Vf32m4 PromoteTo(Df32m4 /* d */, const Vf16m2 v) {
+-  return vfwcvt_f_f_v_f32m4(v);
+-}
+-HWY_API Vf32m8 PromoteTo(Df32m8 /* d */, const Vf16m4 v) {
+-  return vfwcvt_f_f_v_f32m8(v);
+-}
++namespace detail {
+ 
+-HWY_API Vf64m2 PromoteTo(Df64m2 /* d */, const Vf32m1 v) {
+-  return vfwcvt_f_f_v_f64m2(v);
+-}
+-HWY_API Vf64m4 PromoteTo(Df64m4 /* d */, const Vf32m2 v) {
+-  return vfwcvt_f_f_v_f64m4(v);
+-}
+-HWY_API Vf64m8 PromoteTo(Df64m8 /* d */, const Vf32m4 v) {
+-  return vfwcvt_f_f_v_f64m8(v);
++// Saturates unsigned vectors to half/quarter-width TN.
++template <typename TN, class VU>
++VU SaturateU(VU v) {
++  return detail::MinN(v, static_cast<TFromV<VU>>(LimitsMax<TN>()));
+ }
+ 
+-HWY_API Vf64m2 PromoteTo(Df64m2 /* d */, const Vi32m1 v) {
+-  return vfwcvt_f_x_v_f64m2(v);
+-}
+-HWY_API Vf64m4 PromoteTo(Df64m4 /* d */, const Vi32m2 v) {
+-  return vfwcvt_f_x_v_f64m4(v);
+-}
+-HWY_API Vf64m8 PromoteTo(Df64m8 /* d */, const Vi32m4 v) {
+-  return vfwcvt_f_x_v_f64m8(v);
++// Saturates unsigned vectors to half/quarter-width TN.
++template <typename TN, class VI>
++VI SaturateI(VI v) {
++  const DFromV<VI> di;
++  return detail::MinN(detail::MaxN(v, LimitsMin<TN>()), LimitsMax<TN>());
+ }
+ 
+-// ------------------------------ DemoteTo U
+-
+-// First clamp negative numbers to zero to match x86 packus.
+-HWY_API Vu16m1 DemoteTo(Du16m1 /* d */, const Vi32m2 v) {
+-  return vnclipu_wx_u16m1(detail::BitCastToUnsigned(detail::Max(v, 0)), 0);
+-}
+-HWY_API Vu16m2 DemoteTo(Du16m2 /* d */, const Vi32m4 v) {
+-  return vnclipu_wx_u16m2(detail::BitCastToUnsigned(detail::Max(v, 0)), 0);
+-}
+-HWY_API Vu16m4 DemoteTo(Du16m4 /* d */, const Vi32m8 v) {
+-  return vnclipu_wx_u16m4(detail::BitCastToUnsigned(detail::Max(v, 0)), 0);
+-}
++}  // namespace detail
+ 
+-HWY_API Vu8m1 DemoteTo(Du8m1 /* d */, const Vi32m4 v) {
+-  return vnclipu_wx_u8m1(DemoteTo(Du16m2(), v), 0);
+-}
+-HWY_API Vu8m2 DemoteTo(Du8m2 /* d */, const Vi32m8 v) {
+-  return vnclipu_wx_u8m2(DemoteTo(Du16m4(), v), 0);
++template <size_t N>
++HWY_API svuint8_t DemoteTo(Simd<uint8_t, N> dn, const svint16_t v) {
++  const DFromV<decltype(v)> di;
++  const RebindToUnsigned<decltype(di)> du;
++  using TN = TFromD<decltype(dn)>;
++  // First clamp negative numbers to zero and cast to unsigned.
++  const svuint16_t clamped = BitCast(du, Max(Zero(di), v));
++  // Saturate to unsigned-max and halve the width.
++  const svuint8_t vn = BitCast(dn, detail::SaturateU<TN>(clamped));
++  return svuzp1_u8(vn, vn);
+ }
+ 
+-HWY_API Vu8m1 DemoteTo(Du8m1 /* d */, const Vi16m2 v) {
+-  return vnclipu_wx_u8m1(detail::BitCastToUnsigned(detail::Max(v, 0)), 0);
+-}
+-HWY_API Vu8m2 DemoteTo(Du8m2 /* d */, const Vi16m4 v) {
+-  return vnclipu_wx_u8m2(detail::BitCastToUnsigned(detail::Max(v, 0)), 0);
+-}
+-HWY_API Vu8m4 DemoteTo(Du8m4 /* d */, const Vi16m8 v) {
+-  return vnclipu_wx_u8m4(detail::BitCastToUnsigned(detail::Max(v, 0)), 0);
++template <size_t N>
++HWY_API svuint16_t DemoteTo(Simd<uint16_t, N> dn, const svint32_t v) {
++  const DFromV<decltype(v)> di;
++  const RebindToUnsigned<decltype(di)> du;
++  using TN = TFromD<decltype(dn)>;
++  // First clamp negative numbers to zero and cast to unsigned.
++  const svuint32_t clamped = BitCast(du, Max(Zero(di), v));
++  // Saturate to unsigned-max and halve the width.
++  const svuint16_t vn = BitCast(dn, detail::SaturateU<TN>(clamped));
++  return svuzp1_u16(vn, vn);
+ }
+ 
+-HWY_API Vu8m1 U8FromU32(const Vu32m4 v) {
+-  return vnclipu_wx_u8m1(vnclipu_wx_u16m2(v, 0), 0);
+-}
+-HWY_API Vu8m2 U8FromU32(const Vu32m8 v) {
+-  return vnclipu_wx_u8m2(vnclipu_wx_u16m4(v, 0), 0);
++template <size_t N>
++HWY_API svuint8_t DemoteTo(Simd<uint8_t, N> dn, const svint32_t v) {
++  const DFromV<decltype(v)> di;
++  const RebindToUnsigned<decltype(di)> du;
++  const RepartitionToNarrow<decltype(du)> d2;
++  using TN = TFromD<decltype(dn)>;
++  // First clamp negative numbers to zero and cast to unsigned.
++  const svuint32_t clamped = BitCast(du, Max(Zero(di), v));
++  // Saturate to unsigned-max and quarter the width.
++  const svuint16_t cast16 = BitCast(d2, detail::SaturateU<TN>(clamped));
++  const svuint8_t x2 = BitCast(dn, svuzp1_u16(cast16, cast16));
++  return svuzp1_u8(x2, x2);
++}
++
++HWY_API svuint8_t U8FromU32(const svuint32_t v) {
++  const DFromV<svuint32_t> du32;
++  const RepartitionToNarrow<decltype(du32)> du16;
++  const RepartitionToNarrow<decltype(du16)> du8;
++
++  const svuint16_t cast16 = BitCast(du16, v);
++  const svuint16_t x2 = svuzp1_u16(cast16, cast16);
++  const svuint8_t cast8 = BitCast(du8, x2);
++  return svuzp1_u8(cast8, cast8);
+ }
+ 
+ // ------------------------------ DemoteTo I
+ 
+-HWY_API Vi8m1 DemoteTo(Di8m1 /* d */, const Vi16m2 v) {
+-  return vnclip_wx_i8m1(v, 0);
+-}
+-HWY_API Vi8m2 DemoteTo(Di8m2 /* d */, const Vi16m4 v) {
+-  return vnclip_wx_i8m2(v, 0);
+-}
+-HWY_API Vi8m4 DemoteTo(Di8m4 /* d */, const Vi16m8 v) {
+-  return vnclip_wx_i8m4(v, 0);
++template <size_t N>
++HWY_API svint8_t DemoteTo(Simd<int8_t, N> dn, const svint16_t v) {
++  const DFromV<decltype(v)> di;
++  using TN = TFromD<decltype(dn)>;
++#if HWY_TARGET == HWY_SVE2
++  const svint8_t vn = BitCast(dn, svqxtnb_s16(v));
++#else
++  const svint8_t vn = BitCast(dn, detail::SaturateI<TN>(v));
++#endif
++  return svuzp1_s8(vn, vn);
+ }
+ 
+-HWY_API Vi16m1 DemoteTo(Di16m1 /* d */, const Vi32m2 v) {
+-  return vnclip_wx_i16m1(v, 0);
+-}
+-HWY_API Vi16m2 DemoteTo(Di16m2 /* d */, const Vi32m4 v) {
+-  return vnclip_wx_i16m2(v, 0);
+-}
+-HWY_API Vi16m4 DemoteTo(Di16m4 /* d */, const Vi32m8 v) {
+-  return vnclip_wx_i16m4(v, 0);
++template <size_t N>
++HWY_API svint16_t DemoteTo(Simd<int16_t, N> dn, const svint32_t v) {
++  const DFromV<decltype(v)> di;
++  using TN = TFromD<decltype(dn)>;
++#if HWY_TARGET == HWY_SVE2
++  const svint16_t vn = BitCast(dn, svqxtnb_s32(v));
++#else
++  const svint16_t vn = BitCast(dn, detail::SaturateI<TN>(v));
++#endif
++  return svuzp1_s16(vn, vn);
+ }
+ 
+-HWY_API Vi8m1 DemoteTo(Di8m1 d, const Vi32m4 v) {
+-  return DemoteTo(d, DemoteTo(Di16m2(), v));
+-}
+-HWY_API Vi8m2 DemoteTo(Di8m2 d, const Vi32m8 v) {
+-  return DemoteTo(d, DemoteTo(Di16m4(), v));
++template <size_t N>
++HWY_API svint8_t DemoteTo(Simd<int8_t, N> dn, const svint32_t v) {
++  const DFromV<decltype(v)> di;
++  using TN = TFromD<decltype(dn)>;
++  const RepartitionToWide<decltype(dn)> d2;
++#if HWY_TARGET == HWY_SVE2
++  const svint16_t cast16 = BitCast(d2, svqxtnb_s16(svqxtnb_s32(v)));
++#else
++  const svint16_t cast16 = BitCast(d2, detail::SaturateI<TN>(v));
++#endif
++  const svint8_t v2 = BitCast(dn, svuzp1_s16(cast16, cast16));
++  return BitCast(dn, svuzp1_s8(v2, v2));
+ }
+ 
+ // ------------------------------ DemoteTo F
+ 
+-HWY_API Vf16m1 DemoteTo(Df16m1 /* d */, const Vf32m2 v) {
+-  return vfncvt_rod_f_f_w_f16m1(v);
+-}
+-HWY_API Vf16m2 DemoteTo(Df16m2 /* d */, const Vf32m4 v) {
+-  return vfncvt_rod_f_f_w_f16m2(v);
+-}
+-HWY_API Vf16m4 DemoteTo(Df16m4 /* d */, const Vf32m8 v) {
+-  return vfncvt_rod_f_f_w_f16m4(v);
++template <size_t N>
++HWY_API svfloat16_t DemoteTo(Simd<float16_t, N> d, const svfloat32_t v) {
++  return svcvt_f16_f32_x(detail::PTrue(d), v);
+ }
+ 
+-HWY_API Vf32m1 DemoteTo(Df32m1 /* d */, const Vf64m2 v) {
+-  return vfncvt_rod_f_f_w_f32m1(v);
+-}
+-HWY_API Vf32m2 DemoteTo(Df32m2 /* d */, const Vf64m4 v) {
+-  return vfncvt_rod_f_f_w_f32m2(v);
+-}
+-HWY_API Vf32m4 DemoteTo(Df32m4 /* d */, const Vf64m8 v) {
+-  return vfncvt_rod_f_f_w_f32m4(v);
++template <size_t N>
++HWY_API svfloat32_t DemoteTo(Simd<float32_t, N> d, const svfloat64_t v) {
++  return svcvt_f32_f64_x(detail::PTrue(d), v);
+ }
+ 
+-HWY_API Vi32m1 DemoteTo(Di32m1 /* d */, const Vf64m2 v) {
+-  return vfncvt_rtz_x_f_w_i32m1(v);
+-}
+-HWY_API Vi32m2 DemoteTo(Di32m2 /* d */, const Vf64m4 v) {
+-  return vfncvt_rtz_x_f_w_i32m2(v);
+-}
+-HWY_API Vi32m4 DemoteTo(Di32m4 /* d */, const Vf64m8 v) {
+-  return vfncvt_rtz_x_f_w_i32m4(v);
++template <size_t N>
++HWY_API svint32_t DemoteTo(Simd<int32_t, N> d, const svfloat64_t v) {
++  return svcvt_s32_f64_x(detail::PTrue(d), v);
+ }
+ 
+ // ------------------------------ ConvertTo F
+ 
+-#define HWY_SVE_CONVERT(BASE, CHAR, BITS, NAME, OP)                      \
+-  HWY_API HWY_SVE_V(BASE, BITS)                                          \
+-      ConvertTo(HWY_SVE_D(CHAR, BITS) /* d */, HWY_SVE_V(int, BITS) v) { \
+-    return vfcvt_f_x_v_f##BITS(v);                                       \
+-  }                                                                      \
+-  /* Truncates (rounds toward zero). */                                  \
+-  HWY_API HWY_SVE_V(int, BITS)                                           \
+-      ConvertTo(HWY_SVE_D(i, BITS) /* d */, HWY_SVE_V(BASE, BITS) v) {   \
+-    return vfcvt_rtz_x_f_v_i##BITS(v);                                   \
+-  }                                                                      \
+-  /* Uses default rounding mode. */                                      \
+-  HWY_API HWY_SVE_V(int, BITS) NearestInt(HWY_SVE_V(BASE, BITS) v) {     \
+-    return vfcvt_x_f_v_i##BITS(v);                                       \
++#define HWY_SVE_CONVERT(BASE, CHAR, BITS, NAME, OP)                     \
++  template <size_t N>                                                   \
++  HWY_API HWY_SVE_V(BASE, BITS)                                         \
++      NAME(HWY_SVE_D(BASE, BITS, N) /* d */, HWY_SVE_V(int, BITS) v) {  \
++    return sv##OP##_##CHAR##BITS##_s##BITS##_x(HWY_SVE_PTRUE(BITS), v); \
++  }                                                                     \
++  /* Truncates (rounds toward zero). */                                 \
++  template <size_t N>                                                   \
++  HWY_API HWY_SVE_V(int, BITS)                                          \
++      NAME(HWY_SVE_D(int, BITS, N) /* d */, HWY_SVE_V(BASE, BITS) v) {  \
++    return sv##OP##_s##BITS##_##CHAR##BITS##_x(HWY_SVE_PTRUE(BITS), v); \
+   }
+ 
+-// API only requires f32 but we provide f64 for internal use (otherwise, it
+-// seems difficult to implement Iota without a _mf2 vector half).
+-HWY_SVE_FOREACH_F(HWY_SVE_CONVERT, _, _)
++// API only requires f32 but we provide f64 for use by Iota.
++HWY_SVE_FOREACH_F(HWY_SVE_CONVERT, ConvertTo, cvt)
+ #undef HWY_SVE_CONVERT
+ 
+-// ================================================== SWIZZLE
++// ------------------------------ NearestInt (Round, ConvertTo)
+ 
+-// ------------------------------ Compress
++template <class VF, class DI = RebindToSigned<DFromV<VF>>>
++HWY_API VFromD<DI> NearestInt(VF v) {
++  // No single instruction, round then truncate.
++  return ConvertTo(DI(), Round(v));
++}
+ 
+-#define HWY_SVE_COMPRESS(BASE, CHAR, BITS, NAME, OP)        \
+-  HWY_API HWY_SVE_V(BASE, BITS)                             \
+-      NAME(HWY_SVE_V(BASE, BITS) v, HWY_SVE_M(MLEN) mask) { \
+-    return v##OP##_vm_##CHAR##BITS(mask, v, v);             \
+-  }
++// ------------------------------ Iota (Add, ConvertTo)
+ 
+-HWY_SVE_FOREACH_UI16(HWY_SVE_COMPRESS, Compress, compress)
+-HWY_SVE_FOREACH_UI32(HWY_SVE_COMPRESS, Compress, compress)
+-HWY_SVE_FOREACH_UI64(HWY_SVE_COMPRESS, Compress, compress)
+-HWY_SVE_FOREACH_F(HWY_SVE_COMPRESS, Compress, compress)
+-#undef HWY_SVE_COMPRESS
++#define HWY_SVE_IOTA(BASE, CHAR, BITS, NAME, OP)                      \
++  template <size_t N>                                                 \
++  HWY_API HWY_SVE_V(BASE, BITS)                                       \
++      NAME(HWY_SVE_D(BASE, BITS, N) d, HWY_SVE_T(BASE, BITS) first) { \
++    return sv##OP##_##CHAR##BITS(first, 1);                           \
++  }
+ 
+-// ------------------------------ CompressStore
++HWY_SVE_FOREACH_UI(HWY_SVE_IOTA, Iota, index)
++#undef HWY_SVE_IOTA
+ 
+-template <class V, class M, class D>
+-HWY_API size_t CompressStore(const V v, const M mask, const D d,
+-                             TFromD<D>* HWY_RESTRICT aligned) {
+-  Store(Compress(v, mask), d, aligned);
+-  return CountTrue(mask);
++template <class D, HWY_IF_FLOAT_D(D)>
++HWY_API VFromD<D> Iota(const D d, TFromD<D> first) {
++  const RebindToSigned<D> di;
++  return detail::AddN(ConvertTo(d, Iota(di, 0)), first);
+ }
+ 
+-// ------------------------------ TableLookupLanes
++// ================================================== COMBINE
+ 
+-template <class D, class DU = RebindToUnsigned<D>>
+-HWY_API VFromD<DU> SetTableIndices(D d, const TFromD<DU>* idx) {
+-#if !defined(NDEBUG) || defined(ADDRESS_SANITIZER)
+-  const size_t N = Lanes(d);
+-  for (size_t i = 0; i < N; ++i) {
+-    HWY_DASSERT(0 <= idx[i] && idx[i] < static_cast<TFromD<DU>>(N));
+-  }
+-#endif
+-  return Load(DU(), idx);
++namespace detail {
++
++template <typename T, size_t N>
++svbool_t MaskLowerHalf(Simd<T, N> d) {
++  return FirstN(d, Lanes(d) / 2);
++}
++template <typename T, size_t N>
++svbool_t MaskUpperHalf(Simd<T, N> d) {
++  // For Splice to work as intended, make sure bits above Lanes(d) are zero.
++  return AndNot(MaskLowerHalf(d), detail::Mask(d));
+ }
+ 
+-// <32bit are not part of Highway API, but used in Broadcast. This limits VLMAX
+-// to 2048! We could instead use vrgatherei16.
+-#define HWY_SVE_TABLE(BASE, CHAR, BITS, NAME, OP)                \
++// Right-shift vector pair by constexpr; can be used to slide down (=N) or up
++// (=Lanes()-N).
++#define HWY_SVE_EXT(BASE, CHAR, BITS, NAME, OP)                  \
++  template <size_t kIndex>                                       \
+   HWY_API HWY_SVE_V(BASE, BITS)                                  \
+-      NAME(HWY_SVE_V(BASE, BITS) v, HWY_SVE_V(uint, BITS) idx) { \
+-    return v##OP##_vv_##CHAR##BITS(v, idx);                      \
++      NAME(HWY_SVE_V(BASE, BITS) hi, HWY_SVE_V(BASE, BITS) lo) { \
++    return sv##OP##_##CHAR##BITS(lo, hi, kIndex);                \
+   }
++HWY_SVE_FOREACH(HWY_SVE_EXT, Ext, ext)
++#undef HWY_SVE_EXT
++
++// Used to slide up / shift whole register left; mask indicates which range
++// to take from lo, and the rest is filled from hi starting at its lowest.
++#define HWY_SVE_SPLICE(BASE, CHAR, BITS, NAME, OP)                         \
++  HWY_API HWY_SVE_V(BASE, BITS) NAME(                                      \
++      HWY_SVE_V(BASE, BITS) hi, HWY_SVE_V(BASE, BITS) lo, svbool_t mask) { \
++    return sv##OP##_##CHAR##BITS(mask, lo, hi);                            \
++  }
++HWY_SVE_FOREACH(HWY_SVE_SPLICE, Splice, splice)
++#undef HWY_SVE_SPLICE
+ 
+-HWY_SVE_FOREACH(HWY_SVE_TABLE, TableLookupLanes, rgather)
+-#undef HWY_SVE_TABLE
+-
+-// ------------------------------ Shuffle01
++}  // namespace detail
+ 
+-template <class V>
+-HWY_API V Shuffle01(const V v) {
+-  using D = DFromV<V>;
+-  static_assert(sizeof(TFromD<D>) == 8, "Defined for 64-bit types");
+-  const auto idx = detail::Xor(detail::Iota0(D()), 1);
+-  return TableLookupLanes(v, idx);
++// ------------------------------ ConcatUpperLower
++template <class D, class V>
++HWY_API V ConcatUpperLower(const D d, const V hi, const V lo) {
++  return IfThenElse(detail::MaskLowerHalf(d), lo, hi);
+ }
+ 
+-// ------------------------------ Shuffle2301
+-
+-template <class V>
+-HWY_API V Shuffle2301(const V v) {
+-  using D = DFromV<V>;
+-  static_assert(sizeof(TFromD<D>) == 4, "Defined for 32-bit types");
+-  const auto idx = detail::Xor(detail::Iota0(D()), 1);
+-  return TableLookupLanes(v, idx);
++// ------------------------------ ConcatLowerLower
++template <class D, class V>
++HWY_API V ConcatLowerLower(const D d, const V hi, const V lo) {
++  return detail::Splice(hi, lo, detail::MaskLowerHalf(d));
+ }
+ 
+-// ------------------------------ Shuffle1032
+-
+-template <class V>
+-HWY_API V Shuffle1032(const V v) {
+-  using D = DFromV<V>;
+-  static_assert(sizeof(TFromD<D>) == 4, "Defined for 32-bit types");
+-  const auto idx = detail::Xor(detail::Iota0(D()), 2);
+-  return TableLookupLanes(v, idx);
++// ------------------------------ ConcatLowerUpper
++template <class D, class V>
++HWY_API V ConcatLowerUpper(const D d, const V hi, const V lo) {
++  return detail::Splice(hi, lo, detail::MaskUpperHalf(d));
+ }
+ 
+-// ------------------------------ Shuffle0123
+-
+-template <class V>
+-HWY_API V Shuffle0123(const V v) {
+-  using D = DFromV<V>;
+-  static_assert(sizeof(TFromD<D>) == 4, "Defined for 32-bit types");
+-  const auto idx = detail::Xor(detail::Iota0(D()), 3);
+-  return TableLookupLanes(v, idx);
++// ------------------------------ ConcatUpperUpper
++template <class D, class V>
++HWY_API V ConcatUpperUpper(const D d, const V hi, const V lo) {
++  const svbool_t mask_upper = detail::MaskUpperHalf(d);
++  const V lo_upper = detail::Splice(lo, lo, mask_upper);
++  return IfThenElse(mask_upper, hi, lo_upper);
+ }
+ 
+-// ------------------------------ Shuffle2103
+-
+-template <class V>
+-HWY_API V Shuffle2103(const V v) {
+-  using D = DFromV<V>;
+-  static_assert(sizeof(TFromD<D>) == 4, "Defined for 32-bit types");
+-  // This shuffle is a rotation. We can compute subtraction modulo 4 (number of
+-  // lanes per 128-bit block) via bitwise ops.
+-  const auto i = detail::Xor(detail::Iota0(D()), 1);
+-  const auto lsb = detail::And(i, 1);
+-  const auto borrow = Add(lsb, lsb);
+-  const auto idx = Xor(i, borrow);
+-  return TableLookupLanes(v, idx);
++// ------------------------------ Combine
++template <class D, class V2>
++HWY_API VFromD<D> Combine(const D d, const V2 hi, const V2 lo) {
++  return ConcatLowerLower(d, hi, lo);
+ }
+ 
+-// ------------------------------ Shuffle0321
++// ------------------------------ ZeroExtendVector
+ 
+-template <class V>
+-HWY_API V Shuffle0321(const V v) {
+-  using D = DFromV<V>;
+-  static_assert(sizeof(TFromD<D>) == 4, "Defined for 32-bit types");
+-  // This shuffle is a rotation. We can compute subtraction modulo 4 (number of
+-  // lanes per 128-bit block) via bitwise ops.
+-  const auto i = detail::Xor(detail::Iota0(D()), 3);
+-  const auto lsb = detail::And(i, 1);
+-  const auto borrow = Add(lsb, lsb);
+-  const auto idx = Xor(i, borrow);
+-  return TableLookupLanes(v, idx);
++template <class D, class V>
++HWY_API V ZeroExtendVector(const D d, const V lo) {
++  return Combine(d, Zero(Half<D>()), lo);
+ }
+ 
+-// ------------------------------ TableLookupBytes
+-
+-namespace detail {
++// ------------------------------ Lower/UpperHalf
+ 
+-// For x86-compatible behaviour mandated by Highway API: TableLookupBytes
+-// offsets are implicitly relative to the start of their 128-bit block.
+-template <class D>
+-constexpr size_t LanesPerBlock(D) {
+-  return 16 / sizeof(TFromD<D>);
++template <class D2, class V>
++HWY_API V LowerHalf(D2 /* tag */, const V v) {
++  return v;
+ }
+ 
+-template <class D, class V>
+-HWY_API V OffsetsOf128BitBlocks(const D d, const V iota0) {
+-  using T = MakeUnsigned<TFromD<D>>;
+-  return detail::And(iota0, static_cast<T>(~(LanesPerBlock(d) - 1)));
+-}
+-
+-}  // namespace detail
+-
+ template <class V>
+-HWY_API V TableLookupBytes(const V v, const V idx) {
+-  using D = DFromV<V>;
+-  const Repartition<uint8_t, D> d8;
+-  const auto offsets128 = detail::OffsetsOf128BitBlocks(d8, detail::Iota0(d8));
+-  const auto idx8 = Add(BitCast(d8, idx), offsets128);
+-  return BitCast(D(), TableLookupLanes(BitCast(d8, v), idx8));
++HWY_API V LowerHalf(const V v) {
++  return v;
+ }
+ 
+-// ------------------------------ Broadcast
+-
+-template <int kLane, class V>
+-HWY_API V Broadcast(const V v) {
+-  const DFromV<V> d;
+-  constexpr size_t kLanesPerBlock = detail::LanesPerBlock(d);
+-  static_assert(0 <= kLane && kLane < kLanesPerBlock, "Invalid lane");
+-  auto idx = detail::OffsetsOf128BitBlocks(d, detail::Iota0(d));
+-  if (kLane != 0) {
+-    idx = detail::Add(idx, kLane);
+-  }
+-  return TableLookupLanes(v, idx);
++template <class D2, class V>
++HWY_API V UpperHalf(const D2 d2, const V v) {
++  return detail::Splice(v, v, detail::MaskUpperHalf(Twice<D2>()));
+ }
+ 
++// ================================================== SWIZZLE
++
+ // ------------------------------ GetLane
+ 
+ #define HWY_SVE_GET_LANE(BASE, CHAR, BITS, NAME, OP)            \
+   HWY_API HWY_SVE_T(BASE, BITS) NAME(HWY_SVE_V(BASE, BITS) v) { \
+-    return v##OP##_s_##CHAR##BITS##_##CHAR##BITS(v);            \
++    return sv##OP##_##CHAR##BITS(detail::PFalse(), v);          \
+   }
+ 
+-HWY_SVE_FOREACH_UI(HWY_SVE_GET_LANE, GetLane, mv_x)
+-HWY_SVE_FOREACH_F(HWY_SVE_GET_LANE, GetLane, fmv_f)
++HWY_SVE_FOREACH(HWY_SVE_GET_LANE, GetLane, lasta)
+ #undef HWY_SVE_GET_LANE
+ 
+-// ------------------------------ ShiftLeftLanes
+-
+-// vector = f(vector, vector, size_t)
+-#define HWY_SVE_SLIDE(BASE, CHAR, BITS, NAME, OP)                           \
+-  HWY_API HWY_SVE_V(BASE, BITS) NAME(                                       \
+-      HWY_SVE_V(BASE, BITS) dst, HWY_SVE_V(BASE, BITS) src, size_t lanes) { \
+-    return v##OP##_vx_##CHAR##BITS(dst, src, lanes);                        \
+-  }
++// ------------------------------ OddEven
+ 
+ namespace detail {
+-HWY_SVE_FOREACH(HWY_SVE_SLIDE, SlideUp, slideup)
++HWY_SVE_FOREACH(HWY_SVE_RETV_ARGVN, Insert, insr_n)
++HWY_SVE_FOREACH(HWY_SVE_RETV_ARGVV, InterleaveEven, trn1)
++HWY_SVE_FOREACH(HWY_SVE_RETV_ARGVV, InterleaveOdd, trn2)
+ }  // namespace detail
+ 
+-template <size_t kLanes, class V>
+-HWY_API V ShiftLeftLanes(const V v) {
+-  using D = DFromV<V>;
+-  const RebindToSigned<D> di;
+-  const auto shifted = detail::SlideUp(v, v, kLanes);
+-  // Match x86 semantics by zeroing lower lanes in 128-bit blocks
+-  constexpr size_t kLanesPerBlock = detail::LanesPerBlock(di);
+-  const auto idx_mod = detail::And(detail::Iota0(di), kLanesPerBlock - 1);
+-  const auto clear = Lt(BitCast(di, idx_mod), Set(di, kLanes));
+-  return IfThenZeroElse(clear, shifted);
++template <class V>
++HWY_API V OddEven(const V odd, const V even) {
++  const auto even_in_odd = detail::Insert(even, 0);
++  return detail::InterleaveOdd(even_in_odd, odd);
+ }
+ 
+-// ------------------------------ ShiftLeftBytes
++// ------------------------------ TableLookupLanes
+ 
+-template <int kBytes, class V>
+-HWY_API V ShiftLeftBytes(const V v) {
+-  using D = DFromV<V>;
+-  const Repartition<uint8_t, D> d8;
+-  Lanes(d8);
+-  return BitCast(D(), ShiftLeftLanes<kBytes>(BitCast(d8, v)));
++template <class D, class DI = RebindToSigned<D>>
++HWY_API VFromD<DI> SetTableIndices(D d, const TFromD<DI>* idx) {
++#if !defined(NDEBUG) || defined(ADDRESS_SANITIZER)
++  const size_t N = Lanes(d);
++  for (size_t i = 0; i < N; ++i) {
++    HWY_DASSERT(0 <= idx[i] && idx[i] < static_cast<TFromD<DI>>(N));
++  }
++#endif
++  return Load(DI(), idx);
+ }
+ 
+-// ------------------------------ ShiftRightLanes
++// <32bit are not part of Highway API, but used in Broadcast.
++#define HWY_SVE_TABLE(BASE, CHAR, BITS, NAME, OP)                             \
++  HWY_API HWY_SVE_V(BASE, BITS)                                               \
++      NAME(HWY_SVE_V(BASE, BITS) v, HWY_SVE_V(int, BITS) idx) {               \
++    const auto idx_u = BitCast(RebindToUnsigned<DFromV<decltype(v)>>(), idx); \
++    return sv##OP##_##CHAR##BITS(v, idx_u);                                   \
++  }
++
++HWY_SVE_FOREACH(HWY_SVE_TABLE, TableLookupLanes, tbl)
++#undef HWY_SVE_TABLE
++
++// ------------------------------ Compress (PromoteTo)
+ 
+ namespace detail {
+-HWY_SVE_FOREACH(HWY_SVE_SLIDE, SlideDown, slidedown)
++
++#define HWY_SVE_CONCAT_EVERY_SECOND(BASE, CHAR, BITS, NAME, OP)  \
++  HWY_API HWY_SVE_V(BASE, BITS)                                  \
++      NAME(HWY_SVE_V(BASE, BITS) hi, HWY_SVE_V(BASE, BITS) lo) { \
++    return sv##OP##_##CHAR##BITS(lo, hi);                        \
++  }
++HWY_SVE_FOREACH(HWY_SVE_CONCAT_EVERY_SECOND, ConcatEven, uzp1)
++HWY_SVE_FOREACH(HWY_SVE_CONCAT_EVERY_SECOND, ConcatOdd, uzp2)
++#undef HWY_SVE_CONCAT_EVERY_SECOND
++
+ }  // namespace detail
+ 
+-#undef HWY_SVE_SLIDE
++#define HWY_SVE_COMPRESS(BASE, CHAR, BITS, NAME, OP)                           \
++  HWY_API HWY_SVE_V(BASE, BITS) NAME(HWY_SVE_V(BASE, BITS) v, svbool_t mask) { \
++    return sv##OP##_##CHAR##BITS(mask, v);                                     \
++  }
+ 
+-template <size_t kLanes, class V>
+-HWY_API V ShiftRightLanes(const V v) {
+-  using D = DFromV<V>;
+-  const RebindToSigned<D> di;
+-  const auto shifted = detail::SlideDown(v, v, kLanes);
+-  // Match x86 semantics by zeroing upper lanes in 128-bit blocks
+-  constexpr size_t kLanesPerBlock = detail::LanesPerBlock(di);
+-  const auto idx_mod = detail::And(detail::Iota0(di), kLanesPerBlock - 1);
+-  const auto keep = Lt(BitCast(di, idx_mod), Set(di, kLanesPerBlock - kLanes));
+-  return IfThenElseZero(keep, shifted);
++HWY_SVE_FOREACH_UIF3264(HWY_SVE_COMPRESS, Compress, compact)
++#undef HWY_SVE_COMPRESS
++
++template <class V, HWY_IF_LANE_SIZE_V(V, 2)>
++HWY_API V Compress(V v, svbool_t mask16) {
++  static_assert(!IsSame<V, svfloat16_t>(), "Must use overload");
++  const DFromV<V> d16;
++
++  // Promote vector and mask to 32-bit
++  const RepartitionToWide<decltype(d16)> dw;
++  const auto v32L = PromoteTo(dw, v);
++  const auto v32H = detail::PromoteUpperTo(dw, v);
++  const svbool_t mask32L = svunpklo_b(mask16);
++  const svbool_t mask32H = svunpkhi_b(mask16);
++
++  const auto compressedL = Compress(v32L, mask32L);
++  const auto compressedH = Compress(v32H, mask32H);
++
++  // Demote to 16-bit (already in range) - separately so we can splice
++  const V evenL = BitCast(d16, compressedL);
++  const V evenH = BitCast(d16, compressedH);
++  const V v16L = detail::ConcatEven(evenL, evenL);
++  const V v16H = detail::ConcatEven(evenH, evenH);
++
++  // We need to combine two vectors of non-constexpr length, so the only option
++  // is Splice, which requires us to synthesize a mask. NOTE: this function uses
++  // full vectors (SV_ALL instead of SV_POW2), hence we need unmasked svcnt.
++  const size_t countL = detail::CountTrueFull(dw, mask32L);
++  const auto compressed_maskL = FirstN(d16, countL);
++  return detail::Splice(v16H, v16L, compressed_maskL);
++}
++
++// Must treat float16_t as integers so we can ConcatEven.
++HWY_API svfloat16_t Compress(svfloat16_t v, svbool_t mask16) {
++  const DFromV<decltype(v)> df;
++  const RebindToSigned<decltype(df)> di;
++  return BitCast(df, Compress(BitCast(di, v), mask16));
+ }
+ 
+-// ------------------------------ ShiftRightBytes
++// ------------------------------ CompressStore
+ 
+-template <int kBytes, class V>
+-HWY_API V ShiftRightBytes(const V v) {
+-  using D = DFromV<V>;
+-  const Repartition<uint8_t, D> d8;
+-  Lanes(d8);
+-  return BitCast(D(), ShiftRightLanes<kBytes>(BitCast(d8, v)));
++template <class V, class M, class D>
++HWY_API size_t CompressStore(const V v, const M mask, const D d,
++                             TFromD<D>* HWY_RESTRICT aligned) {
++  Store(Compress(v, mask), d, aligned);
++  return CountTrue(d, mask);
+ }
+ 
+-// ------------------------------ OddEven
++// ================================================== BLOCKWISE
+ 
+-template <class V>
+-HWY_API V OddEven(const V a, const V b) {
+-  const RebindToUnsigned<DFromV<V>> du;  // Iota0 is unsigned only
+-  const auto is_even = Eq(detail::And(detail::Iota0(du), 1), Zero(du));
+-  return IfThenElse(is_even, b, a);
+-}
++// ------------------------------ CombineShiftRightBytes
+ 
+-// ------------------------------ ConcatUpperLower
++namespace detail {
+ 
+-template <class V>
+-HWY_API V ConcatUpperLower(const V hi, const V lo) {
+-  const RebindToSigned<DFromV<V>> di;
+-  const auto idx_half = Set(di, Lanes(di) / 2);
+-  const auto is_lower_half = Lt(BitCast(di, detail::Iota0(di)), idx_half);
+-  return IfThenElse(is_lower_half, lo, hi);
++// For x86-compatible behaviour mandated by Highway API: TableLookupBytes
++// offsets are implicitly relative to the start of their 128-bit block.
++template <typename T, size_t N>
++constexpr size_t LanesPerBlock(Simd<T, N> /* tag */) {
++  // We might have a capped vector smaller than a block, so honor that.
++  return HWY_MIN(16 / sizeof(T), N);
+ }
+ 
+-// ------------------------------ ConcatLowerLower
++template <class D, class V>
++HWY_INLINE V OffsetsOf128BitBlocks(const D d, const V iota0) {
++  using T = MakeUnsigned<TFromD<D>>;
++  return detail::AndNotN(static_cast<T>(LanesPerBlock(d) - 1), iota0);
++}
+ 
+-template <class V>
+-HWY_API V ConcatLowerLower(const V hi, const V lo) {
+-  // Move lower half into upper
+-  const auto hi_up = detail::SlideUp(hi, hi, Lanes(DFromV<V>()) / 2);
+-  return ConcatUpperLower(hi_up, lo);
++template <size_t kLanes, class D>
++svbool_t FirstNPerBlock(D d) {
++  const RebindToSigned<D> di;
++  constexpr size_t kLanesPerBlock = detail::LanesPerBlock(di);
++  const auto idx_mod = detail::AndN(Iota(di, 0), kLanesPerBlock - 1);
++  return detail::LtN(BitCast(di, idx_mod), kLanes);
+ }
+ 
+-// ------------------------------ ConcatUpperUpper
++}  // namespace detail
+ 
+-template <class V>
+-HWY_API V ConcatUpperUpper(const V hi, const V lo) {
+-  // Move upper half into lower
+-  const auto lo_down = detail::SlideDown(lo, lo, Lanes(DFromV<V>()) / 2);
+-  return ConcatUpperLower(hi, lo_down);
++template <size_t kBytes, class D, class V = VFromD<D>>
++HWY_API V CombineShiftRightBytes(const D d, const V hi, const V lo) {
++  const Repartition<uint8_t, decltype(d)> d8;
++  const auto hi8 = BitCast(d8, hi);
++  const auto lo8 = BitCast(d8, lo);
++  const auto hi_up = detail::Splice(hi8, hi8, FirstN(d8, 16 - kBytes));
++  const auto lo_down = detail::Ext<kBytes>(lo8, lo8);
++  const svbool_t is_lo = detail::FirstNPerBlock<16 - kBytes>(d8);
++  return BitCast(d, IfThenElse(is_lo, lo_down, hi_up));
+ }
+ 
+-// ------------------------------ ConcatLowerUpper
++// ------------------------------ Shuffle2301
+ 
+-template <class V>
+-HWY_API V ConcatLowerUpper(const V hi, const V lo) {
+-  // Move half of both inputs to the other half
+-  const auto hi_up = detail::SlideUp(hi, hi, Lanes(DFromV<V>()) / 2);
+-  const auto lo_down = detail::SlideDown(lo, lo, Lanes(DFromV<V>()) / 2);
+-  return ConcatUpperLower(hi_up, lo_down);
+-}
++#define HWY_SVE_SHUFFLE_2301(BASE, CHAR, BITS, NAME, OP)                      \
++  HWY_API HWY_SVE_V(BASE, BITS) NAME(HWY_SVE_V(BASE, BITS) v) {               \
++    const DFromV<decltype(v)> d;                                              \
++    const svuint64_t vu64 = BitCast(Repartition<uint64_t, decltype(d)>(), v); \
++    return BitCast(d, sv##OP##_u64_x(HWY_SVE_PTRUE(64), vu64));               \
++  }
+ 
+-// ------------------------------ InterleaveLower
++HWY_SVE_FOREACH_UI32(HWY_SVE_SHUFFLE_2301, Shuffle2301, revw)
++#undef HWY_SVE_SHUFFLE_2301
++
++template <class V, HWY_IF_FLOAT_V(V)>
++HWY_API V Shuffle2301(const V v) {
++  const DFromV<V> df;
++  const RebindToUnsigned<decltype(df)> du;
++  return BitCast(df, Shuffle2301(BitCast(du, v)));
++}
+ 
++// ------------------------------ Shuffle2103
+ template <class V>
+-HWY_API V InterleaveLower(const V a, const V b) {
++HWY_API V Shuffle2103(const V v) {
+   const DFromV<V> d;
+-  const RebindToUnsigned<decltype(d)> du;
+-  constexpr size_t kLanesPerBlock = detail::LanesPerBlock(d);
+-  const auto i = detail::Iota0(d);
+-  const auto idx_mod = ShiftRight<1>(detail::And(i, kLanesPerBlock - 1));
+-  const auto idx = Add(idx_mod, detail::OffsetsOf128BitBlocks(d, i));
+-  const auto is_even = Eq(detail::And(i, 1), Zero(du));
+-  return IfThenElse(is_even, TableLookupLanes(a, idx),
+-                    TableLookupLanes(b, idx));
++  const Repartition<uint8_t, decltype(d)> d8;
++  static_assert(sizeof(TFromD<decltype(d)>) == 4, "Defined for 32-bit types");
++  const svuint8_t v8 = BitCast(d8, v);
++  return BitCast(d, CombineShiftRightBytes<12>(d8, v8, v8));
+ }
+ 
+-// ------------------------------ InterleaveUpper
+-
++// ------------------------------ Shuffle0321
+ template <class V>
+-HWY_API V InterleaveUpper(const V a, const V b) {
++HWY_API V Shuffle0321(const V v) {
+   const DFromV<V> d;
+-  const RebindToUnsigned<decltype(d)> du;
+-  constexpr size_t kLanesPerBlock = detail::LanesPerBlock(d);
+-  const auto i = detail::Iota0(d);
+-  const auto idx_mod = ShiftRight<1>(detail::And(i, kLanesPerBlock - 1));
+-  const auto idx_lower = Add(idx_mod, detail::OffsetsOf128BitBlocks(d, i));
+-  const auto idx = detail::Add(idx_lower, kLanesPerBlock / 2);
+-  const auto is_even = Eq(detail::And(i, 1), Zero(du));
+-  return IfThenElse(is_even, TableLookupLanes(a, idx),
+-                    TableLookupLanes(b, idx));
++  const Repartition<uint8_t, decltype(d)> d8;
++  static_assert(sizeof(TFromD<decltype(d)>) == 4, "Defined for 32-bit types");
++  const svuint8_t v8 = BitCast(d8, v);
++  return BitCast(d, CombineShiftRightBytes<4>(d8, v8, v8));
+ }
+ 
+-// ------------------------------ ZipLower
+-
++// ------------------------------ Shuffle1032
+ template <class V>
+-HWY_API VFromD<RepartitionToWide<DFromV<V>>> ZipLower(const V a, const V b) {
+-  RepartitionToWide<DFromV<V>> dw;
+-  return BitCast(dw, InterleaveLower(a, b));
++HWY_API V Shuffle1032(const V v) {
++  const DFromV<V> d;
++  const Repartition<uint8_t, decltype(d)> d8;
++  static_assert(sizeof(TFromD<decltype(d)>) == 4, "Defined for 32-bit types");
++  const svuint8_t v8 = BitCast(d8, v);
++  return BitCast(d, CombineShiftRightBytes<8>(d8, v8, v8));
+ }
+ 
+-// ------------------------------ ZipUpper
+-
++// ------------------------------ Shuffle01
+ template <class V>
+-HWY_API VFromD<RepartitionToWide<DFromV<V>>> ZipUpper(const V a, const V b) {
+-  RepartitionToWide<DFromV<V>> dw;
+-  return BitCast(dw, InterleaveUpper(a, b));
++HWY_API V Shuffle01(const V v) {
++  const DFromV<V> d;
++  const Repartition<uint8_t, decltype(d)> d8;
++  static_assert(sizeof(TFromD<decltype(d)>) == 8, "Defined for 64-bit types");
++  const svuint8_t v8 = BitCast(d8, v);
++  return BitCast(d, CombineShiftRightBytes<8>(d8, v8, v8));
+ }
+ 
+-// ------------------------------ Combine
+-
+-// TODO(janwas): implement after LMUL ext/trunc
+-#if 0
+-
++// ------------------------------ Shuffle0123
+ template <class V>
+-HWY_API V Combine(const V a, const V b) {
+-  using D = DFromV<V>;
+-  // double LMUL of inputs, then SlideUp with Lanes().
++HWY_API V Shuffle0123(const V v) {
++  return Shuffle2301(Shuffle1032(v));
+ }
+ 
++// ------------------------------ TableLookupBytes
++
++template <class V, class VI>
++HWY_API VI TableLookupBytes(const V v, const VI idx) {
++  const DFromV<VI> d;
++  const Repartition<uint8_t, decltype(d)> du8;
++  const Repartition<int8_t, decltype(d)> di8;
++  const auto offsets128 = detail::OffsetsOf128BitBlocks(du8, Iota(du8, 0));
++  const auto idx8 = BitCast(di8, Add(BitCast(du8, idx), offsets128));
++  return BitCast(d, TableLookupLanes(BitCast(du8, v), idx8));
++}
++
++template <class V, class VI>
++HWY_API VI TableLookupBytesOr0(const V v, const VI idx) {
++  const DFromV<VI> d;
++  // Mask size must match vector type, so cast everything to this type.
++  const Repartition<int8_t, decltype(d)> di8;
++
++  auto idx8 = BitCast(di8, idx);
++  const auto msb = Lt(idx8, Zero(di8));
++// Prevent overflow in table lookups (unnecessary if native)
++#if defined(HWY_EMULATE_SVE)
++  idx8 = IfThenZeroElse(msb, idx8);
+ #endif
+ 
+-// ================================================== REDUCE
++  const auto lookup = TableLookupBytes(BitCast(di8, v), idx8);
++  return BitCast(d, IfThenZeroElse(msb, lookup));
++}
+ 
+-// vector = f(vector, zero_m1)
+-#define HWY_SVE_REDUCE(BASE, CHAR, BITS, NAME, OP)                       \
+-  HWY_API HWY_SVE_V(BASE, BITS)                                          \
+-      NAME(HWY_SVE_V(BASE, BITS) v, HWY_SVE_V(BASE, BITS, 1) v0) {       \
+-    vsetvlmax_e##BITS();                                                 \
+-    return Set(                                                          \
+-        HWY_SVE_D(CHAR, BITS)(),                                         \
+-        GetLane(v##OP##_vs_##CHAR##BITS##_##CHAR##BITS##m1(v0, v, v0))); \
++// ------------------------------ Broadcast
++
++template <int kLane, class V>
++HWY_API V Broadcast(const V v) {
++  const DFromV<V> d;
++  const RebindToSigned<decltype(d)> di;
++  constexpr size_t kLanesPerBlock = detail::LanesPerBlock(di);
++  static_assert(0 <= kLane && kLane < kLanesPerBlock, "Invalid lane");
++  auto idx = detail::OffsetsOf128BitBlocks(di, Iota(di, 0));
++  if (kLane != 0) {
++    idx = detail::AddN(idx, kLane);
+   }
++  return TableLookupLanes(v, idx);
++}
+ 
+-// ------------------------------ SumOfLanes
++// ------------------------------ ShiftLeftLanes
+ 
+-namespace detail {
++template <size_t kLanes, class D, class V = VFromD<D>>
++HWY_API V ShiftLeftLanes(D d, const V v) {
++  const RebindToSigned<decltype(d)> di;
++  const auto zero = Zero(d);
++  const auto shifted = detail::Splice(v, zero, FirstN(d, kLanes));
++  // Match x86 semantics by zeroing lower lanes in 128-bit blocks
++  return IfThenElse(detail::FirstNPerBlock<kLanes>(d), zero, shifted);
++}
+ 
+-HWY_SVE_FOREACH_UI(HWY_SVE_REDUCE, RedSum, redsum)
+-HWY_SVE_FOREACH_F(HWY_SVE_REDUCE, RedSum, fredsum)
++template <size_t kLanes, class V>
++HWY_API V ShiftLeftLanes(const V v) {
++  return ShiftLeftLanes<kLanes>(DFromV<V>(), v);
++}
+ 
+-}  // namespace detail
++// ------------------------------ ShiftRightLanes
++template <size_t kLanes, typename T, size_t N, class V = VFromD<Simd<T, N>>>
++HWY_API V ShiftRightLanes(Simd<T, N> d, V v) {
++  const RebindToSigned<decltype(d)> di;
++  // For partial vectors, clear upper lanes so we shift in zeros.
++  if (N != HWY_LANES(T)) {
++    v = IfThenElseZero(detail::Mask(d), v);
++  }
+ 
+-template <class V>
+-HWY_API V SumOfLanes(const V v) {
+-  using T = TFromV<V>;
+-  const auto v0 = Zero(Simd<T, HWY_LANES(T)>());  // always m1
+-  return detail::RedSum(v, v0);
++  const auto shifted = detail::Ext<kLanes>(v, v);
++  // Match x86 semantics by zeroing upper lanes in 128-bit blocks
++  constexpr size_t kLanesPerBlock = detail::LanesPerBlock(d);
++  const svbool_t mask = detail::FirstNPerBlock<kLanesPerBlock - kLanes>(d);
++  return IfThenElseZero(mask, shifted);
+ }
+ 
+-// ------------------------------ MinOfLanes
+-namespace detail {
++// ------------------------------ ShiftLeftBytes
+ 
+-HWY_SVE_FOREACH_U(HWY_SVE_REDUCE, RedMin, redminu)
+-HWY_SVE_FOREACH_I(HWY_SVE_REDUCE, RedMin, redmin)
+-HWY_SVE_FOREACH_F(HWY_SVE_REDUCE, RedMin, fredmin)
++template <int kBytes, class D, class V = VFromD<D>>
++HWY_API V ShiftLeftBytes(const D d, const V v) {
++  const Repartition<uint8_t, decltype(d)> d8;
++  return BitCast(d, ShiftLeftLanes<kBytes>(BitCast(d8, v)));
++}
+ 
+-}  // namespace detail
++template <int kBytes, class V>
++HWY_API V ShiftLeftBytes(const V v) {
++  return ShiftLeftBytes<kBytes>(DFromV<V>(), v);
++}
+ 
+-template <class V>
+-HWY_API V MinOfLanes(const V v) {
+-  using T = TFromV<V>;
+-  const Simd<T, HWY_LANES(T)> d1;  // always m1
+-  const auto neutral = Set(d1, HighestValue<T>());
+-  return detail::RedMin(v, neutral);
++// ------------------------------ ShiftRightBytes
++template <int kBytes, class D, class V = VFromD<D>>
++HWY_API V ShiftRightBytes(const D d, const V v) {
++  const Repartition<uint8_t, decltype(d)> d8;
++  return BitCast(d, ShiftRightLanes<kBytes>(d8, BitCast(d8, v)));
+ }
+ 
+-// ------------------------------ MaxOfLanes
++// ------------------------------ InterleaveLower
++
+ namespace detail {
++HWY_SVE_FOREACH(HWY_SVE_RETV_ARGVV, ZipLower, zip1)
++}  // namespace detail
+ 
+-HWY_SVE_FOREACH_U(HWY_SVE_REDUCE, RedMax, redmaxu)
+-HWY_SVE_FOREACH_I(HWY_SVE_REDUCE, RedMax, redmax)
+-HWY_SVE_FOREACH_F(HWY_SVE_REDUCE, RedMax, fredmax)
++template <class D, class V>
++HWY_API V InterleaveLower(D d, const V a, const V b) {
++  static_assert(IsSame<TFromD<D>, TFromV<V>>(), "D/V mismatch");
++  // Move lower halves of blocks to lower half of vector.
++  const Repartition<uint64_t, decltype(d)> d64;
++  const auto a64 = BitCast(d64, a);
++  const auto b64 = BitCast(d64, b);
++  const auto a_blocks = detail::ConcatEven(a64, a64);
++  const auto b_blocks = detail::ConcatEven(b64, b64);
+ 
+-}  // namespace detail
++  return detail::ZipLower(BitCast(d, a_blocks), BitCast(d, b_blocks));
++}
+ 
+ template <class V>
+-HWY_API V MaxOfLanes(const V v) {
+-  using T = TFromV<V>;
+-  const Simd<T, HWY_LANES(T)> d1;  // always m1
+-  const auto neutral = Set(d1, LowestValue<T>());
+-  return detail::RedMax(v, neutral);
++HWY_API V InterleaveLower(const V a, const V b) {
++  return InterleaveLower(DFromV<V>(), a, b);
+ }
+ 
+-#undef HWY_SVE_REDUCE
+-
+-// ================================================== Ops with dependencies
+-
+-// ------------------------------ LoadDup128
+-
+-template <class D>
+-HWY_API VFromD<D> LoadDup128(D d, const TFromD<D>* const HWY_RESTRICT p) {
+-  // TODO(janwas): set VL
+-  const auto loaded = Load(d, p);
+-  constexpr size_t kLanesPerBlock = detail::LanesPerBlock(d);
+-  // Broadcast the first block
+-  const auto idx = detail::And(detail::Iota0(d), kLanesPerBlock - 1);
+-  return TableLookupLanes(loaded, idx);
+-}
++// ------------------------------ InterleaveUpper
+ 
+-// ------------------------------ StoreMaskBits
+-#define HWY_SVE_STORE_MASK_BITS(MLEN, NAME, OP)                 \
+-  HWY_API size_t StoreMaskBits(HWY_SVE_M(MLEN) m, uint8_t* p) { \
+-    /* LMUL=1 is always enough */                               \
+-    Simd<uint8_t, HWY_LANES(uint8_t)> d8;                       \
+-    const size_t num_bytes = (Lanes(d8) + MLEN - 1) / MLEN;     \
+-    /* TODO(janwas): how to convert vbool* to vuint?*/          \
+-    /*Store(m, d8, p);*/                                        \
+-    (void)m;                                                    \
+-    (void)p;                                                    \
+-    return num_bytes;                                           \
++// Full vector: guaranteed to have at least one block
++template <typename T, class V = VFromD<Full<T>>>
++HWY_API V InterleaveUpper(Simd<T, HWY_LANES(T)> d, const V a, const V b) {
++  // Move upper halves of blocks to lower half of vector.
++  const Repartition<uint64_t, decltype(d)> d64;
++  const auto a64 = BitCast(d64, a);
++  const auto b64 = BitCast(d64, b);
++  const auto a_blocks = detail::ConcatOdd(a64, a64);
++  const auto b_blocks = detail::ConcatOdd(b64, b64);
++  return detail::ZipLower(BitCast(d, a_blocks), BitCast(d, b_blocks));
++}
++
++// Capped: less than one block
++template <typename T, size_t N, HWY_IF_LE64(T, N), class V = VFromD<Simd<T, N>>>
++HWY_API V InterleaveUpper(Simd<T, N> d, const V a, const V b) {
++  static_assert(IsSame<T, TFromV<V>>(), "D/V mismatch");
++  const Half<decltype(d)> d2;
++  return InterleaveLower(d, UpperHalf(d2, a), UpperHalf(d2, b));
++}
++
++// Partial: need runtime check
++template <typename T, size_t N,
++          hwy::EnableIf<(N < HWY_LANES(T) && N * sizeof(T) >= 16)>* = nullptr,
++          class V = VFromD<Simd<T, N>>>
++HWY_API V InterleaveUpper(Simd<T, N> d, const V a, const V b) {
++  static_assert(IsSame<T, TFromV<V>>(), "D/V mismatch");
++  // Less than one block: treat as capped
++  if (Lanes(d) * sizeof(T) < 16) {
++    const Half<decltype(d)> d2;
++    return InterleaveLower(d, UpperHalf(d2, a), UpperHalf(d2, b));
+   }
+-HWY_SVE_FOREACH_B(HWY_SVE_STORE_MASK_BITS, _, _)
+-#undef HWY_SVE_STORE_MASK_BITS
++  return InterleaveUpper(Full<T>(), a, b);
++}
+ 
+-// ------------------------------ FirstN (Iota0, Lt, RebindMask, SlideUp)
++// ------------------------------ ZipLower
+ 
+-// Disallow for 8-bit because Iota is likely to overflow.
+-template <class D, HWY_IF_NOT_LANE_SIZE_D(D, 1)>
+-HWY_API MFromD<D> FirstN(const D d, const size_t n) {
+-  const RebindToSigned<D> di;
+-  return RebindMask(d, Lt(BitCast(di, detail::Iota0(d)), Set(di, n)));
++template <class V, class DW = RepartitionToWide<DFromV<V>>>
++HWY_API VFromD<DW> ZipLower(DW dw, V a, V b) {
++  const RepartitionToNarrow<DW> dn;
++  static_assert(IsSame<TFromD<decltype(dn)>, TFromV<V>>(), "D/V mismatch");
++  return BitCast(dw, InterleaveLower(dn, a, b));
+ }
+-
+-template <class D, HWY_IF_LANE_SIZE_D(D, 1)>
+-HWY_API MFromD<D> FirstN(const D d, const size_t n) {
+-  const auto zero = Zero(d);
+-  const auto one = Set(d, 1);
+-  return Eq(detail::SlideUp(one, zero, n), one);
++template <class V, class D = DFromV<V>, class DW = RepartitionToWide<D>>
++HWY_API VFromD<DW> ZipLower(const V a, const V b) {
++  return BitCast(DW(), InterleaveLower(D(), a, b));
+ }
+ 
+-// ------------------------------ Neg
+-
+-template <class V, HWY_IF_SIGNED_V(V)>
+-HWY_API V Neg(const V v) {
+-  return Sub(Zero(DFromV<V>()), v);
++// ------------------------------ ZipUpper
++template <class V, class DW = RepartitionToWide<DFromV<V>>>
++HWY_API VFromD<DW> ZipUpper(DW dw, V a, V b) {
++  const RepartitionToNarrow<DW> dn;
++  static_assert(IsSame<TFromD<decltype(dn)>, TFromV<V>>(), "D/V mismatch");
++  return BitCast(dw, InterleaveUpper(dn, a, b));
+ }
+ 
+-// vector = f(vector), but argument is repeated
+-#define HWY_SVE_RETV_ARGV2(BASE, CHAR, BITS, NAME, OP)          \
+-  HWY_API HWY_SVE_V(BASE, BITS) NAME(HWY_SVE_V(BASE, BITS) v) { \
+-    return v##OP##_vv_##CHAR##BITS(v, v);                       \
++// ================================================== REDUCE
++
++#define HWY_SVE_REDUCE(BASE, CHAR, BITS, NAME, OP)                \
++  template <size_t N>                                             \
++  HWY_API HWY_SVE_V(BASE, BITS)                                   \
++      NAME(HWY_SVE_D(BASE, BITS, N) d, HWY_SVE_V(BASE, BITS) v) { \
++    return Set(d, sv##OP##_##CHAR##BITS(detail::Mask(d), v));     \
+   }
+ 
+-HWY_SVE_FOREACH_F(HWY_SVE_RETV_ARGV2, Neg, fsgnjn)
++HWY_SVE_FOREACH(HWY_SVE_REDUCE, SumOfLanes, addv)
++HWY_SVE_FOREACH_UI(HWY_SVE_REDUCE, MinOfLanes, minv)
++HWY_SVE_FOREACH_UI(HWY_SVE_REDUCE, MaxOfLanes, maxv)
++// NaN if all are
++HWY_SVE_FOREACH_F(HWY_SVE_REDUCE, MinOfLanes, minnmv)
++HWY_SVE_FOREACH_F(HWY_SVE_REDUCE, MaxOfLanes, maxnmv)
+ 
+-// ------------------------------ Abs
++#undef HWY_SVE_REDUCE
+ 
+-template <class V, HWY_IF_SIGNED_V(V)>
+-HWY_API V Abs(const V v) {
+-  return Max(v, Neg(v));
+-}
++// ================================================== Ops with dependencies
+ 
+-HWY_SVE_FOREACH_F(HWY_SVE_RETV_ARGV2, Abs, fsgnjx)
++// ------------------------------ ZeroIfNegative (Lt, IfThenElse)
++template <class V>
++HWY_API V ZeroIfNegative(const V v) {
++  const auto v0 = Zero(DFromV<V>());
++  // We already have a zero constant, so avoid IfThenZeroElse.
++  return IfThenElse(Lt(v, v0), v0, v);
++}
+ 
+-#undef HWY_SVE_RETV_ARGV2
++// ------------------------------ BroadcastSignBit (ShiftRight)
++template <class V>
++HWY_API V BroadcastSignBit(const V v) {
++  return ShiftRight<sizeof(TFromV<V>) * 8 - 1>(v);
++}
+ 
+-// ------------------------------ AbsDiff
++// ------------------------------ AverageRound (ShiftRight)
+ 
++#if HWY_TARGET == HWY_SVE2
++HWY_SVE_FOREACH_U08(HWY_SVE_RETV_ARGPVV, AverageRound, rhadd)
++HWY_SVE_FOREACH_U16(HWY_SVE_RETV_ARGPVV, AverageRound, rhadd)
++#else
+ template <class V>
+-HWY_API V AbsDiff(const V a, const V b) {
+-  return Abs(Sub(a, b));
++V AverageRound(const V a, const V b) {
++  return ShiftRight<1>(Add(Add(a, b), Set(DFromV<V>(), 1)));
+ }
++#endif  // HWY_TARGET == HWY_SVE2
+ 
+-// ------------------------------ Round
+-
+-// IEEE-754 roundToIntegralTiesToEven returns floating-point, but we do not have
+-// a dedicated instruction for that. Rounding to integer and converting back to
+-// float is correct except when the input magnitude is large, in which case the
+-// input was already an integer (because mantissa >> exponent is zero).
++// ------------------------------ StoreMaskBits
+ 
+ namespace detail {
+-enum RoundingModes { kNear, kTrunc, kDown, kUp };
+ 
+-template <class V>
+-HWY_API auto UseInt(const V v) -> decltype(MaskFromVec(v)) {
+-  return Lt(Abs(v), Set(DFromV<V>(), MantissaEnd<TFromV<V>>()));
++// Returns mask ? 1 : 0 in BYTE lanes.
++template <typename T, size_t N, HWY_IF_LANE_SIZE(T, 1)>
++HWY_API svuint8_t BoolFromMask(Simd<T, N> d, svbool_t m) {
++  return svdup_n_u8_z(m, 1);
++}
++template <typename T, size_t N, HWY_IF_LANE_SIZE(T, 2)>
++HWY_API svuint8_t BoolFromMask(Simd<T, N> d, svbool_t m) {
++  const Repartition<uint8_t, decltype(d)> d8;
++  const svuint8_t b16 = BitCast(d8, svdup_n_u16_z(m, 1));
++  return detail::ConcatEven(b16, b16);
++}
++template <typename T, size_t N, HWY_IF_LANE_SIZE(T, 4)>
++HWY_API svuint8_t BoolFromMask(Simd<T, N> d, svbool_t m) {
++  return U8FromU32(svdup_n_u32_z(m, 1));
++}
++template <typename T, size_t N, HWY_IF_LANE_SIZE(T, 8)>
++HWY_API svuint8_t BoolFromMask(Simd<T, N> d, svbool_t m) {
++  const Repartition<uint32_t, decltype(d)> d32;
++  const svuint32_t b64 = BitCast(d32, svdup_n_u64_z(m, 1));
++  return U8FromU32(detail::ConcatEven(b64, b64));
+ }
+ 
+ }  // namespace detail
+ 
+-template <class V>
+-HWY_API V Round(const V v) {
+-  const DFromV<V> df;
+-
+-  const auto integer = NearestInt(v);  // round using current mode
+-  const auto int_f = ConvertTo(df, integer);
++template <typename T, size_t N>
++HWY_API size_t StoreMaskBits(Simd<T, N> d, svbool_t m, uint8_t* p) {
++  const Repartition<uint8_t, decltype(d)> d8;
++  const Repartition<uint16_t, decltype(d)> d16;
++  const Repartition<uint32_t, decltype(d)> d32;
++  const Repartition<uint64_t, decltype(d)> d64;
++  auto x = detail::BoolFromMask(d, m);
++  // Compact bytes to bits. Could use SVE2 BDEP, but it's optional.
++  x = Or(x, BitCast(d8, ShiftRight<7>(BitCast(d16, x))));
++  x = Or(x, BitCast(d8, ShiftRight<14>(BitCast(d32, x))));
++  x = Or(x, BitCast(d8, ShiftRight<28>(BitCast(d64, x))));
++
++  const size_t num_bits = Lanes(d);
++  const size_t num_bytes = (num_bits + 8 - 1) / 8;  // Round up, see below
++
++  // Truncate to 8 bits and store.
++  svst1b_u64(FirstN(d64, num_bytes), p, BitCast(d64, x));
++
++  // Non-full byte, need to clear the undefined upper bits. Can happen for
++  // capped/partial vectors or large T and small hardware vectors.
++  if (num_bits < 8) {
++    const int mask = (1 << num_bits) - 1;
++    p[num_bytes - 1] = static_cast<uint8_t>(p[num_bytes - 1] & mask);
++  }
++  // Else: we wrote full bytes because num_bits is a power of two >= 8.
+ 
+-  return IfThenElse(detail::UseInt(v), CopySign(int_f, v), v);
++  return num_bytes;
+ }
+ 
+-// ------------------------------ Trunc
+-
+-template <class V>
+-HWY_API V Trunc(const V v) {
+-  const DFromV<V> df;
+-  const RebindToSigned<decltype(df)> di;
++// ------------------------------ MulEven (InterleaveEven)
+ 
+-  const auto integer = ConvertTo(di, v);  // round toward 0
+-  const auto int_f = ConvertTo(df, integer);
++#if HWY_TARGET == HWY_SVE2
++namespace detail {
++HWY_SVE_FOREACH_UI32(HWY_SVE_RETV_ARGPVV, MulEven, mullb)
++}  // namespace detail
++#endif
+ 
+-  return IfThenElse(detail::UseInt(v), CopySign(int_f, v), v);
++template <class V, class DW = RepartitionToWide<DFromV<V>>>
++HWY_API VFromD<DW> MulEven(const V a, const V b) {
++#if HWY_TARGET == HWY_SVE2
++  return BitCast(DW(), detail::MulEven(a, b));
++#else
++  const auto lo = Mul(a, b);
++  const auto hi = detail::MulHigh(a, b);
++  return BitCast(DW(), detail::InterleaveEven(lo, hi));
++#endif
+ }
+ 
+-// ------------------------------ Ceil
++HWY_API svuint64_t MulEven(const svuint64_t a, const svuint64_t b) {
++  const auto lo = Mul(a, b);
++  const auto hi = detail::MulHigh(a, b);
++  return detail::InterleaveEven(lo, hi);
++}
+ 
+-template <class V>
+-HWY_API V Ceil(const V v) {
+-  asm volatile("fsrm %0" ::"r"(detail::kUp));
+-  const auto ret = Round(v);
+-  asm volatile("fsrm %0" ::"r"(detail::kNear));
+-  return ret;
++HWY_API svuint64_t MulOdd(const svuint64_t a, const svuint64_t b) {
++  const auto lo = Mul(a, b);
++  const auto hi = detail::MulHigh(a, b);
++  return detail::InterleaveOdd(lo, hi);
+ }
+ 
+-// ------------------------------ Floor
++// ------------------------------ AESRound / CLMul
+ 
+-template <class V>
+-HWY_API V Floor(const V v) {
+-  asm volatile("fsrm %0" ::"r"(detail::kDown));
+-  const auto ret = Round(v);
+-  asm volatile("fsrm %0" ::"r"(detail::kNear));
+-  return ret;
+-}
++#if defined(__ARM_FEATURE_SVE2_AES)
+ 
+-// ------------------------------ Iota
++// Per-target flag to prevent generic_ops-inl.h from defining AESRound.
++#ifdef HWY_NATIVE_AES
++#undef HWY_NATIVE_AES
++#else
++#define HWY_NATIVE_AES
++#endif
+ 
+-template <class D, HWY_IF_UNSIGNED_D(D)>
+-HWY_API VFromD<D> Iota(const D d, TFromD<D> first) {
+-  return Add(detail::Iota0(d), Set(d, first));
++HWY_API svuint8_t AESRound(svuint8_t state, svuint8_t round_key) {
++  // NOTE: it is important that AESE and AESMC be consecutive instructions so
++  // they can be fused. AESE includes AddRoundKey, which is a different ordering
++  // than the AES-NI semantics we adopted, so XOR by 0 and later with the actual
++  // round key (the compiler will hopefully optimize this for multiple rounds).
++  const svuint8_t zero = Zero(HWY_FULL(uint8_t)());
++  return Xor(vaesmcq_u8(vaeseq_u8(state, zero), round_key));
+ }
+ 
+-template <class D, HWY_IF_SIGNED_D(D)>
+-HWY_API VFromD<D> Iota(const D d, TFromD<D> first) {
+-  const RebindToUnsigned<D> du;
+-  return Add(BitCast(d, detail::Iota0(du)), Set(d, first));
++HWY_API svuint64_t CLMulLower(const svuint64_t a, const svuint64_t b) {
++  return svpmullb_pair(a, b);
+ }
+ 
+-template <class D, HWY_IF_FLOAT_D(D)>
+-HWY_API VFromD<D> Iota(const D d, TFromD<D> first) {
+-  const RebindToUnsigned<D> du;
+-  const RebindToSigned<D> di;
+-  return detail::Add(ConvertTo(d, BitCast(di, detail::Iota0(du))), first);
++HWY_API svuint64_t CLMulUpper(const svuint64_t a, const svuint64_t b) {
++  return svpmullt_pair(a, b);
+ }
+ 
+-// ------------------------------ MulEven
+-
+-// Using vwmul does not work for m8, so use mulh instead. Highway only provides
+-// MulHigh for 16-bit, so use a private wrapper.
+-namespace detail {
+-
+-HWY_SVE_FOREACH_U32(HWY_SVE_RETV_ARGVV, MulHigh, mulhu)
+-HWY_SVE_FOREACH_I32(HWY_SVE_RETV_ARGVV, MulHigh, mulh)
+-
+-}  // namespace detail
+-
+-template <class V>
+-HWY_API VFromD<RepartitionToWide<DFromV<V>>> MulEven(const V a, const V b) {
+-  const DFromV<V> d;
+-  Lanes(d);
+-  const auto lo = Mul(a, b);
+-  const auto hi = detail::MulHigh(a, b);
+-  const RepartitionToWide<DFromV<V>> dw;
+-  return BitCast(dw, OddEven(detail::SlideUp(hi, hi, 1), lo));
+-}
++#endif  // __ARM_FEATURE_SVE2_AES
+ 
+ // ================================================== END MACROS
+ namespace detail {  // for code folding
+ #undef HWY_IF_FLOAT_V
++#undef HWY_IF_LANE_SIZE_V
+ #undef HWY_IF_SIGNED_V
+ #undef HWY_IF_UNSIGNED_V
+-
++#undef HWY_SVE_D
+ #undef HWY_SVE_FOREACH
+-#undef HWY_SVE_FOREACH_08
+-#undef HWY_SVE_FOREACH_16
+-#undef HWY_SVE_FOREACH_32
+-#undef HWY_SVE_FOREACH_64
+-#undef HWY_SVE_FOREACH_B
+ #undef HWY_SVE_FOREACH_F
++#undef HWY_SVE_FOREACH_F16
+ #undef HWY_SVE_FOREACH_F32
+ #undef HWY_SVE_FOREACH_F64
+ #undef HWY_SVE_FOREACH_I
+@@ -1671,25 +1818,28 @@ namespace detail {  // for code folding
+ #undef HWY_SVE_FOREACH_I16
+ #undef HWY_SVE_FOREACH_I32
+ #undef HWY_SVE_FOREACH_I64
++#undef HWY_SVE_FOREACH_IF
+ #undef HWY_SVE_FOREACH_U
+ #undef HWY_SVE_FOREACH_U08
+ #undef HWY_SVE_FOREACH_U16
+ #undef HWY_SVE_FOREACH_U32
+ #undef HWY_SVE_FOREACH_U64
+ #undef HWY_SVE_FOREACH_UI
++#undef HWY_SVE_FOREACH_UI08
+ #undef HWY_SVE_FOREACH_UI16
+ #undef HWY_SVE_FOREACH_UI32
+ #undef HWY_SVE_FOREACH_UI64
+-
++#undef HWY_SVE_FOREACH_UIF3264
++#undef HWY_SVE_PTRUE
+ #undef HWY_SVE_RETV_ARGD
++#undef HWY_SVE_RETV_ARGPV
++#undef HWY_SVE_RETV_ARGPVN
++#undef HWY_SVE_RETV_ARGPVV
+ #undef HWY_SVE_RETV_ARGV
+-#undef HWY_SVE_RETV_ARGVS
++#undef HWY_SVE_RETV_ARGVN
+ #undef HWY_SVE_RETV_ARGVV
+-
+ #undef HWY_SVE_T
+-#undef HWY_SVE_D
+ #undef HWY_SVE_V
+-#undef HWY_SVE_M
+ 
+ }  // namespace detail
+ // NOLINTNEXTLINE(google-readability-namespace-comments)
+diff --git a/third_party/highway/hwy/ops/generic_ops-inl.h b/third_party/highway/hwy/ops/generic_ops-inl.h
+new file mode 100644
+index 0000000000000..35cec12f75f03
+--- /dev/null
++++ b/third_party/highway/hwy/ops/generic_ops-inl.h
+@@ -0,0 +1,324 @@
++// Copyright 2021 Google LLC
++//
++// Licensed under the Apache License, Version 2.0 (the "License");
++// you may not use this file except in compliance with the License.
++// You may obtain a copy of the License at
++//
++//      http://www.apache.org/licenses/LICENSE-2.0
++//
++// Unless required by applicable law or agreed to in writing, software
++// distributed under the License is distributed on an "AS IS" BASIS,
++// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
++// See the License for the specific language governing permissions and
++// limitations under the License.
++
++// Target-independent types/functions defined after target-specific ops.
++
++// Relies on the external include guard in highway.h.
++HWY_BEFORE_NAMESPACE();
++namespace hwy {
++namespace HWY_NAMESPACE {
++
++// The lane type of a vector type, e.g. float for Vec<Simd<float, 4>>.
++template <class V>
++using LaneType = decltype(GetLane(V()));
++
++// Vector type, e.g. Vec128<float> for Simd<float, 4>. Useful as the return type
++// of functions that do not take a vector argument, or as an argument type if
++// the function only has a template argument for D, or for explicit type names
++// instead of auto. This may be a built-in type.
++template <class D>
++using Vec = decltype(Zero(D()));
++
++// Mask type. Useful as the return type of functions that do not take a mask
++// argument, or as an argument type if the function only has a template argument
++// for D, or for explicit type names instead of auto.
++template <class D>
++using Mask = decltype(MaskFromVec(Zero(D())));
++
++// Returns the closest value to v within [lo, hi].
++template <class V>
++HWY_API V Clamp(const V v, const V lo, const V hi) {
++  return Min(Max(lo, v), hi);
++}
++
++// CombineShiftRightBytes (and -Lanes) are not available for the scalar target,
++// and RVV has its own implementation of -Lanes.
++#if HWY_TARGET != HWY_SCALAR && HWY_TARGET != HWY_RVV
++
++template <size_t kLanes, class D, class V = VFromD<D>>
++HWY_API V CombineShiftRightLanes(D d, const V hi, const V lo) {
++  constexpr size_t kBytes = kLanes * sizeof(LaneType<V>);
++  static_assert(kBytes < 16, "Shift count is per-block");
++  return CombineShiftRightBytes<kBytes>(d, hi, lo);
++}
++
++// DEPRECATED
++template <size_t kLanes, class V>
++HWY_API V CombineShiftRightLanes(const V hi, const V lo) {
++  return CombineShiftRightLanes<kLanes>(DFromV<V>(), hi, lo);
++}
++
++#endif
++
++// Returns lanes with the most significant bit set and all other bits zero.
++template <class D>
++HWY_API Vec<D> SignBit(D d) {
++  using Unsigned = MakeUnsigned<TFromD<D>>;
++  const Unsigned bit = Unsigned(1) << (sizeof(Unsigned) * 8 - 1);
++  return BitCast(d, Set(Rebind<Unsigned, D>(), bit));
++}
++
++// Returns quiet NaN.
++template <class D>
++HWY_API Vec<D> NaN(D d) {
++  const RebindToSigned<D> di;
++  // LimitsMax sets all exponent and mantissa bits to 1. The exponent plus
++  // mantissa MSB (to indicate quiet) would be sufficient.
++  return BitCast(d, Set(di, LimitsMax<TFromD<decltype(di)>>()));
++}
++
++// ------------------------------ AESRound
++
++// Cannot implement on scalar: need at least 16 bytes for TableLookupBytes.
++#if HWY_TARGET != HWY_SCALAR
++
++// Define for white-box testing, even if native instructions are available.
++namespace detail {
++
++// Constant-time: computes inverse in GF(2^4) based on "Accelerating AES with
++// Vector Permute Instructions" and the accompanying assembly language
++// implementation: https://crypto.stanford.edu/vpaes/vpaes.tgz. See also Botan:
++// https://botan.randombit.net/doxygen/aes__vperm_8cpp_source.html .
++//
++// A brute-force 256 byte table lookup can also be made constant-time, and
++// possibly competitive on NEON, but this is more performance-portable
++// especially for x86 and large vectors.
++template <class V>  // u8
++HWY_INLINE V SubBytes(V state) {
++  const DFromV<V> du;
++  const auto mask = Set(du, 0xF);
++
++  // Change polynomial basis to GF(2^4)
++  {
++    alignas(16) static constexpr uint8_t basisL[16] = {
++        0x00, 0x70, 0x2A, 0x5A, 0x98, 0xE8, 0xB2, 0xC2,
++        0x08, 0x78, 0x22, 0x52, 0x90, 0xE0, 0xBA, 0xCA};
++    alignas(16) static constexpr uint8_t basisU[16] = {
++        0x00, 0x4D, 0x7C, 0x31, 0x7D, 0x30, 0x01, 0x4C,
++        0x81, 0xCC, 0xFD, 0xB0, 0xFC, 0xB1, 0x80, 0xCD};
++    const auto sL = And(state, mask);
++    const auto sU = ShiftRight<4>(state);  // byte shift => upper bits are zero
++    const auto gf4L = TableLookupBytes(LoadDup128(du, basisL), sL);
++    const auto gf4U = TableLookupBytes(LoadDup128(du, basisU), sU);
++    state = Xor(gf4L, gf4U);
++  }
++
++  // Inversion in GF(2^4). Elements 0 represent "infinity" (division by 0) and
++  // cause TableLookupBytesOr0 to return 0.
++  alignas(16) static constexpr uint8_t kZetaInv[16] = {
++      0x80, 7, 11, 15, 6, 10, 4, 1, 9, 8, 5, 2, 12, 14, 13, 3};
++  alignas(16) static constexpr uint8_t kInv[16] = {
++      0x80, 1, 8, 13, 15, 6, 5, 14, 2, 12, 11, 10, 9, 3, 7, 4};
++  const auto tbl = LoadDup128(du, kInv);
++  const auto sL = And(state, mask);      // L=low nibble, U=upper
++  const auto sU = ShiftRight<4>(state);  // byte shift => upper bits are zero
++  const auto sX = Xor(sU, sL);
++  const auto invL = TableLookupBytes(LoadDup128(du, kZetaInv), sL);
++  const auto invU = TableLookupBytes(tbl, sU);
++  const auto invX = TableLookupBytes(tbl, sX);
++  const auto outL = Xor(sX, TableLookupBytesOr0(tbl, Xor(invL, invU)));
++  const auto outU = Xor(sU, TableLookupBytesOr0(tbl, Xor(invL, invX)));
++
++  // Linear skew (cannot bake 0x63 bias into the table because out* indices
++  // may have the infinity flag set).
++  alignas(16) static constexpr uint8_t kAffineL[16] = {
++      0x00, 0xC7, 0xBD, 0x6F, 0x17, 0x6D, 0xD2, 0xD0,
++      0x78, 0xA8, 0x02, 0xC5, 0x7A, 0xBF, 0xAA, 0x15};
++  alignas(16) static constexpr uint8_t kAffineU[16] = {
++      0x00, 0x6A, 0xBB, 0x5F, 0xA5, 0x74, 0xE4, 0xCF,
++      0xFA, 0x35, 0x2B, 0x41, 0xD1, 0x90, 0x1E, 0x8E};
++  const auto affL = TableLookupBytesOr0(LoadDup128(du, kAffineL), outL);
++  const auto affU = TableLookupBytesOr0(LoadDup128(du, kAffineU), outU);
++  return Xor(Xor(affL, affU), Set(du, 0x63));
++}
++
++}  // namespace detail
++
++#endif  // HWY_TARGET != HWY_SCALAR
++
++// "Include guard": skip if native AES instructions are available.
++#if (defined(HWY_NATIVE_AES) == defined(HWY_TARGET_TOGGLE))
++#ifdef HWY_NATIVE_AES
++#undef HWY_NATIVE_AES
++#else
++#define HWY_NATIVE_AES
++#endif
++
++// (Must come after HWY_TARGET_TOGGLE, else we don't reset it for scalar)
++#if HWY_TARGET != HWY_SCALAR
++
++namespace detail {
++
++template <class V>  // u8
++HWY_API V ShiftRows(const V state) {
++  const DFromV<V> du;
++  alignas(16) static constexpr uint8_t kShiftRow[16] = {
++      0,  5,  10, 15,  // transposed: state is column major
++      4,  9,  14, 3,   //
++      8,  13, 2,  7,   //
++      12, 1,  6,  11};
++  const auto shift_row = LoadDup128(du, kShiftRow);
++  return TableLookupBytes(state, shift_row);
++}
++
++template <class V>  // u8
++HWY_API V MixColumns(const V state) {
++  const DFromV<V> du;
++  // For each column, the rows are the sum of GF(2^8) matrix multiplication by:
++  // 2 3 1 1  // Let s := state*1, d := state*2, t := state*3.
++  // 1 2 3 1  // d are on diagonal, no permutation needed.
++  // 1 1 2 3  // t1230 indicates column indices of threes for the 4 rows.
++  // 3 1 1 2  // We also need to compute s2301 and s3012 (=1230 o 2301).
++  alignas(16) static constexpr uint8_t k2301[16] = {
++      2, 3, 0, 1, 6, 7, 4, 5, 10, 11, 8, 9, 14, 15, 12, 13};
++  alignas(16) static constexpr uint8_t k1230[16] = {
++      1, 2, 3, 0, 5, 6, 7, 4, 9, 10, 11, 8, 13, 14, 15, 12};
++  const RebindToSigned<decltype(du)> di;  // can only do signed comparisons
++  const auto msb = Lt(BitCast(di, state), Zero(di));
++  const auto overflow = BitCast(du, IfThenElseZero(msb, Set(di, 0x1B)));
++  const auto d = Xor(Add(state, state), overflow);  // = state*2 in GF(2^8).
++  const auto s2301 = TableLookupBytes(state, LoadDup128(du, k2301));
++  const auto d_s2301 = Xor(d, s2301);
++  const auto t_s2301 = Xor(state, d_s2301);  // t(s*3) = XOR-sum {s, d(s*2)}
++  const auto t1230_s3012 = TableLookupBytes(t_s2301, LoadDup128(du, k1230));
++  return Xor(d_s2301, t1230_s3012);  // XOR-sum of 4 terms
++}
++
++}  // namespace detail
++
++template <class V>  // u8
++HWY_API V AESRound(V state, const V round_key) {
++  // Intel docs swap the first two steps, but it does not matter because
++  // ShiftRows is a permutation and SubBytes is independent of lane index.
++  state = detail::SubBytes(state);
++  state = detail::ShiftRows(state);
++  state = detail::MixColumns(state);
++  state = Xor(state, round_key);  // AddRoundKey
++  return state;
++}
++
++// Constant-time implementation inspired by
++// https://www.bearssl.org/constanttime.html, but about half the cost because we
++// use 64x64 multiplies and 128-bit XORs.
++template <class V>
++HWY_API V CLMulLower(V a, V b) {
++  const DFromV<V> d;
++  static_assert(IsSame<TFromD<decltype(d)>, uint64_t>(), "V must be u64");
++  const auto k1 = Set(d, 0x1111111111111111ULL);
++  const auto k2 = Set(d, 0x2222222222222222ULL);
++  const auto k4 = Set(d, 0x4444444444444444ULL);
++  const auto k8 = Set(d, 0x8888888888888888ULL);
++  const auto a0 = And(a, k1);
++  const auto a1 = And(a, k2);
++  const auto a2 = And(a, k4);
++  const auto a3 = And(a, k8);
++  const auto b0 = And(b, k1);
++  const auto b1 = And(b, k2);
++  const auto b2 = And(b, k4);
++  const auto b3 = And(b, k8);
++
++  auto m0 = Xor(MulEven(a0, b0), MulEven(a1, b3));
++  auto m1 = Xor(MulEven(a0, b1), MulEven(a1, b0));
++  auto m2 = Xor(MulEven(a0, b2), MulEven(a1, b1));
++  auto m3 = Xor(MulEven(a0, b3), MulEven(a1, b2));
++  m0 = Xor(m0, Xor(MulEven(a2, b2), MulEven(a3, b1)));
++  m1 = Xor(m1, Xor(MulEven(a2, b3), MulEven(a3, b2)));
++  m2 = Xor(m2, Xor(MulEven(a2, b0), MulEven(a3, b3)));
++  m3 = Xor(m3, Xor(MulEven(a2, b1), MulEven(a3, b0)));
++  return Or(Or(And(m0, k1), And(m1, k2)), Or(And(m2, k4), And(m3, k8)));
++}
++
++template <class V>
++HWY_API V CLMulUpper(V a, V b) {
++  const DFromV<V> d;
++  static_assert(IsSame<TFromD<decltype(d)>, uint64_t>(), "V must be u64");
++  const auto k1 = Set(d, 0x1111111111111111ULL);
++  const auto k2 = Set(d, 0x2222222222222222ULL);
++  const auto k4 = Set(d, 0x4444444444444444ULL);
++  const auto k8 = Set(d, 0x8888888888888888ULL);
++  const auto a0 = And(a, k1);
++  const auto a1 = And(a, k2);
++  const auto a2 = And(a, k4);
++  const auto a3 = And(a, k8);
++  const auto b0 = And(b, k1);
++  const auto b1 = And(b, k2);
++  const auto b2 = And(b, k4);
++  const auto b3 = And(b, k8);
++
++  auto m0 = Xor(MulOdd(a0, b0), MulOdd(a1, b3));
++  auto m1 = Xor(MulOdd(a0, b1), MulOdd(a1, b0));
++  auto m2 = Xor(MulOdd(a0, b2), MulOdd(a1, b1));
++  auto m3 = Xor(MulOdd(a0, b3), MulOdd(a1, b2));
++  m0 = Xor(m0, Xor(MulOdd(a2, b2), MulOdd(a3, b1)));
++  m1 = Xor(m1, Xor(MulOdd(a2, b3), MulOdd(a3, b2)));
++  m2 = Xor(m2, Xor(MulOdd(a2, b0), MulOdd(a3, b3)));
++  m3 = Xor(m3, Xor(MulOdd(a2, b1), MulOdd(a3, b0)));
++  return Or(Or(And(m0, k1), And(m1, k2)), Or(And(m2, k4), And(m3, k8)));
++}
++
++#endif  // HWY_NATIVE_AES
++#endif  // HWY_TARGET != HWY_SCALAR
++
++// "Include guard": skip if native POPCNT-related instructions are available.
++#if (defined(HWY_NATIVE_POPCNT) == defined(HWY_TARGET_TOGGLE))
++#ifdef HWY_NATIVE_POPCNT
++#undef HWY_NATIVE_POPCNT
++#else
++#define HWY_NATIVE_POPCNT
++#endif
++
++template <typename V, HWY_IF_LANES_ARE(uint8_t, V)>
++HWY_API V PopulationCount(V v) {
++  constexpr DFromV<V> d;
++  HWY_ALIGN constexpr uint8_t kLookup[16] = {
++      0, 1, 1, 2, 1, 2, 2, 3, 1, 2, 2, 3, 2, 3, 3, 4,
++  };
++  auto lo = And(v, Set(d, 0xF));
++  auto hi = ShiftRight<4>(v);
++  auto lookup = LoadDup128(Simd<uint8_t, HWY_MAX(16, MaxLanes(d))>(), kLookup);
++  return Add(TableLookupBytes(lookup, hi), TableLookupBytes(lookup, lo));
++}
++
++template <typename V, HWY_IF_LANES_ARE(uint16_t, V)>
++HWY_API V PopulationCount(V v) {
++  const DFromV<V> d;
++  Repartition<uint8_t, decltype(d)> d8;
++  auto vals = BitCast(d, PopulationCount(BitCast(d8, v)));
++  return Add(ShiftRight<8>(vals), And(vals, Set(d, 0xFF)));
++}
++
++template <typename V, HWY_IF_LANES_ARE(uint32_t, V)>
++HWY_API V PopulationCount(V v) {
++  const DFromV<V> d;
++  Repartition<uint16_t, decltype(d)> d16;
++  auto vals = BitCast(d, PopulationCount(BitCast(d16, v)));
++  return Add(ShiftRight<16>(vals), And(vals, Set(d, 0xFF)));
++}
++
++#if HWY_CAP_INTEGER64
++template <typename V, HWY_IF_LANES_ARE(uint64_t, V)>
++HWY_API V PopulationCount(V v) {
++  const DFromV<V> d;
++  Repartition<uint32_t, decltype(d)> d32;
++  auto vals = BitCast(d, PopulationCount(BitCast(d32, v)));
++  return Add(ShiftRight<32>(vals), And(vals, Set(d, 0xFF)));
++}
++#endif
++
++#endif  // HWY_NATIVE_POPCNT
++
++// NOLINTNEXTLINE(google-readability-namespace-comments)
++}  // namespace HWY_NAMESPACE
++}  // namespace hwy
++HWY_AFTER_NAMESPACE();
+diff --git a/third_party/highway/hwy/ops/rvv-inl.h b/third_party/highway/hwy/ops/rvv-inl.h
+index 6da8720995974..236069603a333 100644
+--- a/third_party/highway/hwy/ops/rvv-inl.h
++++ b/third_party/highway/hwy/ops/rvv-inl.h
+@@ -29,15 +29,16 @@ namespace HWY_NAMESPACE {
+ template <class V>
+ struct DFromV_t {};  // specialized in macros
+ template <class V>
+-using DFromV = typename DFromV_t<V>::type;
++using DFromV = typename DFromV_t<RemoveConst<V>>::type;
+ 
+ template <class V>
+ using TFromV = TFromD<DFromV<V>>;
+ 
+-#define HWY_IF_UNSIGNED_V(V) hwy::EnableIf<!IsSigned<TFromV<V>>()>* = nullptr
+-#define HWY_IF_SIGNED_V(V) \
+-  hwy::EnableIf<IsSigned<TFromV<V>>() && !IsFloat<TFromV<V>>()>* = nullptr
+-#define HWY_IF_FLOAT_V(V) hwy::EnableIf<IsFloat<TFromV<V>>()>* = nullptr
++template <typename T, size_t N>
++HWY_INLINE constexpr size_t MLenFromD(Simd<T, N> /* tag */) {
++  // Returns divisor = type bits / LMUL
++  return sizeof(T) * 8 / (N / HWY_LANES(T));
++}
+ 
+ // kShift = log2 of multiplier: 0 for m1, 1 for m2, -2 for mf4
+ template <typename T, int kShift = 0>
+@@ -202,7 +203,7 @@ HWY_RVV_FOREACH(HWY_SPECIALIZE, _, _)
+     return v##OP##_v_##CHAR##SEW##LMUL(v);                                \
+   }
+ 
+-// vector = f(vector, scalar), e.g. detail::Add
++// vector = f(vector, scalar), e.g. detail::AddS
+ #define HWY_RVV_RETV_ARGVS(BASE, CHAR, SEW, LMUL, SHIFT, MLEN, NAME, OP) \
+   HWY_API HWY_RVV_V(BASE, SEW, LMUL)                                     \
+       NAME(HWY_RVV_V(BASE, SEW, LMUL) a, HWY_RVV_T(BASE, SEW) b) {       \
+@@ -340,7 +341,7 @@ HWY_API VFromD<Simd<T, N>> BitCast(Simd<T, N> /*tag*/, FromV v) {
+ namespace detail {
+ 
+ template <class V, class DU = RebindToUnsigned<DFromV<V>>>
+-HWY_API VFromD<DU> BitCastToUnsigned(V v) {
++HWY_INLINE VFromD<DU> BitCastToUnsigned(V v) {
+   return BitCast(DU(), v);
+ }
+ 
+@@ -353,14 +354,14 @@ namespace detail {
+ HWY_RVV_FOREACH_U(HWY_RVV_RETV_ARGD, Iota0, id_v)
+ 
+ template <class D, class DU = RebindToUnsigned<D>>
+-HWY_API VFromD<DU> Iota0(const D /*d*/) {
++HWY_INLINE VFromD<DU> Iota0(const D /*d*/) {
+   Lanes(DU());
+   return BitCastToUnsigned(Iota0(DU()));
+ }
+ 
+ // Partial
+ template <typename T, size_t N, HWY_IF_LE128(T, N)>
+-HWY_API VFromD<Simd<T, N>> Iota0(Simd<T, N> /*tag*/) {
++HWY_INLINE VFromD<Simd<T, N>> Iota0(Simd<T, N> /*tag*/) {
+   return Iota0(Full<T>());
+ }
+ 
+@@ -383,7 +384,7 @@ HWY_API V Not(const V v) {
+ 
+ // Non-vector version (ideally immediate) for use with Iota0
+ namespace detail {
+-HWY_RVV_FOREACH_UI(HWY_RVV_RETV_ARGVS, And, and_vx)
++HWY_RVV_FOREACH_UI(HWY_RVV_RETV_ARGVS, AndS, and_vx)
+ }  // namespace detail
+ 
+ HWY_RVV_FOREACH_UI(HWY_RVV_RETV_ARGVV, And, and)
+@@ -397,18 +398,6 @@ HWY_API V And(const V a, const V b) {
+ 
+ // ------------------------------ Or
+ 
+-// Scalar argument plus mask. Used by VecFromMask.
+-#define HWY_RVV_OR_MASK(BASE, CHAR, SEW, LMUL, SHIFT, MLEN, NAME, OP)    \
+-  HWY_API HWY_RVV_V(BASE, SEW, LMUL)                                     \
+-      NAME(HWY_RVV_V(BASE, SEW, LMUL) v, HWY_RVV_T(BASE, SEW) imm,       \
+-           HWY_RVV_M(MLEN) mask, HWY_RVV_V(BASE, SEW, LMUL) maskedoff) { \
+-    return v##OP##_##CHAR##SEW##LMUL##_m(mask, maskedoff, v, imm);       \
+-  }
+-
+-namespace detail {
+-HWY_RVV_FOREACH_UI(HWY_RVV_OR_MASK, Or, or_vx)
+-}  // namespace detail
+-
+ #undef HWY_RVV_OR_MASK
+ 
+ HWY_RVV_FOREACH_UI(HWY_RVV_RETV_ARGVV, Or, or)
+@@ -424,7 +413,7 @@ HWY_API V Or(const V a, const V b) {
+ 
+ // Non-vector version (ideally immediate) for use with Iota0
+ namespace detail {
+-HWY_RVV_FOREACH_UI(HWY_RVV_RETV_ARGVS, Xor, xor_vx)
++HWY_RVV_FOREACH_UI(HWY_RVV_RETV_ARGVS, XorS, xor_vx)
+ }  // namespace detail
+ 
+ HWY_RVV_FOREACH_UI(HWY_RVV_RETV_ARGVV, Xor, xor)
+@@ -458,8 +447,8 @@ HWY_API V CopySignToAbs(const V abs, const V sign) {
+ // ------------------------------ Add
+ 
+ namespace detail {
+-HWY_RVV_FOREACH_UI(HWY_RVV_RETV_ARGVS, Add, add_vx)
+-HWY_RVV_FOREACH_F(HWY_RVV_RETV_ARGVS, Add, fadd_vf)
++HWY_RVV_FOREACH_UI(HWY_RVV_RETV_ARGVS, AddS, add_vx)
++HWY_RVV_FOREACH_F(HWY_RVV_RETV_ARGVS, AddS, fadd_vf)
+ }  // namespace detail
+ 
+ HWY_RVV_FOREACH_UI(HWY_RVV_RETV_ARGVV, Add, add)
+@@ -560,17 +549,30 @@ HWY_RVV_FOREACH_F(HWY_RVV_RETV_ARGVV, Max, fmax)
+ 
+ // ------------------------------ Mul
+ 
++// Only for internal use (Highway only promises Mul for 16/32-bit inputs).
++// Used by MulLower.
++namespace detail {
++HWY_RVV_FOREACH_U64(HWY_RVV_RETV_ARGVV, Mul, mul)
++}  // namespace detail
++
+ HWY_RVV_FOREACH_UI16(HWY_RVV_RETV_ARGVV, Mul, mul)
+ HWY_RVV_FOREACH_UI32(HWY_RVV_RETV_ARGVV, Mul, mul)
+ HWY_RVV_FOREACH_F(HWY_RVV_RETV_ARGVV, Mul, fmul)
+ 
+ // ------------------------------ MulHigh
+ 
++// Only for internal use (Highway only promises MulHigh for 16-bit inputs).
++// Used by MulEven; vwmul does not work for m8.
++namespace detail {
++HWY_RVV_FOREACH_I32(HWY_RVV_RETV_ARGVV, MulHigh, mulh)
++HWY_RVV_FOREACH_U32(HWY_RVV_RETV_ARGVV, MulHigh, mulhu)
++HWY_RVV_FOREACH_U64(HWY_RVV_RETV_ARGVV, MulHigh, mulhu)
++}  // namespace detail
++
+ HWY_RVV_FOREACH_U16(HWY_RVV_RETV_ARGVV, MulHigh, mulhu)
+ HWY_RVV_FOREACH_I16(HWY_RVV_RETV_ARGVV, MulHigh, mulh)
+ 
+ // ------------------------------ Div
+-
+ HWY_RVV_FOREACH_F(HWY_RVV_RETV_ARGVV, Div, fdiv)
+ 
+ // ------------------------------ ApproximateReciprocal
+@@ -630,6 +632,14 @@ HWY_RVV_FOREACH_F(HWY_RVV_FMA, NegMulSub, fnmacc)
+     return v##OP##_vv_##CHAR##SEW##LMUL##_b##MLEN(a, b);                 \
+   }
+ 
++// mask = f(vector, scalar)
++#define HWY_RVV_RETM_ARGVS(BASE, CHAR, SEW, LMUL, SHIFT, MLEN, NAME, OP) \
++  HWY_API HWY_RVV_M(MLEN)                                                \
++      NAME(HWY_RVV_V(BASE, SEW, LMUL) a, HWY_RVV_T(BASE, SEW) b) {       \
++    (void)Lanes(DFromV<decltype(a)>());                                  \
++    return v##OP##_vx_##CHAR##SEW##LMUL##_b##MLEN(a, b);                 \
++  }
++
+ // ------------------------------ Eq
+ HWY_RVV_FOREACH_UI(HWY_RVV_RETM_ARGVV, Eq, mseq)
+ HWY_RVV_FOREACH_F(HWY_RVV_RETM_ARGVV, Eq, mfeq)
+@@ -642,27 +652,29 @@ HWY_RVV_FOREACH_F(HWY_RVV_RETM_ARGVV, Ne, mfne)
+ HWY_RVV_FOREACH_I(HWY_RVV_RETM_ARGVV, Lt, mslt)
+ HWY_RVV_FOREACH_F(HWY_RVV_RETM_ARGVV, Lt, mflt)
+ 
+-// ------------------------------ Gt
+-
+-template <class V>
+-HWY_API auto Gt(const V a, const V b) -> decltype(Lt(a, b)) {
+-  return Lt(b, a);
+-}
++namespace detail {
++HWY_RVV_FOREACH_I(HWY_RVV_RETM_ARGVS, LtS, mslt)
++}  // namespace detail
+ 
+ // ------------------------------ Le
+ HWY_RVV_FOREACH_F(HWY_RVV_RETM_ARGVV, Le, mfle)
+ 
+ #undef HWY_RVV_RETM_ARGVV
++#undef HWY_RVV_RETM_ARGVS
+ 
+-// ------------------------------ Ge
++// ------------------------------ Gt/Ge
+ 
+ template <class V>
+ HWY_API auto Ge(const V a, const V b) -> decltype(Le(a, b)) {
+   return Le(b, a);
+ }
+ 
+-// ------------------------------ TestBit
++template <class V>
++HWY_API auto Gt(const V a, const V b) -> decltype(Lt(a, b)) {
++  return Lt(b, a);
++}
+ 
++// ------------------------------ TestBit
+ template <class V>
+ HWY_API auto TestBit(const V a, const V bit) -> decltype(Eq(a, bit)) {
+   return Ne(And(a, bit), Zero(DFromV<V>()));
+@@ -671,9 +683,9 @@ HWY_API auto TestBit(const V a, const V bit) -> decltype(Eq(a, bit)) {
+ // ------------------------------ Not
+ 
+ // mask = f(mask)
+-#define HWY_RVV_RETM_ARGM(MLEN, NAME, OP)           \
+-  HWY_API HWY_RVV_M(MLEN) NAME(HWY_RVV_M(MLEN) m) { \
+-    return vm##OP##_m_b##MLEN(m);                   \
++#define HWY_RVV_RETM_ARGM(MLEN, NAME, OP)                 \
++  HWY_API HWY_RVV_M(MLEN) NAME(HWY_RVV_M(MLEN) m) {       \
++    return vm##OP##_m_b##MLEN(m);                         \
+   }
+ 
+ HWY_RVV_FOREACH_B(HWY_RVV_RETM_ARGM, Not, not )
+@@ -712,15 +724,14 @@ HWY_RVV_FOREACH_B(HWY_RVV_RETM_ARGMM, Xor, xor)
+ HWY_RVV_FOREACH(HWY_RVV_IF_THEN_ELSE, IfThenElse, merge)
+ 
+ #undef HWY_RVV_IF_THEN_ELSE
+-// ------------------------------ IfThenElseZero
+ 
++// ------------------------------ IfThenElseZero
+ template <class M, class V>
+ HWY_API V IfThenElseZero(const M mask, const V yes) {
+   return IfThenElse(mask, yes, Zero(DFromV<V>()));
+ }
+ 
+ // ------------------------------ IfThenZeroElse
+-
+ template <class M, class V>
+ HWY_API V IfThenZeroElse(const M mask, const V no) {
+   return IfThenElse(mask, Zero(DFromV<V>()), no);
+@@ -745,10 +756,20 @@ HWY_API MFromD<D> RebindMask(const D /*d*/, const MFrom mask) {
+ 
+ // ------------------------------ VecFromMask
+ 
++namespace detail {
++#define HWY_RVV_VEC_FROM_MASK(BASE, CHAR, SEW, LMUL, SHIFT, MLEN, NAME, OP) \
++  HWY_API HWY_RVV_V(BASE, SEW, LMUL)                                        \
++      NAME(HWY_RVV_V(BASE, SEW, LMUL) v0, HWY_RVV_M(MLEN) m) {              \
++    return v##OP##_##CHAR##SEW##LMUL##_m(m, v0, v0, 1);                     \
++  }
++
++HWY_RVV_FOREACH_UI(HWY_RVV_VEC_FROM_MASK, SubS, sub_vx)
++#undef HWY_RVV_VEC_FROM_MASK
++}  // namespace detail
++
+ template <class D, HWY_IF_NOT_FLOAT_D(D)>
+ HWY_API VFromD<D> VecFromMask(const D d, MFromD<D> mask) {
+-  const auto v0 = Zero(d);
+-  return detail::Or(v0, -1, mask, v0);
++  return detail::SubS(Zero(d), mask);
+ }
+ 
+ template <class D, HWY_IF_FLOAT_D(D)>
+@@ -757,7 +778,6 @@ HWY_API VFromD<D> VecFromMask(const D d, MFromD<D> mask) {
+ }
+ 
+ // ------------------------------ ZeroIfNegative
+-
+ template <class V>
+ HWY_API V ZeroIfNegative(const V v) {
+   const auto v0 = Zero(DFromV<V>());
+@@ -766,7 +786,6 @@ HWY_API V ZeroIfNegative(const V v) {
+ }
+ 
+ // ------------------------------ BroadcastSignBit
+-
+ template <class V>
+ HWY_API V BroadcastSignBit(const V v) {
+   return ShiftRight<sizeof(TFromV<V>) * 8 - 1>(v);
+@@ -774,29 +793,61 @@ HWY_API V BroadcastSignBit(const V v) {
+ 
+ // ------------------------------ AllFalse
+ 
+-#define HWY_RVV_ALL_FALSE(MLEN, NAME, OP)          \
+-  HWY_API bool AllFalse(const HWY_RVV_M(MLEN) m) { \
+-    return vfirst_m_b##MLEN(m) < 0;                \
++#define HWY_RVV_ALL_FALSE(MLEN, NAME, OP)                     \
++  template <class D>                                          \
++  HWY_API bool AllFalse(const D d, const HWY_RVV_M(MLEN) m) { \
++    static_assert(MLenFromD(d) == MLEN, "Type mismatch");     \
++    return vfirst_m_b##MLEN(m) < 0;                           \
++  }                                                           \
++  /* DEPRECATED */                                            \
++  HWY_API bool AllFalse(const HWY_RVV_M(MLEN) m) {            \
++    return vfirst_m_b##MLEN(m) < 0;                           \
+   }
+ HWY_RVV_FOREACH_B(HWY_RVV_ALL_FALSE, _, _)
+ #undef HWY_RVV_ALL_FALSE
+ 
+ // ------------------------------ AllTrue
+ 
+-#define HWY_RVV_ALL_TRUE(MLEN, NAME, OP)    \
+-  HWY_API bool AllTrue(HWY_RVV_M(MLEN) m) { \
+-    return AllFalse(vmnot_m_b##MLEN(m));    \
++#define HWY_RVV_ALL_TRUE(MLEN, NAME, OP)                  \
++  template <class D>                                      \
++  HWY_API bool AllTrue(D d, HWY_RVV_M(MLEN) m) {          \
++    static_assert(MLenFromD(d) == MLEN, "Type mismatch"); \
++    return AllFalse(vmnot_m_b##MLEN(m));                  \
++  }                                                       \
++  /* DEPRECATED */                                        \
++  HWY_API bool AllTrue(HWY_RVV_M(MLEN) m) {               \
++    return AllFalse(vmnot_m_b##MLEN(m));                  \
+   }
++
+ HWY_RVV_FOREACH_B(HWY_RVV_ALL_TRUE, _, _)
+ #undef HWY_RVV_ALL_TRUE
+ 
+ // ------------------------------ CountTrue
+ 
+-#define HWY_RVV_COUNT_TRUE(MLEN, NAME, OP) \
++#define HWY_RVV_COUNT_TRUE(MLEN, NAME, OP)                \
++  template <class D>                                      \
++  HWY_API size_t CountTrue(D d, HWY_RVV_M(MLEN) m) {      \
++    static_assert(MLenFromD(d) == MLEN, "Type mismatch"); \
++    return vpopc_m_b##MLEN(m);                            \
++  }                                                       \
++  /* DEPRECATED */                                        \
+   HWY_API size_t CountTrue(HWY_RVV_M(MLEN) m) { return vpopc_m_b##MLEN(m); }
++
+ HWY_RVV_FOREACH_B(HWY_RVV_COUNT_TRUE, _, _)
+ #undef HWY_RVV_COUNT_TRUE
+ 
++// ------------------------------ FindFirstTrue
++
++#define HWY_RVV_FIND_FIRST_TRUE(MLEN, NAME, OP)           \
++  template <class D>                                      \
++  HWY_API intptr_t FindFirstTrue(D d, HWY_RVV_M(MLEN) m) { \
++    static_assert(MLenFromD(d) == MLEN, "Type mismatch"); \
++    return vfirst_m_b##MLEN(m);                           \
++  }
++
++HWY_RVV_FOREACH_B(HWY_RVV_FIND_FIRST_TRUE, _, _)
++#undef HWY_RVV_FIND_FIRST_TRUE
++
+ // ================================================== MEMORY
+ 
+ // ------------------------------ Load
+@@ -852,7 +903,6 @@ HWY_API void StoreU(const V v, D d, TFromD<D>* HWY_RESTRICT p) {
+ }
+ 
+ // ------------------------------ Stream
+-
+ template <class V, class D, typename T>
+ HWY_API void Stream(const V v, D d, T* HWY_RESTRICT aligned) {
+   Store(v, d, aligned);
+@@ -1167,29 +1217,110 @@ HWY_API VFromD<Simd<T, N>> ConvertTo(Simd<T, N> /*tag*/, FromV v) {
+   return ConvertTo(Full<T>(), v);
+ }
+ 
+-// ================================================== SWIZZLE
++// ================================================== COMBINE
+ 
+-// ------------------------------ Compress
++namespace detail {
+ 
+-#define HWY_RVV_COMPRESS(BASE, CHAR, SEW, LMUL, SHIFT, MLEN, NAME, OP) \
+-  HWY_API HWY_RVV_V(BASE, SEW, LMUL)                                   \
+-      NAME(HWY_RVV_V(BASE, SEW, LMUL) v, HWY_RVV_M(MLEN) mask) {       \
+-    return v##OP##_vm_##CHAR##SEW##LMUL(mask, v, v);                   \
++// For x86-compatible behaviour mandated by Highway API: TableLookupBytes
++// offsets are implicitly relative to the start of their 128-bit block.
++template <typename T, size_t N>
++constexpr size_t LanesPerBlock(Simd<T, N> /* tag */) {
++  // Also cap to the limit imposed by D (for fixed-size <= 128-bit vectors).
++  return HWY_MIN(16 / sizeof(T), N);
++}
++
++template <class D, class V>
++HWY_INLINE V OffsetsOf128BitBlocks(const D d, const V iota0) {
++  using T = MakeUnsigned<TFromD<D>>;
++  return AndS(iota0, static_cast<T>(~(LanesPerBlock(d) - 1)));
++}
++
++template <size_t kLanes, class D>
++HWY_INLINE MFromD<D> FirstNPerBlock(D /* tag */) {
++  const RebindToUnsigned<D> du;
++  const RebindToSigned<D> di;
++  constexpr size_t kLanesPerBlock = LanesPerBlock(du);
++  const auto idx_mod = AndS(Iota0(du), kLanesPerBlock - 1);
++  return LtS(BitCast(di, idx_mod), static_cast<TFromD<decltype(di)>>(kLanes));
++}
++
++// vector = f(vector, vector, size_t)
++#define HWY_RVV_SLIDE(BASE, CHAR, SEW, LMUL, SHIFT, MLEN, NAME, OP)        \
++  HWY_API HWY_RVV_V(BASE, SEW, LMUL)                                       \
++      NAME(HWY_RVV_V(BASE, SEW, LMUL) dst, HWY_RVV_V(BASE, SEW, LMUL) src, \
++           size_t lanes) {                                                 \
++    return v##OP##_vx_##CHAR##SEW##LMUL(dst, src, lanes);                  \
+   }
+ 
+-HWY_RVV_FOREACH_UI16(HWY_RVV_COMPRESS, Compress, compress)
+-HWY_RVV_FOREACH_UI32(HWY_RVV_COMPRESS, Compress, compress)
+-HWY_RVV_FOREACH_UI64(HWY_RVV_COMPRESS, Compress, compress)
+-HWY_RVV_FOREACH_F(HWY_RVV_COMPRESS, Compress, compress)
+-#undef HWY_RVV_COMPRESS
++HWY_RVV_FOREACH(HWY_RVV_SLIDE, SlideUp, slideup)
++HWY_RVV_FOREACH(HWY_RVV_SLIDE, SlideDown, slidedown)
+ 
+-// ------------------------------ CompressStore
++#undef HWY_RVV_SLIDE
+ 
+-template <class V, class M, class D>
+-HWY_API size_t CompressStore(const V v, const M mask, const D d,
+-                             TFromD<D>* HWY_RESTRICT aligned) {
+-  Store(Compress(v, mask), d, aligned);
+-  return CountTrue(mask);
++}  // namespace detail
++
++// ------------------------------ ConcatUpperLower
++template <class V>
++HWY_API V ConcatUpperLower(const V hi, const V lo) {
++  const RebindToSigned<DFromV<V>> di;
++  return IfThenElse(FirstN(di, Lanes(di) / 2), lo, hi);
++}
++
++// ------------------------------ ConcatLowerLower
++template <class V>
++HWY_API V ConcatLowerLower(const V hi, const V lo) {
++  return detail::SlideUp(lo, hi, Lanes(DFromV<V>()) / 2);
++}
++
++// ------------------------------ ConcatUpperUpper
++template <class V>
++HWY_API V ConcatUpperUpper(const V hi, const V lo) {
++  // Move upper half into lower
++  const auto lo_down = detail::SlideDown(lo, lo, Lanes(DFromV<V>()) / 2);
++  return ConcatUpperLower(hi, lo_down);
++}
++
++// ------------------------------ ConcatLowerUpper
++template <class V>
++HWY_API V ConcatLowerUpper(const V hi, const V lo) {
++  // Move half of both inputs to the other half
++  const auto hi_up = detail::SlideUp(hi, hi, Lanes(DFromV<V>()) / 2);
++  const auto lo_down = detail::SlideDown(lo, lo, Lanes(DFromV<V>()) / 2);
++  return ConcatUpperLower(hi_up, lo_down);
++}
++
++// ------------------------------ Combine
++
++// TODO(janwas): implement after LMUL ext/trunc
++#if 0
++
++template <class V>
++HWY_API V Combine(const V a, const V b) {
++  using D = DFromV<V>;
++  // double LMUL of inputs, then SlideUp with Lanes().
++}
++
++#endif
++
++// ================================================== SWIZZLE
++
++// ------------------------------ GetLane
++
++#define HWY_RVV_GET_LANE(BASE, CHAR, SEW, LMUL, SHIFT, MLEN, NAME, OP) \
++  HWY_API HWY_RVV_T(BASE, SEW) NAME(HWY_RVV_V(BASE, SEW, LMUL) v) {    \
++    return v##OP##_s_##CHAR##SEW##LMUL##_##CHAR##SEW(v);               \
++  }
++
++HWY_RVV_FOREACH_UI(HWY_RVV_GET_LANE, GetLane, mv_x)
++HWY_RVV_FOREACH_F(HWY_RVV_GET_LANE, GetLane, fmv_f)
++#undef HWY_RVV_GET_LANE
++
++// ------------------------------ OddEven
++template <class V>
++HWY_API V OddEven(const V a, const V b) {
++  const RebindToUnsigned<DFromV<V>> du;  // Iota0 is unsigned only
++  const auto is_even = Eq(detail::AndS(detail::Iota0(du), 1), Zero(du));
++  return IfThenElse(is_even, b, a);
+ }
+ 
+ // ------------------------------ TableLookupLanes
+@@ -1216,105 +1347,134 @@ HWY_API VFromD<DU> SetTableIndices(D d, const TFromD<DU>* idx) {
+ HWY_RVV_FOREACH(HWY_RVV_TABLE, TableLookupLanes, rgather)
+ #undef HWY_RVV_TABLE
+ 
+-// ------------------------------ Shuffle01
++// ------------------------------ Compress
+ 
+-template <class V>
+-HWY_API V Shuffle01(const V v) {
+-  using D = DFromV<V>;
+-  static_assert(sizeof(TFromD<D>) == 8, "Defined for 64-bit types");
+-  const auto idx = detail::Xor(detail::Iota0(D()), 1);
+-  return TableLookupLanes(v, idx);
+-}
++#define HWY_RVV_COMPRESS(BASE, CHAR, SEW, LMUL, SHIFT, MLEN, NAME, OP) \
++  HWY_API HWY_RVV_V(BASE, SEW, LMUL)                                   \
++      NAME(HWY_RVV_V(BASE, SEW, LMUL) v, HWY_RVV_M(MLEN) mask) {       \
++    return v##OP##_vm_##CHAR##SEW##LMUL(mask, v, v);                   \
++  }
+ 
+-// ------------------------------ Shuffle2301
++HWY_RVV_FOREACH_UI16(HWY_RVV_COMPRESS, Compress, compress)
++HWY_RVV_FOREACH_UI32(HWY_RVV_COMPRESS, Compress, compress)
++HWY_RVV_FOREACH_UI64(HWY_RVV_COMPRESS, Compress, compress)
++HWY_RVV_FOREACH_F(HWY_RVV_COMPRESS, Compress, compress)
++#undef HWY_RVV_COMPRESS
+ 
+-template <class V>
+-HWY_API V Shuffle2301(const V v) {
+-  using D = DFromV<V>;
+-  static_assert(sizeof(TFromD<D>) == 4, "Defined for 32-bit types");
+-  const auto idx = detail::Xor(detail::Iota0(D()), 1);
+-  return TableLookupLanes(v, idx);
++// ------------------------------ CompressStore
++template <class V, class M, class D>
++HWY_API size_t CompressStore(const V v, const M mask, const D d,
++                             TFromD<D>* HWY_RESTRICT aligned) {
++  Store(Compress(v, mask), d, aligned);
++  return CountTrue(d, mask);
+ }
+ 
+-// ------------------------------ Shuffle1032
++// ================================================== BLOCKWISE
+ 
+-template <class V>
+-HWY_API V Shuffle1032(const V v) {
+-  using D = DFromV<V>;
+-  static_assert(sizeof(TFromD<D>) == 4, "Defined for 32-bit types");
+-  const auto idx = detail::Xor(detail::Iota0(D()), 2);
+-  return TableLookupLanes(v, idx);
++// ------------------------------ CombineShiftRightBytes
++template <size_t kBytes, class D, class V = VFromD<D>>
++HWY_API V CombineShiftRightBytes(const D d, const V hi, V lo) {
++  const Repartition<uint8_t, decltype(d)> d8;
++  Lanes(d8);
++  const auto hi8 = BitCast(d8, hi);
++  const auto lo8 = BitCast(d8, lo);
++  const auto hi_up = detail::SlideUp(hi8, hi8, 16 - kBytes);
++  const auto lo_down = detail::SlideDown(lo8, lo8, kBytes);
++  const auto is_lo = detail::FirstNPerBlock<16 - kBytes>(d8);
++  const auto combined = BitCast(d, IfThenElse(is_lo, lo_down, hi_up));
++  Lanes(d);
++  return combined;
+ }
+ 
+-// ------------------------------ Shuffle0123
++// ------------------------------ CombineShiftRightLanes
++template <size_t kLanes, class D, class V = VFromD<D>>
++HWY_API V CombineShiftRightLanes(const D d, const V hi, V lo) {
++  constexpr size_t kLanesUp = 16 / sizeof(TFromV<V>) - kLanes;
++  const auto hi_up = detail::SlideUp(hi, hi, kLanesUp);
++  const auto lo_down = detail::SlideDown(lo, lo, kLanes);
++  const auto is_lo = detail::FirstNPerBlock<kLanesUp>(d);
++  return IfThenElse(is_lo, lo_down, hi_up);
++}
+ 
++// ------------------------------ Shuffle2301 (ShiftLeft)
+ template <class V>
+-HWY_API V Shuffle0123(const V v) {
+-  using D = DFromV<V>;
+-  static_assert(sizeof(TFromD<D>) == 4, "Defined for 32-bit types");
+-  const auto idx = detail::Xor(detail::Iota0(D()), 3);
+-  return TableLookupLanes(v, idx);
++HWY_API V Shuffle2301(const V v) {
++  const DFromV<V> d;
++  static_assert(sizeof(TFromD<decltype(d)>) == 4, "Defined for 32-bit types");
++  const Repartition<uint64_t, decltype(d)> du64;
++  const auto v64 = BitCast(du64, v);
++  Lanes(du64);
++  const auto rotated = BitCast(d, Or(ShiftRight<32>(v64), ShiftLeft<32>(v64)));
++  Lanes(d);
++  return rotated;
+ }
+ 
+ // ------------------------------ Shuffle2103
+-
+ template <class V>
+ HWY_API V Shuffle2103(const V v) {
+-  using D = DFromV<V>;
+-  static_assert(sizeof(TFromD<D>) == 4, "Defined for 32-bit types");
+-  // This shuffle is a rotation. We can compute subtraction modulo 4 (number of
+-  // lanes per 128-bit block) via bitwise ops.
+-  const auto i = detail::Xor(detail::Iota0(D()), 1);
+-  const auto lsb = detail::And(i, 1);
+-  const auto borrow = Add(lsb, lsb);
+-  const auto idx = Xor(i, borrow);
+-  return TableLookupLanes(v, idx);
++  const DFromV<V> d;
++  static_assert(sizeof(TFromD<decltype(d)>) == 4, "Defined for 32-bit types");
++  return CombineShiftRightLanes<3>(d, v, v);
+ }
+ 
+ // ------------------------------ Shuffle0321
+-
+ template <class V>
+ HWY_API V Shuffle0321(const V v) {
+-  using D = DFromV<V>;
+-  static_assert(sizeof(TFromD<D>) == 4, "Defined for 32-bit types");
+-  // This shuffle is a rotation. We can compute subtraction modulo 4 (number of
+-  // lanes per 128-bit block) via bitwise ops.
+-  const auto i = detail::Xor(detail::Iota0(D()), 3);
+-  const auto lsb = detail::And(i, 1);
+-  const auto borrow = Add(lsb, lsb);
+-  const auto idx = Xor(i, borrow);
+-  return TableLookupLanes(v, idx);
++  const DFromV<V> d;
++  static_assert(sizeof(TFromD<decltype(d)>) == 4, "Defined for 32-bit types");
++  return CombineShiftRightLanes<1>(d, v, v);
+ }
+ 
+-// ------------------------------ TableLookupBytes
+-
+-namespace detail {
+-
+-// For x86-compatible behaviour mandated by Highway API: TableLookupBytes
+-// offsets are implicitly relative to the start of their 128-bit block.
+-template <class D>
+-constexpr size_t LanesPerBlock(D) {
+-  return 16 / sizeof(TFromD<D>);
++// ------------------------------ Shuffle1032
++template <class V>
++HWY_API V Shuffle1032(const V v) {
++  const DFromV<V> d;
++  static_assert(sizeof(TFromD<decltype(d)>) == 4, "Defined for 32-bit types");
++  return CombineShiftRightLanes<2>(d, v, v);
+ }
+ 
+-template <class D, class V>
+-HWY_API V OffsetsOf128BitBlocks(const D d, const V iota0) {
+-  using T = MakeUnsigned<TFromD<D>>;
+-  return detail::And(iota0, static_cast<T>(~(LanesPerBlock(d) - 1)));
++// ------------------------------ Shuffle01
++template <class V>
++HWY_API V Shuffle01(const V v) {
++  const DFromV<V> d;
++  static_assert(sizeof(TFromD<decltype(d)>) == 8, "Defined for 64-bit types");
++  return CombineShiftRightLanes<1>(d, v, v);
+ }
+-}  // namespace detail
+ 
++// ------------------------------ Shuffle0123
+ template <class V>
+-HWY_API V TableLookupBytes(const V v, const V idx) {
+-  using D = DFromV<V>;
+-  const Repartition<uint8_t, D> d8;
++HWY_API V Shuffle0123(const V v) {
++  return Shuffle2301(Shuffle1032(v));
++}
++
++// ------------------------------ TableLookupBytes
++
++template <class V, class VI>
++HWY_API VI TableLookupBytes(const V v, const VI idx) {
++  const DFromV<VI> d;
++  const Repartition<uint8_t, decltype(d)> d8;
++  Lanes(d8);
+   const auto offsets128 = detail::OffsetsOf128BitBlocks(d8, detail::Iota0(d8));
+   const auto idx8 = Add(BitCast(d8, idx), offsets128);
+-  return BitCast(D(), TableLookupLanes(BitCast(d8, v), idx8));
++  const auto out = BitCast(d, TableLookupLanes(BitCast(d8, v), idx8));
++  Lanes(d);
++  return out;
++}
++
++template <class V, class VI>
++HWY_API V TableLookupBytesOr0(const VI v, const V idx) {
++  const DFromV<VI> d;
++  // Mask size must match vector type, so cast everything to this type.
++  const Repartition<int8_t, decltype(d)> di8;
++  Lanes(di8);
++  const auto lookup = TableLookupBytes(BitCast(di8, v), BitCast(di8, idx));
++  const auto msb = Lt(BitCast(di8, idx), Zero(di8));
++  const auto out = BitCast(d, IfThenZeroElse(msb, lookup));
++  Lanes(d);
++  return out;
+ }
+ 
+ // ------------------------------ Broadcast
+-
+ template <int kLane, class V>
+ HWY_API V Broadcast(const V v) {
+   const DFromV<V> d;
+@@ -1322,195 +1482,134 @@ HWY_API V Broadcast(const V v) {
+   static_assert(0 <= kLane && kLane < kLanesPerBlock, "Invalid lane");
+   auto idx = detail::OffsetsOf128BitBlocks(d, detail::Iota0(d));
+   if (kLane != 0) {
+-    idx = detail::Add(idx, kLane);
++    idx = detail::AddS(idx, kLane);
+   }
+   return TableLookupLanes(v, idx);
+ }
+ 
+-// ------------------------------ GetLane
+-
+-#define HWY_RVV_GET_LANE(BASE, CHAR, SEW, LMUL, SHIFT, MLEN, NAME, OP) \
+-  HWY_API HWY_RVV_T(BASE, SEW) NAME(HWY_RVV_V(BASE, SEW, LMUL) v) {    \
+-    return v##OP##_s_##CHAR##SEW##LMUL##_##CHAR##SEW(v);               \
+-  }
+-
+-HWY_RVV_FOREACH_UI(HWY_RVV_GET_LANE, GetLane, mv_x)
+-HWY_RVV_FOREACH_F(HWY_RVV_GET_LANE, GetLane, fmv_f)
+-#undef HWY_RVV_GET_LANE
+-
+ // ------------------------------ ShiftLeftLanes
+ 
+-// vector = f(vector, vector, size_t)
+-#define HWY_RVV_SLIDE(BASE, CHAR, SEW, LMUL, SHIFT, MLEN, NAME, OP)        \
+-  HWY_API HWY_RVV_V(BASE, SEW, LMUL)                                       \
+-      NAME(HWY_RVV_V(BASE, SEW, LMUL) dst, HWY_RVV_V(BASE, SEW, LMUL) src, \
+-           size_t lanes) {                                                 \
+-    return v##OP##_vx_##CHAR##SEW##LMUL(dst, src, lanes);                  \
+-  }
+-
+-namespace detail {
+-HWY_RVV_FOREACH(HWY_RVV_SLIDE, SlideUp, slideup)
+-}  // namespace detail
+-
+-template <size_t kLanes, class V>
+-HWY_API V ShiftLeftLanes(const V v) {
+-  using D = DFromV<V>;
+-  const RebindToSigned<D> di;
++template <size_t kLanes, class D, class V = VFromD<D>>
++HWY_API V ShiftLeftLanes(const D d, const V v) {
++  const RebindToSigned<decltype(d)> di;
+   const auto shifted = detail::SlideUp(v, v, kLanes);
+   // Match x86 semantics by zeroing lower lanes in 128-bit blocks
+   constexpr size_t kLanesPerBlock = detail::LanesPerBlock(di);
+-  const auto idx_mod = detail::And(detail::Iota0(di), kLanesPerBlock - 1);
++  const auto idx_mod = detail::AndS(detail::Iota0(di), kLanesPerBlock - 1);
+   const auto clear = Lt(BitCast(di, idx_mod), Set(di, kLanes));
+   return IfThenZeroElse(clear, shifted);
+ }
+ 
++template <size_t kLanes, class V>
++HWY_API V ShiftLeftLanes(const V v) {
++  return ShiftLeftLanes<kLanes>(DFromV<V>(), v);
++}
++
+ // ------------------------------ ShiftLeftBytes
+ 
+ template <int kBytes, class V>
+-HWY_API V ShiftLeftBytes(const V v) {
+-  using D = DFromV<V>;
+-  const Repartition<uint8_t, D> d8;
++HWY_API V ShiftLeftBytes(DFromV<V> d, const V v) {
++  const Repartition<uint8_t, decltype(d)> d8;
+   Lanes(d8);
+-  return BitCast(D(), ShiftLeftLanes<kBytes>(BitCast(d8, v)));
++  const auto shifted = BitCast(d, ShiftLeftLanes<kBytes>(BitCast(d8, v)));
++  Lanes(d);
++  return shifted;
+ }
+ 
+-// ------------------------------ ShiftRightLanes
+-
+-namespace detail {
+-HWY_RVV_FOREACH(HWY_RVV_SLIDE, SlideDown, slidedown)
+-}  // namespace detail
++template <int kBytes, class V>
++HWY_API V ShiftLeftBytes(const V v) {
++  return ShiftLeftBytes<kBytes>(DFromV<V>(), v);
++}
+ 
+-#undef HWY_RVV_SLIDE
++// ------------------------------ ShiftRightLanes
++template <size_t kLanes, typename T, size_t N, class V = VFromD<Simd<T, N>>>
++HWY_API V ShiftRightLanes(const Simd<T, N> d, V v) {
++  const RebindToSigned<decltype(d)> di;
++  // For partial vectors, clear upper lanes so we shift in zeros.
++  if (N <= 16 / sizeof(T)) {
++    v = IfThenElseZero(FirstN(d, N), v);
++  }
+ 
+-template <size_t kLanes, class V>
+-HWY_API V ShiftRightLanes(const V v) {
+-  using D = DFromV<V>;
+-  const RebindToSigned<D> di;
+   const auto shifted = detail::SlideDown(v, v, kLanes);
+   // Match x86 semantics by zeroing upper lanes in 128-bit blocks
+   constexpr size_t kLanesPerBlock = detail::LanesPerBlock(di);
+-  const auto idx_mod = detail::And(detail::Iota0(di), kLanesPerBlock - 1);
++  const auto idx_mod = detail::AndS(detail::Iota0(di), kLanesPerBlock - 1);
+   const auto keep = Lt(BitCast(di, idx_mod), Set(di, kLanesPerBlock - kLanes));
+   return IfThenElseZero(keep, shifted);
+ }
+ 
+ // ------------------------------ ShiftRightBytes
+-
+-template <int kBytes, class V>
+-HWY_API V ShiftRightBytes(const V v) {
+-  using D = DFromV<V>;
+-  const Repartition<uint8_t, D> d8;
++template <int kBytes, class D, class V = VFromD<D>>
++HWY_API V ShiftRightBytes(const D d, const V v) {
++  const Repartition<uint8_t, decltype(d)> d8;
+   Lanes(d8);
+-  return BitCast(D(), ShiftRightLanes<kBytes>(BitCast(d8, v)));
+-}
+-
+-// ------------------------------ OddEven
+-
+-template <class V>
+-HWY_API V OddEven(const V a, const V b) {
+-  const RebindToUnsigned<DFromV<V>> du;  // Iota0 is unsigned only
+-  const auto is_even = Eq(detail::And(detail::Iota0(du), 1), Zero(du));
+-  return IfThenElse(is_even, b, a);
+-}
+-
+-// ------------------------------ ConcatUpperLower
+-
+-template <class V>
+-HWY_API V ConcatUpperLower(const V hi, const V lo) {
+-  const RebindToSigned<DFromV<V>> di;
+-  const auto idx_half = Set(di, Lanes(di) / 2);
+-  const auto is_lower_half = Lt(BitCast(di, detail::Iota0(di)), idx_half);
+-  return IfThenElse(is_lower_half, lo, hi);
+-}
+-
+-// ------------------------------ ConcatLowerLower
+-
+-template <class V>
+-HWY_API V ConcatLowerLower(const V hi, const V lo) {
+-  // Move lower half into upper
+-  const auto hi_up = detail::SlideUp(hi, hi, Lanes(DFromV<V>()) / 2);
+-  return ConcatUpperLower(hi_up, lo);
+-}
+-
+-// ------------------------------ ConcatUpperUpper
+-
+-template <class V>
+-HWY_API V ConcatUpperUpper(const V hi, const V lo) {
+-  // Move upper half into lower
+-  const auto lo_down = detail::SlideDown(lo, lo, Lanes(DFromV<V>()) / 2);
+-  return ConcatUpperLower(hi, lo_down);
+-}
+-
+-// ------------------------------ ConcatLowerUpper
+-
+-template <class V>
+-HWY_API V ConcatLowerUpper(const V hi, const V lo) {
+-  // Move half of both inputs to the other half
+-  const auto hi_up = detail::SlideUp(hi, hi, Lanes(DFromV<V>()) / 2);
+-  const auto lo_down = detail::SlideDown(lo, lo, Lanes(DFromV<V>()) / 2);
+-  return ConcatUpperLower(hi_up, lo_down);
++  const auto shifted = BitCast(d, ShiftRightLanes<kBytes>(d8, BitCast(d8, v)));
++  Lanes(d);
++  return shifted;
+ }
+ 
+ // ------------------------------ InterleaveLower
+ 
+-template <class V>
+-HWY_API V InterleaveLower(const V a, const V b) {
+-  const DFromV<V> d;
++// TODO(janwas): PromoteTo(LowerHalf), slide1up, add
++template <class D, class V>
++HWY_API V InterleaveLower(D d, const V a, const V b) {
++  static_assert(IsSame<TFromD<D>, TFromV<V>>(), "D/V mismatch");
+   const RebindToUnsigned<decltype(d)> du;
+-  constexpr size_t kLanesPerBlock = detail::LanesPerBlock(d);
+-  const auto i = detail::Iota0(d);
+-  const auto idx_mod = ShiftRight<1>(detail::And(i, kLanesPerBlock - 1));
++  constexpr size_t kLanesPerBlock = detail::LanesPerBlock(du);
++  const auto i = detail::Iota0(du);
++  const auto idx_mod = ShiftRight<1>(detail::AndS(i, kLanesPerBlock - 1));
+   const auto idx = Add(idx_mod, detail::OffsetsOf128BitBlocks(d, i));
+-  const auto is_even = Eq(detail::And(i, 1), Zero(du));
++  const auto is_even = Eq(detail::AndS(i, 1), Zero(du));
+   return IfThenElse(is_even, TableLookupLanes(a, idx),
+                     TableLookupLanes(b, idx));
+ }
+ 
++template <class V>
++HWY_API V InterleaveLower(const V a, const V b) {
++  return InterleaveLower(DFromV<V>(), a, b);
++}
++
+ // ------------------------------ InterleaveUpper
+ 
+-template <class V>
+-HWY_API V InterleaveUpper(const V a, const V b) {
+-  const DFromV<V> d;
++template <class D, class V>
++HWY_API V InterleaveUpper(const D d, const V a, const V b) {
++  static_assert(IsSame<TFromD<D>, TFromV<V>>(), "D/V mismatch");
+   const RebindToUnsigned<decltype(d)> du;
+-  constexpr size_t kLanesPerBlock = detail::LanesPerBlock(d);
+-  const auto i = detail::Iota0(d);
+-  const auto idx_mod = ShiftRight<1>(detail::And(i, kLanesPerBlock - 1));
++  constexpr size_t kLanesPerBlock = detail::LanesPerBlock(du);
++  const auto i = detail::Iota0(du);
++  const auto idx_mod = ShiftRight<1>(detail::AndS(i, kLanesPerBlock - 1));
+   const auto idx_lower = Add(idx_mod, detail::OffsetsOf128BitBlocks(d, i));
+-  const auto idx = detail::Add(idx_lower, kLanesPerBlock / 2);
+-  const auto is_even = Eq(detail::And(i, 1), Zero(du));
++  const auto idx = detail::AddS(idx_lower, kLanesPerBlock / 2);
++  const auto is_even = Eq(detail::AndS(i, 1), Zero(du));
+   return IfThenElse(is_even, TableLookupLanes(a, idx),
+                     TableLookupLanes(b, idx));
+ }
+ 
+ // ------------------------------ ZipLower
+ 
+-template <class V>
+-HWY_API VFromD<RepartitionToWide<DFromV<V>>> ZipLower(const V a, const V b) {
+-  RepartitionToWide<DFromV<V>> dw;
+-  return BitCast(dw, InterleaveLower(a, b));
++template <class V, class DW = RepartitionToWide<DFromV<V>>>
++HWY_API VFromD<DW> ZipLower(DW dw, V a, V b) {
++  const RepartitionToNarrow<DW> dn;
++  static_assert(IsSame<TFromD<decltype(dn)>, TFromV<V>>(), "D/V mismatch");
++  const auto zipped = BitCast(dw, InterleaveLower(dn, a, b));
++  Lanes(dw);
++  return zipped;
+ }
+-
+-// ------------------------------ ZipUpper
+-
+-template <class V>
+-HWY_API VFromD<RepartitionToWide<DFromV<V>>> ZipUpper(const V a, const V b) {
+-  RepartitionToWide<DFromV<V>> dw;
+-  return BitCast(dw, InterleaveUpper(a, b));
++template <class V, class DW = RepartitionToWide<DFromV<V>>>
++HWY_API VFromD<DW> ZipLower(const V a, const V b) {
++  return ZipLower(DW(), a, b);
+ }
+ 
+-// ------------------------------ Combine
+-
+-// TODO(janwas): implement after LMUL ext/trunc
+-#if 0
+-
+-template <class V>
+-HWY_API V Combine(const V a, const V b) {
+-  using D = DFromV<V>;
+-  // double LMUL of inputs, then SlideUp with Lanes().
++// ------------------------------ ZipUpper
++template <class DW, class V>
++HWY_API VFromD<DW> ZipUpper(DW dw, V a, V b) {
++  const RepartitionToNarrow<DW> dn;
++  static_assert(IsSame<TFromD<decltype(dn)>, TFromV<V>>(), "D/V mismatch");
++  const auto zipped = BitCast(dw, InterleaveUpper(dn, a, b));
++  Lanes(dw);
++  return zipped;
+ }
+ 
+-#endif
+-
+ // ================================================== REDUCE
+ 
+ // vector = f(vector, zero_m1)
+@@ -1530,10 +1629,9 @@ HWY_RVV_FOREACH_UI(HWY_RVV_REDUCE, RedSum, redsum)
+ HWY_RVV_FOREACH_F(HWY_RVV_REDUCE, RedSum, fredsum)
+ }  // namespace detail
+ 
+-template <class V>
+-HWY_API V SumOfLanes(const V v) {
+-  using T = TFromV<V>;
+-  const auto v0 = Zero(Full<T>());  // always m1
++template <class D>
++HWY_API VFromD<D> SumOfLanes(D /* d */, const VFromD<D> v) {
++  const auto v0 = Zero(Full<TFromD<D>>());  // always m1
+   return detail::RedSum(v, v0);
+ }
+ 
+@@ -1544,9 +1642,9 @@ HWY_RVV_FOREACH_I(HWY_RVV_REDUCE, RedMin, redmin)
+ HWY_RVV_FOREACH_F(HWY_RVV_REDUCE, RedMin, fredmin)
+ }  // namespace detail
+ 
+-template <class V>
+-HWY_API V MinOfLanes(const V v) {
+-  using T = TFromV<V>;
++template <class D>
++HWY_API VFromD<D> MinOfLanes(D /* d */, const VFromD<D> v) {
++  using T = TFromD<D>;
+   const Full<T> d1;  // always m1
+   const auto neutral = Set(d1, HighestValue<T>());
+   return detail::RedMin(v, neutral);
+@@ -1559,9 +1657,9 @@ HWY_RVV_FOREACH_I(HWY_RVV_REDUCE, RedMax, redmax)
+ HWY_RVV_FOREACH_F(HWY_RVV_REDUCE, RedMax, fredmax)
+ }  // namespace detail
+ 
+-template <class V>
+-HWY_API V MaxOfLanes(const V v) {
+-  using T = TFromV<V>;
++template <class D>
++HWY_API VFromD<D> MaxOfLanes(D /* d */, const VFromD<D> v) {
++  using T = TFromD<D>;
+   const Full<T> d1;  // always m1
+   const auto neutral = Set(d1, LowestValue<T>());
+   return detail::RedMax(v, neutral);
+@@ -1579,21 +1677,26 @@ HWY_API VFromD<D> LoadDup128(D d, const TFromD<D>* const HWY_RESTRICT p) {
+   const auto loaded = Load(d, p);
+   constexpr size_t kLanesPerBlock = detail::LanesPerBlock(d);
+   // Broadcast the first block
+-  const auto idx = detail::And(detail::Iota0(d), kLanesPerBlock - 1);
++  const auto idx = detail::AndS(detail::Iota0(d), kLanesPerBlock - 1);
+   return TableLookupLanes(loaded, idx);
+ }
+ 
+ // ------------------------------ StoreMaskBits
+-#define HWY_RVV_STORE_MASK_BITS(MLEN, NAME, OP)                 \
+-  HWY_API size_t StoreMaskBits(HWY_RVV_M(MLEN) m, uint8_t* p) { \
+-    /* LMUL=1 is always enough */                               \
+-    Full<uint8_t> d8;                                           \
+-    const size_t num_bytes = (Lanes(d8) + MLEN - 1) / MLEN;     \
+-    /* TODO(janwas): how to convert vbool* to vuint?*/          \
+-    /*Store(m, d8, p);*/                                        \
+-    (void)m;                                                    \
+-    (void)p;                                                    \
+-    return num_bytes;                                           \
++#define HWY_RVV_STORE_MASK_BITS(MLEN, NAME, OP)                              \
++  /* DEPRECATED */                                                           \
++  HWY_API size_t StoreMaskBits(HWY_RVV_M(MLEN) m, uint8_t* p) {              \
++    /* LMUL=1 is always enough */                                            \
++    Full<uint8_t> d8;                                                        \
++    const size_t num_bytes = (Lanes(d8) + MLEN - 1) / MLEN;                  \
++    /* TODO(janwas): how to convert vbool* to vuint?*/                       \
++    /*Store(m, d8, p);*/                                                     \
++    (void)m;                                                                 \
++    (void)p;                                                                 \
++    return num_bytes;                                                        \
++  }                                                                          \
++  template <class D>                                                         \
++  HWY_API size_t StoreMaskBits(D /* tag */, HWY_RVV_M(MLEN) m, uint8_t* p) { \
++    return StoreMaskBits(m, p);                                              \
+   }
+ HWY_RVV_FOREACH_B(HWY_RVV_STORE_MASK_BITS, _, _)
+ #undef HWY_RVV_STORE_MASK_BITS
+@@ -1614,7 +1717,7 @@ HWY_API MFromD<D> FirstN(const D d, const size_t n) {
+   return Eq(detail::SlideUp(one, zero, n), one);
+ }
+ 
+-// ------------------------------ Neg
++// ------------------------------ Neg (Sub)
+ 
+ template <class V, HWY_IF_SIGNED_V(V)>
+ HWY_API V Neg(const V v) {
+@@ -1629,7 +1732,7 @@ HWY_API V Neg(const V v) {
+ 
+ HWY_RVV_FOREACH_F(HWY_RVV_RETV_ARGV2, Neg, fsgnjn)
+ 
+-// ------------------------------ Abs
++// ------------------------------ Abs (Max, Neg)
+ 
+ template <class V, HWY_IF_SIGNED_V(V)>
+ HWY_API V Abs(const V v) {
+@@ -1640,14 +1743,13 @@ HWY_RVV_FOREACH_F(HWY_RVV_RETV_ARGV2, Abs, fsgnjx)
+ 
+ #undef HWY_RVV_RETV_ARGV2
+ 
+-// ------------------------------ AbsDiff
+-
++// ------------------------------ AbsDiff (Abs, Sub)
+ template <class V>
+ HWY_API V AbsDiff(const V a, const V b) {
+   return Abs(Sub(a, b));
+ }
+ 
+-// ------------------------------ Round
++// ------------------------------ Round  (NearestInt, ConvertTo, CopySign)
+ 
+ // IEEE-754 roundToIntegralTiesToEven returns floating-point, but we do not have
+ // a dedicated instruction for that. Rounding to integer and converting back to
+@@ -1658,9 +1760,10 @@ namespace detail {
+ enum RoundingModes { kNear, kTrunc, kDown, kUp };
+ 
+ template <class V>
+-HWY_API auto UseInt(const V v) -> decltype(MaskFromVec(v)) {
++HWY_INLINE auto UseInt(const V v) -> decltype(MaskFromVec(v)) {
+   return Lt(Abs(v), Set(DFromV<V>(), MantissaEnd<TFromV<V>>()));
+ }
++
+ }  // namespace detail
+ 
+ template <class V>
+@@ -1673,8 +1776,7 @@ HWY_API V Round(const V v) {
+   return IfThenElse(detail::UseInt(v), CopySign(int_f, v), v);
+ }
+ 
+-// ------------------------------ Trunc
+-
++// ------------------------------ Trunc (ConvertTo)
+ template <class V>
+ HWY_API V Trunc(const V v) {
+   const DFromV<V> df;
+@@ -1687,7 +1789,6 @@ HWY_API V Trunc(const V v) {
+ }
+ 
+ // ------------------------------ Ceil
+-
+ template <class V>
+ HWY_API V Ceil(const V v) {
+   asm volatile("fsrm %0" ::"r"(detail::kUp));
+@@ -1697,7 +1798,6 @@ HWY_API V Ceil(const V v) {
+ }
+ 
+ // ------------------------------ Floor
+-
+ template <class V>
+ HWY_API V Floor(const V v) {
+   asm volatile("fsrm %0" ::"r"(detail::kDown));
+@@ -1706,7 +1806,7 @@ HWY_API V Floor(const V v) {
+   return ret;
+ }
+ 
+-// ------------------------------ Iota
++// ------------------------------ Iota (ConvertTo)
+ 
+ template <class D, HWY_IF_UNSIGNED_D(D)>
+ HWY_API VFromD<D> Iota(const D d, TFromD<D> first) {
+@@ -1723,26 +1823,50 @@ template <class D, HWY_IF_FLOAT_D(D)>
+ HWY_API VFromD<D> Iota(const D d, TFromD<D> first) {
+   const RebindToUnsigned<D> du;
+   const RebindToSigned<D> di;
+-  return detail::Add(ConvertTo(d, BitCast(di, detail::Iota0(du))), first);
++  return detail::AddS(ConvertTo(d, BitCast(di, detail::Iota0(du))), first);
+ }
+ 
+-// ------------------------------ MulEven
++// ------------------------------ MulEven/Odd (Mul, OddEven)
+ 
+-// Using vwmul does not work for m8, so use mulh instead. Highway only provides
+-// MulHigh for 16-bit, so use a private wrapper.
+ namespace detail {
+-HWY_RVV_FOREACH_U32(HWY_RVV_RETV_ARGVV, MulHigh, mulhu)
+-HWY_RVV_FOREACH_I32(HWY_RVV_RETV_ARGVV, MulHigh, mulh)
++// Special instruction for 1 lane is presumably faster?
++#define HWY_RVV_SLIDE1(BASE, CHAR, SEW, LMUL, SHIFT, MLEN, NAME, OP)      \
++  HWY_API HWY_RVV_V(BASE, SEW, LMUL) NAME(HWY_RVV_V(BASE, SEW, LMUL) v) { \
++    return v##OP##_vx_##CHAR##SEW##LMUL(v, 0);                            \
++  }
++
++HWY_RVV_FOREACH_UI32(HWY_RVV_SLIDE1, Slide1Up, slide1up)
++HWY_RVV_FOREACH_U64(HWY_RVV_SLIDE1, Slide1Up, slide1up)
++HWY_RVV_FOREACH_UI32(HWY_RVV_SLIDE1, Slide1Down, slide1down)
++HWY_RVV_FOREACH_U64(HWY_RVV_SLIDE1, Slide1Down, slide1down)
++#undef HWY_RVV_SLIDE1
+ }  // namespace detail
+ 
+-template <class V>
++template <class V, HWY_IF_LANE_SIZE_V(V, 4)>
+ HWY_API VFromD<RepartitionToWide<DFromV<V>>> MulEven(const V a, const V b) {
+   const DFromV<V> d;
+   Lanes(d);
+   const auto lo = Mul(a, b);
+   const auto hi = detail::MulHigh(a, b);
+   const RepartitionToWide<DFromV<V>> dw;
+-  return BitCast(dw, OddEven(detail::SlideUp(hi, hi, 1), lo));
++  const auto wide = BitCast(dw, OddEven(detail::Slide1Up(hi), lo));
++  Lanes(dw);
++  return wide;
++}
++
++// There is no 64x64 vwmul.
++template <class V, HWY_IF_LANE_SIZE_V(V, 8)>
++HWY_INLINE V MulEven(const V a, const V b) {
++  const auto lo = detail::Mul(a, b);
++  const auto hi = detail::MulHigh(a, b);
++  return OddEven(detail::Slide1Up(hi), lo);
++}
++
++template <class V, HWY_IF_LANE_SIZE_V(V, 8)>
++HWY_INLINE V MulOdd(const V a, const V b) {
++  const auto lo = detail::Mul(a, b);
++  const auto hi = detail::MulHigh(a, b);
++  return OddEven(hi, detail::Slide1Down(lo));
+ }
+ 
+ // ================================================== END MACROS
+diff --git a/third_party/highway/hwy/ops/scalar-inl.h b/third_party/highway/hwy/ops/scalar-inl.h
+index a32d88692e6f4..1fa37c19b1613 100644
+--- a/third_party/highway/hwy/ops/scalar-inl.h
++++ b/third_party/highway/hwy/ops/scalar-inl.h
+@@ -18,8 +18,6 @@
+ #include <stddef.h>
+ #include <stdint.h>
+ 
+-#include <algorithm>  // std::min
+-
+ #include "hwy/base.h"
+ #include "hwy/ops/shared-inl.h"
+ 
+@@ -79,10 +77,25 @@ class Mask1 {
+   Raw bits;
+ };
+ 
++namespace detail {
++
++// Deduce Sisd<T> from Vec1<T>
++struct Deduce1 {
++  template <typename T>
++  Sisd<T> operator()(Vec1<T>) const {
++    return Sisd<T>();
++  }
++};
++
++}  // namespace detail
++
++template <class V>
++using DFromV = decltype(detail::Deduce1()(V()));
++
+ // ------------------------------ BitCast
+ 
+ template <typename T, typename FromT>
+-HWY_INLINE Vec1<T> BitCast(Sisd<T> /* tag */, Vec1<FromT> v) {
++HWY_API Vec1<T> BitCast(Sisd<T> /* tag */, Vec1<FromT> v) {
+   static_assert(sizeof(T) <= sizeof(FromT), "Promoting is undefined");
+   T to;
+   CopyBytes<sizeof(FromT)>(&v.raw, &to);
+@@ -92,22 +105,22 @@ HWY_INLINE Vec1<T> BitCast(Sisd<T> /* tag */, Vec1<FromT> v) {
+ // ------------------------------ Set
+ 
+ template <typename T>
+-HWY_INLINE Vec1<T> Zero(Sisd<T> /* tag */) {
++HWY_API Vec1<T> Zero(Sisd<T> /* tag */) {
+   return Vec1<T>(T(0));
+ }
+ 
+ template <typename T, typename T2>
+-HWY_INLINE Vec1<T> Set(Sisd<T> /* tag */, const T2 t) {
++HWY_API Vec1<T> Set(Sisd<T> /* tag */, const T2 t) {
+   return Vec1<T>(static_cast<T>(t));
+ }
+ 
+ template <typename T>
+-HWY_INLINE Vec1<T> Undefined(Sisd<T> d) {
++HWY_API Vec1<T> Undefined(Sisd<T> d) {
+   return Zero(d);
+ }
+ 
+ template <typename T, typename T2>
+-Vec1<T> Iota(const Sisd<T> /* tag */, const T2 first) {
++HWY_API Vec1<T> Iota(const Sisd<T> /* tag */, const T2 first) {
+   return Vec1<T>(static_cast<T>(first));
+ }
+ 
+@@ -116,7 +129,7 @@ Vec1<T> Iota(const Sisd<T> /* tag */, const T2 first) {
+ // ------------------------------ Not
+ 
+ template <typename T>
+-HWY_INLINE Vec1<T> Not(const Vec1<T> v) {
++HWY_API Vec1<T> Not(const Vec1<T> v) {
+   using TU = MakeUnsigned<T>;
+   const Sisd<TU> du;
+   return BitCast(Sisd<T>(), Vec1<TU>(~BitCast(du, v).raw));
+@@ -125,20 +138,20 @@ HWY_INLINE Vec1<T> Not(const Vec1<T> v) {
+ // ------------------------------ And
+ 
+ template <typename T>
+-HWY_INLINE Vec1<T> And(const Vec1<T> a, const Vec1<T> b) {
++HWY_API Vec1<T> And(const Vec1<T> a, const Vec1<T> b) {
+   using TU = MakeUnsigned<T>;
+   const Sisd<TU> du;
+   return BitCast(Sisd<T>(), Vec1<TU>(BitCast(du, a).raw & BitCast(du, b).raw));
+ }
+ template <typename T>
+-HWY_INLINE Vec1<T> operator&(const Vec1<T> a, const Vec1<T> b) {
++HWY_API Vec1<T> operator&(const Vec1<T> a, const Vec1<T> b) {
+   return And(a, b);
+ }
+ 
+ // ------------------------------ AndNot
+ 
+ template <typename T>
+-HWY_INLINE Vec1<T> AndNot(const Vec1<T> a, const Vec1<T> b) {
++HWY_API Vec1<T> AndNot(const Vec1<T> a, const Vec1<T> b) {
+   using TU = MakeUnsigned<T>;
+   const Sisd<TU> du;
+   return BitCast(Sisd<T>(), Vec1<TU>(~BitCast(du, a).raw & BitCast(du, b).raw));
+@@ -147,26 +160,26 @@ HWY_INLINE Vec1<T> AndNot(const Vec1<T> a, const Vec1<T> b) {
+ // ------------------------------ Or
+ 
+ template <typename T>
+-HWY_INLINE Vec1<T> Or(const Vec1<T> a, const Vec1<T> b) {
++HWY_API Vec1<T> Or(const Vec1<T> a, const Vec1<T> b) {
+   using TU = MakeUnsigned<T>;
+   const Sisd<TU> du;
+   return BitCast(Sisd<T>(), Vec1<TU>(BitCast(du, a).raw | BitCast(du, b).raw));
+ }
+ template <typename T>
+-HWY_INLINE Vec1<T> operator|(const Vec1<T> a, const Vec1<T> b) {
++HWY_API Vec1<T> operator|(const Vec1<T> a, const Vec1<T> b) {
+   return Or(a, b);
+ }
+ 
+ // ------------------------------ Xor
+ 
+ template <typename T>
+-HWY_INLINE Vec1<T> Xor(const Vec1<T> a, const Vec1<T> b) {
++HWY_API Vec1<T> Xor(const Vec1<T> a, const Vec1<T> b) {
+   using TU = MakeUnsigned<T>;
+   const Sisd<TU> du;
+   return BitCast(Sisd<T>(), Vec1<TU>(BitCast(du, a).raw ^ BitCast(du, b).raw));
+ }
+ template <typename T>
+-HWY_INLINE Vec1<T> operator^(const Vec1<T> a, const Vec1<T> b) {
++HWY_API Vec1<T> operator^(const Vec1<T> a, const Vec1<T> b) {
+   return Xor(a, b);
+ }
+ 
+@@ -193,6 +206,19 @@ HWY_API Vec1<T> BroadcastSignBit(const Vec1<T> v) {
+   return v.raw < 0 ? Vec1<T>(T(-1)) : Vec1<T>(0);
+ }
+ 
++// ------------------------------ PopulationCount
++
++#ifdef HWY_NATIVE_POPCNT
++#undef HWY_NATIVE_POPCNT
++#else
++#define HWY_NATIVE_POPCNT
++#endif
++
++template <typename T>
++HWY_API Vec1<T> PopulationCount(Vec1<T> v) {
++  return Vec1<T>(static_cast<T>(PopCount(v.raw)));
++}
++
+ // ------------------------------ Mask
+ 
+ template <typename TFrom, typename TTo>
+@@ -203,7 +229,7 @@ HWY_API Mask1<TTo> RebindMask(Sisd<TTo> /*tag*/, Mask1<TFrom> m) {
+ 
+ // v must be 0 or FF..FF.
+ template <typename T>
+-HWY_INLINE Mask1<T> MaskFromVec(const Vec1<T> v) {
++HWY_API Mask1<T> MaskFromVec(const Vec1<T> v) {
+   Mask1<T> mask;
+   CopyBytes<sizeof(mask.bits)>(&v.raw, &mask.bits);
+   return mask;
+@@ -224,29 +250,29 @@ Vec1<T> VecFromMask(Sisd<T> /* tag */, const Mask1<T> mask) {
+ }
+ 
+ template <typename T>
+-HWY_INLINE Mask1<T> FirstN(Sisd<T> /*tag*/, size_t n) {
++HWY_API Mask1<T> FirstN(Sisd<T> /*tag*/, size_t n) {
+   return Mask1<T>::FromBool(n != 0);
+ }
+ 
+ // Returns mask ? yes : no.
+ template <typename T>
+-HWY_INLINE Vec1<T> IfThenElse(const Mask1<T> mask, const Vec1<T> yes,
+-                              const Vec1<T> no) {
++HWY_API Vec1<T> IfThenElse(const Mask1<T> mask, const Vec1<T> yes,
++                           const Vec1<T> no) {
+   return mask.bits ? yes : no;
+ }
+ 
+ template <typename T>
+-HWY_INLINE Vec1<T> IfThenElseZero(const Mask1<T> mask, const Vec1<T> yes) {
++HWY_API Vec1<T> IfThenElseZero(const Mask1<T> mask, const Vec1<T> yes) {
+   return mask.bits ? yes : Vec1<T>(0);
+ }
+ 
+ template <typename T>
+-HWY_INLINE Vec1<T> IfThenZeroElse(const Mask1<T> mask, const Vec1<T> no) {
++HWY_API Vec1<T> IfThenZeroElse(const Mask1<T> mask, const Vec1<T> no) {
+   return mask.bits ? Vec1<T>(0) : no;
+ }
+ 
+ template <typename T>
+-HWY_INLINE Vec1<T> ZeroIfNegative(const Vec1<T> v) {
++HWY_API Vec1<T> ZeroIfNegative(const Vec1<T> v) {
+   return v.raw < 0 ? Vec1<T>(0) : v;
+ }
+ 
+@@ -254,8 +280,7 @@ HWY_INLINE Vec1<T> ZeroIfNegative(const Vec1<T> v) {
+ 
+ template <typename T>
+ HWY_API Mask1<T> Not(const Mask1<T> m) {
+-  const Sisd<T> d;
+-  return MaskFromVec(Not(VecFromMask(d, m)));
++  return MaskFromVec(Not(VecFromMask(Sisd<T>(), m)));
+ }
+ 
+ template <typename T>
+@@ -287,13 +312,13 @@ HWY_API Mask1<T> Xor(const Mask1<T> a, Mask1<T> b) {
+ // ------------------------------ ShiftLeft (BroadcastSignBit)
+ 
+ template <int kBits, typename T>
+-HWY_INLINE Vec1<T> ShiftLeft(const Vec1<T> v) {
++HWY_API Vec1<T> ShiftLeft(const Vec1<T> v) {
+   static_assert(0 <= kBits && kBits < sizeof(T) * 8, "Invalid shift");
+   return Vec1<T>(static_cast<hwy::MakeUnsigned<T>>(v.raw) << kBits);
+ }
+ 
+ template <int kBits, typename T>
+-HWY_INLINE Vec1<T> ShiftRight(const Vec1<T> v) {
++HWY_API Vec1<T> ShiftRight(const Vec1<T> v) {
+   static_assert(0 <= kBits && kBits < sizeof(T) * 8, "Invalid shift");
+ #if __cplusplus >= 202002L
+   // Signed right shift is now guaranteed to be arithmetic (rounding toward
+@@ -318,12 +343,12 @@ HWY_INLINE Vec1<T> ShiftRight(const Vec1<T> v) {
+ // ------------------------------ ShiftLeftSame (BroadcastSignBit)
+ 
+ template <typename T>
+-HWY_INLINE Vec1<T> ShiftLeftSame(const Vec1<T> v, int bits) {
++HWY_API Vec1<T> ShiftLeftSame(const Vec1<T> v, int bits) {
+   return Vec1<T>(static_cast<hwy::MakeUnsigned<T>>(v.raw) << bits);
+ }
+ 
+ template <typename T>
+-HWY_INLINE Vec1<T> ShiftRightSame(const Vec1<T> v, int bits) {
++HWY_API Vec1<T> ShiftRightSame(const Vec1<T> v, int bits) {
+ #if __cplusplus >= 202002L
+   // Signed right shift is now guaranteed to be arithmetic (rounding toward
+   // negative infinity, i.e. shifting in the sign bit).
+@@ -348,40 +373,40 @@ HWY_INLINE Vec1<T> ShiftRightSame(const Vec1<T> v, int bits) {
+ 
+ // Single-lane => same as ShiftLeftSame except for the argument type.
+ template <typename T>
+-HWY_INLINE Vec1<T> operator<<(const Vec1<T> v, const Vec1<T> bits) {
++HWY_API Vec1<T> operator<<(const Vec1<T> v, const Vec1<T> bits) {
+   return ShiftLeftSame(v, static_cast<int>(bits.raw));
+ }
+ 
+ template <typename T>
+-HWY_INLINE Vec1<T> operator>>(const Vec1<T> v, const Vec1<T> bits) {
++HWY_API Vec1<T> operator>>(const Vec1<T> v, const Vec1<T> bits) {
+   return ShiftRightSame(v, static_cast<int>(bits.raw));
+ }
+ 
+ // ================================================== ARITHMETIC
+ 
+ template <typename T>
+-HWY_INLINE Vec1<T> operator+(Vec1<T> a, Vec1<T> b) {
++HWY_API Vec1<T> operator+(Vec1<T> a, Vec1<T> b) {
+   const uint64_t a64 = static_cast<uint64_t>(a.raw);
+   const uint64_t b64 = static_cast<uint64_t>(b.raw);
+   return Vec1<T>(static_cast<T>((a64 + b64) & static_cast<uint64_t>(~T(0))));
+ }
+-HWY_INLINE Vec1<float> operator+(const Vec1<float> a, const Vec1<float> b) {
++HWY_API Vec1<float> operator+(const Vec1<float> a, const Vec1<float> b) {
+   return Vec1<float>(a.raw + b.raw);
+ }
+-HWY_INLINE Vec1<double> operator+(const Vec1<double> a, const Vec1<double> b) {
++HWY_API Vec1<double> operator+(const Vec1<double> a, const Vec1<double> b) {
+   return Vec1<double>(a.raw + b.raw);
+ }
+ 
+ template <typename T>
+-HWY_INLINE Vec1<T> operator-(Vec1<T> a, Vec1<T> b) {
++HWY_API Vec1<T> operator-(Vec1<T> a, Vec1<T> b) {
+   const uint64_t a64 = static_cast<uint64_t>(a.raw);
+   const uint64_t b64 = static_cast<uint64_t>(b.raw);
+   return Vec1<T>(static_cast<T>((a64 - b64) & static_cast<uint64_t>(~T(0))));
+ }
+-HWY_INLINE Vec1<float> operator-(const Vec1<float> a, const Vec1<float> b) {
++HWY_API Vec1<float> operator-(const Vec1<float> a, const Vec1<float> b) {
+   return Vec1<float>(a.raw - b.raw);
+ }
+-HWY_INLINE Vec1<double> operator-(const Vec1<double> a, const Vec1<double> b) {
++HWY_API Vec1<double> operator-(const Vec1<double> a, const Vec1<double> b) {
+   return Vec1<double>(a.raw - b.raw);
+ }
+ 
+@@ -390,25 +415,24 @@ HWY_INLINE Vec1<double> operator-(const Vec1<double> a, const Vec1<double> b) {
+ // Returns a + b clamped to the destination range.
+ 
+ // Unsigned
+-HWY_INLINE Vec1<uint8_t> SaturatedAdd(const Vec1<uint8_t> a,
+-                                      const Vec1<uint8_t> b) {
++HWY_API Vec1<uint8_t> SaturatedAdd(const Vec1<uint8_t> a,
++                                   const Vec1<uint8_t> b) {
+   return Vec1<uint8_t>(
+       static_cast<uint8_t>(HWY_MIN(HWY_MAX(0, a.raw + b.raw), 255)));
+ }
+-HWY_INLINE Vec1<uint16_t> SaturatedAdd(const Vec1<uint16_t> a,
+-                                       const Vec1<uint16_t> b) {
++HWY_API Vec1<uint16_t> SaturatedAdd(const Vec1<uint16_t> a,
++                                    const Vec1<uint16_t> b) {
+   return Vec1<uint16_t>(
+       static_cast<uint16_t>(HWY_MIN(HWY_MAX(0, a.raw + b.raw), 65535)));
+ }
+ 
+ // Signed
+-HWY_INLINE Vec1<int8_t> SaturatedAdd(const Vec1<int8_t> a,
+-                                     const Vec1<int8_t> b) {
++HWY_API Vec1<int8_t> SaturatedAdd(const Vec1<int8_t> a, const Vec1<int8_t> b) {
+   return Vec1<int8_t>(
+       static_cast<int8_t>(HWY_MIN(HWY_MAX(-128, a.raw + b.raw), 127)));
+ }
+-HWY_INLINE Vec1<int16_t> SaturatedAdd(const Vec1<int16_t> a,
+-                                      const Vec1<int16_t> b) {
++HWY_API Vec1<int16_t> SaturatedAdd(const Vec1<int16_t> a,
++                                   const Vec1<int16_t> b) {
+   return Vec1<int16_t>(
+       static_cast<int16_t>(HWY_MIN(HWY_MAX(-32768, a.raw + b.raw), 32767)));
+ }
+@@ -418,25 +442,24 @@ HWY_INLINE Vec1<int16_t> SaturatedAdd(const Vec1<int16_t> a,
+ // Returns a - b clamped to the destination range.
+ 
+ // Unsigned
+-HWY_INLINE Vec1<uint8_t> SaturatedSub(const Vec1<uint8_t> a,
+-                                      const Vec1<uint8_t> b) {
++HWY_API Vec1<uint8_t> SaturatedSub(const Vec1<uint8_t> a,
++                                   const Vec1<uint8_t> b) {
+   return Vec1<uint8_t>(
+       static_cast<uint8_t>(HWY_MIN(HWY_MAX(0, a.raw - b.raw), 255)));
+ }
+-HWY_INLINE Vec1<uint16_t> SaturatedSub(const Vec1<uint16_t> a,
+-                                       const Vec1<uint16_t> b) {
++HWY_API Vec1<uint16_t> SaturatedSub(const Vec1<uint16_t> a,
++                                    const Vec1<uint16_t> b) {
+   return Vec1<uint16_t>(
+       static_cast<uint16_t>(HWY_MIN(HWY_MAX(0, a.raw - b.raw), 65535)));
+ }
+ 
+ // Signed
+-HWY_INLINE Vec1<int8_t> SaturatedSub(const Vec1<int8_t> a,
+-                                     const Vec1<int8_t> b) {
++HWY_API Vec1<int8_t> SaturatedSub(const Vec1<int8_t> a, const Vec1<int8_t> b) {
+   return Vec1<int8_t>(
+       static_cast<int8_t>(HWY_MIN(HWY_MAX(-128, a.raw - b.raw), 127)));
+ }
+-HWY_INLINE Vec1<int16_t> SaturatedSub(const Vec1<int16_t> a,
+-                                      const Vec1<int16_t> b) {
++HWY_API Vec1<int16_t> SaturatedSub(const Vec1<int16_t> a,
++                                   const Vec1<int16_t> b) {
+   return Vec1<int16_t>(
+       static_cast<int16_t>(HWY_MIN(HWY_MAX(-32768, a.raw - b.raw), 32767)));
+ }
+@@ -445,50 +468,50 @@ HWY_INLINE Vec1<int16_t> SaturatedSub(const Vec1<int16_t> a,
+ 
+ // Returns (a + b + 1) / 2
+ 
+-HWY_INLINE Vec1<uint8_t> AverageRound(const Vec1<uint8_t> a,
+-                                      const Vec1<uint8_t> b) {
++HWY_API Vec1<uint8_t> AverageRound(const Vec1<uint8_t> a,
++                                   const Vec1<uint8_t> b) {
+   return Vec1<uint8_t>(static_cast<uint8_t>((a.raw + b.raw + 1) / 2));
+ }
+-HWY_INLINE Vec1<uint16_t> AverageRound(const Vec1<uint16_t> a,
+-                                       const Vec1<uint16_t> b) {
++HWY_API Vec1<uint16_t> AverageRound(const Vec1<uint16_t> a,
++                                    const Vec1<uint16_t> b) {
+   return Vec1<uint16_t>(static_cast<uint16_t>((a.raw + b.raw + 1) / 2));
+ }
+ 
+ // ------------------------------ Absolute value
+ 
+ template <typename T>
+-HWY_INLINE Vec1<T> Abs(const Vec1<T> a) {
++HWY_API Vec1<T> Abs(const Vec1<T> a) {
+   const T i = a.raw;
+   return (i >= 0 || i == hwy::LimitsMin<T>()) ? a : Vec1<T>(-i);
+ }
+-HWY_INLINE Vec1<float> Abs(const Vec1<float> a) {
++HWY_API Vec1<float> Abs(const Vec1<float> a) {
+   return Vec1<float>(std::abs(a.raw));
+ }
+-HWY_INLINE Vec1<double> Abs(const Vec1<double> a) {
++HWY_API Vec1<double> Abs(const Vec1<double> a) {
+   return Vec1<double>(std::abs(a.raw));
+ }
+ 
+ // ------------------------------ min/max
+ 
+ template <typename T, HWY_IF_NOT_FLOAT(T)>
+-HWY_INLINE Vec1<T> Min(const Vec1<T> a, const Vec1<T> b) {
++HWY_API Vec1<T> Min(const Vec1<T> a, const Vec1<T> b) {
+   return Vec1<T>(HWY_MIN(a.raw, b.raw));
+ }
+ 
+ template <typename T, HWY_IF_FLOAT(T)>
+-HWY_INLINE Vec1<T> Min(const Vec1<T> a, const Vec1<T> b) {
++HWY_API Vec1<T> Min(const Vec1<T> a, const Vec1<T> b) {
+   if (std::isnan(a.raw)) return b;
+   if (std::isnan(b.raw)) return a;
+   return Vec1<T>(HWY_MIN(a.raw, b.raw));
+ }
+ 
+ template <typename T, HWY_IF_NOT_FLOAT(T)>
+-HWY_INLINE Vec1<T> Max(const Vec1<T> a, const Vec1<T> b) {
++HWY_API Vec1<T> Max(const Vec1<T> a, const Vec1<T> b) {
+   return Vec1<T>(HWY_MAX(a.raw, b.raw));
+ }
+ 
+ template <typename T, HWY_IF_FLOAT(T)>
+-HWY_INLINE Vec1<T> Max(const Vec1<T> a, const Vec1<T> b) {
++HWY_API Vec1<T> Max(const Vec1<T> a, const Vec1<T> b) {
+   if (std::isnan(a.raw)) return b;
+   if (std::isnan(b.raw)) return a;
+   return Vec1<T>(HWY_MAX(a.raw, b.raw));
+@@ -497,19 +520,19 @@ HWY_INLINE Vec1<T> Max(const Vec1<T> a, const Vec1<T> b) {
+ // ------------------------------ Floating-point negate
+ 
+ template <typename T, HWY_IF_FLOAT(T)>
+-HWY_INLINE Vec1<T> Neg(const Vec1<T> v) {
++HWY_API Vec1<T> Neg(const Vec1<T> v) {
+   return Xor(v, SignBit(Sisd<T>()));
+ }
+ 
+ template <typename T, HWY_IF_NOT_FLOAT(T)>
+-HWY_INLINE Vec1<T> Neg(const Vec1<T> v) {
++HWY_API Vec1<T> Neg(const Vec1<T> v) {
+   return Zero(Sisd<T>()) - v;
+ }
+ 
+ // ------------------------------ mul/div
+ 
+ template <typename T>
+-HWY_INLINE Vec1<T> operator*(const Vec1<T> a, const Vec1<T> b) {
++HWY_API Vec1<T> operator*(const Vec1<T> a, const Vec1<T> b) {
+   if (hwy::IsFloat<T>()) {
+     return Vec1<T>(static_cast<T>(double(a.raw) * b.raw));
+   } else if (hwy::IsSigned<T>()) {
+@@ -520,16 +543,15 @@ HWY_INLINE Vec1<T> operator*(const Vec1<T> a, const Vec1<T> b) {
+ }
+ 
+ template <typename T>
+-HWY_INLINE Vec1<T> operator/(const Vec1<T> a, const Vec1<T> b) {
++HWY_API Vec1<T> operator/(const Vec1<T> a, const Vec1<T> b) {
+   return Vec1<T>(a.raw / b.raw);
+ }
+ 
+ // Returns the upper 16 bits of a * b in each lane.
+-HWY_INLINE Vec1<int16_t> MulHigh(const Vec1<int16_t> a, const Vec1<int16_t> b) {
++HWY_API Vec1<int16_t> MulHigh(const Vec1<int16_t> a, const Vec1<int16_t> b) {
+   return Vec1<int16_t>(static_cast<int16_t>((a.raw * b.raw) >> 16));
+ }
+-HWY_INLINE Vec1<uint16_t> MulHigh(const Vec1<uint16_t> a,
+-                                  const Vec1<uint16_t> b) {
++HWY_API Vec1<uint16_t> MulHigh(const Vec1<uint16_t> a, const Vec1<uint16_t> b) {
+   // Cast to uint32_t first to prevent overflow. Otherwise the result of
+   // uint16_t * uint16_t is in "int" which may overflow. In practice the result
+   // is the same but this way it is also defined.
+@@ -538,18 +560,17 @@ HWY_INLINE Vec1<uint16_t> MulHigh(const Vec1<uint16_t> a,
+ }
+ 
+ // Multiplies even lanes (0, 2 ..) and returns the double-wide result.
+-HWY_INLINE Vec1<int64_t> MulEven(const Vec1<int32_t> a, const Vec1<int32_t> b) {
++HWY_API Vec1<int64_t> MulEven(const Vec1<int32_t> a, const Vec1<int32_t> b) {
+   const int64_t a64 = a.raw;
+   return Vec1<int64_t>(a64 * b.raw);
+ }
+-HWY_INLINE Vec1<uint64_t> MulEven(const Vec1<uint32_t> a,
+-                                  const Vec1<uint32_t> b) {
++HWY_API Vec1<uint64_t> MulEven(const Vec1<uint32_t> a, const Vec1<uint32_t> b) {
+   const uint64_t a64 = a.raw;
+   return Vec1<uint64_t>(a64 * b.raw);
+ }
+ 
+ // Approximate reciprocal
+-HWY_INLINE Vec1<float> ApproximateReciprocal(const Vec1<float> v) {
++HWY_API Vec1<float> ApproximateReciprocal(const Vec1<float> v) {
+   // Zero inputs are allowed, but callers are responsible for replacing the
+   // return value with something else (typically using IfThenElse). This check
+   // avoids a ubsan error. The return value is arbitrary.
+@@ -558,40 +579,38 @@ HWY_INLINE Vec1<float> ApproximateReciprocal(const Vec1<float> v) {
+ }
+ 
+ // Absolute value of difference.
+-HWY_INLINE Vec1<float> AbsDiff(const Vec1<float> a, const Vec1<float> b) {
++HWY_API Vec1<float> AbsDiff(const Vec1<float> a, const Vec1<float> b) {
+   return Abs(a - b);
+ }
+ 
+ // ------------------------------ Floating-point multiply-add variants
+ 
+ template <typename T>
+-HWY_INLINE Vec1<T> MulAdd(const Vec1<T> mul, const Vec1<T> x,
+-                          const Vec1<T> add) {
++HWY_API Vec1<T> MulAdd(const Vec1<T> mul, const Vec1<T> x, const Vec1<T> add) {
+   return mul * x + add;
+ }
+ 
+ template <typename T>
+-HWY_INLINE Vec1<T> NegMulAdd(const Vec1<T> mul, const Vec1<T> x,
+-                             const Vec1<T> add) {
++HWY_API Vec1<T> NegMulAdd(const Vec1<T> mul, const Vec1<T> x,
++                          const Vec1<T> add) {
+   return add - mul * x;
+ }
+ 
+ template <typename T>
+-HWY_INLINE Vec1<T> MulSub(const Vec1<T> mul, const Vec1<T> x,
+-                          const Vec1<T> sub) {
++HWY_API Vec1<T> MulSub(const Vec1<T> mul, const Vec1<T> x, const Vec1<T> sub) {
+   return mul * x - sub;
+ }
+ 
+ template <typename T>
+-HWY_INLINE Vec1<T> NegMulSub(const Vec1<T> mul, const Vec1<T> x,
+-                             const Vec1<T> sub) {
++HWY_API Vec1<T> NegMulSub(const Vec1<T> mul, const Vec1<T> x,
++                          const Vec1<T> sub) {
+   return Neg(mul) * x - sub;
+ }
+ 
+ // ------------------------------ Floating-point square root
+ 
+ // Approximate reciprocal square root
+-HWY_INLINE Vec1<float> ApproximateReciprocalSqrt(const Vec1<float> v) {
++HWY_API Vec1<float> ApproximateReciprocalSqrt(const Vec1<float> v) {
+   float f = v.raw;
+   const float half = f * 0.5f;
+   uint32_t bits;
+@@ -604,17 +623,17 @@ HWY_INLINE Vec1<float> ApproximateReciprocalSqrt(const Vec1<float> v) {
+ }
+ 
+ // Square root
+-HWY_INLINE Vec1<float> Sqrt(const Vec1<float> v) {
++HWY_API Vec1<float> Sqrt(const Vec1<float> v) {
+   return Vec1<float>(std::sqrt(v.raw));
+ }
+-HWY_INLINE Vec1<double> Sqrt(const Vec1<double> v) {
++HWY_API Vec1<double> Sqrt(const Vec1<double> v) {
+   return Vec1<double>(std::sqrt(v.raw));
+ }
+ 
+ // ------------------------------ Floating-point rounding
+ 
+ template <typename T>
+-HWY_INLINE Vec1<T> Round(const Vec1<T> v) {
++HWY_API Vec1<T> Round(const Vec1<T> v) {
+   using TI = MakeSigned<T>;
+   if (!(Abs(v).raw < MantissaEnd<T>())) {  // Huge or NaN
+     return v;
+@@ -630,7 +649,7 @@ HWY_INLINE Vec1<T> Round(const Vec1<T> v) {
+ }
+ 
+ // Round-to-nearest even.
+-HWY_INLINE Vec1<int32_t> NearestInt(const Vec1<float> v) {
++HWY_API Vec1<int32_t> NearestInt(const Vec1<float> v) {
+   using T = float;
+   using TI = int32_t;
+ 
+@@ -655,7 +674,7 @@ HWY_INLINE Vec1<int32_t> NearestInt(const Vec1<float> v) {
+ }
+ 
+ template <typename T>
+-HWY_INLINE Vec1<T> Trunc(const Vec1<T> v) {
++HWY_API Vec1<T> Trunc(const Vec1<T> v) {
+   using TI = MakeSigned<T>;
+   if (!(Abs(v).raw <= MantissaEnd<T>())) {  // Huge or NaN
+     return v;
+@@ -730,49 +749,54 @@ V Floor(const V v) {
+ }
+ 
+ // Toward +infinity, aka ceiling
+-HWY_INLINE Vec1<float> Ceil(const Vec1<float> v) {
++HWY_API Vec1<float> Ceil(const Vec1<float> v) {
+   return Ceiling<float, uint32_t, 23, 8>(v);
+ }
+-HWY_INLINE Vec1<double> Ceil(const Vec1<double> v) {
++HWY_API Vec1<double> Ceil(const Vec1<double> v) {
+   return Ceiling<double, uint64_t, 52, 11>(v);
+ }
+ 
+ // Toward -infinity, aka floor
+-HWY_INLINE Vec1<float> Floor(const Vec1<float> v) {
++HWY_API Vec1<float> Floor(const Vec1<float> v) {
+   return Floor<float, uint32_t, 23, 8>(v);
+ }
+-HWY_INLINE Vec1<double> Floor(const Vec1<double> v) {
++HWY_API Vec1<double> Floor(const Vec1<double> v) {
+   return Floor<double, uint64_t, 52, 11>(v);
+ }
+ 
+ // ================================================== COMPARE
+ 
+ template <typename T>
+-HWY_INLINE Mask1<T> operator==(const Vec1<T> a, const Vec1<T> b) {
++HWY_API Mask1<T> operator==(const Vec1<T> a, const Vec1<T> b) {
+   return Mask1<T>::FromBool(a.raw == b.raw);
+ }
+ 
+ template <typename T>
+-HWY_INLINE Mask1<T> TestBit(const Vec1<T> v, const Vec1<T> bit) {
++HWY_API Mask1<T> operator!=(const Vec1<T> a, const Vec1<T> b) {
++  return Mask1<T>::FromBool(a.raw != b.raw);
++}
++
++template <typename T>
++HWY_API Mask1<T> TestBit(const Vec1<T> v, const Vec1<T> bit) {
+   static_assert(!hwy::IsFloat<T>(), "Only integer vectors supported");
+   return (v & bit) == bit;
+ }
+ 
+ template <typename T>
+-HWY_INLINE Mask1<T> operator<(const Vec1<T> a, const Vec1<T> b) {
++HWY_API Mask1<T> operator<(const Vec1<T> a, const Vec1<T> b) {
+   return Mask1<T>::FromBool(a.raw < b.raw);
+ }
+ template <typename T>
+-HWY_INLINE Mask1<T> operator>(const Vec1<T> a, const Vec1<T> b) {
++HWY_API Mask1<T> operator>(const Vec1<T> a, const Vec1<T> b) {
+   return Mask1<T>::FromBool(a.raw > b.raw);
+ }
+ 
+ template <typename T>
+-HWY_INLINE Mask1<T> operator<=(const Vec1<T> a, const Vec1<T> b) {
++HWY_API Mask1<T> operator<=(const Vec1<T> a, const Vec1<T> b) {
+   return Mask1<T>::FromBool(a.raw <= b.raw);
+ }
+ template <typename T>
+-HWY_INLINE Mask1<T> operator>=(const Vec1<T> a, const Vec1<T> b) {
++HWY_API Mask1<T> operator>=(const Vec1<T> a, const Vec1<T> b) {
+   return Mask1<T>::FromBool(a.raw >= b.raw);
+ }
+ 
+@@ -781,33 +805,33 @@ HWY_INLINE Mask1<T> operator>=(const Vec1<T> a, const Vec1<T> b) {
+ // ------------------------------ Load
+ 
+ template <typename T>
+-HWY_INLINE Vec1<T> Load(Sisd<T> /* tag */, const T* HWY_RESTRICT aligned) {
++HWY_API Vec1<T> Load(Sisd<T> /* tag */, const T* HWY_RESTRICT aligned) {
+   T t;
+   CopyBytes<sizeof(T)>(aligned, &t);
+   return Vec1<T>(t);
+ }
+ 
+ template <typename T>
+-HWY_INLINE Vec1<T> LoadU(Sisd<T> d, const T* HWY_RESTRICT p) {
++HWY_API Vec1<T> LoadU(Sisd<T> d, const T* HWY_RESTRICT p) {
+   return Load(d, p);
+ }
+ 
+ // In some use cases, "load single lane" is sufficient; otherwise avoid this.
+ template <typename T>
+-HWY_INLINE Vec1<T> LoadDup128(Sisd<T> d, const T* HWY_RESTRICT aligned) {
++HWY_API Vec1<T> LoadDup128(Sisd<T> d, const T* HWY_RESTRICT aligned) {
+   return Load(d, aligned);
+ }
+ 
+ // ------------------------------ Store
+ 
+ template <typename T>
+-HWY_INLINE void Store(const Vec1<T> v, Sisd<T> /* tag */,
+-                      T* HWY_RESTRICT aligned) {
++HWY_API void Store(const Vec1<T> v, Sisd<T> /* tag */,
++                   T* HWY_RESTRICT aligned) {
+   CopyBytes<sizeof(T)>(&v.raw, aligned);
+ }
+ 
+ template <typename T>
+-HWY_INLINE void StoreU(const Vec1<T> v, Sisd<T> d, T* HWY_RESTRICT p) {
++HWY_API void StoreU(const Vec1<T> v, Sisd<T> d, T* HWY_RESTRICT p) {
+   return Store(v, d, p);
+ }
+ 
+@@ -834,23 +858,23 @@ HWY_API void StoreInterleaved4(const Vec1<uint8_t> v0, const Vec1<uint8_t> v1,
+ // ------------------------------ Stream
+ 
+ template <typename T>
+-HWY_INLINE void Stream(const Vec1<T> v, Sisd<T> d, T* HWY_RESTRICT aligned) {
++HWY_API void Stream(const Vec1<T> v, Sisd<T> d, T* HWY_RESTRICT aligned) {
+   return Store(v, d, aligned);
+ }
+ 
+ // ------------------------------ Scatter
+ 
+ template <typename T, typename Offset>
+-HWY_INLINE void ScatterOffset(Vec1<T> v, Sisd<T> d, T* base,
+-                              const Vec1<Offset> offset) {
++HWY_API void ScatterOffset(Vec1<T> v, Sisd<T> d, T* base,
++                           const Vec1<Offset> offset) {
+   static_assert(sizeof(T) == sizeof(Offset), "Must match for portability");
+   uint8_t* const base8 = reinterpret_cast<uint8_t*>(base) + offset.raw;
+   return Store(v, d, reinterpret_cast<T*>(base8));
+ }
+ 
+ template <typename T, typename Index>
+-HWY_INLINE void ScatterIndex(Vec1<T> v, Sisd<T> d, T* HWY_RESTRICT base,
+-                             const Vec1<Index> index) {
++HWY_API void ScatterIndex(Vec1<T> v, Sisd<T> d, T* HWY_RESTRICT base,
++                          const Vec1<Index> index) {
+   static_assert(sizeof(T) == sizeof(Index), "Must match for portability");
+   return Store(v, d, base + index.raw);
+ }
+@@ -858,16 +882,16 @@ HWY_INLINE void ScatterIndex(Vec1<T> v, Sisd<T> d, T* HWY_RESTRICT base,
+ // ------------------------------ Gather
+ 
+ template <typename T, typename Offset>
+-HWY_INLINE Vec1<T> GatherOffset(Sisd<T> d, const T* base,
+-                                const Vec1<Offset> offset) {
++HWY_API Vec1<T> GatherOffset(Sisd<T> d, const T* base,
++                             const Vec1<Offset> offset) {
+   static_assert(sizeof(T) == sizeof(Offset), "Must match for portability");
+   const uintptr_t addr = reinterpret_cast<uintptr_t>(base) + offset.raw;
+   return Load(d, reinterpret_cast<const T*>(addr));
+ }
+ 
+ template <typename T, typename Index>
+-HWY_INLINE Vec1<T> GatherIndex(Sisd<T> d, const T* HWY_RESTRICT base,
+-                               const Vec1<Index> index) {
++HWY_API Vec1<T> GatherIndex(Sisd<T> d, const T* HWY_RESTRICT base,
++                            const Vec1<Index> index) {
+   static_assert(sizeof(T) == sizeof(Index), "Must match for portability");
+   return Load(d, base + index.raw);
+ }
+@@ -878,14 +902,14 @@ HWY_INLINE Vec1<T> GatherIndex(Sisd<T> d, const T* HWY_RESTRICT base,
+ // (rounding toward zero).
+ 
+ template <typename FromT, typename ToT>
+-HWY_INLINE Vec1<ToT> PromoteTo(Sisd<ToT> /* tag */, Vec1<FromT> from) {
++HWY_API Vec1<ToT> PromoteTo(Sisd<ToT> /* tag */, Vec1<FromT> from) {
+   static_assert(sizeof(ToT) > sizeof(FromT), "Not promoting");
+   // For bits Y > X, floatX->floatY and intX->intY are always representable.
+   return Vec1<ToT>(static_cast<ToT>(from.raw));
+ }
+ 
+ template <typename FromT, typename ToT, HWY_IF_FLOAT(FromT)>
+-HWY_INLINE Vec1<ToT> DemoteTo(Sisd<ToT> /* tag */, Vec1<FromT> from) {
++HWY_API Vec1<ToT> DemoteTo(Sisd<ToT> /* tag */, Vec1<FromT> from) {
+   static_assert(sizeof(ToT) < sizeof(FromT), "Not demoting");
+ 
+   // Prevent ubsan errors when converting float to narrower integer/float
+@@ -898,17 +922,15 @@ HWY_INLINE Vec1<ToT> DemoteTo(Sisd<ToT> /* tag */, Vec1<FromT> from) {
+ }
+ 
+ template <typename FromT, typename ToT, HWY_IF_NOT_FLOAT(FromT)>
+-HWY_INLINE Vec1<ToT> DemoteTo(Sisd<ToT> /* tag */, Vec1<FromT> from) {
++HWY_API Vec1<ToT> DemoteTo(Sisd<ToT> /* tag */, Vec1<FromT> from) {
+   static_assert(sizeof(ToT) < sizeof(FromT), "Not demoting");
+ 
+   // Int to int: choose closest value in ToT to `from` (avoids UB)
+-  from.raw = std::min<FromT>(std::max<FromT>(LimitsMin<ToT>(), from.raw),
+-                             LimitsMax<ToT>());
++  from.raw = HWY_MIN(HWY_MAX(LimitsMin<ToT>(), from.raw), LimitsMax<ToT>());
+   return Vec1<ToT>(static_cast<ToT>(from.raw));
+ }
+ 
+-static HWY_INLINE Vec1<float> PromoteTo(Sisd<float> /* tag */,
+-                                        const Vec1<float16_t> v) {
++HWY_API Vec1<float> PromoteTo(Sisd<float> /* tag */, const Vec1<float16_t> v) {
+ #if HWY_NATIVE_FLOAT16
+   uint16_t bits16;
+   CopyBytes<2>(&v.raw, &bits16);
+@@ -935,8 +957,8 @@ static HWY_INLINE Vec1<float> PromoteTo(Sisd<float> /* tag */,
+   return Vec1<float>(out);
+ }
+ 
+-static HWY_INLINE Vec1<float16_t> DemoteTo(Sisd<float16_t> /* tag */,
+-                                           const Vec1<float> v) {
++HWY_API Vec1<float16_t> DemoteTo(Sisd<float16_t> /* tag */,
++                                 const Vec1<float> v) {
+   uint32_t bits32;
+   CopyBytes<4>(&v.raw, &bits32);
+   const uint32_t sign = bits32 >> 31;
+@@ -985,7 +1007,7 @@ static HWY_INLINE Vec1<float16_t> DemoteTo(Sisd<float16_t> /* tag */,
+ }
+ 
+ template <typename FromT, typename ToT, HWY_IF_FLOAT(FromT)>
+-HWY_INLINE Vec1<ToT> ConvertTo(Sisd<ToT> /* tag */, Vec1<FromT> from) {
++HWY_API Vec1<ToT> ConvertTo(Sisd<ToT> /* tag */, Vec1<FromT> from) {
+   static_assert(sizeof(ToT) == sizeof(FromT), "Should have same size");
+   // float## -> int##: return closest representable value. We cannot exactly
+   // represent LimitsMax<ToT> in FromT, so use double.
+@@ -999,35 +1021,65 @@ HWY_INLINE Vec1<ToT> ConvertTo(Sisd<ToT> /* tag */, Vec1<FromT> from) {
+ }
+ 
+ template <typename FromT, typename ToT, HWY_IF_NOT_FLOAT(FromT)>
+-HWY_INLINE Vec1<ToT> ConvertTo(Sisd<ToT> /* tag */, Vec1<FromT> from) {
++HWY_API Vec1<ToT> ConvertTo(Sisd<ToT> /* tag */, Vec1<FromT> from) {
+   static_assert(sizeof(ToT) == sizeof(FromT), "Should have same size");
+   // int## -> float##: no check needed
+   return Vec1<ToT>(static_cast<ToT>(from.raw));
+ }
+ 
+-HWY_INLINE Vec1<uint8_t> U8FromU32(const Vec1<uint32_t> v) {
++HWY_API Vec1<uint8_t> U8FromU32(const Vec1<uint32_t> v) {
+   return DemoteTo(Sisd<uint8_t>(), v);
+ }
+ 
+-// ================================================== SWIZZLE
++// ================================================== COMBINE
++// UpperHalf, ZeroExtendVector, Combine, Concat* are unsupported.
+ 
+-// Unsupported: Shift*Bytes, CombineShiftRightBytes, Interleave*, Shuffle*,
+-// UpperHalf - these require more than one lane and/or actual 128-bit vectors.
++template <typename T>
++HWY_API Vec1<T> LowerHalf(Vec1<T> v) {
++  return v;
++}
++
++template <typename T>
++HWY_API Vec1<T> LowerHalf(Sisd<T> /* tag */, Vec1<T> v) {
++  return v;
++}
++
++// ================================================== SWIZZLE
++// OddEven is unsupported.
+ 
+ template <typename T>
+-HWY_INLINE T GetLane(const Vec1<T> v) {
++HWY_API T GetLane(const Vec1<T> v) {
+   return v.raw;
+ }
+ 
++// ------------------------------ TableLookupLanes
++
++// Returned by SetTableIndices for use by TableLookupLanes.
++template <typename T>
++struct Indices1 {
++  int raw;
++};
++
++template <typename T>
++HWY_API Indices1<T> SetTableIndices(Sisd<T>, const int32_t* idx) {
++#if !defined(NDEBUG) || defined(ADDRESS_SANITIZER)
++  HWY_DASSERT(idx[0] == 0);
++#endif
++  return Indices1<T>{idx[0]};
++}
++
+ template <typename T>
+-HWY_INLINE Vec1<T> LowerHalf(Vec1<T> v) {
++HWY_API Vec1<T> TableLookupLanes(const Vec1<T> v, const Indices1<T> /* idx */) {
+   return v;
+ }
+ 
++// ================================================== BLOCKWISE
++// Shift*Bytes, CombineShiftRightBytes, Interleave*, Shuffle* are unsupported.
++
+ // ------------------------------ Broadcast/splat any lane
+ 
+ template <int kLane, typename T>
+-HWY_INLINE Vec1<T> Broadcast(const Vec1<T> v) {
++HWY_API Vec1<T> Broadcast(const Vec1<T> v) {
+   static_assert(kLane == 0, "Scalar only has one lane");
+   return v;
+ }
+@@ -1051,75 +1103,77 @@ HWY_API Vec1<T> TableLookupBytes(const Vec1<T> in, const Vec1<T> from) {
+   return Vec1<T>{out};
+ }
+ 
+-// ------------------------------ TableLookupLanes
+-
+-// Returned by SetTableIndices for use by TableLookupLanes.
+ template <typename T>
+-struct Indices1 {
+-  int raw;
+-};
+-
+-template <typename T>
+-HWY_API Indices1<T> SetTableIndices(Sisd<T>, const int32_t* idx) {
+-#if !defined(NDEBUG) || defined(ADDRESS_SANITIZER)
+-  HWY_DASSERT(idx[0] == 0);
+-#endif
+-  return Indices1<T>{idx[0]};
+-}
+-
+-template <typename T>
+-HWY_API Vec1<T> TableLookupLanes(const Vec1<T> v, const Indices1<T> /* idx */) {
+-  return v;
++HWY_API Vec1<T> TableLookupBytesOr0(const Vec1<T> in, const Vec1<T> from) {
++  uint8_t in_bytes[sizeof(T)];
++  uint8_t from_bytes[sizeof(T)];
++  uint8_t out_bytes[sizeof(T)];
++  CopyBytes<sizeof(T)>(&in, &in_bytes);
++  CopyBytes<sizeof(T)>(&from, &from_bytes);
++  for (size_t i = 0; i < sizeof(T); ++i) {
++    out_bytes[i] = from_bytes[i] & 0x80 ? 0 : in_bytes[from_bytes[i]];
++  }
++  T out;
++  CopyBytes<sizeof(T)>(&out_bytes, &out);
++  return Vec1<T>{out};
+ }
+ 
+-// ------------------------------ Zip/unpack
++// ------------------------------ ZipLower
+ 
+-HWY_INLINE Vec1<uint16_t> ZipLower(const Vec1<uint8_t> a,
+-                                   const Vec1<uint8_t> b) {
++HWY_API Vec1<uint16_t> ZipLower(const Vec1<uint8_t> a, const Vec1<uint8_t> b) {
+   return Vec1<uint16_t>(static_cast<uint16_t>((uint32_t(b.raw) << 8) + a.raw));
+ }
+-HWY_INLINE Vec1<uint32_t> ZipLower(const Vec1<uint16_t> a,
+-                                   const Vec1<uint16_t> b) {
++HWY_API Vec1<uint32_t> ZipLower(const Vec1<uint16_t> a,
++                                const Vec1<uint16_t> b) {
+   return Vec1<uint32_t>((uint32_t(b.raw) << 16) + a.raw);
+ }
+-HWY_INLINE Vec1<uint64_t> ZipLower(const Vec1<uint32_t> a,
+-                                   const Vec1<uint32_t> b) {
++HWY_API Vec1<uint64_t> ZipLower(const Vec1<uint32_t> a,
++                                const Vec1<uint32_t> b) {
+   return Vec1<uint64_t>((uint64_t(b.raw) << 32) + a.raw);
+ }
+-HWY_INLINE Vec1<int16_t> ZipLower(const Vec1<int8_t> a, const Vec1<int8_t> b) {
++HWY_API Vec1<int16_t> ZipLower(const Vec1<int8_t> a, const Vec1<int8_t> b) {
+   return Vec1<int16_t>(static_cast<int16_t>((int32_t(b.raw) << 8) + a.raw));
+ }
+-HWY_INLINE Vec1<int32_t> ZipLower(const Vec1<int16_t> a,
+-                                  const Vec1<int16_t> b) {
++HWY_API Vec1<int32_t> ZipLower(const Vec1<int16_t> a, const Vec1<int16_t> b) {
+   return Vec1<int32_t>((int32_t(b.raw) << 16) + a.raw);
+ }
+-HWY_INLINE Vec1<int64_t> ZipLower(const Vec1<int32_t> a,
+-                                  const Vec1<int32_t> b) {
++HWY_API Vec1<int64_t> ZipLower(const Vec1<int32_t> a, const Vec1<int32_t> b) {
+   return Vec1<int64_t>((int64_t(b.raw) << 32) + a.raw);
+ }
+ 
+-// ------------------------------ Mask
++template <typename T, typename TW = MakeWide<T>, class VW = Vec1<TW>>
++HWY_API VW ZipLower(Sisd<TW> /* tag */, Vec1<T> a, Vec1<T> b) {
++  return VW((TW(b.raw) << (sizeof(T) * 8)) + a.raw);
++}
++
++// ================================================== MASK
+ 
+ template <typename T>
+-HWY_INLINE bool AllFalse(const Mask1<T> mask) {
++HWY_API bool AllFalse(Sisd<T> /* tag */, const Mask1<T> mask) {
+   return mask.bits == 0;
+ }
+ 
+ template <typename T>
+-HWY_INLINE bool AllTrue(const Mask1<T> mask) {
++HWY_API bool AllTrue(Sisd<T> /* tag */, const Mask1<T> mask) {
+   return mask.bits != 0;
+ }
+ 
+ template <typename T>
+-HWY_INLINE size_t StoreMaskBits(const Mask1<T> mask, uint8_t* p) {
+-  *p = AllTrue(mask);
++HWY_API size_t StoreMaskBits(Sisd<T> d, const Mask1<T> mask, uint8_t* p) {
++  *p = AllTrue(d, mask);
+   return 1;
+ }
++
+ template <typename T>
+-HWY_INLINE size_t CountTrue(const Mask1<T> mask) {
++HWY_API size_t CountTrue(Sisd<T> /* tag */, const Mask1<T> mask) {
+   return mask.bits == 0 ? 0 : 1;
+ }
+ 
++template <typename T>
++HWY_API intptr_t FindFirstTrue(Sisd<T> /* tag */, const Mask1<T> mask) {
++  return mask.bits == 0 ? -1 : 0;
++}
++
+ template <typename T>
+ HWY_API Vec1<T> Compress(Vec1<T> v, const Mask1<T> /* mask */) {
+   // Upper lanes are undefined, so result is the same independent of mask.
+@@ -1132,25 +1186,60 @@ template <typename T>
+ HWY_API size_t CompressStore(Vec1<T> v, const Mask1<T> mask, Sisd<T> d,
+                              T* HWY_RESTRICT aligned) {
+   Store(Compress(v, mask), d, aligned);
+-  return CountTrue(mask);
++  return CountTrue(d, mask);
+ }
+ 
+-// ------------------------------ Reductions
++// ================================================== REDUCTIONS
+ 
+ // Sum of all lanes, i.e. the only one.
+ template <typename T>
+-HWY_INLINE Vec1<T> SumOfLanes(const Vec1<T> v0) {
+-  return v0;
++HWY_API Vec1<T> SumOfLanes(Sisd<T> /* tag */, const Vec1<T> v) {
++  return v;
+ }
+ template <typename T>
+-HWY_INLINE Vec1<T> MinOfLanes(const Vec1<T> v) {
++HWY_API Vec1<T> MinOfLanes(Sisd<T> /* tag */, const Vec1<T> v) {
+   return v;
+ }
+ template <typename T>
+-HWY_INLINE Vec1<T> MaxOfLanes(const Vec1<T> v) {
++HWY_API Vec1<T> MaxOfLanes(Sisd<T> /* tag */, const Vec1<T> v) {
+   return v;
+ }
+ 
++// ================================================== DEPRECATED
++
++template <typename T>
++HWY_API size_t StoreMaskBits(const Mask1<T> mask, uint8_t* p) {
++  return StoreMaskBits(Sisd<T>(), mask, p);
++}
++
++template <typename T>
++HWY_API bool AllTrue(const Mask1<T> mask) {
++  return AllTrue(Sisd<T>(), mask);
++}
++
++template <typename T>
++HWY_API bool AllFalse(const Mask1<T> mask) {
++  return AllFalse(Sisd<T>(), mask);
++}
++
++template <typename T>
++HWY_API size_t CountTrue(const Mask1<T> mask) {
++  return CountTrue(Sisd<T>(), mask);
++}
++
++template <typename T>
++HWY_API Vec1<T> SumOfLanes(const Vec1<T> v) {
++  return SumOfLanes(Sisd<T>(), v);
++}
++template <typename T>
++HWY_API Vec1<T> MinOfLanes(const Vec1<T> v) {
++  return MinOfLanes(Sisd<T>(), v);
++}
++template <typename T>
++HWY_API Vec1<T> MaxOfLanes(const Vec1<T> v) {
++  return MaxOfLanes(Sisd<T>(), v);
++}
++
+ // ================================================== Operator wrapper
+ 
+ template <class V>
+@@ -1185,6 +1274,10 @@ HWY_API auto Eq(V a, V b) -> decltype(a == b) {
+   return a == b;
+ }
+ template <class V>
++HWY_API auto Ne(V a, V b) -> decltype(a == b) {
++  return a != b;
++}
++template <class V>
+ HWY_API auto Lt(V a, V b) -> decltype(a == b) {
+   return a < b;
+ }
+diff --git a/third_party/highway/hwy/ops/set_macros-inl.h b/third_party/highway/hwy/ops/set_macros-inl.h
+index 8188d56e3bca5..a4faf7935317c 100644
+--- a/third_party/highway/hwy/ops/set_macros-inl.h
++++ b/third_party/highway/hwy/ops/set_macros-inl.h
+@@ -25,35 +25,81 @@
+ 
+ #endif  // HWY_SET_MACROS_PER_TARGET
+ 
+-#include "hwy/targets.h"
++#include "hwy/detect_targets.h"
+ 
+ #undef HWY_NAMESPACE
+ #undef HWY_ALIGN
+ #undef HWY_LANES
+ 
+ #undef HWY_CAP_INTEGER64
++#undef HWY_CAP_FLOAT16
+ #undef HWY_CAP_FLOAT64
+ #undef HWY_CAP_GE256
+ #undef HWY_CAP_GE512
+ 
+ #undef HWY_TARGET_STR
+ 
++#if defined(HWY_DISABLE_PCLMUL_AES)
++#define HWY_TARGET_STR_PCLMUL_AES ""
++#else
++#define HWY_TARGET_STR_PCLMUL_AES ",pclmul,aes"
++#endif
++
++#if defined(HWY_DISABLE_BMI2_FMA)
++#define HWY_TARGET_STR_BMI2_FMA ""
++#else
++#define HWY_TARGET_STR_BMI2_FMA ",bmi,bmi2,fma"
++#endif
++
++#if defined(HWY_DISABLE_F16C)
++#define HWY_TARGET_STR_F16C ""
++#else
++#define HWY_TARGET_STR_F16C ",f16c"
++#endif
++
++#define HWY_TARGET_STR_SSSE3 "sse2,ssse3"
++
++#define HWY_TARGET_STR_SSE4 \
++  HWY_TARGET_STR_SSSE3 ",sse4.1,sse4.2" HWY_TARGET_STR_PCLMUL_AES
++// Include previous targets, which are the half-vectors of the next target.
++#define HWY_TARGET_STR_AVX2 \
++  HWY_TARGET_STR_SSE4 ",avx,avx2" HWY_TARGET_STR_BMI2_FMA HWY_TARGET_STR_F16C
++#define HWY_TARGET_STR_AVX3 \
++  HWY_TARGET_STR_AVX2 ",avx512f,avx512vl,avx512dq,avx512bw"
++
+ // Before include guard so we redefine HWY_TARGET_STR on each include,
+ // governed by the current HWY_TARGET.
+ //-----------------------------------------------------------------------------
++// SSSE3
++#if HWY_TARGET == HWY_SSSE3
++
++#define HWY_NAMESPACE N_SSSE3
++#define HWY_ALIGN alignas(16)
++#define HWY_LANES(T) (16 / sizeof(T))
++
++#define HWY_CAP_INTEGER64 1
++#define HWY_CAP_FLOAT16 1
++#define HWY_CAP_FLOAT64 1
++#define HWY_CAP_AES 0
++#define HWY_CAP_GE256 0
++#define HWY_CAP_GE512 0
++
++#define HWY_TARGET_STR HWY_TARGET_STR_SSSE3
++//-----------------------------------------------------------------------------
+ // SSE4
+-#if HWY_TARGET == HWY_SSE4
++#elif HWY_TARGET == HWY_SSE4
+ 
+ #define HWY_NAMESPACE N_SSE4
+ #define HWY_ALIGN alignas(16)
+ #define HWY_LANES(T) (16 / sizeof(T))
+ 
+ #define HWY_CAP_INTEGER64 1
++#define HWY_CAP_FLOAT16 1
+ #define HWY_CAP_FLOAT64 1
+ #define HWY_CAP_GE256 0
+ #define HWY_CAP_GE512 0
+ 
+-#define HWY_TARGET_STR "sse2,ssse3,sse4.1"
++#define HWY_TARGET_STR HWY_TARGET_STR_SSE4
+ 
+ //-----------------------------------------------------------------------------
+ // AVX2
+@@ -64,35 +110,36 @@
+ #define HWY_LANES(T) (32 / sizeof(T))
+ 
+ #define HWY_CAP_INTEGER64 1
++#define HWY_CAP_FLOAT16 1
+ #define HWY_CAP_FLOAT64 1
+ #define HWY_CAP_GE256 1
+ #define HWY_CAP_GE512 0
+ 
+-#if defined(HWY_DISABLE_BMI2_FMA)
+-#define HWY_TARGET_STR "avx,avx2,f16c"
+-#else
+-#define HWY_TARGET_STR "avx,avx2,bmi,bmi2,fma,f16c"
+-#endif
++#define HWY_TARGET_STR HWY_TARGET_STR_AVX2
+ 
+ //-----------------------------------------------------------------------------
+-// AVX3
+-#elif HWY_TARGET == HWY_AVX3
++// AVX3[_DL]
++#elif HWY_TARGET == HWY_AVX3 || HWY_TARGET == HWY_AVX3_DL
+ 
+ #define HWY_ALIGN alignas(64)
+ #define HWY_LANES(T) (64 / sizeof(T))
+ 
+ #define HWY_CAP_INTEGER64 1
++#define HWY_CAP_FLOAT16 1
+ #define HWY_CAP_FLOAT64 1
+ #define HWY_CAP_GE256 1
+ #define HWY_CAP_GE512 1
+ 
++#if HWY_TARGET == HWY_AVX3
+ #define HWY_NAMESPACE N_AVX3
+-
+-// Must include AVX2 because an AVX3 test may call AVX2 functions (e.g. when
+-// converting to half-vectors). HWY_DISABLE_BMI2_FMA is not relevant because if
+-// we have AVX3, we should also have BMI2/FMA.
++#define HWY_TARGET_STR HWY_TARGET_STR_AVX3
++#elif HWY_TARGET == HWY_AVX3_DL
++#define HWY_NAMESPACE N_AVX3_DL
+ #define HWY_TARGET_STR \
+-  "avx,avx2,bmi,bmi2,fma,f16c,avx512f,avx512vl,avx512dq,avx512bw"
++  HWY_TARGET_STR_AVX3 ",vpclmulqdq,vaes,avxvnni,avx512bitalg,avx512vpopcntdq"
++#else
++#error "Logic error"
++#endif  // HWY_TARGET == HWY_AVX3_DL
+ 
+ //-----------------------------------------------------------------------------
+ // PPC8
+@@ -102,6 +149,7 @@
+ #define HWY_LANES(T) (16 / sizeof(T))
+ 
+ #define HWY_CAP_INTEGER64 1
++#define HWY_CAP_FLOAT16 0
+ #define HWY_CAP_FLOAT64 1
+ #define HWY_CAP_GE256 0
+ #define HWY_CAP_GE512 0
+@@ -118,6 +166,7 @@
+ #define HWY_LANES(T) (16 / sizeof(T))
+ 
+ #define HWY_CAP_INTEGER64 1
++#define HWY_CAP_FLOAT16 1
+ #define HWY_CAP_GE256 0
+ #define HWY_CAP_GE512 0
+ 
+@@ -135,12 +184,24 @@
+ // SVE[2]
+ #elif HWY_TARGET == HWY_SVE2 || HWY_TARGET == HWY_SVE
+ 
++#if defined(HWY_EMULATE_SVE) && !defined(__F16C__)
++#error "Disable HWY_CAP_FLOAT16 or ensure farm_sve actually converts to f16"
++#endif
++
+ // SVE only requires lane alignment, not natural alignment of the entire vector.
+ #define HWY_ALIGN alignas(8)
+-// Upper bound, not the actual lane count!
+-#define HWY_LANES(T) (256 / sizeof(T))
++
++// <= 16 bytes: exact size (from HWY_CAPPED). 2048 bytes denotes a full vector.
++// In between: fraction of the full length, a power of two; HWY_LANES(T)/4
++// denotes 1/4 the actual length (a power of two because we use SV_POW2).
++//
++// The upper bound for SVE is actually 256 bytes, but we need to be able to
++// differentiate 1/8th of a vector, subsequently demoted to 1/4 the lane width,
++// from an exact size <= 16 bytes.
++#define HWY_LANES(T) (2048 / sizeof(T))
+ 
+ #define HWY_CAP_INTEGER64 1
++#define HWY_CAP_FLOAT16 1
+ #define HWY_CAP_FLOAT64 1
+ #define HWY_CAP_GE256 0
+ #define HWY_CAP_GE512 0
+@@ -151,7 +212,7 @@
+ #define HWY_NAMESPACE N_SVE
+ #endif
+ 
+-// HWY_TARGET_STR remains undefined - TODO(janwas): attribute for SVE?
++// HWY_TARGET_STR remains undefined
+ 
+ //-----------------------------------------------------------------------------
+ // WASM
+@@ -161,6 +222,7 @@
+ #define HWY_LANES(T) (16 / sizeof(T))
+ 
+ #define HWY_CAP_INTEGER64 0
++#define HWY_CAP_FLOAT16 1
+ #define HWY_CAP_FLOAT64 0
+ #define HWY_CAP_GE256 0
+ #define HWY_CAP_GE512 0
+@@ -178,15 +240,21 @@
+ #define HWY_ALIGN
+ 
+ // Arbitrary constant, not the actual lane count! Large enough that we can
+-// mul/div by 8 for LMUL. Value matches kMaxVectorSize, see base.h.
++// mul/div by 8 for LMUL.
++// TODO(janwas): update to actual upper bound 64K, plus headroom for 1/8.
+ #define HWY_LANES(T) (4096 / sizeof(T))
+ 
+-
+ #define HWY_CAP_INTEGER64 1
+ #define HWY_CAP_FLOAT64 1
+ #define HWY_CAP_GE256 0
+ #define HWY_CAP_GE512 0
+ 
++#if defined(__riscv_zfh)
++#define HWY_CAP_FLOAT16 1
++#else
++#define HWY_CAP_FLOAT16 0
++#endif
++
+ #define HWY_NAMESPACE N_RVV
+ 
+ // HWY_TARGET_STR remains undefined so HWY_ATTR is a no-op.
+@@ -201,6 +269,7 @@
+ #define HWY_LANES(T) 1
+ 
+ #define HWY_CAP_INTEGER64 1
++#define HWY_CAP_FLOAT16 1
+ #define HWY_CAP_FLOAT64 1
+ #define HWY_CAP_GE256 0
+ #define HWY_CAP_GE512 0
+diff --git a/third_party/highway/hwy/ops/shared-inl.h b/third_party/highway/hwy/ops/shared-inl.h
+index 11a7b616f806e..95ee01c199ab9 100644
+--- a/third_party/highway/hwy/ops/shared-inl.h
++++ b/third_party/highway/hwy/ops/shared-inl.h
+@@ -98,6 +98,17 @@ using Twice = typename D::Twice;
+ #define HWY_IF_LANE_SIZE_D(D, bytes) HWY_IF_LANE_SIZE(TFromD<D>, bytes)
+ #define HWY_IF_NOT_LANE_SIZE_D(D, bytes) HWY_IF_NOT_LANE_SIZE(TFromD<D>, bytes)
+ 
++// Same, but with a vector argument.
++#define HWY_IF_UNSIGNED_V(V) HWY_IF_UNSIGNED(TFromV<V>)
++#define HWY_IF_SIGNED_V(V) HWY_IF_SIGNED(TFromV<V>)
++#define HWY_IF_FLOAT_V(V) HWY_IF_FLOAT(TFromV<V>)
++#define HWY_IF_LANE_SIZE_V(V, bytes) HWY_IF_LANE_SIZE(TFromV<V>, bytes)
++
++// For implementing functions for a specific type.
++// IsSame<...>() in template arguments is broken on MSVC2015.
++#define HWY_IF_LANES_ARE(T, V) \
++  EnableIf<IsSameT<T, TFromD<DFromV<V>>>::value>* = nullptr
++
+ // Compile-time-constant, (typically but not guaranteed) an upper bound on the
+ // number of lanes.
+ // Prefer instead using Lanes() and dynamic allocation, or Rebind, or
+diff --git a/third_party/highway/hwy/ops/wasm_128-inl.h b/third_party/highway/hwy/ops/wasm_128-inl.h
+index e235d10b45122..a5c43bb65ce53 100644
+--- a/third_party/highway/hwy/ops/wasm_128-inl.h
++++ b/third_party/highway/hwy/ops/wasm_128-inl.h
+@@ -22,10 +22,37 @@
+ #include "hwy/base.h"
+ #include "hwy/ops/shared-inl.h"
+ 
++#ifdef HWY_WASM_OLD_NAMES
++#define wasm_i8x16_shuffle wasm_v8x16_shuffle
++#define wasm_i16x8_shuffle wasm_v16x8_shuffle
++#define wasm_i32x4_shuffle wasm_v32x4_shuffle
++#define wasm_i64x2_shuffle wasm_v64x2_shuffle
++#define wasm_u16x8_extend_low_u8x16 wasm_i16x8_widen_low_u8x16
++#define wasm_u32x4_extend_low_u16x8 wasm_i32x4_widen_low_u16x8
++#define wasm_i32x4_extend_low_i16x8 wasm_i32x4_widen_low_i16x8
++#define wasm_i16x8_extend_low_i8x16 wasm_i16x8_widen_low_i8x16
++#define wasm_u32x4_extend_high_u16x8 wasm_i32x4_widen_high_u16x8
++#define wasm_i32x4_extend_high_i16x8 wasm_i32x4_widen_high_i16x8
++#define wasm_i32x4_trunc_sat_f32x4 wasm_i32x4_trunc_saturate_f32x4
++#define wasm_u8x16_add_sat wasm_u8x16_add_saturate
++#define wasm_u8x16_sub_sat wasm_u8x16_sub_saturate
++#define wasm_u16x8_add_sat wasm_u16x8_add_saturate
++#define wasm_u16x8_sub_sat wasm_u16x8_sub_saturate
++#define wasm_i8x16_add_sat wasm_i8x16_add_saturate
++#define wasm_i8x16_sub_sat wasm_i8x16_sub_saturate
++#define wasm_i16x8_add_sat wasm_i16x8_add_saturate
++#define wasm_i16x8_sub_sat wasm_i16x8_sub_saturate
++#endif
++
+ HWY_BEFORE_NAMESPACE();
+ namespace hwy {
+ namespace HWY_NAMESPACE {
+ 
++template <typename T>
++using Full128 = Simd<T, 16 / sizeof(T)>;
++
++namespace detail {
++
+ template <typename T>
+ struct Raw128 {
+   using type = __v128_u;
+@@ -35,12 +62,11 @@ struct Raw128<float> {
+   using type = __f32x4;
+ };
+ 
+-template <typename T>
+-using Full128 = Simd<T, 16 / sizeof(T)>;
++} // namespace detail
+ 
+ template <typename T, size_t N = 16 / sizeof(T)>
+ class Vec128 {
+-  using Raw = typename Raw128<T>::type;
++  using Raw = typename detail::Raw128<T>::type;
+ 
+  public:
+   // Compound assignment. Only usable if there is a corresponding non-member
+@@ -70,29 +96,41 @@ class Vec128 {
+   Raw raw;
+ };
+ 
+-// Integer: FF..FF or 0. Float: MSB, all other bits undefined - see README.
++// FF..FF or 0.
+ template <typename T, size_t N = 16 / sizeof(T)>
+-class Mask128 {
+-  using Raw = typename Raw128<T>::type;
++struct Mask128 {
++  typename detail::Raw128<T>::type raw;
++};
+ 
+- public:
+-  Raw raw;
++namespace detail {
++
++// Deduce Simd<T, N> from Vec128<T, N>
++struct DeduceD {
++  template <typename T, size_t N>
++  Simd<T, N> operator()(Vec128<T, N>) const {
++    return Simd<T, N>();
++  }
+ };
+ 
++}  // namespace detail
++
++template <class V>
++using DFromV = decltype(detail::DeduceD()(V()));
++
+ // ------------------------------ BitCast
+ 
+ namespace detail {
+ 
+-HWY_API __v128_u BitCastToInteger(__v128_u v) { return v; }
+-HWY_API __v128_u BitCastToInteger(__f32x4 v) {
++HWY_INLINE __v128_u BitCastToInteger(__v128_u v) { return v; }
++HWY_INLINE __v128_u BitCastToInteger(__f32x4 v) {
+   return static_cast<__v128_u>(v);
+ }
+-HWY_API __v128_u BitCastToInteger(__f64x2 v) {
++HWY_INLINE __v128_u BitCastToInteger(__f64x2 v) {
+   return static_cast<__v128_u>(v);
+ }
+ 
+ template <typename T, size_t N>
+-HWY_API Vec128<uint8_t, N * sizeof(T)> BitCastToByte(Vec128<T, N> v) {
++HWY_INLINE Vec128<uint8_t, N * sizeof(T)> BitCastToByte(Vec128<T, N> v) {
+   return Vec128<uint8_t, N * sizeof(T)>{BitCastToInteger(v.raw)};
+ }
+ 
+@@ -107,8 +145,8 @@ struct BitCastFromInteger128<float> {
+ };
+ 
+ template <typename T, size_t N>
+-HWY_API Vec128<T, N> BitCastFromByte(Simd<T, N> /* tag */,
+-                                     Vec128<uint8_t, N * sizeof(T)> v) {
++HWY_INLINE Vec128<T, N> BitCastFromByte(Simd<T, N> /* tag */,
++                                        Vec128<uint8_t, N * sizeof(T)> v) {
+   return Vec128<T, N>{BitCastFromInteger128<T>()(v.raw)};
+ }
+ 
+@@ -120,7 +158,7 @@ HWY_API Vec128<T, N> BitCast(Simd<T, N> d,
+   return detail::BitCastFromByte(d, detail::BitCastToByte(v));
+ }
+ 
+-// ------------------------------ Set
++// ------------------------------ Zero
+ 
+ // Returns an all-zero vector/part.
+ template <typename T, size_t N, HWY_IF_LE128(T, N)>
+@@ -132,6 +170,11 @@ HWY_API Vec128<float, N> Zero(Simd<float, N> /* tag */) {
+   return Vec128<float, N>{wasm_f32x4_splat(0.0f)};
+ }
+ 
++template <class D>
++using VFromD = decltype(Zero(D()));
++
++// ------------------------------ Set
++
+ // Returns a vector/part with all lanes set to "t".
+ template <size_t N, HWY_IF_LE128(uint8_t, N)>
+ HWY_API Vec128<uint8_t, N> Set(Simd<uint8_t, N> /* tag */, const uint8_t t) {
+@@ -145,6 +188,10 @@ template <size_t N, HWY_IF_LE128(uint32_t, N)>
+ HWY_API Vec128<uint32_t, N> Set(Simd<uint32_t, N> /* tag */, const uint32_t t) {
+   return Vec128<uint32_t, N>{wasm_i32x4_splat(t)};
+ }
++template <size_t N, HWY_IF_LE128(uint64_t, N)>
++HWY_API Vec128<uint64_t, N> Set(Simd<uint64_t, N> /* tag */, const uint64_t t) {
++  return Vec128<uint64_t, N>{wasm_i64x2_splat(t)};
++}
+ 
+ template <size_t N, HWY_IF_LE128(int8_t, N)>
+ HWY_API Vec128<int8_t, N> Set(Simd<int8_t, N> /* tag */, const int8_t t) {
+@@ -158,6 +205,10 @@ template <size_t N, HWY_IF_LE128(int32_t, N)>
+ HWY_API Vec128<int32_t, N> Set(Simd<int32_t, N> /* tag */, const int32_t t) {
+   return Vec128<int32_t, N>{wasm_i32x4_splat(t)};
+ }
++template <size_t N, HWY_IF_LE128(int64_t, N)>
++HWY_API Vec128<int64_t, N> Set(Simd<int64_t, N> /* tag */, const int64_t t) {
++  return Vec128<int64_t, N>{wasm_i64x2_splat(t)};
++}
+ 
+ template <size_t N, HWY_IF_LE128(float, N)>
+ HWY_API Vec128<float, N> Set(Simd<float, N> /* tag */, const float t) {
+@@ -281,24 +332,24 @@ HWY_API Vec128<float, N> operator-(const Vec128<float, N> a,
+ template <size_t N>
+ HWY_API Vec128<uint8_t, N> SaturatedAdd(const Vec128<uint8_t, N> a,
+                                         const Vec128<uint8_t, N> b) {
+-  return Vec128<uint8_t, N>{wasm_u8x16_add_saturate(a.raw, b.raw)};
++  return Vec128<uint8_t, N>{wasm_u8x16_add_sat(a.raw, b.raw)};
+ }
+ template <size_t N>
+ HWY_API Vec128<uint16_t, N> SaturatedAdd(const Vec128<uint16_t, N> a,
+                                          const Vec128<uint16_t, N> b) {
+-  return Vec128<uint16_t, N>{wasm_u16x8_add_saturate(a.raw, b.raw)};
++  return Vec128<uint16_t, N>{wasm_u16x8_add_sat(a.raw, b.raw)};
+ }
+ 
+ // Signed
+ template <size_t N>
+ HWY_API Vec128<int8_t, N> SaturatedAdd(const Vec128<int8_t, N> a,
+                                        const Vec128<int8_t, N> b) {
+-  return Vec128<int8_t, N>{wasm_i8x16_add_saturate(a.raw, b.raw)};
++  return Vec128<int8_t, N>{wasm_i8x16_add_sat(a.raw, b.raw)};
+ }
+ template <size_t N>
+ HWY_API Vec128<int16_t, N> SaturatedAdd(const Vec128<int16_t, N> a,
+                                         const Vec128<int16_t, N> b) {
+-  return Vec128<int16_t, N>{wasm_i16x8_add_saturate(a.raw, b.raw)};
++  return Vec128<int16_t, N>{wasm_i16x8_add_sat(a.raw, b.raw)};
+ }
+ 
+ // ------------------------------ Saturating subtraction
+@@ -309,24 +360,24 @@ HWY_API Vec128<int16_t, N> SaturatedAdd(const Vec128<int16_t, N> a,
+ template <size_t N>
+ HWY_API Vec128<uint8_t, N> SaturatedSub(const Vec128<uint8_t, N> a,
+                                         const Vec128<uint8_t, N> b) {
+-  return Vec128<uint8_t, N>{wasm_u8x16_sub_saturate(a.raw, b.raw)};
++  return Vec128<uint8_t, N>{wasm_u8x16_sub_sat(a.raw, b.raw)};
+ }
+ template <size_t N>
+ HWY_API Vec128<uint16_t, N> SaturatedSub(const Vec128<uint16_t, N> a,
+                                          const Vec128<uint16_t, N> b) {
+-  return Vec128<uint16_t, N>{wasm_u16x8_sub_saturate(a.raw, b.raw)};
++  return Vec128<uint16_t, N>{wasm_u16x8_sub_sat(a.raw, b.raw)};
+ }
+ 
+ // Signed
+ template <size_t N>
+ HWY_API Vec128<int8_t, N> SaturatedSub(const Vec128<int8_t, N> a,
+                                        const Vec128<int8_t, N> b) {
+-  return Vec128<int8_t, N>{wasm_i8x16_sub_saturate(a.raw, b.raw)};
++  return Vec128<int8_t, N>{wasm_i8x16_sub_sat(a.raw, b.raw)};
+ }
+ template <size_t N>
+ HWY_API Vec128<int16_t, N> SaturatedSub(const Vec128<int16_t, N> a,
+                                         const Vec128<int16_t, N> b) {
+-  return Vec128<int16_t, N>{wasm_i16x8_sub_saturate(a.raw, b.raw)};
++  return Vec128<int16_t, N>{wasm_i16x8_sub_sat(a.raw, b.raw)};
+ }
+ 
+ // ------------------------------ Average
+@@ -362,9 +413,7 @@ HWY_API Vec128<int32_t, N> Abs(const Vec128<int32_t, N> v) {
+ }
+ template <size_t N>
+ HWY_API Vec128<int64_t, N> Abs(const Vec128<int64_t, N> v) {
+-  // TODO(janwas): use wasm_i64x2_abs when available
+-  const Vec128<int64_t, N> mask = wasm_i64x2_shr(v.raw, 63);
+-  return ((v ^ mask) - mask);
++  return Vec128<int32_t, N>{wasm_i62x2_abs(v.raw)};
+ }
+ 
+ template <size_t N>
+@@ -537,12 +586,10 @@ HWY_API Vec128<uint64_t, N> Min(const Vec128<uint64_t, N> a,
+                                 const Vec128<uint64_t, N> b) {
+   alignas(16) float min[4];
+   min[0] =
+-      std::min(wasm_u64x2_extract_lane(a, 0), wasm_u64x2_extract_lane(b, 0));
++      HWY_MIN(wasm_u64x2_extract_lane(a, 0), wasm_u64x2_extract_lane(b, 0));
+   min[1] =
+-      std::min(wasm_u64x2_extract_lane(a, 1), wasm_u64x2_extract_lane(b, 1));
++      HWY_MIN(wasm_u64x2_extract_lane(a, 1), wasm_u64x2_extract_lane(b, 1));
+   return Vec128<uint64_t, N>{wasm_v128_load(min)};
+-  // TODO(janwas): new op?
+-  // return Vec128<uint64_t, N>{wasm_u64x2_min(a.raw, b.raw)};
+ }
+ 
+ // Signed
+@@ -566,12 +613,10 @@ HWY_API Vec128<int64_t, N> Min(const Vec128<int64_t, N> a,
+                                const Vec128<int64_t, N> b) {
+   alignas(16) float min[4];
+   min[0] =
+-      std::min(wasm_i64x2_extract_lane(a, 0), wasm_i64x2_extract_lane(b, 0));
++      HWY_MIN(wasm_i64x2_extract_lane(a, 0), wasm_i64x2_extract_lane(b, 0));
+   min[1] =
+-      std::min(wasm_i64x2_extract_lane(a, 1), wasm_i64x2_extract_lane(b, 1));
++      HWY_MIN(wasm_i64x2_extract_lane(a, 1), wasm_i64x2_extract_lane(b, 1));
+   return Vec128<int64_t, N>{wasm_v128_load(min)};
+-  // TODO(janwas): new op? (also do not yet have wasm_u64x2_make)
+-  // return Vec128<int64_t, N>{wasm_i64x2_min(a.raw, b.raw)};
+ }
+ 
+ // Float
+@@ -604,12 +649,10 @@ HWY_API Vec128<uint64_t, N> Max(const Vec128<uint64_t, N> a,
+                                 const Vec128<uint64_t, N> b) {
+   alignas(16) float max[4];
+   max[0] =
+-      std::max(wasm_u64x2_extract_lane(a, 0), wasm_u64x2_extract_lane(b, 0));
++      HWY_MAX(wasm_u64x2_extract_lane(a, 0), wasm_u64x2_extract_lane(b, 0));
+   max[1] =
+-      std::max(wasm_u64x2_extract_lane(a, 1), wasm_u64x2_extract_lane(b, 1));
++      HWY_MAX(wasm_u64x2_extract_lane(a, 1), wasm_u64x2_extract_lane(b, 1));
+   return Vec128<int64_t, N>{wasm_v128_load(max)};
+-  // TODO(janwas): new op? (also do not yet have wasm_u64x2_make)
+-  // return Vec128<uint64_t, N>{wasm_u64x2_max(a.raw, b.raw)};
+ }
+ 
+ // Signed
+@@ -633,12 +676,10 @@ HWY_API Vec128<int64_t, N> Max(const Vec128<int64_t, N> a,
+                                const Vec128<int64_t, N> b) {
+   alignas(16) float max[4];
+   max[0] =
+-      std::max(wasm_i64x2_extract_lane(a, 0), wasm_i64x2_extract_lane(b, 0));
++      HWY_MAX(wasm_i64x2_extract_lane(a, 0), wasm_i64x2_extract_lane(b, 0));
+   max[1] =
+-      std::max(wasm_i64x2_extract_lane(a, 1), wasm_i64x2_extract_lane(b, 1));
++      HWY_MAX(wasm_i64x2_extract_lane(a, 1), wasm_i64x2_extract_lane(b, 1));
+   return Vec128<int64_t, N>{wasm_v128_load(max)};
+-  // TODO(janwas): new op? (also do not yet have wasm_u64x2_make)
+-  // return Vec128<int64_t, N>{wasm_i64x2_max(a.raw, b.raw)};
+ }
+ 
+ // Float
+@@ -679,29 +720,29 @@ template <size_t N>
+ HWY_API Vec128<uint16_t, N> MulHigh(const Vec128<uint16_t, N> a,
+                                     const Vec128<uint16_t, N> b) {
+   // TODO(eustas): replace, when implemented in WASM.
+-  const auto al = wasm_i32x4_widen_low_u16x8(a.raw);
+-  const auto ah = wasm_i32x4_widen_high_u16x8(a.raw);
+-  const auto bl = wasm_i32x4_widen_low_u16x8(b.raw);
+-  const auto bh = wasm_i32x4_widen_high_u16x8(b.raw);
++  const auto al = wasm_u32x4_extend_low_u16x8(a.raw);
++  const auto ah = wasm_u32x4_extend_high_u16x8(a.raw);
++  const auto bl = wasm_u32x4_extend_low_u16x8(b.raw);
++  const auto bh = wasm_u32x4_extend_high_u16x8(b.raw);
+   const auto l = wasm_i32x4_mul(al, bl);
+   const auto h = wasm_i32x4_mul(ah, bh);
+   // TODO(eustas): shift-right + narrow?
+   return Vec128<uint16_t, N>{
+-      wasm_v16x8_shuffle(l, h, 1, 3, 5, 7, 9, 11, 13, 15)};
++      wasm_i16x8_shuffle(l, h, 1, 3, 5, 7, 9, 11, 13, 15)};
+ }
+ template <size_t N>
+ HWY_API Vec128<int16_t, N> MulHigh(const Vec128<int16_t, N> a,
+                                    const Vec128<int16_t, N> b) {
+   // TODO(eustas): replace, when implemented in WASM.
+-  const auto al = wasm_i32x4_widen_low_i16x8(a.raw);
+-  const auto ah = wasm_i32x4_widen_high_i16x8(a.raw);
+-  const auto bl = wasm_i32x4_widen_low_i16x8(b.raw);
+-  const auto bh = wasm_i32x4_widen_high_i16x8(b.raw);
++  const auto al = wasm_i32x4_extend_low_i16x8(a.raw);
++  const auto ah = wasm_i32x4_extend_high_i16x8(a.raw);
++  const auto bl = wasm_i32x4_extend_low_i16x8(b.raw);
++  const auto bh = wasm_i32x4_extend_high_i16x8(b.raw);
+   const auto l = wasm_i32x4_mul(al, bl);
+   const auto h = wasm_i32x4_mul(ah, bh);
+   // TODO(eustas): shift-right + narrow?
+   return Vec128<int16_t, N>{
+-      wasm_v16x8_shuffle(l, h, 1, 3, 5, 7, 9, 11, 13, 15)};
++      wasm_i16x8_shuffle(l, h, 1, 3, 5, 7, 9, 11, 13, 15)};
+ }
+ 
+ // Multiplies even lanes (0, 2 ..) and returns the double-width result.
+@@ -764,7 +805,6 @@ HWY_API Vec128<float, N> operator/(const Vec128<float, N> a,
+ // Approximate reciprocal
+ template <size_t N>
+ HWY_API Vec128<float, N> ApproximateReciprocal(const Vec128<float, N> v) {
+-  // TODO(eustas): replace, when implemented in WASM.
+   const Vec128<float, N> one = Vec128<float, N>{wasm_f32x4_splat(1.0f)};
+   return one / v;
+ }
+@@ -837,76 +877,25 @@ HWY_API Vec128<float, N> ApproximateReciprocalSqrt(const Vec128<float, N> v) {
+ // Toward nearest integer, ties to even
+ template <size_t N>
+ HWY_API Vec128<float, N> Round(const Vec128<float, N> v) {
+-  // IEEE-754 roundToIntegralTiesToEven returns floating-point, but we do not
+-  // yet have an instruction for that (f32x4.nearest is not implemented). We
+-  // rely on rounding after addition with a large value such that no mantissa
+-  // bits remain (assuming the current mode is nearest-even). We may need a
+-  // compiler flag for precise floating-point to prevent "optimizing" this out.
+-  const Simd<float, N> df;
+-  const auto max = Set(df, MantissaEnd<float>());
+-  const auto large = CopySignToAbs(max, v);
+-  const auto added = large + v;
+-  const auto rounded = added - large;
+-
+-  // Keep original if NaN or the magnitude is large (already an int).
+-  return IfThenElse(Abs(v) < max, rounded, v);
++  return Vec128<float, N>{wasm_f32x4_nearest(v.raw)};
+ }
+ 
+-namespace detail {
+-
+-// Truncating to integer and converting back to float is correct except when the
+-// input magnitude is large, in which case the input was already an integer
+-// (because mantissa >> exponent is zero).
+-template <size_t N>
+-HWY_API Mask128<float, N> UseInt(const Vec128<float, N> v) {
+-  return Abs(v) < Set(Simd<float, N>(), MantissaEnd<float>());
+-}
+-
+-}  // namespace detail
+-
+ // Toward zero, aka truncate
+ template <size_t N>
+ HWY_API Vec128<float, N> Trunc(const Vec128<float, N> v) {
+-  // TODO(eustas): is it f32x4.trunc? (not implemented yet)
+-  const Simd<float, N> df;
+-  const RebindToSigned<decltype(df)> di;
+-
+-  const auto integer = ConvertTo(di, v);  // round toward 0
+-  const auto int_f = ConvertTo(df, integer);
+-
+-  return IfThenElse(detail::UseInt(v), CopySign(int_f, v), v);
++  return Vec128<float, N>{wasm_f32x4_trunc(v.raw)};
+ }
+ 
+ // Toward +infinity, aka ceiling
+ template <size_t N>
+-HWY_INLINE Vec128<float, N> Ceil(const Vec128<float, N> v) {
+-  // TODO(eustas): is it f32x4.ceil? (not implemented yet)
+-  const Simd<float, N> df;
+-  const RebindToSigned<decltype(df)> di;
+-
+-  const auto integer = ConvertTo(di, v);  // round toward 0
+-  const auto int_f = ConvertTo(df, integer);
+-
+-  // Truncating a positive non-integer ends up smaller; if so, add 1.
+-  const auto neg1 = ConvertTo(df, VecFromMask(di, RebindMask(di, int_f < v)));
+-
+-  return IfThenElse(detail::UseInt(v), int_f - neg1, v);
++HWY_API Vec128<float, N> Ceil(const Vec128<float, N> v) {
++  return Vec128<float, N>{wasm_f32x4_ceil(v.raw)};
+ }
+ 
+ // Toward -infinity, aka floor
+ template <size_t N>
+-HWY_INLINE Vec128<float, N> Floor(const Vec128<float, N> v) {
+-  // TODO(eustas): is it f32x4.floor? (not implemented yet)
+-  const Simd<float, N> df;
+-  const RebindToSigned<decltype(df)> di;
+-
+-  const auto integer = ConvertTo(di, v);  // round toward 0
+-  const auto int_f = ConvertTo(df, integer);
+-
+-  // Truncating a negative non-integer ends up larger; if so, subtract 1.
+-  const auto neg1 = ConvertTo(df, VecFromMask(di, RebindMask(di, int_f > v)));
+-
+-  return IfThenElse(detail::UseInt(v), int_f + neg1, v);
++HWY_API Vec128<float, N> Floor(const Vec128<float, N> v) {
++  return Vec128<float, N>{wasm_f32x4_floor(v.raw)};
+ }
+ 
+ // ================================================== COMPARE
+@@ -919,6 +908,12 @@ HWY_API Mask128<TTo, N> RebindMask(Simd<TTo, N> /*tag*/, Mask128<TFrom, N> m) {
+   return Mask128<TTo, N>{m.raw};
+ }
+ 
++template <typename T, size_t N>
++HWY_API Mask128<T, N> TestBit(Vec128<T, N> v, Vec128<T, N> bit) {
++  static_assert(!hwy::IsFloat<T>(), "Only integer vectors supported");
++  return (v & bit) == bit;
++}
++
+ // ------------------------------ Equality
+ 
+ // Unsigned
+@@ -962,10 +957,47 @@ HWY_API Mask128<float, N> operator==(const Vec128<float, N> a,
+   return Mask128<float, N>{wasm_f32x4_eq(a.raw, b.raw)};
+ }
+ 
+-template <typename T, size_t N>
+-HWY_API Mask128<T, N> TestBit(Vec128<T, N> v, Vec128<T, N> bit) {
+-  static_assert(!hwy::IsFloat<T>(), "Only integer vectors supported");
+-  return (v & bit) == bit;
++// ------------------------------ Inequality
++
++// Unsigned
++template <size_t N>
++HWY_API Mask128<uint8_t, N> operator!=(const Vec128<uint8_t, N> a,
++                                       const Vec128<uint8_t, N> b) {
++  return Mask128<uint8_t, N>{wasm_i8x16_ne(a.raw, b.raw)};
++}
++template <size_t N>
++HWY_API Mask128<uint16_t, N> operator!=(const Vec128<uint16_t, N> a,
++                                        const Vec128<uint16_t, N> b) {
++  return Mask128<uint16_t, N>{wasm_i16x8_ne(a.raw, b.raw)};
++}
++template <size_t N>
++HWY_API Mask128<uint32_t, N> operator!=(const Vec128<uint32_t, N> a,
++                                        const Vec128<uint32_t, N> b) {
++  return Mask128<uint32_t, N>{wasm_i32x4_ne(a.raw, b.raw)};
++}
++
++// Signed
++template <size_t N>
++HWY_API Mask128<int8_t, N> operator!=(const Vec128<int8_t, N> a,
++                                      const Vec128<int8_t, N> b) {
++  return Mask128<int8_t, N>{wasm_i8x16_ne(a.raw, b.raw)};
++}
++template <size_t N>
++HWY_API Mask128<int16_t, N> operator!=(Vec128<int16_t, N> a,
++                                       Vec128<int16_t, N> b) {
++  return Mask128<int16_t, N>{wasm_i16x8_ne(a.raw, b.raw)};
++}
++template <size_t N>
++HWY_API Mask128<int32_t, N> operator!=(const Vec128<int32_t, N> a,
++                                       const Vec128<int32_t, N> b) {
++  return Mask128<int32_t, N>{wasm_i32x4_ne(a.raw, b.raw)};
++}
++
++// Float
++template <size_t N>
++HWY_API Mask128<float, N> operator!=(const Vec128<float, N> a,
++                                     const Vec128<float, N> b) {
++  return Mask128<float, N>{wasm_f32x4_ne(a.raw, b.raw)};
+ }
+ 
+ // ------------------------------ Strict inequality
+@@ -997,12 +1029,12 @@ HWY_API Mask128<int64_t, N> operator>(const Vec128<int64_t, N> a,
+ 
+   // Otherwise, the lower half decides.
+   const auto m_eq = a32 == b32;
+-  const auto lo_in_hi = wasm_v32x4_shuffle(m_gt, m_gt, 2, 2, 0, 0);
++  const auto lo_in_hi = wasm_i32x4_shuffle(m_gt, m_gt, 2, 2, 0, 0);
+   const auto lo_gt = And(m_eq, lo_in_hi);
+ 
+   const auto gt = Or(lo_gt, m_gt);
+   // Copy result in upper 32 bits to lower 32 bits.
+-  return Mask128<int64_t, N>{wasm_v32x4_shuffle(gt, gt, 3, 3, 1, 1)};
++  return Mask128<int64_t, N>{wasm_i32x4_shuffle(gt, gt, 3, 3, 1, 1)};
+ }
+ 
+ template <size_t N>
+@@ -1170,8 +1202,7 @@ HWY_API Vec128<T, N> ZeroIfNegative(Vec128<T, N> v) {
+ 
+ template <typename T, size_t N>
+ HWY_API Mask128<T, N> Not(const Mask128<T, N> m) {
+-  const Simd<T, N> d;
+-  return MaskFromVec(Not(VecFromMask(d, m)));
++  return MaskFromVec(Not(VecFromMask(Simd<T, N>(), m)));
+ }
+ 
+ template <typename T, size_t N>
+@@ -1479,36 +1510,37 @@ template <size_t N>
+ HWY_API int32_t GetLane(const Vec128<int32_t, N> v) {
+   return wasm_i32x4_extract_lane(v.raw, 0);
+ }
++template <size_t N>
++HWY_API uint64_t GetLane(const Vec128<uint64_t, N> v) {
++  return wasm_i64x2_extract_lane(v.raw, 0);
++}
++template <size_t N>
++HWY_API int64_t GetLane(const Vec128<int64_t, N> v) {
++  return wasm_i64x2_extract_lane(v.raw, 0);
++}
++
+ template <size_t N>
+ HWY_API float GetLane(const Vec128<float, N> v) {
+   return wasm_f32x4_extract_lane(v.raw, 0);
+ }
+ 
+-// ------------------------------ Extract half
++// ------------------------------ LowerHalf
+ 
+-// Returns upper/lower half of a vector.
+ template <typename T, size_t N>
+-HWY_API Vec128<T, N / 2> LowerHalf(Vec128<T, N> v) {
++HWY_API Vec128<T, N / 2> LowerHalf(Simd<T, N / 2> /* tag */, Vec128<T, N> v) {
+   return Vec128<T, N / 2>{v.raw};
+ }
+ 
+-// These copy hi into lo (smaller instruction encoding than shifts).
+-template <typename T>
+-HWY_API Vec128<T, 8 / sizeof(T)> UpperHalf(Vec128<T> v) {
+-  // TODO(eustas): use swizzle?
+-  return Vec128<T, 8 / sizeof(T)>{wasm_v32x4_shuffle(v.raw, v.raw, 2, 3, 2, 3)};
+-}
+-template <>
+-HWY_INLINE Vec128<float, 2> UpperHalf(Vec128<float> v) {
+-  // TODO(eustas): use swizzle?
+-  return Vec128<float, 2>{wasm_v32x4_shuffle(v.raw, v.raw, 2, 3, 2, 3)};
++template <typename T, size_t N>
++HWY_API Vec128<T, N / 2> LowerHalf(Vec128<T, N> v) {
++  return LowerHalf(Simd<T, N / 2>(), v);
+ }
+ 
+-// ------------------------------ Shift vector by constant #bytes
++// ------------------------------ ShiftLeftBytes
+ 
+ // 0x01..0F, kBytes = 1 => 0x02..0F00
+-template <int kBytes, typename T>
+-HWY_API Vec128<T> ShiftLeftBytes(const Vec128<T> v) {
++template <int kBytes, typename T, size_t N>
++HWY_API Vec128<T, N> ShiftLeftBytes(Simd<T, N> /* tag */, Vec128<T, N> v) {
+   static_assert(0 <= kBytes && kBytes <= 16, "Invalid kBytes");
+   const __i8x16 zero = wasm_i8x16_splat(0);
+   switch (kBytes) {
+@@ -1516,275 +1548,328 @@ HWY_API Vec128<T> ShiftLeftBytes(const Vec128<T> v) {
+       return v;
+ 
+     case 1:
+-      return Vec128<T>{wasm_v8x16_shuffle(v.raw, zero, 16, 0, 1, 2, 3, 4, 5, 6,
+-                                          7, 8, 9, 10, 11, 12, 13, 14)};
++      return Vec128<T, N>{wasm_i8x16_shuffle(v.raw, zero, 16, 0, 1, 2, 3, 4, 5,
++                                             6, 7, 8, 9, 10, 11, 12, 13, 14)};
+ 
+     case 2:
+-      return Vec128<T>{wasm_v8x16_shuffle(v.raw, zero, 16, 16, 0, 1, 2, 3, 4, 5,
+-                                          6, 7, 8, 9, 10, 11, 12, 13)};
++      return Vec128<T, N>{wasm_i8x16_shuffle(v.raw, zero, 16, 16, 0, 1, 2, 3, 4,
++                                             5, 6, 7, 8, 9, 10, 11, 12, 13)};
+ 
+     case 3:
+-      return Vec128<T>{wasm_v8x16_shuffle(v.raw, zero, 16, 16, 16, 0, 1, 2, 3,
+-                                          4, 5, 6, 7, 8, 9, 10, 11, 12)};
++      return Vec128<T, N>{wasm_i8x16_shuffle(v.raw, zero, 16, 16, 16, 0, 1, 2,
++                                             3, 4, 5, 6, 7, 8, 9, 10, 11, 12)};
+ 
+     case 4:
+-      return Vec128<T>{wasm_v8x16_shuffle(v.raw, zero, 16, 16, 16, 16, 0, 1, 2,
+-                                          3, 4, 5, 6, 7, 8, 9, 10, 11)};
++      return Vec128<T, N>{wasm_i8x16_shuffle(v.raw, zero, 16, 16, 16, 16, 0, 1,
++                                             2, 3, 4, 5, 6, 7, 8, 9, 10, 11)};
+ 
+     case 5:
+-      return Vec128<T>{wasm_v8x16_shuffle(v.raw, zero, 16, 16, 16, 16, 16, 0, 1,
+-                                          2, 3, 4, 5, 6, 7, 8, 9, 10)};
++      return Vec128<T, N>{wasm_i8x16_shuffle(v.raw, zero, 16, 16, 16, 16, 16, 0,
++                                             1, 2, 3, 4, 5, 6, 7, 8, 9, 10)};
+ 
+     case 6:
+-      return Vec128<T>{wasm_v8x16_shuffle(v.raw, zero, 16, 16, 16, 16, 16, 16,
+-                                          0, 1, 2, 3, 4, 5, 6, 7, 8, 9)};
++      return Vec128<T, N>{wasm_i8x16_shuffle(v.raw, zero, 16, 16, 16, 16, 16,
++                                             16, 0, 1, 2, 3, 4, 5, 6, 7, 8, 9)};
+ 
+     case 7:
+-      return Vec128<T>{wasm_v8x16_shuffle(v.raw, zero, 16, 16, 16, 16, 16, 16,
+-                                          16, 0, 1, 2, 3, 4, 5, 6, 7, 8)};
++      return Vec128<T, N>{wasm_i8x16_shuffle(
++          v.raw, zero, 16, 16, 16, 16, 16, 16, 16, 0, 1, 2, 3, 4, 5, 6, 7, 8)};
+ 
+     case 8:
+-      return Vec128<T>{wasm_v8x16_shuffle(v.raw, zero, 16, 16, 16, 16, 16, 16,
+-                                          16, 16, 0, 1, 2, 3, 4, 5, 6, 7)};
++      return Vec128<T, N>{wasm_i8x16_shuffle(
++          v.raw, zero, 16, 16, 16, 16, 16, 16, 16, 16, 0, 1, 2, 3, 4, 5, 6, 7)};
+ 
+     case 9:
+-      return Vec128<T>{wasm_v8x16_shuffle(v.raw, zero, 16, 16, 16, 16, 16, 16,
+-                                          16, 16, 16, 0, 1, 2, 3, 4, 5, 6)};
++      return Vec128<T, N>{wasm_i8x16_shuffle(v.raw, zero, 16, 16, 16, 16, 16,
++                                             16, 16, 16, 16, 0, 1, 2, 3, 4, 5,
++                                             6)};
+ 
+     case 10:
+-      return Vec128<T>{wasm_v8x16_shuffle(v.raw, zero, 16, 16, 16, 16, 16, 16,
+-                                          16, 16, 16, 16, 0, 1, 2, 3, 4, 5)};
++      return Vec128<T, N>{wasm_i8x16_shuffle(v.raw, zero, 16, 16, 16, 16, 16,
++                                             16, 16, 16, 16, 16, 0, 1, 2, 3, 4,
++                                             5)};
+ 
+     case 11:
+-      return Vec128<T>{wasm_v8x16_shuffle(v.raw, zero, 16, 16, 16, 16, 16, 16,
+-                                          16, 16, 16, 16, 16, 0, 1, 2, 3, 4)};
++      return Vec128<T, N>{wasm_i8x16_shuffle(v.raw, zero, 16, 16, 16, 16, 16,
++                                             16, 16, 16, 16, 16, 16, 0, 1, 2, 3,
++                                             4)};
+ 
+     case 12:
+-      return Vec128<T>{wasm_v8x16_shuffle(v.raw, zero, 16, 16, 16, 16, 16, 16,
+-                                          16, 16, 16, 16, 16, 16, 0, 1, 2, 3)};
++      return Vec128<T, N>{wasm_i8x16_shuffle(v.raw, zero, 16, 16, 16, 16, 16,
++                                             16, 16, 16, 16, 16, 16, 16, 0, 1,
++                                             2, 3)};
+ 
+     case 13:
+-      return Vec128<T>{wasm_v8x16_shuffle(v.raw, zero, 16, 16, 16, 16, 16, 16,
+-                                          16, 16, 16, 16, 16, 16, 16, 0, 1, 2)};
++      return Vec128<T, N>{wasm_i8x16_shuffle(v.raw, zero, 16, 16, 16, 16, 16,
++                                             16, 16, 16, 16, 16, 16, 16, 16, 0,
++                                             1, 2)};
+ 
+     case 14:
+-      return Vec128<T>{wasm_v8x16_shuffle(v.raw, zero, 16, 16, 16, 16, 16, 16,
+-                                          16, 16, 16, 16, 16, 16, 16, 16, 0,
+-                                          1)};
++      return Vec128<T, N>{wasm_i8x16_shuffle(v.raw, zero, 16, 16, 16, 16, 16,
++                                             16, 16, 16, 16, 16, 16, 16, 16, 16,
++                                             0, 1)};
+ 
+     case 15:
+-      return Vec128<T>{wasm_v8x16_shuffle(v.raw, zero, 16, 16, 16, 16, 16, 16,
+-                                          16, 16, 16, 16, 16, 16, 16, 16, 16,
+-                                          0)};
++      return Vec128<T, N>{wasm_i8x16_shuffle(v.raw, zero, 16, 16, 16, 16, 16,
++                                             16, 16, 16, 16, 16, 16, 16, 16, 16,
++                                             16, 0)};
+   }
+-  return Vec128<T>{zero};
++  return Vec128<T, N>{zero};
++}
++
++template <int kBytes, typename T, size_t N>
++HWY_API Vec128<T, N> ShiftLeftBytes(Vec128<T, N> v) {
++  return ShiftLeftBytes<kBytes>(Simd<T, N>(), v);
+ }
+ 
++// ------------------------------ ShiftLeftLanes
++
+ template <int kLanes, typename T, size_t N>
+-HWY_API Vec128<T, N> ShiftLeftLanes(const Vec128<T, N> v) {
+-  const Simd<uint8_t, N * sizeof(T)> d8;
+-  const Simd<T, N> d;
++HWY_API Vec128<T, N> ShiftLeftLanes(Simd<T, N> d, const Vec128<T, N> v) {
++  const Repartition<uint8_t, decltype(d)> d8;
+   return BitCast(d, ShiftLeftBytes<kLanes * sizeof(T)>(BitCast(d8, v)));
+ }
+ 
+-// 0x01..0F, kBytes = 1 => 0x0001..0E
+-template <int kBytes, typename T>
+-HWY_API Vec128<T> ShiftRightBytes(const Vec128<T> v) {
++template <int kLanes, typename T, size_t N>
++HWY_API Vec128<T, N> ShiftLeftLanes(const Vec128<T, N> v) {
++  return ShiftLeftLanes<kLanes>(Simd<T, N>(), v);
++}
++
++// ------------------------------ ShiftRightBytes
++namespace detail {
++
++// Helper function allows zeroing invalid lanes in caller.
++template <int kBytes, typename T, size_t N>
++HWY_API __i8x16 ShrBytes(const Vec128<T, N> v) {
+   static_assert(0 <= kBytes && kBytes <= 16, "Invalid kBytes");
+   const __i8x16 zero = wasm_i8x16_splat(0);
++
+   switch (kBytes) {
+     case 0:
+-      return v;
++      return v.raw;
+ 
+     case 1:
+-      return Vec128<T>{wasm_v8x16_shuffle(v.raw, zero, 1, 2, 3, 4, 5, 6, 7, 8,
+-                                          9, 10, 11, 12, 13, 14, 15, 16)};
++      return wasm_i8x16_shuffle(v.raw, zero, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11,
++                                12, 13, 14, 15, 16);
+ 
+     case 2:
+-      return Vec128<T>{wasm_v8x16_shuffle(v.raw, zero, 2, 3, 4, 5, 6, 7, 8, 9,
+-                                          10, 11, 12, 13, 14, 15, 16, 16)};
++      return wasm_i8x16_shuffle(v.raw, zero, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12,
++                                13, 14, 15, 16, 16);
+ 
+     case 3:
+-      return Vec128<T>{wasm_v8x16_shuffle(v.raw, zero, 3, 4, 5, 6, 7, 8, 9, 10,
+-                                          11, 12, 13, 14, 15, 16, 16, 16)};
++      return wasm_i8x16_shuffle(v.raw, zero, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12,
++                                13, 14, 15, 16, 16, 16);
+ 
+     case 4:
+-      return Vec128<T>{wasm_v8x16_shuffle(v.raw, zero, 4, 5, 6, 7, 8, 9, 10, 11,
+-                                          12, 13, 14, 15, 16, 16, 16, 16)};
++      return wasm_i8x16_shuffle(v.raw, zero, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13,
++                                14, 15, 16, 16, 16, 16);
+ 
+     case 5:
+-      return Vec128<T>{wasm_v8x16_shuffle(v.raw, zero, 5, 6, 7, 8, 9, 10, 11,
+-                                          12, 13, 14, 15, 16, 16, 16, 16, 16)};
++      return wasm_i8x16_shuffle(v.raw, zero, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14,
++                                15, 16, 16, 16, 16, 16);
+ 
+     case 6:
+-      return Vec128<T>{wasm_v8x16_shuffle(v.raw, zero, 6, 7, 8, 9, 10, 11, 12,
+-                                          13, 14, 15, 16, 16, 16, 16, 16, 16)};
++      return wasm_i8x16_shuffle(v.raw, zero, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15,
++                                16, 16, 16, 16, 16, 16);
+ 
+     case 7:
+-      return Vec128<T>{wasm_v8x16_shuffle(v.raw, zero, 7, 8, 9, 10, 11, 12, 13,
+-                                          14, 15, 16, 16, 16, 16, 16, 16, 16)};
++      return wasm_i8x16_shuffle(v.raw, zero, 7, 8, 9, 10, 11, 12, 13, 14, 15,
++                                16, 16, 16, 16, 16, 16, 16);
+ 
+     case 8:
+-      return Vec128<T>{wasm_v8x16_shuffle(v.raw, zero, 8, 9, 10, 11, 12, 13, 14,
+-                                          15, 16, 16, 16, 16, 16, 16, 16, 16)};
++      return wasm_i8x16_shuffle(v.raw, zero, 8, 9, 10, 11, 12, 13, 14, 15, 16,
++                                16, 16, 16, 16, 16, 16, 16);
+ 
+     case 9:
+-      return Vec128<T>{wasm_v8x16_shuffle(v.raw, zero, 9, 10, 11, 12, 13, 14,
+-                                          15, 16, 16, 16, 16, 16, 16, 16, 16,
+-                                          16)};
++      return wasm_i8x16_shuffle(v.raw, zero, 9, 10, 11, 12, 13, 14, 15, 16, 16,
++                                16, 16, 16, 16, 16, 16, 16);
+ 
+     case 10:
+-      return Vec128<T>{wasm_v8x16_shuffle(v.raw, zero, 10, 11, 12, 13, 14, 15,
+-                                          16, 16, 16, 16, 16, 16, 16, 16, 16,
+-                                          16)};
++      return wasm_i8x16_shuffle(v.raw, zero, 10, 11, 12, 13, 14, 15, 16, 16, 16,
++                                16, 16, 16, 16, 16, 16, 16);
+ 
+     case 11:
+-      return Vec128<T>{wasm_v8x16_shuffle(v.raw, zero, 11, 12, 13, 14, 15, 16,
+-                                          16, 16, 16, 16, 16, 16, 16, 16, 16,
+-                                          16)};
++      return wasm_i8x16_shuffle(v.raw, zero, 11, 12, 13, 14, 15, 16, 16, 16, 16,
++                                16, 16, 16, 16, 16, 16, 16);
+ 
+     case 12:
+-      return Vec128<T>{wasm_v8x16_shuffle(v.raw, zero, 12, 13, 14, 15, 16, 16,
+-                                          16, 16, 16, 16, 16, 16, 16, 16, 16,
+-                                          16)};
++      return wasm_i8x16_shuffle(v.raw, zero, 12, 13, 14, 15, 16, 16, 16, 16, 16,
++                                16, 16, 16, 16, 16, 16, 16);
+ 
+     case 13:
+-      return Vec128<T>{wasm_v8x16_shuffle(v.raw, zero, 13, 14, 15, 16, 16, 16,
+-                                          16, 16, 16, 16, 16, 16, 16, 16, 16,
+-                                          16)};
++      return wasm_i8x16_shuffle(v.raw, zero, 13, 14, 15, 16, 16, 16, 16, 16, 16,
++                                16, 16, 16, 16, 16, 16, 16);
+ 
+     case 14:
+-      return Vec128<T>{wasm_v8x16_shuffle(v.raw, zero, 14, 15, 16, 16, 16, 16,
+-                                          16, 16, 16, 16, 16, 16, 16, 16, 16,
+-                                          16)};
++      return wasm_i8x16_shuffle(v.raw, zero, 14, 15, 16, 16, 16, 16, 16, 16, 16,
++                                16, 16, 16, 16, 16, 16, 16);
+ 
+     case 15:
+-      return Vec128<T>{wasm_v8x16_shuffle(v.raw, zero, 15, 16, 16, 16, 16, 16,
+-                                          16, 16, 16, 16, 16, 16, 16, 16, 16,
+-                                          16)};
++      return wasm_i8x16_shuffle(v.raw, zero, 15, 16, 16, 16, 16, 16, 16, 16, 16,
++                                16, 16, 16, 16, 16, 16, 16);
++    case 16:
++      return zero;
+   }
+-  return Vec128<T>{zero};
+ }
+ 
++}  // namespace detail
++
++// 0x01..0F, kBytes = 1 => 0x0001..0E
++template <int kBytes, typename T, size_t N>
++HWY_API Vec128<T, N> ShiftRightBytes(Simd<T, N> /* tag */, Vec128<T, N> v) {
++  // For partial vectors, clear upper lanes so we shift in zeros.
++  if (N != 16 / sizeof(T)) {
++    const Vec128<T> vfull{v.raw};
++    v = Vec128<T, N>{IfThenElseZero(FirstN(Full128<T>(), N), vfull).raw};
++  }
++  return Vec128<T, N>{detail::ShrBytes<kBytes>(v)};
++}
++
++// ------------------------------ ShiftRightLanes
+ template <int kLanes, typename T, size_t N>
+-HWY_API Vec128<T, N> ShiftRightLanes(const Vec128<T, N> v) {
+-  const Simd<uint8_t, N * sizeof(T)> d8;
+-  const Simd<T, N> d;
++HWY_API Vec128<T, N> ShiftRightLanes(Simd<T, N> d, const Vec128<T, N> v) {
++  const Repartition<uint8_t, decltype(d)> d8;
+   return BitCast(d, ShiftRightBytes<kLanes * sizeof(T)>(BitCast(d8, v)));
+ }
+ 
+-// ------------------------------ Extract from 2x 128-bit at constant offset
++// ------------------------------ UpperHalf (ShiftRightBytes)
++
++// Full input: copy hi into lo (smaller instruction encoding than shifts).
++template <typename T>
++HWY_API Vec128<T, 8 / sizeof(T)> UpperHalf(Half<Full128<T>> /* tag */,
++                                           const Vec128<T> v) {
++  return Vec128<T, 8 / sizeof(T)>{wasm_i32x4_shuffle(v.raw, v.raw, 2, 3, 2, 3)};
++}
++HWY_API Vec128<float, 2> UpperHalf(Half<Full128<float>> /* tag */,
++                                   const Vec128<float> v) {
++  return Vec128<float, 2>{wasm_i32x4_shuffle(v.raw, v.raw, 2, 3, 2, 3)};
++}
+ 
+-// Extracts 128 bits from <hi, lo> by skipping the least-significant kBytes.
+-template <int kBytes, typename T>
+-HWY_API Vec128<T> CombineShiftRightBytes(const Vec128<T> hi,
+-                                         const Vec128<T> lo) {
++// Partial
++template <typename T, size_t N, HWY_IF_LE64(T, N)>
++HWY_API Vec128<T, (N + 1) / 2> UpperHalf(Half<Simd<T, N>> /* tag */,
++                                         Vec128<T, N> v) {
++  const Simd<T, N> d;
++  const auto vu = BitCast(RebindToUnsigned<decltype(d)>(), v);
++  const auto upper = BitCast(d, ShiftRightBytes<N * sizeof(T) / 2>(vu));
++  return Vec128<T, (N + 1) / 2>{upper.raw};
++}
++
++// ------------------------------ CombineShiftRightBytes
++
++template <int kBytes, typename T, class V = Vec128<T>>
++HWY_API V CombineShiftRightBytes(Full128<T> /* tag */, V hi, V lo) {
+   static_assert(0 <= kBytes && kBytes <= 16, "Invalid kBytes");
+   switch (kBytes) {
+     case 0:
+       return lo;
+ 
+     case 1:
+-      return Vec128<T>{wasm_v8x16_shuffle(lo.raw, hi.raw, 1, 2, 3, 4, 5, 6, 7,
+-                                          8, 9, 10, 11, 12, 13, 14, 15, 16)};
++      return V{wasm_i8x16_shuffle(lo.raw, hi.raw, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10,
++                                  11, 12, 13, 14, 15, 16)};
+ 
+     case 2:
+-      return Vec128<T>{wasm_v8x16_shuffle(lo.raw, hi.raw, 2, 3, 4, 5, 6, 7, 8,
+-                                          9, 10, 11, 12, 13, 14, 15, 16, 17)};
++      return V{wasm_i8x16_shuffle(lo.raw, hi.raw, 2, 3, 4, 5, 6, 7, 8, 9, 10,
++                                  11, 12, 13, 14, 15, 16, 17)};
+ 
+     case 3:
+-      return Vec128<T>{wasm_v8x16_shuffle(lo.raw, hi.raw, 3, 4, 5, 6, 7, 8, 9,
+-                                          10, 11, 12, 13, 14, 15, 16, 17, 18)};
++      return V{wasm_i8x16_shuffle(lo.raw, hi.raw, 3, 4, 5, 6, 7, 8, 9, 10, 11,
++                                  12, 13, 14, 15, 16, 17, 18)};
+ 
+     case 4:
+-      return Vec128<T>{wasm_v8x16_shuffle(lo.raw, hi.raw, 4, 5, 6, 7, 8, 9, 10,
+-                                          11, 12, 13, 14, 15, 16, 17, 18, 19)};
++      return V{wasm_i8x16_shuffle(lo.raw, hi.raw, 4, 5, 6, 7, 8, 9, 10, 11, 12,
++                                  13, 14, 15, 16, 17, 18, 19)};
+ 
+     case 5:
+-      return Vec128<T>{wasm_v8x16_shuffle(lo.raw, hi.raw, 5, 6, 7, 8, 9, 10, 11,
+-                                          12, 13, 14, 15, 16, 17, 18, 19, 20)};
++      return V{wasm_i8x16_shuffle(lo.raw, hi.raw, 5, 6, 7, 8, 9, 10, 11, 12, 13,
++                                  14, 15, 16, 17, 18, 19, 20)};
+ 
+     case 6:
+-      return Vec128<T>{wasm_v8x16_shuffle(lo.raw, hi.raw, 6, 7, 8, 9, 10, 11,
+-                                          12, 13, 14, 15, 16, 17, 18, 19, 20,
+-                                          21)};
++      return V{wasm_i8x16_shuffle(lo.raw, hi.raw, 6, 7, 8, 9, 10, 11, 12, 13,
++                                  14, 15, 16, 17, 18, 19, 20, 21)};
+ 
+     case 7:
+-      return Vec128<T>{wasm_v8x16_shuffle(lo.raw, hi.raw, 7, 8, 9, 10, 11, 12,
+-                                          13, 14, 15, 16, 17, 18, 19, 20, 21,
+-                                          22)};
++      return V{wasm_i8x16_shuffle(lo.raw, hi.raw, 7, 8, 9, 10, 11, 12, 13, 14,
++                                  15, 16, 17, 18, 19, 20, 21, 22)};
+ 
+     case 8:
+-      return Vec128<T>{wasm_v8x16_shuffle(lo.raw, hi.raw, 8, 9, 10, 11, 12, 13,
+-                                          14, 15, 16, 17, 18, 19, 20, 21, 22,
+-                                          23)};
++      return V{wasm_i8x16_shuffle(lo.raw, hi.raw, 8, 9, 10, 11, 12, 13, 14, 15,
++                                  16, 17, 18, 19, 20, 21, 22, 23)};
+ 
+     case 9:
+-      return Vec128<T>{wasm_v8x16_shuffle(lo.raw, hi.raw, 9, 10, 11, 12, 13, 14,
+-                                          15, 16, 17, 18, 19, 20, 21, 22, 23,
+-                                          24)};
++      return V{wasm_i8x16_shuffle(lo.raw, hi.raw, 9, 10, 11, 12, 13, 14, 15, 16,
++                                  17, 18, 19, 20, 21, 22, 23, 24)};
+ 
+     case 10:
+-      return Vec128<T>{wasm_v8x16_shuffle(lo.raw, hi.raw, 10, 11, 12, 13, 14,
+-                                          15, 16, 17, 18, 19, 20, 21, 22, 23,
+-                                          24, 25)};
++      return V{wasm_i8x16_shuffle(lo.raw, hi.raw, 10, 11, 12, 13, 14, 15, 16,
++                                  17, 18, 19, 20, 21, 22, 23, 24, 25)};
+ 
+     case 11:
+-      return Vec128<T>{wasm_v8x16_shuffle(lo.raw, hi.raw, 11, 12, 13, 14, 15,
+-                                          16, 17, 18, 19, 20, 21, 22, 23, 24,
+-                                          25, 26)};
++      return V{wasm_i8x16_shuffle(lo.raw, hi.raw, 11, 12, 13, 14, 15, 16, 17,
++                                  18, 19, 20, 21, 22, 23, 24, 25, 26)};
+ 
+     case 12:
+-      return Vec128<T>{wasm_v8x16_shuffle(lo.raw, hi.raw, 12, 13, 14, 15, 16,
+-                                          17, 18, 19, 20, 21, 22, 23, 24, 25,
+-                                          26, 27)};
++      return V{wasm_i8x16_shuffle(lo.raw, hi.raw, 12, 13, 14, 15, 16, 17, 18,
++                                  19, 20, 21, 22, 23, 24, 25, 26, 27)};
+ 
+     case 13:
+-      return Vec128<T>{wasm_v8x16_shuffle(lo.raw, hi.raw, 13, 14, 15, 16, 17,
+-                                          18, 19, 20, 21, 22, 23, 24, 25, 26,
+-                                          27, 28)};
++      return V{wasm_i8x16_shuffle(lo.raw, hi.raw, 13, 14, 15, 16, 17, 18, 19,
++                                  20, 21, 22, 23, 24, 25, 26, 27, 28)};
+ 
+     case 14:
+-      return Vec128<T>{wasm_v8x16_shuffle(lo.raw, hi.raw, 14, 15, 16, 17, 18,
+-                                          19, 20, 21, 22, 23, 24, 25, 26, 27,
+-                                          28, 29)};
++      return V{wasm_i8x16_shuffle(lo.raw, hi.raw, 14, 15, 16, 17, 18, 19, 20,
++                                  21, 22, 23, 24, 25, 26, 27, 28, 29)};
+ 
+     case 15:
+-      return Vec128<T>{wasm_v8x16_shuffle(lo.raw, hi.raw, 15, 16, 17, 18, 19,
+-                                          20, 21, 22, 23, 24, 25, 26, 27, 28,
+-                                          29, 30)};
++      return V{wasm_i8x16_shuffle(lo.raw, hi.raw, 15, 16, 17, 18, 19, 20, 21,
++                                  22, 23, 24, 25, 26, 27, 28, 29, 30)};
+   }
+   return hi;
+ }
+ 
++template <int kBytes, typename T, size_t N, HWY_IF_LE64(T, N),
++          class V = Vec128<T, N>>
++HWY_API V CombineShiftRightBytes(Simd<T, N> d, V hi, V lo) {
++  constexpr size_t kSize = N * sizeof(T);
++  static_assert(0 < kBytes && kBytes < kSize, "kBytes invalid");
++  const Repartition<uint8_t, decltype(d)> d8;
++  const Full128<uint8_t> d_full8;
++  using V8 = VFromD<decltype(d_full8)>;
++  const V8 hi8{BitCast(d8, hi).raw};
++  // Move into most-significant bytes
++  const V8 lo8 = ShiftLeftBytes<16 - kSize>(V8{BitCast(d8, lo).raw});
++  const V8 r = CombineShiftRightBytes<16 - kSize + kBytes>(d_full8, hi8, lo8);
++  return V{BitCast(Full128<T>(), r).raw};
++}
++
+ // ------------------------------ Broadcast/splat any lane
+ 
+ // Unsigned
+ template <int kLane, size_t N>
+ HWY_API Vec128<uint16_t, N> Broadcast(const Vec128<uint16_t, N> v) {
+   static_assert(0 <= kLane && kLane < N, "Invalid lane");
+-  return Vec128<uint16_t, N>{wasm_v16x8_shuffle(
++  return Vec128<uint16_t, N>{wasm_i16x8_shuffle(
+       v.raw, v.raw, kLane, kLane, kLane, kLane, kLane, kLane, kLane, kLane)};
+ }
+ template <int kLane, size_t N>
+ HWY_API Vec128<uint32_t, N> Broadcast(const Vec128<uint32_t, N> v) {
+   static_assert(0 <= kLane && kLane < N, "Invalid lane");
+   return Vec128<uint32_t, N>{
+-      wasm_v32x4_shuffle(v.raw, v.raw, kLane, kLane, kLane, kLane)};
++      wasm_i32x4_shuffle(v.raw, v.raw, kLane, kLane, kLane, kLane)};
+ }
+ 
+ // Signed
+ template <int kLane, size_t N>
+ HWY_API Vec128<int16_t, N> Broadcast(const Vec128<int16_t, N> v) {
+   static_assert(0 <= kLane && kLane < N, "Invalid lane");
+-  return Vec128<int16_t, N>{wasm_v16x8_shuffle(
++  return Vec128<int16_t, N>{wasm_i16x8_shuffle(
+       v.raw, v.raw, kLane, kLane, kLane, kLane, kLane, kLane, kLane, kLane)};
+ }
+ template <int kLane, size_t N>
+ HWY_API Vec128<int32_t, N> Broadcast(const Vec128<int32_t, N> v) {
+   static_assert(0 <= kLane && kLane < N, "Invalid lane");
+   return Vec128<int32_t, N>{
+-      wasm_v32x4_shuffle(v.raw, v.raw, kLane, kLane, kLane, kLane)};
++      wasm_i32x4_shuffle(v.raw, v.raw, kLane, kLane, kLane, kLane)};
+ }
+ 
+ // Float
+@@ -1792,22 +1877,22 @@ template <int kLane, size_t N>
+ HWY_API Vec128<float, N> Broadcast(const Vec128<float, N> v) {
+   static_assert(0 <= kLane && kLane < N, "Invalid lane");
+   return Vec128<float, N>{
+-      wasm_v32x4_shuffle(v.raw, v.raw, kLane, kLane, kLane, kLane)};
++      wasm_i32x4_shuffle(v.raw, v.raw, kLane, kLane, kLane, kLane)};
+ }
+ 
+-// ------------------------------ Shuffle bytes with variable indices
++// ------------------------------ TableLookupBytes
+ 
+ // Returns vector of bytes[from[i]]. "from" is also interpreted as bytes, i.e.
+ // lane indices in [0, 16).
+-template <typename T, size_t N>
+-HWY_API Vec128<T, N> TableLookupBytes(const Vec128<T, N> bytes,
+-                                      const Vec128<T, N> from) {
++template <typename T, size_t N, typename TI, size_t NI>
++HWY_API Vec128<TI, NI> TableLookupBytes(const Vec128<T, N> bytes,
++                                        const Vec128<TI, NI> from) {
+ // Not yet available in all engines, see
+ // https://github.com/WebAssembly/simd/blob/bdcc304b2d379f4601c2c44ea9b44ed9484fde7e/proposals/simd/ImplementationStatus.md
+ // V8 implementation of this had a bug, fixed on 2021-04-03:
+ // https://chromium-review.googlesource.com/c/v8/v8/+/2822951
+ #if 0
+-  return Vec128<T, N>{wasm_v8x16_swizzle(bytes.raw, from.raw)};
++  return Vec128<TI, NI>{wasm_i8x16_swizzle(bytes.raw, from.raw)};
+ #else
+   alignas(16) uint8_t control[16];
+   alignas(16) uint8_t input[16];
+@@ -1817,10 +1902,23 @@ HWY_API Vec128<T, N> TableLookupBytes(const Vec128<T, N> bytes,
+   for (size_t i = 0; i < 16; ++i) {
+     output[i] = control[i] < 16 ? input[control[i]] : 0;
+   }
+-  return Vec128<T, N>{wasm_v128_load(output)};
++  return Vec128<TI, NI>{wasm_v128_load(output)};
+ #endif
+ }
+ 
++template <typename T, size_t N, typename TI, size_t NI>
++HWY_API Vec128<TI, NI> TableLookupBytesOr0(const Vec128<T, N> bytes,
++                                           const Vec128<TI, NI> from) {
++  const Simd<TI, NI> d;
++  // Mask size must match vector type, so cast everything to this type.
++  Repartition<int8_t, decltype(d)> di8;
++  Repartition<int8_t, Simd<T, N>> d_bytes8;
++  const auto msb = BitCast(di8, from) < Zero(di8);
++  const auto lookup =
++      TableLookupBytes(BitCast(d_bytes8, bytes), BitCast(di8, from));
++  return BitCast(d, IfThenZeroElse(msb, lookup));
++}
++
+ // ------------------------------ Hard-coded shuffles
+ 
+ // Notation: let Vec128<int32_t> have lanes 3,2,1,0 (0 is least-significant).
+@@ -1830,56 +1928,56 @@ HWY_API Vec128<T, N> TableLookupBytes(const Vec128<T, N> bytes,
+ 
+ // Swap 32-bit halves in 64-bit halves.
+ HWY_API Vec128<uint32_t> Shuffle2301(const Vec128<uint32_t> v) {
+-  return Vec128<uint32_t>{wasm_v32x4_shuffle(v.raw, v.raw, 1, 0, 3, 2)};
++  return Vec128<uint32_t>{wasm_i32x4_shuffle(v.raw, v.raw, 1, 0, 3, 2)};
+ }
+ HWY_API Vec128<int32_t> Shuffle2301(const Vec128<int32_t> v) {
+-  return Vec128<int32_t>{wasm_v32x4_shuffle(v.raw, v.raw, 1, 0, 3, 2)};
++  return Vec128<int32_t>{wasm_i32x4_shuffle(v.raw, v.raw, 1, 0, 3, 2)};
+ }
+ HWY_API Vec128<float> Shuffle2301(const Vec128<float> v) {
+-  return Vec128<float>{wasm_v32x4_shuffle(v.raw, v.raw, 1, 0, 3, 2)};
++  return Vec128<float>{wasm_i32x4_shuffle(v.raw, v.raw, 1, 0, 3, 2)};
+ }
+ 
+ // Swap 64-bit halves
+ HWY_API Vec128<uint32_t> Shuffle1032(const Vec128<uint32_t> v) {
+-  return Vec128<uint32_t>{wasm_v64x2_shuffle(v.raw, v.raw, 1, 0)};
++  return Vec128<uint32_t>{wasm_i64x2_shuffle(v.raw, v.raw, 1, 0)};
+ }
+ HWY_API Vec128<int32_t> Shuffle1032(const Vec128<int32_t> v) {
+-  return Vec128<int32_t>{wasm_v64x2_shuffle(v.raw, v.raw, 1, 0)};
++  return Vec128<int32_t>{wasm_i64x2_shuffle(v.raw, v.raw, 1, 0)};
+ }
+ HWY_API Vec128<float> Shuffle1032(const Vec128<float> v) {
+-  return Vec128<float>{wasm_v64x2_shuffle(v.raw, v.raw, 1, 0)};
++  return Vec128<float>{wasm_i64x2_shuffle(v.raw, v.raw, 1, 0)};
+ }
+ 
+ // Rotate right 32 bits
+ HWY_API Vec128<uint32_t> Shuffle0321(const Vec128<uint32_t> v) {
+-  return Vec128<uint32_t>{wasm_v32x4_shuffle(v.raw, v.raw, 1, 2, 3, 0)};
++  return Vec128<uint32_t>{wasm_i32x4_shuffle(v.raw, v.raw, 1, 2, 3, 0)};
+ }
+ HWY_API Vec128<int32_t> Shuffle0321(const Vec128<int32_t> v) {
+-  return Vec128<int32_t>{wasm_v32x4_shuffle(v.raw, v.raw, 1, 2, 3, 0)};
++  return Vec128<int32_t>{wasm_i32x4_shuffle(v.raw, v.raw, 1, 2, 3, 0)};
+ }
+ HWY_API Vec128<float> Shuffle0321(const Vec128<float> v) {
+-  return Vec128<float>{wasm_v32x4_shuffle(v.raw, v.raw, 1, 2, 3, 0)};
++  return Vec128<float>{wasm_i32x4_shuffle(v.raw, v.raw, 1, 2, 3, 0)};
+ }
+ // Rotate left 32 bits
+ HWY_API Vec128<uint32_t> Shuffle2103(const Vec128<uint32_t> v) {
+-  return Vec128<uint32_t>{wasm_v32x4_shuffle(v.raw, v.raw, 3, 0, 1, 2)};
++  return Vec128<uint32_t>{wasm_i32x4_shuffle(v.raw, v.raw, 3, 0, 1, 2)};
+ }
+ HWY_API Vec128<int32_t> Shuffle2103(const Vec128<int32_t> v) {
+-  return Vec128<int32_t>{wasm_v32x4_shuffle(v.raw, v.raw, 3, 0, 1, 2)};
++  return Vec128<int32_t>{wasm_i32x4_shuffle(v.raw, v.raw, 3, 0, 1, 2)};
+ }
+ HWY_API Vec128<float> Shuffle2103(const Vec128<float> v) {
+-  return Vec128<float>{wasm_v32x4_shuffle(v.raw, v.raw, 3, 0, 1, 2)};
++  return Vec128<float>{wasm_i32x4_shuffle(v.raw, v.raw, 3, 0, 1, 2)};
+ }
+ 
+ // Reverse
+ HWY_API Vec128<uint32_t> Shuffle0123(const Vec128<uint32_t> v) {
+-  return Vec128<uint32_t>{wasm_v32x4_shuffle(v.raw, v.raw, 3, 2, 1, 0)};
++  return Vec128<uint32_t>{wasm_i32x4_shuffle(v.raw, v.raw, 3, 2, 1, 0)};
+ }
+ HWY_API Vec128<int32_t> Shuffle0123(const Vec128<int32_t> v) {
+-  return Vec128<int32_t>{wasm_v32x4_shuffle(v.raw, v.raw, 3, 2, 1, 0)};
++  return Vec128<int32_t>{wasm_i32x4_shuffle(v.raw, v.raw, 3, 2, 1, 0)};
+ }
+ HWY_API Vec128<float> Shuffle0123(const Vec128<float> v) {
+-  return Vec128<float>{wasm_v32x4_shuffle(v.raw, v.raw, 3, 2, 1, 0)};
++  return Vec128<float>{wasm_i32x4_shuffle(v.raw, v.raw, 3, 2, 1, 0)};
+ }
+ 
+ // ------------------------------ TableLookupLanes
+@@ -1928,176 +2026,274 @@ HWY_API Vec128<float, N> TableLookupLanes(const Vec128<float, N> v,
+                  TableLookupBytes(BitCast(di, v), Vec128<int32_t, N>{idx.raw}));
+ }
+ 
+-// ------------------------------ Zip lanes
+-
+-// Same as Interleave*, except that the return lanes are double-width integers;
+-// this is necessary because the single-lane scalar cannot return two values.
++// ------------------------------ InterleaveLower
+ 
+ template <size_t N>
+-HWY_API Vec128<uint16_t, (N + 1) / 2> ZipLower(const Vec128<uint8_t, N> a,
+-                                               const Vec128<uint8_t, N> b) {
+-  return Vec128<uint16_t, (N + 1) / 2>{wasm_v8x16_shuffle(
++HWY_API Vec128<uint8_t, N> InterleaveLower(Vec128<uint8_t, N> a,
++                                           Vec128<uint8_t, N> b) {
++  return Vec128<uint8_t, N>{wasm_i8x16_shuffle(
+       a.raw, b.raw, 0, 16, 1, 17, 2, 18, 3, 19, 4, 20, 5, 21, 6, 22, 7, 23)};
+ }
+ template <size_t N>
+-HWY_API Vec128<uint32_t, (N + 1) / 2> ZipLower(const Vec128<uint16_t, N> a,
+-                                               const Vec128<uint16_t, N> b) {
+-  return Vec128<uint32_t, (N + 1) / 2>{
+-      wasm_v16x8_shuffle(a.raw, b.raw, 0, 8, 1, 9, 2, 10, 3, 11)};
++HWY_API Vec128<uint16_t, N> InterleaveLower(Vec128<uint16_t, N> a,
++                                            Vec128<uint16_t, N> b) {
++  return Vec128<uint16_t, N>{
++      wasm_i16x8_shuffle(a.raw, b.raw, 0, 8, 1, 9, 2, 10, 3, 11)};
++}
++template <size_t N>
++HWY_API Vec128<uint32_t, N> InterleaveLower(Vec128<uint32_t, N> a,
++                                            Vec128<uint32_t, N> b) {
++  return Vec128<uint32_t, N>{wasm_i32x4_shuffle(a.raw, b.raw, 0, 4, 1, 5)};
++}
++template <size_t N>
++HWY_API Vec128<uint64_t, N> InterleaveLower(Vec128<uint64_t, N> a,
++                                            Vec128<uint64_t, N> b) {
++  return Vec128<uint64_t, N>{wasm_i64x2_shuffle(a.raw, b.raw, 0, 2)};
+ }
+ 
+ template <size_t N>
+-HWY_API Vec128<int16_t, (N + 1) / 2> ZipLower(const Vec128<int8_t, N> a,
+-                                              const Vec128<int8_t, N> b) {
+-  return Vec128<int16_t, (N + 1) / 2>{wasm_v8x16_shuffle(
++HWY_API Vec128<int8_t, N> InterleaveLower(Vec128<int8_t, N> a,
++                                          Vec128<int8_t, N> b) {
++  return Vec128<int8_t, N>{wasm_i8x16_shuffle(
+       a.raw, b.raw, 0, 16, 1, 17, 2, 18, 3, 19, 4, 20, 5, 21, 6, 22, 7, 23)};
+ }
+ template <size_t N>
+-HWY_API Vec128<int32_t, (N + 1) / 2> ZipLower(const Vec128<int16_t, N> a,
+-                                              const Vec128<int16_t, N> b) {
+-  return Vec128<int32_t, (N + 1) / 2>{
+-      wasm_v16x8_shuffle(a.raw, b.raw, 0, 8, 1, 9, 2, 10, 3, 11)};
++HWY_API Vec128<int16_t, N> InterleaveLower(Vec128<int16_t, N> a,
++                                           Vec128<int16_t, N> b) {
++  return Vec128<int16_t, N>{
++      wasm_i16x8_shuffle(a.raw, b.raw, 0, 8, 1, 9, 2, 10, 3, 11)};
++}
++template <size_t N>
++HWY_API Vec128<int32_t, N> InterleaveLower(Vec128<int32_t, N> a,
++                                           Vec128<int32_t, N> b) {
++  return Vec128<int32_t, N>{wasm_i32x4_shuffle(a.raw, b.raw, 0, 4, 1, 5)};
++}
++template <size_t N>
++HWY_API Vec128<int64_t, N> InterleaveLower(Vec128<int64_t, N> a,
++                                           Vec128<int64_t, N> b) {
++  return Vec128<int64_t, N>{wasm_i64x2_shuffle(a.raw, b.raw, 0, 2)};
+ }
+ 
+ template <size_t N>
+-HWY_API Vec128<uint16_t, N / 2> ZipUpper(const Vec128<uint8_t, N> a,
+-                                         const Vec128<uint8_t, N> b) {
+-  return Vec128<uint16_t, N / 2>{wasm_v8x16_shuffle(a.raw, b.raw, 8, 24, 9, 25,
+-                                                    10, 26, 11, 27, 12, 28, 13,
+-                                                    29, 14, 30, 15, 31)};
++HWY_API Vec128<float, N> InterleaveLower(Vec128<float, N> a,
++                                         Vec128<float, N> b) {
++  return Vec128<float, N>{wasm_i32x4_shuffle(a.raw, b.raw, 0, 4, 1, 5)};
+ }
++
++// Additional overload for the optional Simd<> tag.
++template <typename T, size_t N, class V = Vec128<T, N>>
++HWY_API V InterleaveLower(Simd<T, N> /* tag */, V a, V b) {
++  return InterleaveLower(a, b);
++}
++
++// ------------------------------ InterleaveUpper (UpperHalf)
++
++// All functions inside detail lack the required D parameter.
++namespace detail {
++
+ template <size_t N>
+-HWY_API Vec128<uint32_t, N / 2> ZipUpper(const Vec128<uint16_t, N> a,
+-                                         const Vec128<uint16_t, N> b) {
+-  return Vec128<uint32_t, N / 2>{
+-      wasm_v16x8_shuffle(a.raw, b.raw, 4, 12, 5, 13, 6, 14, 7, 15)};
++HWY_API Vec128<uint8_t, N> InterleaveUpper(Vec128<uint8_t, N> a,
++                                           Vec128<uint8_t, N> b) {
++  return Vec128<uint8_t, N>{wasm_i8x16_shuffle(a.raw, b.raw, 8, 24, 9, 25, 10,
++                                               26, 11, 27, 12, 28, 13, 29, 14,
++                                               30, 15, 31)};
++}
++template <size_t N>
++HWY_API Vec128<uint16_t, N> InterleaveUpper(Vec128<uint16_t, N> a,
++                                            Vec128<uint16_t, N> b) {
++  return Vec128<uint16_t, N>{
++      wasm_i16x8_shuffle(a.raw, b.raw, 4, 12, 5, 13, 6, 14, 7, 15)};
++}
++template <size_t N>
++HWY_API Vec128<uint32_t, N> InterleaveUpper(Vec128<uint32_t, N> a,
++                                            Vec128<uint32_t, N> b) {
++  return Vec128<uint32_t, N>{wasm_i32x4_shuffle(a.raw, b.raw, 2, 6, 3, 7)};
++}
++template <size_t N>
++HWY_API Vec128<uint64_t, N> InterleaveUpper(Vec128<uint64_t, N> a,
++                                            Vec128<uint64_t, N> b) {
++  return Vec128<uint64_t, N>{wasm_i64x2_shuffle(a.raw, b.raw, 1, 3)};
+ }
+ 
+ template <size_t N>
+-HWY_API Vec128<int16_t, N / 2> ZipUpper(const Vec128<int8_t, N> a,
+-                                        const Vec128<int8_t, N> b) {
+-  return Vec128<int16_t, N / 2>{wasm_v8x16_shuffle(a.raw, b.raw, 8, 24, 9, 25,
+-                                                   10, 26, 11, 27, 12, 28, 13,
+-                                                   29, 14, 30, 15, 31)};
++HWY_API Vec128<int8_t, N> InterleaveUpper(Vec128<int8_t, N> a,
++                                          Vec128<int8_t, N> b) {
++  return Vec128<int8_t, N>{wasm_i8x16_shuffle(a.raw, b.raw, 8, 24, 9, 25, 10,
++                                              26, 11, 27, 12, 28, 13, 29, 14,
++                                              30, 15, 31)};
+ }
+ template <size_t N>
+-HWY_API Vec128<int32_t, N / 2> ZipUpper(const Vec128<int16_t, N> a,
+-                                        const Vec128<int16_t, N> b) {
+-  return Vec128<int32_t, N / 2>{
+-      wasm_v16x8_shuffle(a.raw, b.raw, 4, 12, 5, 13, 6, 14, 7, 15)};
++HWY_API Vec128<int16_t, N> InterleaveUpper(Vec128<int16_t, N> a,
++                                           Vec128<int16_t, N> b) {
++  return Vec128<int16_t, N>{
++      wasm_i16x8_shuffle(a.raw, b.raw, 4, 12, 5, 13, 6, 14, 7, 15)};
++}
++template <size_t N>
++HWY_API Vec128<int32_t, N> InterleaveUpper(Vec128<int32_t, N> a,
++                                           Vec128<int32_t, N> b) {
++  return Vec128<int32_t, N>{wasm_i32x4_shuffle(a.raw, b.raw, 2, 6, 3, 7)};
++}
++template <size_t N>
++HWY_API Vec128<int64_t, N> InterleaveUpper(Vec128<int64_t, N> a,
++                                           Vec128<int64_t, N> b) {
++  return Vec128<int64_t, N>{wasm_i64x2_shuffle(a.raw, b.raw, 1, 3)};
+ }
+ 
+-// ------------------------------ Interleave lanes
++template <size_t N>
++HWY_API Vec128<float, N> InterleaveUpper(Vec128<float, N> a,
++                                         Vec128<float, N> b) {
++  return Vec128<float, N>{wasm_i32x4_shuffle(a.raw, b.raw, 2, 6, 3, 7)};
++}
+ 
+-// Interleaves lanes from halves of the 128-bit blocks of "a" (which provides
+-// the least-significant lane) and "b". To concatenate two half-width integers
+-// into one, use ZipLower/Upper instead (also works with scalar).
++}  // namespace detail
+ 
+-template <typename T>
+-HWY_API Vec128<T> InterleaveLower(const Vec128<T> a, const Vec128<T> b) {
+-  return Vec128<T>{ZipLower(a, b).raw};
++// Full
++template <typename T, class V = Vec128<T>>
++HWY_API V InterleaveUpper(Full128<T> /* tag */, V a, V b) {
++  return detail::InterleaveUpper(a, b);
+ }
+-template <>
+-HWY_INLINE Vec128<uint32_t> InterleaveLower<uint32_t>(
+-    const Vec128<uint32_t> a, const Vec128<uint32_t> b) {
+-  return Vec128<uint32_t>{wasm_v32x4_shuffle(a.raw, b.raw, 0, 4, 1, 5)};
++
++// Partial
++template <typename T, size_t N, HWY_IF_LE64(T, N), class V = Vec128<T, N>>
++HWY_API V InterleaveUpper(Simd<T, N> d, V a, V b) {
++  const Half<decltype(d)> d2;
++  return InterleaveLower(d, V{UpperHalf(d2, a).raw}, V{UpperHalf(d2, b).raw});
+ }
+-template <>
+-HWY_INLINE Vec128<int32_t> InterleaveLower<int32_t>(const Vec128<int32_t> a,
+-                                                    const Vec128<int32_t> b) {
+-  return Vec128<int32_t>{wasm_v32x4_shuffle(a.raw, b.raw, 0, 4, 1, 5)};
++
++// ------------------------------ ZipLower/ZipUpper (InterleaveLower)
++
++// Same as Interleave*, except that the return lanes are double-width integers;
++// this is necessary because the single-lane scalar cannot return two values.
++template <typename T, size_t N, class DW = RepartitionToWide<Simd<T, N>>>
++HWY_API VFromD<DW> ZipLower(Vec128<T, N> a, Vec128<T, N> b) {
++  return BitCast(DW(), InterleaveLower(a, b));
+ }
+-template <>
+-HWY_INLINE Vec128<float> InterleaveLower<float>(const Vec128<float> a,
+-                                                const Vec128<float> b) {
+-  return Vec128<float>{wasm_v32x4_shuffle(a.raw, b.raw, 0, 4, 1, 5)};
++template <typename T, size_t N, class D = Simd<T, N>,
++          class DW = RepartitionToWide<D>>
++HWY_API VFromD<DW> ZipLower(DW dw, Vec128<T, N> a, Vec128<T, N> b) {
++  return BitCast(dw, InterleaveLower(D(), a, b));
+ }
+ 
+-template <typename T>
+-HWY_API Vec128<T> InterleaveUpper(const Vec128<T> a, const Vec128<T> b) {
+-  return Vec128<T>{ZipUpper(a, b).raw};
++template <typename T, size_t N, class D = Simd<T, N>,
++          class DW = RepartitionToWide<D>>
++HWY_API VFromD<DW> ZipUpper(DW dw, Vec128<T, N> a, Vec128<T, N> b) {
++  return BitCast(dw, InterleaveUpper(D(), a, b));
+ }
+-template <>
+-HWY_INLINE Vec128<uint32_t> InterleaveUpper<uint32_t>(
+-    const Vec128<uint32_t> a, const Vec128<uint32_t> b) {
+-  return Vec128<uint32_t>{wasm_v32x4_shuffle(a.raw, b.raw, 2, 6, 3, 7)};
+-}
+-template <>
+-HWY_INLINE Vec128<int32_t> InterleaveUpper<int32_t>(const Vec128<int32_t> a,
+-                                                    const Vec128<int32_t> b) {
+-  return Vec128<int32_t>{wasm_v32x4_shuffle(a.raw, b.raw, 2, 6, 3, 7)};
++
++// ================================================== COMBINE
++
++// ------------------------------ Combine (InterleaveLower)
++
++// N = N/2 + N/2 (upper half undefined)
++template <typename T, size_t N>
++HWY_API Vec128<T, N> Combine(Simd<T, N> d, Vec128<T, N / 2> hi_half,
++                             Vec128<T, N / 2> lo_half) {
++  const Half<decltype(d)> d2;
++  const RebindToUnsigned<decltype(d2)> du2;
++  // Treat half-width input as one lane, and expand to two lanes.
++  using VU = Vec128<UnsignedFromSize<N * sizeof(T) / 2>, 2>;
++  const VU lo{BitCast(du2, lo_half).raw};
++  const VU hi{BitCast(du2, hi_half).raw};
++  return BitCast(d, InterleaveLower(lo, hi));
+ }
+-template <>
+-HWY_INLINE Vec128<float> InterleaveUpper<float>(const Vec128<float> a,
+-                                                const Vec128<float> b) {
+-  return Vec128<float>{wasm_v32x4_shuffle(a.raw, b.raw, 2, 6, 3, 7)};
++
++// ------------------------------ ZeroExtendVector (Combine, IfThenElseZero)
++
++template <typename T, size_t N>
++HWY_API Vec128<T, N> ZeroExtendVector(Simd<T, N> d, Vec128<T, N / 2> lo) {
++  return IfThenElseZero(FirstN(d, N / 2), Vec128<T, N>{lo.raw});
+ }
+ 
+-// ------------------------------ Blocks
++// ------------------------------ ConcatLowerLower
+ 
+ // hiH,hiL loH,loL |-> hiL,loL (= lower halves)
+ template <typename T>
+-HWY_API Vec128<T> ConcatLowerLower(const Vec128<T> hi, const Vec128<T> lo) {
+-  return Vec128<T>{wasm_v64x2_shuffle(lo.raw, hi.raw, 0, 2)};
++HWY_API Vec128<T> ConcatLowerLower(Full128<T> /* tag */, const Vec128<T> hi,
++                                   const Vec128<T> lo) {
++  return Vec128<T>{wasm_i64x2_shuffle(lo.raw, hi.raw, 0, 2)};
++}
++template <typename T, size_t N, HWY_IF_LE64(T, N)>
++HWY_API Vec128<T, N> ConcatLowerLower(Simd<T, N> d, const Vec128<T, N> hi,
++                                      const Vec128<T, N> lo) {
++  const Half<decltype(d)> d2;
++  return Combine(LowerHalf(d2, hi), LowerHalf(d2, lo));
+ }
+ 
+-// hiH,hiL loH,loL |-> hiH,loH (= upper halves)
++// ------------------------------ ConcatUpperUpper
++
+ template <typename T>
+-HWY_API Vec128<T> ConcatUpperUpper(const Vec128<T> hi, const Vec128<T> lo) {
+-  return Vec128<T>{wasm_v64x2_shuffle(lo.raw, hi.raw, 1, 3)};
++HWY_API Vec128<T> ConcatUpperUpper(Full128<T> /* tag */, const Vec128<T> hi,
++                                   const Vec128<T> lo) {
++  return Vec128<T>{wasm_i64x2_shuffle(lo.raw, hi.raw, 1, 3)};
++}
++template <typename T, size_t N, HWY_IF_LE64(T, N)>
++HWY_API Vec128<T, N> ConcatUpperUpper(Simd<T, N> d, const Vec128<T, N> hi,
++                                      const Vec128<T, N> lo) {
++  const Half<decltype(d)> d2;
++  return Combine(UpperHalf(d2, hi), UpperHalf(d2, lo));
+ }
+ 
+-// hiH,hiL loH,loL |-> hiL,loH (= inner halves)
++// ------------------------------ ConcatLowerUpper
++
+ template <typename T>
+-HWY_API Vec128<T> ConcatLowerUpper(const Vec128<T> hi, const Vec128<T> lo) {
+-  return CombineShiftRightBytes<8>(hi, lo);
++HWY_API Vec128<T> ConcatLowerUpper(Full128<T> d, const Vec128<T> hi,
++                                   const Vec128<T> lo) {
++  return CombineShiftRightBytes<8>(d, hi, lo);
++}
++template <typename T, size_t N, HWY_IF_LE64(T, N)>
++HWY_API Vec128<T, N> ConcatLowerUpper(Simd<T, N> d, const Vec128<T, N> hi,
++                                      const Vec128<T, N> lo) {
++  const Half<decltype(d)> d2;
++  return Combine(LowerHalf(d2, hi), UpperHalf(d2, lo));
+ }
+ 
+-// hiH,hiL loH,loL |-> hiH,loL (= outer halves)
+-template <typename T>
+-HWY_API Vec128<T> ConcatUpperLower(const Vec128<T> hi, const Vec128<T> lo) {
+-  return Vec128<T>{wasm_v64x2_shuffle(lo.raw, hi.raw, 0, 3)};
++// ------------------------------ ConcatUpperLower
++template <typename T, size_t N>
++HWY_API Vec128<T, N> ConcatUpperLower(Simd<T, N> d, const Vec128<T, N> hi,
++                                      const Vec128<T, N> lo) {
++  return IfThenElse(FirstN(d, Lanes(d) / 2), lo, hi);
+ }
+ 
+-// ------------------------------ Odd/even lanes
++// ------------------------------ OddEven
+ 
+-namespace {
++namespace detail {
+ 
+-template <typename T>
+-HWY_API Vec128<T> odd_even_impl(hwy::SizeTag<1> /* tag */, const Vec128<T> a,
+-                                const Vec128<T> b) {
+-  const Full128<T> d;
+-  const Full128<uint8_t> d8;
++template <typename T, size_t N>
++HWY_INLINE Vec128<T, N> OddEven(hwy::SizeTag<1> /* tag */, const Vec128<T, N> a,
++                                const Vec128<T, N> b) {
++  const Simd<T, N> d;
++  const Repartition<uint8_t, decltype(d)> d8;
+   alignas(16) constexpr uint8_t mask[16] = {0xFF, 0, 0xFF, 0, 0xFF, 0, 0xFF, 0,
+                                             0xFF, 0, 0xFF, 0, 0xFF, 0, 0xFF, 0};
+   return IfThenElse(MaskFromVec(BitCast(d, Load(d8, mask))), b, a);
+ }
+-template <typename T>
+-HWY_API Vec128<T> odd_even_impl(hwy::SizeTag<2> /* tag */, const Vec128<T> a,
+-                                const Vec128<T> b) {
+-  return Vec128<T>{wasm_v16x8_shuffle(a.raw, b.raw, 8, 1, 10, 3, 12, 5, 14, 7)};
++template <typename T, size_t N>
++HWY_INLINE Vec128<T, N> OddEven(hwy::SizeTag<2> /* tag */, const Vec128<T, N> a,
++                                const Vec128<T, N> b) {
++  return Vec128<T, N>{
++      wasm_i16x8_shuffle(a.raw, b.raw, 8, 1, 10, 3, 12, 5, 14, 7)};
+ }
+-template <typename T>
+-HWY_API Vec128<T> odd_even_impl(hwy::SizeTag<4> /* tag */, const Vec128<T> a,
+-                                const Vec128<T> b) {
+-  return Vec128<T>{wasm_v32x4_shuffle(a.raw, b.raw, 4, 1, 6, 3)};
++template <typename T, size_t N>
++HWY_INLINE Vec128<T, N> OddEven(hwy::SizeTag<4> /* tag */, const Vec128<T, N> a,
++                                const Vec128<T, N> b) {
++  return Vec128<T, N>{wasm_i32x4_shuffle(a.raw, b.raw, 4, 1, 6, 3)};
++}
++template <typename T, size_t N>
++HWY_INLINE Vec128<T, N> OddEven(hwy::SizeTag<8> /* tag */, const Vec128<T, N> a,
++                                const Vec128<T, N> b) {
++  return Vec128<T, N>{wasm_i64x2_shuffle(a.raw, b.raw, 2, 1)};
+ }
+-// TODO(eustas): implement
+-// template <typename T>
+-// HWY_API Vec128<T> odd_even_impl(hwy::SizeTag<8> /* tag */,
+-//                                                 const Vec128<T> a,
+-//                                                 const Vec128<T> b)
+ 
+-}  // namespace
++}  // namespace detail
+ 
+-template <typename T>
+-HWY_API Vec128<T> OddEven(const Vec128<T> a, const Vec128<T> b) {
+-  return odd_even_impl(hwy::SizeTag<sizeof(T)>(), a, b);
++template <typename T, size_t N>
++HWY_API Vec128<T, N> OddEven(const Vec128<T, N> a, const Vec128<T, N> b) {
++  return detail::OddEven(hwy::SizeTag<sizeof(T)>(), a, b);
+ }
+-template <>
+-HWY_INLINE Vec128<float> OddEven<float>(const Vec128<float> a,
+-                                        const Vec128<float> b) {
+-  return Vec128<float>{wasm_v32x4_shuffle(a.raw, b.raw, 4, 1, 6, 3)};
++template <size_t N>
++HWY_API Vec128<float, N> OddEven(const Vec128<float, N> a,
++                                 const Vec128<float, N> b) {
++  return Vec128<float, N>{wasm_i32x4_shuffle(a.raw, b.raw, 4, 1, 6, 3)};
+ }
+ 
+ // ================================================== CONVERT
+@@ -2108,69 +2304,63 @@ HWY_INLINE Vec128<float> OddEven<float>(const Vec128<float> a,
+ template <size_t N>
+ HWY_API Vec128<uint16_t, N> PromoteTo(Simd<uint16_t, N> /* tag */,
+                                       const Vec128<uint8_t, N> v) {
+-  return Vec128<uint16_t, N>{wasm_i16x8_widen_low_u8x16(v.raw)};
++  return Vec128<uint16_t, N>{wasm_u16x8_extend_low_u8x16(v.raw)};
+ }
+ template <size_t N>
+ HWY_API Vec128<uint32_t, N> PromoteTo(Simd<uint32_t, N> /* tag */,
+                                       const Vec128<uint8_t, N> v) {
+   return Vec128<uint32_t, N>{
+-      wasm_i32x4_widen_low_u16x8(wasm_i16x8_widen_low_u8x16(v.raw))};
++      wasm_u32x4_extend_low_u16x8(wasm_u16x8_extend_low_u8x16(v.raw))};
+ }
+ template <size_t N>
+ HWY_API Vec128<int16_t, N> PromoteTo(Simd<int16_t, N> /* tag */,
+                                      const Vec128<uint8_t, N> v) {
+-  return Vec128<int16_t, N>{wasm_i16x8_widen_low_u8x16(v.raw)};
++  return Vec128<int16_t, N>{wasm_u16x8_extend_low_u8x16(v.raw)};
+ }
+ template <size_t N>
+ HWY_API Vec128<int32_t, N> PromoteTo(Simd<int32_t, N> /* tag */,
+                                      const Vec128<uint8_t, N> v) {
+   return Vec128<int32_t, N>{
+-      wasm_i32x4_widen_low_u16x8(wasm_i16x8_widen_low_u8x16(v.raw))};
++      wasm_u32x4_extend_low_u16x8(wasm_u16x8_extend_low_u8x16(v.raw))};
+ }
+ template <size_t N>
+ HWY_API Vec128<uint32_t, N> PromoteTo(Simd<uint32_t, N> /* tag */,
+                                       const Vec128<uint16_t, N> v) {
+-  return Vec128<uint32_t, N>{wasm_i32x4_widen_low_u16x8(v.raw)};
++  return Vec128<uint32_t, N>{wasm_u32x4_extend_low_u16x8(v.raw)};
+ }
+ template <size_t N>
+ HWY_API Vec128<int32_t, N> PromoteTo(Simd<int32_t, N> /* tag */,
+                                      const Vec128<uint16_t, N> v) {
+-  return Vec128<int32_t, N>{wasm_i32x4_widen_low_u16x8(v.raw)};
++  return Vec128<int32_t, N>{wasm_u32x4_extend_low_u16x8(v.raw)};
+ }
+ 
+ // Signed: replicate sign bit.
+ template <size_t N>
+ HWY_API Vec128<int16_t, N> PromoteTo(Simd<int16_t, N> /* tag */,
+                                      const Vec128<int8_t, N> v) {
+-  return Vec128<int16_t, N>{wasm_i16x8_widen_low_i8x16(v.raw)};
++  return Vec128<int16_t, N>{wasm_i16x8_extend_low_i8x16(v.raw)};
+ }
+ template <size_t N>
+ HWY_API Vec128<int32_t, N> PromoteTo(Simd<int32_t, N> /* tag */,
+                                      const Vec128<int8_t, N> v) {
+   return Vec128<int32_t, N>{
+-      wasm_i32x4_widen_low_i16x8(wasm_i16x8_widen_low_i8x16(v.raw))};
++      wasm_i32x4_extend_low_i16x8(wasm_i16x8_extend_low_i8x16(v.raw))};
+ }
+ template <size_t N>
+ HWY_API Vec128<int32_t, N> PromoteTo(Simd<int32_t, N> /* tag */,
+                                      const Vec128<int16_t, N> v) {
+-  return Vec128<int32_t, N>{wasm_i32x4_widen_low_i16x8(v.raw)};
++  return Vec128<int32_t, N>{wasm_i32x4_extend_low_i16x8(v.raw)};
+ }
+ 
+ template <size_t N>
+-HWY_API Vec128<double, N> PromoteTo(Simd<double, N> df,
++HWY_API Vec128<double, N> PromoteTo(Simd<double, N> /* tag */,
+                                     const Vec128<int32_t, N> v) {
+-  // TODO(janwas): use https://github.com/WebAssembly/simd/pull/383
+-  alignas(16) int32_t lanes[4];
+-  Store(v, Simd<int32_t, N>(), lanes);
+-  alignas(16) double lanes64[2];
+-  lanes64[0] = lanes[0];
+-  lanes64[1] = N >= 2 ? lanes[1] : 0.0;
+-  return Load(df, lanes64);
++  return Vec128<double, N>{wasm_f64x2_convert_low_i32x4(v.raw)};
+ }
+ 
+ template <size_t N>
+-HWY_INLINE Vec128<float, N> PromoteTo(Simd<float, N> /* tag */,
+-                                      const Vec128<float16_t, N> v) {
++HWY_API Vec128<float, N> PromoteTo(Simd<float, N> /* tag */,
++                                   const Vec128<float16_t, N> v) {
+   const Simd<int32_t, N> di32;
+   const Simd<uint32_t, N> du32;
+   const Simd<float, N> df32;
+@@ -2232,19 +2422,14 @@ HWY_API Vec128<int8_t, N> DemoteTo(Simd<int8_t, N> /* tag */,
+ }
+ 
+ template <size_t N>
+-HWY_API Vec128<int32_t, N> DemoteTo(Simd<int32_t, N> di,
++HWY_API Vec128<int32_t, N> DemoteTo(Simd<int32_t, N> /* di */,
+                                     const Vec128<double, N> v) {
+-  // TODO(janwas): use https://github.com/WebAssembly/simd/pull/383
+-  alignas(16) double lanes64[2];
+-  Store(v, Simd<double, N>(), lanes64);
+-  alignas(16) int32_t lanes[4] = {static_cast<int32_t>(lanes64[0])};
+-  if (N >= 2) lanes[1] = static_cast<int32_t>(lanes64[1]);
+-  return Load(di, lanes);
++  return Vec128<int32_t, N>{wasm_i32x4_trunc_sat_f64x2_zero(v.raw)};
+ }
+ 
+ template <size_t N>
+-HWY_INLINE Vec128<float16_t, N> DemoteTo(Simd<float16_t, N> /* tag */,
+-                                         const Vec128<float, N> v) {
++HWY_API Vec128<float16_t, N> DemoteTo(Simd<float16_t, N> /* tag */,
++                                      const Vec128<float, N> v) {
+   const Simd<int32_t, N> di;
+   const Simd<uint32_t, N> du;
+   const Simd<uint16_t, N> du16;
+@@ -2291,7 +2476,7 @@ HWY_API Vec128<float, N> ConvertTo(Simd<float, N> /* tag */,
+ template <size_t N>
+ HWY_API Vec128<int32_t, N> ConvertTo(Simd<int32_t, N> /* tag */,
+                                      const Vec128<float, N> v) {
+-  return Vec128<int32_t, N>{wasm_i32x4_trunc_saturate_f32x4(v.raw)};
++  return Vec128<int32_t, N>{wasm_i32x4_trunc_sat_f32x4(v.raw)};
+ }
+ 
+ template <size_t N>
+@@ -2305,9 +2490,10 @@ HWY_API Vec128<int32_t, N> NearestInt(const Vec128<float, N> v) {
+ 
+ namespace detail {
+ 
+-template <typename T, size_t N>
+-HWY_API uint64_t BitsFromMask(hwy::SizeTag<1> /*tag*/,
+-                              const Mask128<T, N> mask) {
++// Full
++template <typename T>
++HWY_INLINE uint64_t BitsFromMask(hwy::SizeTag<1> /*tag*/,
++                                 const Mask128<T> mask) {
+   alignas(16) uint64_t lanes[2];
+   wasm_v128_store(lanes, mask.raw);
+ 
+@@ -2317,18 +2503,37 @@ HWY_API uint64_t BitsFromMask(hwy::SizeTag<1> /*tag*/,
+   return (hi + lo);
+ }
+ 
++// 64-bit
++template <typename T>
++HWY_INLINE uint64_t BitsFromMask(hwy::SizeTag<1> /*tag*/,
++                                 const Mask128<T, 8> mask) {
++  constexpr uint64_t kMagic = 0x103070F1F3F80ULL;
++  return (wasm_i64x2_extract_lane(mask.raw, 0) * kMagic) >> 56;
++}
++
++// 32-bit or less: need masking
++template <typename T, size_t N, HWY_IF_LE32(T, N)>
++HWY_INLINE uint64_t BitsFromMask(hwy::SizeTag<1> /*tag*/,
++                                 const Mask128<T, N> mask) {
++  uint64_t bytes = wasm_i64x2_extract_lane(mask.raw, 0);
++  // Clear potentially undefined bytes.
++  bytes &= (1ULL << (N * 8)) - 1;
++  constexpr uint64_t kMagic = 0x103070F1F3F80ULL;
++  return (bytes * kMagic) >> 56;
++}
++
+ template <typename T, size_t N>
+-HWY_API uint64_t BitsFromMask(hwy::SizeTag<2> /*tag*/,
+-                              const Mask128<T, N> mask) {
++HWY_INLINE uint64_t BitsFromMask(hwy::SizeTag<2> /*tag*/,
++                                 const Mask128<T, N> mask) {
+   // Remove useless lower half of each u16 while preserving the sign bit.
+   const __i16x8 zero = wasm_i16x8_splat(0);
+-  const Mask128<T> mask8{wasm_i8x16_narrow_i16x8(mask.raw, zero)};
++  const Mask128<uint8_t, N> mask8{wasm_i8x16_narrow_i16x8(mask.raw, zero)};
+   return BitsFromMask(hwy::SizeTag<1>(), mask8);
+ }
+ 
+ template <typename T, size_t N>
+-HWY_API uint64_t BitsFromMask(hwy::SizeTag<4> /*tag*/,
+-                              const Mask128<T, N> mask) {
++HWY_INLINE uint64_t BitsFromMask(hwy::SizeTag<4> /*tag*/,
++                                 const Mask128<T, N> mask) {
+   const __i32x4 mask_i = static_cast<__i32x4>(mask.raw);
+   const __i32x4 slice = wasm_i32x4_make(1, 2, 4, 8);
+   const __i32x4 sliced_mask = wasm_v128_and(mask_i, slice);
+@@ -2374,22 +2579,22 @@ constexpr __i8x16 BytesAbove() {
+ }
+ 
+ template <typename T, size_t N>
+-HWY_API uint64_t BitsFromMask(const Mask128<T, N> mask) {
++HWY_INLINE uint64_t BitsFromMask(const Mask128<T, N> mask) {
+   return OnlyActive<T, N>(BitsFromMask(hwy::SizeTag<sizeof(T)>(), mask));
+ }
+ 
+ template <typename T>
+-HWY_API size_t CountTrue(hwy::SizeTag<1> tag, const Mask128<T> m) {
++HWY_INLINE size_t CountTrue(hwy::SizeTag<1> tag, const Mask128<T> m) {
+   return PopCount(BitsFromMask(tag, m));
+ }
+ 
+ template <typename T>
+-HWY_API size_t CountTrue(hwy::SizeTag<2> tag, const Mask128<T> m) {
++HWY_INLINE size_t CountTrue(hwy::SizeTag<2> tag, const Mask128<T> m) {
+   return PopCount(BitsFromMask(tag, m));
+ }
+ 
+ template <typename T>
+-HWY_API size_t CountTrue(hwy::SizeTag<4> /*tag*/, const Mask128<T> m) {
++HWY_INLINE size_t CountTrue(hwy::SizeTag<4> /*tag*/, const Mask128<T> m) {
+   const __i32x4 var_shift = wasm_i32x4_make(1, 2, 4, 8);
+   const __i32x4 shifted_bits = wasm_v128_and(m.raw, var_shift);
+   alignas(16) uint64_t lanes[2];
+@@ -2400,76 +2605,85 @@ HWY_API size_t CountTrue(hwy::SizeTag<4> /*tag*/, const Mask128<T> m) {
+ }  // namespace detail
+ 
+ template <typename T, size_t N>
+-HWY_INLINE size_t StoreMaskBits(const Mask128<T, N> mask, uint8_t* p) {
++HWY_API size_t StoreMaskBits(const Simd<T, N> /* tag */,
++                             const Mask128<T, N> mask, uint8_t* p) {
+   const uint64_t bits = detail::BitsFromMask(mask);
+   const size_t kNumBytes = (N + 7) / 8;
+   CopyBytes<kNumBytes>(&bits, p);
+   return kNumBytes;
+ }
+ 
+-template <typename T>
+-HWY_API size_t CountTrue(const Mask128<T> m) {
++template <typename T, size_t N>
++HWY_API size_t CountTrue(const Simd<T, N> /* tag */, const Mask128<T> m) {
+   return detail::CountTrue(hwy::SizeTag<sizeof(T)>(), m);
+ }
+ 
+ // Partial vector
+ template <typename T, size_t N, HWY_IF_LE64(T, N)>
+-HWY_API size_t CountTrue(const Mask128<T, N> m) {
++HWY_API size_t CountTrue(const Simd<T, N> d, const Mask128<T, N> m) {
+   // Ensure all undefined bytes are 0.
+   const Mask128<T, N> mask{detail::BytesAbove<N * sizeof(T)>()};
+-  return CountTrue(Mask128<T>{AndNot(mask, m).raw});
++  return CountTrue(d, Mask128<T>{AndNot(mask, m).raw});
+ }
+ 
+-// Full vector, type-independent
++// Full vector
+ template <typename T>
+-HWY_API bool AllFalse(const Mask128<T> m) {
++HWY_API bool AllFalse(const Full128<T> d, const Mask128<T> m) {
+ #if 0
+   // Casting followed by wasm_i8x16_any_true results in wasm error:
+   // i32.eqz[0] expected type i32, found i8x16.popcnt of type s128
+-  const auto v8 = BitCast(Full128<int8_t>(), VecFromMask(Full128<T>(), m));
++  const auto v8 = BitCast(Full128<int8_t>(), VecFromMask(d, m));
+   return !wasm_i8x16_any_true(v8.raw);
+ #else
++  (void)d;
+   return (wasm_i64x2_extract_lane(m.raw, 0) |
+           wasm_i64x2_extract_lane(m.raw, 1)) == 0;
+ #endif
+ }
+ 
+-// Full vector, type-dependent
++// Full vector
+ namespace detail {
+ template <typename T>
+-HWY_API bool AllTrue(hwy::SizeTag<1> /*tag*/, const Mask128<T> m) {
++HWY_INLINE bool AllTrue(hwy::SizeTag<1> /*tag*/, const Mask128<T> m) {
+   return wasm_i8x16_all_true(m.raw);
+ }
+ template <typename T>
+-HWY_API bool AllTrue(hwy::SizeTag<2> /*tag*/, const Mask128<T> m) {
++HWY_INLINE bool AllTrue(hwy::SizeTag<2> /*tag*/, const Mask128<T> m) {
+   return wasm_i16x8_all_true(m.raw);
+ }
+ template <typename T>
+-HWY_API bool AllTrue(hwy::SizeTag<4> /*tag*/, const Mask128<T> m) {
++HWY_INLINE bool AllTrue(hwy::SizeTag<4> /*tag*/, const Mask128<T> m) {
+   return wasm_i32x4_all_true(m.raw);
+ }
+ 
+ }  // namespace detail
+ 
+-template <typename T>
+-HWY_API bool AllTrue(const Mask128<T> m) {
++template <typename T, size_t N>
++HWY_API bool AllTrue(const Simd<T, N> /* tag */, const Mask128<T> m) {
+   return detail::AllTrue(hwy::SizeTag<sizeof(T)>(), m);
+ }
+ 
+ // Partial vectors
+ 
+ template <typename T, size_t N, HWY_IF_LE64(T, N)>
+-HWY_API bool AllFalse(const Mask128<T, N> m) {
++HWY_API bool AllFalse(Simd<T, N> /* tag */, const Mask128<T, N> m) {
+   // Ensure all undefined bytes are 0.
+   const Mask128<T, N> mask{detail::BytesAbove<N * sizeof(T)>()};
+   return AllFalse(Mask128<T>{AndNot(mask, m).raw});
+ }
+ 
+ template <typename T, size_t N, HWY_IF_LE64(T, N)>
+-HWY_API bool AllTrue(const Mask128<T, N> m) {
++HWY_API bool AllTrue(const Simd<T, N> d, const Mask128<T, N> m) {
+   // Ensure all undefined bytes are FF.
+   const Mask128<T, N> mask{detail::BytesAbove<N * sizeof(T)>()};
+-  return AllTrue(Mask128<T>{Or(mask, m).raw});
++  return AllTrue(d, Mask128<T>{Or(mask, m).raw});
++}
++
++template <typename T, size_t N>
++HWY_API intptr_t FindFirstTrue(const Simd<T, N> /* tag */,
++                               const Mask128<T, N> mask) {
++  const uint64_t bits = detail::BitsFromMask(mask);
++  return bits ? Num0BitsBelowLS1Bit_Nonzero64(bits) : -1;
+ }
+ 
+ // ------------------------------ Compress
+@@ -2661,8 +2875,8 @@ HWY_INLINE Vec128<T, N> Idx64x2FromBits(const uint64_t mask_bits) {
+ // redundant BitsFromMask in the latter.
+ 
+ template <typename T, size_t N>
+-HWY_API Vec128<T, N> Compress(hwy::SizeTag<2> /*tag*/, Vec128<T, N> v,
+-                              const uint64_t mask_bits) {
++HWY_INLINE Vec128<T, N> Compress(hwy::SizeTag<2> /*tag*/, Vec128<T, N> v,
++                                 const uint64_t mask_bits) {
+   const auto idx = detail::Idx16x8FromBits<T, N>(mask_bits);
+   using D = Simd<T, N>;
+   const RebindToSigned<D> di;
+@@ -2670,8 +2884,8 @@ HWY_API Vec128<T, N> Compress(hwy::SizeTag<2> /*tag*/, Vec128<T, N> v,
+ }
+ 
+ template <typename T, size_t N>
+-HWY_API Vec128<T, N> Compress(hwy::SizeTag<4> /*tag*/, Vec128<T, N> v,
+-                              const uint64_t mask_bits) {
++HWY_INLINE Vec128<T, N> Compress(hwy::SizeTag<4> /*tag*/, Vec128<T, N> v,
++                                 const uint64_t mask_bits) {
+   const auto idx = detail::Idx32x4FromBits<T, N>(mask_bits);
+   using D = Simd<T, N>;
+   const RebindToSigned<D> di;
+@@ -2681,9 +2895,9 @@ HWY_API Vec128<T, N> Compress(hwy::SizeTag<4> /*tag*/, Vec128<T, N> v,
+ #if HWY_CAP_INTEGER64 || HWY_CAP_FLOAT64
+ 
+ template <typename T, size_t N>
+-HWY_API Vec128<uint64_t, N> Compress(hwy::SizeTag<8> /*tag*/,
+-                                     Vec128<uint64_t, N> v,
+-                                     const uint64_t mask_bits) {
++HWY_INLINE Vec128<uint64_t, N> Compress(hwy::SizeTag<8> /*tag*/,
++                                        Vec128<uint64_t, N> v,
++                                        const uint64_t mask_bits) {
+   const auto idx = detail::Idx64x2FromBits<uint64_t, N>(mask_bits);
+   using D = Simd<T, N>;
+   const RebindToSigned<D> di;
+@@ -2730,7 +2944,7 @@ HWY_API void StoreInterleaved3(const Vec128<uint8_t> a, const Vec128<uint8_t> b,
+       0x80, 2, 0x80, 0x80, 3, 0x80, 0x80, 4, 0x80, 0x80};
+   const auto shuf_r0 = Load(d, tbl_r0);
+   const auto shuf_g0 = Load(d, tbl_g0);  // cannot reuse r0 due to 5 in MSB
+-  const auto shuf_b0 = CombineShiftRightBytes<15>(shuf_g0, shuf_g0);
++  const auto shuf_b0 = CombineShiftRightBytes<15>(d, shuf_g0, shuf_g0);
+   const auto r0 = TableLookupBytes(a, shuf_r0);  // 5..4..3..2..1..0
+   const auto g0 = TableLookupBytes(b, shuf_g0);  // ..4..3..2..1..0.
+   const auto b0 = TableLookupBytes(c, shuf_b0);  // .4..3..2..1..0..
+@@ -2782,7 +2996,7 @@ HWY_API void StoreInterleaved3(const Vec128<uint8_t, 8> a,
+       0x80, 2, 0x80, 0x80, 3, 0x80, 0x80, 4, 0x80, 0x80};
+   const auto shuf_r0 = Load(d_full, tbl_r0);
+   const auto shuf_g0 = Load(d_full, tbl_g0);  // cannot reuse r0 due to 5 in MSB
+-  const auto shuf_b0 = CombineShiftRightBytes<15>(shuf_g0, shuf_g0);
++  const auto shuf_b0 = CombineShiftRightBytes<15>(d_full, shuf_g0, shuf_g0);
+   const auto r0 = TableLookupBytes(full_a, shuf_r0);  // 5..4..3..2..1..0
+   const auto g0 = TableLookupBytes(full_b, shuf_g0);  // ..4..3..2..1..0.
+   const auto b0 = TableLookupBytes(full_c, shuf_b0);  // .4..3..2..1..0..
+@@ -2820,8 +3034,8 @@ HWY_API void StoreInterleaved3(const Vec128<uint8_t, N> a,
+       0,    0x80, 0x80, 1,   0x80, 0x80, 2, 0x80, 0x80, 3, 0x80, 0x80,  //
+       0x80, 0x80, 0x80, 0x80};
+   const auto shuf_r0 = Load(d_full, tbl_r0);
+-  const auto shuf_g0 = CombineShiftRightBytes<15>(shuf_r0, shuf_r0);
+-  const auto shuf_b0 = CombineShiftRightBytes<14>(shuf_r0, shuf_r0);
++  const auto shuf_g0 = CombineShiftRightBytes<15>(d_full, shuf_r0, shuf_r0);
++  const auto shuf_b0 = CombineShiftRightBytes<14>(d_full, shuf_r0, shuf_r0);
+   const auto r0 = TableLookupBytes(full_a, shuf_r0);  // ......3..2..1..0
+   const auto g0 = TableLookupBytes(full_b, shuf_g0);  // .....3..2..1..0.
+   const auto b0 = TableLookupBytes(full_c, shuf_b0);  // ....3..2..1..0..
+@@ -2837,21 +3051,23 @@ HWY_API void StoreInterleaved3(const Vec128<uint8_t, N> a,
+ HWY_API void StoreInterleaved4(const Vec128<uint8_t> v0,
+                                const Vec128<uint8_t> v1,
+                                const Vec128<uint8_t> v2,
+-                               const Vec128<uint8_t> v3, Full128<uint8_t> d,
++                               const Vec128<uint8_t> v3, Full128<uint8_t> d8,
+                                uint8_t* HWY_RESTRICT unaligned) {
++  const RepartitionToWide<decltype(d8)> d16;
++  const RepartitionToWide<decltype(d16)> d32;
+   // let a,b,c,d denote v0..3.
+-  const auto ba0 = ZipLower(v0, v1);  // b7 a7 .. b0 a0
+-  const auto dc0 = ZipLower(v2, v3);  // d7 c7 .. d0 c0
+-  const auto ba8 = ZipUpper(v0, v1);
+-  const auto dc8 = ZipUpper(v2, v3);
+-  const auto dcba_0 = ZipLower(ba0, dc0);  // d..a3 d..a0
+-  const auto dcba_4 = ZipUpper(ba0, dc0);  // d..a7 d..a4
+-  const auto dcba_8 = ZipLower(ba8, dc8);  // d..aB d..a8
+-  const auto dcba_C = ZipUpper(ba8, dc8);  // d..aF d..aC
+-  StoreU(BitCast(d, dcba_0), d, unaligned + 0 * 16);
+-  StoreU(BitCast(d, dcba_4), d, unaligned + 1 * 16);
+-  StoreU(BitCast(d, dcba_8), d, unaligned + 2 * 16);
+-  StoreU(BitCast(d, dcba_C), d, unaligned + 3 * 16);
++  const auto ba0 = ZipLower(d16, v0, v1);  // b7 a7 .. b0 a0
++  const auto dc0 = ZipLower(d16, v2, v3);  // d7 c7 .. d0 c0
++  const auto ba8 = ZipUpper(d16, v0, v1);
++  const auto dc8 = ZipUpper(d16, v2, v3);
++  const auto dcba_0 = ZipLower(d32, ba0, dc0);  // d..a3 d..a0
++  const auto dcba_4 = ZipUpper(d32, ba0, dc0);  // d..a7 d..a4
++  const auto dcba_8 = ZipLower(d32, ba8, dc8);  // d..aB d..a8
++  const auto dcba_C = ZipUpper(d32, ba8, dc8);  // d..aF d..aC
++  StoreU(BitCast(d8, dcba_0), d8, unaligned + 0 * 16);
++  StoreU(BitCast(d8, dcba_4), d8, unaligned + 1 * 16);
++  StoreU(BitCast(d8, dcba_8), d8, unaligned + 2 * 16);
++  StoreU(BitCast(d8, dcba_C), d8, unaligned + 3 * 16);
+ }
+ 
+ // 64 bits
+@@ -2859,21 +3075,23 @@ HWY_API void StoreInterleaved4(const Vec128<uint8_t, 8> in0,
+                                const Vec128<uint8_t, 8> in1,
+                                const Vec128<uint8_t, 8> in2,
+                                const Vec128<uint8_t, 8> in3,
+-                               Simd<uint8_t, 8> /*tag*/,
++                               Simd<uint8_t, 8> /* tag */,
+                                uint8_t* HWY_RESTRICT unaligned) {
+   // Use full vectors to reduce the number of stores.
++  const Full128<uint8_t> d_full8;
++  const RepartitionToWide<decltype(d_full8)> d16;
++  const RepartitionToWide<decltype(d16)> d32;
+   const Vec128<uint8_t> v0{in0.raw};
+   const Vec128<uint8_t> v1{in1.raw};
+   const Vec128<uint8_t> v2{in2.raw};
+   const Vec128<uint8_t> v3{in3.raw};
+   // let a,b,c,d denote v0..3.
+-  const auto ba0 = ZipLower(v0, v1);       // b7 a7 .. b0 a0
+-  const auto dc0 = ZipLower(v2, v3);       // d7 c7 .. d0 c0
+-  const auto dcba_0 = ZipLower(ba0, dc0);  // d..a3 d..a0
+-  const auto dcba_4 = ZipUpper(ba0, dc0);  // d..a7 d..a4
+-  const Full128<uint8_t> d_full;
+-  StoreU(BitCast(d_full, dcba_0), d_full, unaligned + 0 * 16);
+-  StoreU(BitCast(d_full, dcba_4), d_full, unaligned + 1 * 16);
++  const auto ba0 = ZipLower(d16, v0, v1);       // b7 a7 .. b0 a0
++  const auto dc0 = ZipLower(d16, v2, v3);       // d7 c7 .. d0 c0
++  const auto dcba_0 = ZipLower(d32, ba0, dc0);  // d..a3 d..a0
++  const auto dcba_4 = ZipUpper(d32, ba0, dc0);  // d..a7 d..a4
++  StoreU(BitCast(d_full8, dcba_0), d_full8, unaligned + 0 * 16);
++  StoreU(BitCast(d_full8, dcba_4), d_full8, unaligned + 1 * 16);
+ }
+ 
+ // <= 32 bits
+@@ -2885,38 +3103,58 @@ HWY_API void StoreInterleaved4(const Vec128<uint8_t, N> in0,
+                                Simd<uint8_t, N> /*tag*/,
+                                uint8_t* HWY_RESTRICT unaligned) {
+   // Use full vectors to reduce the number of stores.
++  const Full128<uint8_t> d_full8;
++  const RepartitionToWide<decltype(d_full8)> d16;
++  const RepartitionToWide<decltype(d16)> d32;
+   const Vec128<uint8_t> v0{in0.raw};
+   const Vec128<uint8_t> v1{in1.raw};
+   const Vec128<uint8_t> v2{in2.raw};
+   const Vec128<uint8_t> v3{in3.raw};
+   // let a,b,c,d denote v0..3.
+-  const auto ba0 = ZipLower(v0, v1);       // b3 a3 .. b0 a0
+-  const auto dc0 = ZipLower(v2, v3);       // d3 c3 .. d0 c0
+-  const auto dcba_0 = ZipLower(ba0, dc0);  // d..a3 d..a0
++  const auto ba0 = ZipLower(d16, v0, v1);       // b3 a3 .. b0 a0
++  const auto dc0 = ZipLower(d16, v2, v3);       // d3 c3 .. d0 c0
++  const auto dcba_0 = ZipLower(d32, ba0, dc0);  // d..a3 d..a0
+   alignas(16) uint8_t buf[16];
+-  const Full128<uint8_t> d_full;
+-  StoreU(BitCast(d_full, dcba_0), d_full, buf);
++  StoreU(BitCast(d_full8, dcba_0), d_full8, buf);
+   CopyBytes<4 * N>(buf, unaligned);
+ }
+ 
++// ------------------------------ MulEven/Odd (Load)
++
++HWY_INLINE Vec128<uint64_t> MulEven(const Vec128<uint64_t> a,
++                                    const Vec128<uint64_t> b) {
++  alignas(16) uint64_t mul[2];
++  mul[0] = Mul128(wasm_i64x2_extract_lane(a.raw, 0),
++                  wasm_i64x2_extract_lane(b.raw, 0), &mul[1]);
++  return Load(Full128<uint64_t>(), mul);
++}
++
++HWY_INLINE Vec128<uint64_t> MulOdd(const Vec128<uint64_t> a,
++                                   const Vec128<uint64_t> b) {
++  alignas(16) uint64_t mul[2];
++  mul[0] = Mul128(wasm_i64x2_extract_lane(a.raw, 1),
++                  wasm_i64x2_extract_lane(b.raw, 1), &mul[1]);
++  return Load(Full128<uint64_t>(), mul);
++}
++
+ // ------------------------------ Reductions
+ 
+ namespace detail {
+ 
+ // N=1 for any T: no-op
+ template <typename T>
+-HWY_API Vec128<T, 1> SumOfLanes(hwy::SizeTag<sizeof(T)> /* tag */,
+-                                const Vec128<T, 1> v) {
++HWY_INLINE Vec128<T, 1> SumOfLanes(hwy::SizeTag<sizeof(T)> /* tag */,
++                                   const Vec128<T, 1> v) {
+   return v;
+ }
+ template <typename T>
+-HWY_API Vec128<T, 1> MinOfLanes(hwy::SizeTag<sizeof(T)> /* tag */,
+-                                const Vec128<T, 1> v) {
++HWY_INLINE Vec128<T, 1> MinOfLanes(hwy::SizeTag<sizeof(T)> /* tag */,
++                                   const Vec128<T, 1> v) {
+   return v;
+ }
+ template <typename T>
+-HWY_API Vec128<T, 1> MaxOfLanes(hwy::SizeTag<sizeof(T)> /* tag */,
+-                                const Vec128<T, 1> v) {
++HWY_INLINE Vec128<T, 1> MaxOfLanes(hwy::SizeTag<sizeof(T)> /* tag */,
++                                   const Vec128<T, 1> v) {
+   return v;
+ }
+ 
+@@ -2924,38 +3162,41 @@ HWY_API Vec128<T, 1> MaxOfLanes(hwy::SizeTag<sizeof(T)> /* tag */,
+ 
+ // N=2
+ template <typename T>
+-HWY_API Vec128<T, 2> SumOfLanes(hwy::SizeTag<4> /* tag */,
+-                                const Vec128<T, 2> v10) {
++HWY_INLINE Vec128<T, 2> SumOfLanes(hwy::SizeTag<4> /* tag */,
++                                   const Vec128<T, 2> v10) {
+   return v10 + Vec128<T, 2>{Shuffle2301(Vec128<T>{v10.raw}).raw};
+ }
+ template <typename T>
+-HWY_API Vec128<T, 2> MinOfLanes(hwy::SizeTag<4> /* tag */,
+-                                const Vec128<T, 2> v10) {
++HWY_INLINE Vec128<T, 2> MinOfLanes(hwy::SizeTag<4> /* tag */,
++                                   const Vec128<T, 2> v10) {
+   return Min(v10, Vec128<T, 2>{Shuffle2301(Vec128<T>{v10.raw}).raw});
+ }
+ template <typename T>
+-HWY_API Vec128<T, 2> MaxOfLanes(hwy::SizeTag<4> /* tag */,
+-                                const Vec128<T, 2> v10) {
++HWY_INLINE Vec128<T, 2> MaxOfLanes(hwy::SizeTag<4> /* tag */,
++                                   const Vec128<T, 2> v10) {
+   return Max(v10, Vec128<T, 2>{Shuffle2301(Vec128<T>{v10.raw}).raw});
+ }
+ 
+ // N=4 (full)
+ template <typename T>
+-HWY_API Vec128<T> SumOfLanes(hwy::SizeTag<4> /* tag */, const Vec128<T> v3210) {
++HWY_INLINE Vec128<T> SumOfLanes(hwy::SizeTag<4> /* tag */,
++                                const Vec128<T> v3210) {
+   const Vec128<T> v1032 = Shuffle1032(v3210);
+   const Vec128<T> v31_20_31_20 = v3210 + v1032;
+   const Vec128<T> v20_31_20_31 = Shuffle0321(v31_20_31_20);
+   return v20_31_20_31 + v31_20_31_20;
+ }
+ template <typename T>
+-HWY_API Vec128<T> MinOfLanes(hwy::SizeTag<4> /* tag */, const Vec128<T> v3210) {
++HWY_INLINE Vec128<T> MinOfLanes(hwy::SizeTag<4> /* tag */,
++                                const Vec128<T> v3210) {
+   const Vec128<T> v1032 = Shuffle1032(v3210);
+   const Vec128<T> v31_20_31_20 = Min(v3210, v1032);
+   const Vec128<T> v20_31_20_31 = Shuffle0321(v31_20_31_20);
+   return Min(v20_31_20_31, v31_20_31_20);
+ }
+ template <typename T>
+-HWY_API Vec128<T> MaxOfLanes(hwy::SizeTag<4> /* tag */, const Vec128<T> v3210) {
++HWY_INLINE Vec128<T> MaxOfLanes(hwy::SizeTag<4> /* tag */,
++                                const Vec128<T> v3210) {
+   const Vec128<T> v1032 = Shuffle1032(v3210);
+   const Vec128<T> v31_20_31_20 = Max(v3210, v1032);
+   const Vec128<T> v20_31_20_31 = Shuffle0321(v31_20_31_20);
+@@ -2966,17 +3207,20 @@ HWY_API Vec128<T> MaxOfLanes(hwy::SizeTag<4> /* tag */, const Vec128<T> v3210) {
+ 
+ // N=2 (full)
+ template <typename T>
+-HWY_API Vec128<T> SumOfLanes(hwy::SizeTag<8> /* tag */, const Vec128<T> v10) {
++HWY_INLINE Vec128<T> SumOfLanes(hwy::SizeTag<8> /* tag */,
++                                const Vec128<T> v10) {
+   const Vec128<T> v01 = Shuffle01(v10);
+   return v10 + v01;
+ }
+ template <typename T>
+-HWY_API Vec128<T> MinOfLanes(hwy::SizeTag<8> /* tag */, const Vec128<T> v10) {
++HWY_INLINE Vec128<T> MinOfLanes(hwy::SizeTag<8> /* tag */,
++                                const Vec128<T> v10) {
+   const Vec128<T> v01 = Shuffle01(v10);
+   return Min(v10, v01);
+ }
+ template <typename T>
+-HWY_API Vec128<T> MaxOfLanes(hwy::SizeTag<8> /* tag */, const Vec128<T> v10) {
++HWY_INLINE Vec128<T> MaxOfLanes(hwy::SizeTag<8> /* tag */,
++                                const Vec128<T> v10) {
+   const Vec128<T> v01 = Shuffle01(v10);
+   return Max(v10, v01);
+ }
+@@ -2985,18 +3229,114 @@ HWY_API Vec128<T> MaxOfLanes(hwy::SizeTag<8> /* tag */, const Vec128<T> v10) {
+ 
+ // Supported for u/i/f 32/64. Returns the same value in each lane.
+ template <typename T, size_t N>
+-HWY_API Vec128<T, N> SumOfLanes(const Vec128<T, N> v) {
++HWY_API Vec128<T, N> SumOfLanes(Simd<T, N> /* tag */, const Vec128<T, N> v) {
+   return detail::SumOfLanes(hwy::SizeTag<sizeof(T)>(), v);
+ }
+ template <typename T, size_t N>
+-HWY_API Vec128<T, N> MinOfLanes(const Vec128<T, N> v) {
++HWY_API Vec128<T, N> MinOfLanes(Simd<T, N> /* tag */, const Vec128<T, N> v) {
+   return detail::MinOfLanes(hwy::SizeTag<sizeof(T)>(), v);
+ }
+ template <typename T, size_t N>
+-HWY_API Vec128<T, N> MaxOfLanes(const Vec128<T, N> v) {
++HWY_API Vec128<T, N> MaxOfLanes(Simd<T, N> /* tag */, const Vec128<T, N> v) {
+   return detail::MaxOfLanes(hwy::SizeTag<sizeof(T)>(), v);
+ }
+ 
++// ================================================== DEPRECATED
++
++template <typename T, size_t N>
++HWY_API size_t StoreMaskBits(const Mask128<T, N> mask, uint8_t* p) {
++  return StoreMaskBits(Simd<T, N>(), mask, p);
++}
++
++template <typename T, size_t N>
++HWY_API bool AllTrue(const Mask128<T, N> mask) {
++  return AllTrue(Simd<T, N>(), mask);
++}
++
++template <typename T, size_t N>
++HWY_API bool AllFalse(const Mask128<T, N> mask) {
++  return AllFalse(Simd<T, N>(), mask);
++}
++
++template <typename T, size_t N>
++HWY_API size_t CountTrue(const Mask128<T, N> mask) {
++  return CountTrue(Simd<T, N>(), mask);
++}
++
++template <typename T, size_t N>
++HWY_API Vec128<T, N> SumOfLanes(const Vec128<T, N> v) {
++  return SumOfLanes(Simd<T, N>(), v);
++}
++template <typename T, size_t N>
++HWY_API Vec128<T, N> MinOfLanes(const Vec128<T, N> v) {
++  return MinOfLanes(Simd<T, N>(), v);
++}
++template <typename T, size_t N>
++HWY_API Vec128<T, N> MaxOfLanes(const Vec128<T, N> v) {
++  return MaxOfLanes(Simd<T, N>(), v);
++}
++
++template <typename T, size_t N>
++HWY_API Vec128<T, (N + 1) / 2> UpperHalf(Vec128<T, N> v) {
++  return UpperHalf(Half<Simd<T, N>>(), v);
++}
++
++template <int kBytes, typename T, size_t N>
++HWY_API Vec128<T, N> ShiftRightBytes(const Vec128<T, N> v) {
++  return ShiftRightBytes<kBytes>(Simd<T, N>(), v);
++}
++
++template <int kLanes, typename T, size_t N>
++HWY_API Vec128<T, N> ShiftRightLanes(const Vec128<T, N> v) {
++  return ShiftRightLanes<kLanes>(Simd<T, N>(), v);
++}
++
++template <size_t kBytes, typename T, size_t N>
++HWY_API Vec128<T, N> CombineShiftRightBytes(Vec128<T, N> hi, Vec128<T, N> lo) {
++  return CombineShiftRightBytes<kBytes>(Simd<T, N>(), hi, lo);
++}
++
++template <typename T, size_t N>
++HWY_API Vec128<T, N> InterleaveUpper(Vec128<T, N> a, Vec128<T, N> b) {
++  return InterleaveUpper(Simd<T, N>(), a, b);
++}
++
++template <typename T, size_t N, class D = Simd<T, N>>
++HWY_API VFromD<RepartitionToWide<D>> ZipUpper(Vec128<T, N> a, Vec128<T, N> b) {
++  return InterleaveUpper(RepartitionToWide<D>(), a, b);
++}
++
++template <typename T, size_t N2>
++HWY_API Vec128<T, N2 * 2> Combine(Vec128<T, N2> hi2, Vec128<T, N2> lo2) {
++  return Combine(Simd<T, N2 * 2>(), hi2, lo2);
++}
++
++template <typename T, size_t N2, HWY_IF_LE64(T, N2)>
++HWY_API Vec128<T, N2 * 2> ZeroExtendVector(Vec128<T, N2> lo) {
++  return ZeroExtendVector(Simd<T, N2 * 2>(), lo);
++}
++
++template <typename T, size_t N>
++HWY_API Vec128<T, N> ConcatLowerLower(Vec128<T, N> hi, Vec128<T, N> lo) {
++  return ConcatLowerLower(Simd<T, N>(), hi, lo);
++}
++
++template <typename T, size_t N>
++HWY_API Vec128<T, N> ConcatUpperUpper(Vec128<T, N> hi, Vec128<T, N> lo) {
++  return ConcatUpperUpper(Simd<T, N>(), hi, lo);
++}
++
++template <typename T, size_t N>
++HWY_API Vec128<T, N> ConcatLowerUpper(const Vec128<T, N> hi,
++                                      const Vec128<T, N> lo) {
++  return ConcatLowerUpper(Simd<T, N>(), hi, lo);
++}
++
++template <typename T, size_t N>
++HWY_API Vec128<T, N> ConcatUpperLower(Vec128<T, N> hi, Vec128<T, N> lo) {
++  return ConcatUpperLower(Simd<T, N>(), hi, lo);
++}
++
+ // ================================================== Operator wrapper
+ 
+ template <class V>
+@@ -3031,6 +3371,10 @@ HWY_API auto Eq(V a, V b) -> decltype(a == b) {
+   return a == b;
+ }
+ template <class V>
++HWY_API auto Ne(V a, V b) -> decltype(a == b) {
++  return a != b;
++}
++template <class V>
+ HWY_API auto Lt(V a, V b) -> decltype(a == b) {
+   return a < b;
+ }
+diff --git a/third_party/highway/hwy/ops/x86_128-inl.h b/third_party/highway/hwy/ops/x86_128-inl.h
+index fc275274596cd..0c7da3930fddf 100644
+--- a/third_party/highway/hwy/ops/x86_128-inl.h
++++ b/third_party/highway/hwy/ops/x86_128-inl.h
+@@ -17,7 +17,12 @@
+ // External include guard in highway.h - see comment there.
+ 
+ #include <emmintrin.h>
++#if HWY_TARGET == HWY_SSSE3
++#include <tmmintrin.h>  // SSSE3
++#else
+ #include <smmintrin.h>  // SSE4
++#include <wmmintrin.h>  // CLMUL
++#endif
+ #include <stddef.h>
+ #include <stdint.h>
+ 
+@@ -37,6 +42,11 @@ HWY_BEFORE_NAMESPACE();
+ namespace hwy {
+ namespace HWY_NAMESPACE {
+ 
++template <typename T>
++using Full128 = Simd<T, 16 / sizeof(T)>;
++
++namespace detail {
++
+ template <typename T>
+ struct Raw128 {
+   using type = __m128i;
+@@ -50,12 +60,11 @@ struct Raw128<double> {
+   using type = __m128d;
+ };
+ 
+-template <typename T>
+-using Full128 = Simd<T, 16 / sizeof(T)>;
++}  // namespace detail
+ 
+ template <typename T, size_t N = 16 / sizeof(T)>
+ class Vec128 {
+-  using Raw = typename Raw128<T>::type;
++  using Raw = typename detail::Raw128<T>::type;
+ 
+  public:
+   // Compound assignment. Only usable if there is a corresponding non-member
+@@ -85,25 +94,53 @@ class Vec128 {
+   Raw raw;
+ };
+ 
+-// Integer: FF..FF or 0. Float: MSB, all other bits undefined - see README.
++// Forward-declare for use by DeduceD, see below.
++template <typename T>
++class Vec256;
++template <typename T>
++class Vec512;
++
++// FF..FF or 0.
+ template <typename T, size_t N = 16 / sizeof(T)>
+-class Mask128 {
+-  using Raw = typename Raw128<T>::type;
++struct Mask128 {
++  typename detail::Raw128<T>::type raw;
++};
+ 
+- public:
+-  Raw raw;
++namespace detail {
++
++// Deduce Simd<T, N> from Vec*<T, N> (pointers because Vec256/512 may be
++// incomplete types at this point; this is simpler than avoiding multiple
++// definitions of DFromV via #if)
++struct DeduceD {
++  template <typename T, size_t N>
++  Simd<T, N> operator()(const Vec128<T, N>*) const {
++    return Simd<T, N>();
++  }
++  template <typename T>
++  Simd<T, 32 / sizeof(T)> operator()(const Vec256<T>*) const {
++    return Simd<T, 32 / sizeof(T)>();
++  }
++  template <typename T>
++  Simd<T, 64 / sizeof(T)> operator()(const Vec512<T>*) const {
++    return Simd<T, 64 / sizeof(T)>();
++  }
+ };
+ 
++}  // namespace detail
++
++template <class V>
++using DFromV = decltype(detail::DeduceD()(static_cast<V*>(nullptr)));
++
+ // ------------------------------ BitCast
+ 
+ namespace detail {
+ 
+-HWY_API __m128i BitCastToInteger(__m128i v) { return v; }
+-HWY_API __m128i BitCastToInteger(__m128 v) { return _mm_castps_si128(v); }
+-HWY_API __m128i BitCastToInteger(__m128d v) { return _mm_castpd_si128(v); }
++HWY_INLINE __m128i BitCastToInteger(__m128i v) { return v; }
++HWY_INLINE __m128i BitCastToInteger(__m128 v) { return _mm_castps_si128(v); }
++HWY_INLINE __m128i BitCastToInteger(__m128d v) { return _mm_castpd_si128(v); }
+ 
+ template <typename T, size_t N>
+-HWY_API Vec128<uint8_t, N * sizeof(T)> BitCastToByte(Vec128<T, N> v) {
++HWY_INLINE Vec128<uint8_t, N * sizeof(T)> BitCastToByte(Vec128<T, N> v) {
+   return Vec128<uint8_t, N * sizeof(T)>{BitCastToInteger(v.raw)};
+ }
+ 
+@@ -122,8 +159,8 @@ struct BitCastFromInteger128<double> {
+ };
+ 
+ template <typename T, size_t N>
+-HWY_API Vec128<T, N> BitCastFromByte(Simd<T, N> /* tag */,
+-                                Vec128<uint8_t, N * sizeof(T)> v) {
++HWY_INLINE Vec128<T, N> BitCastFromByte(Simd<T, N> /* tag */,
++                                        Vec128<uint8_t, N * sizeof(T)> v) {
+   return Vec128<T, N>{BitCastFromInteger128<T>()(v.raw)};
+ }
+ 
+@@ -135,7 +172,7 @@ HWY_API Vec128<T, N> BitCast(Simd<T, N> d,
+   return detail::BitCastFromByte(d, detail::BitCastToByte(v));
+ }
+ 
+-// ------------------------------ Set
++// ------------------------------ Zero
+ 
+ // Returns an all-zero vector/part.
+ template <typename T, size_t N, HWY_IF_LE128(T, N)>
+@@ -151,6 +188,11 @@ HWY_API Vec128<double, N> Zero(Simd<double, N> /* tag */) {
+   return Vec128<double, N>{_mm_setzero_pd()};
+ }
+ 
++template <class D>
++using VFromD = decltype(Zero(D()));
++
++// ------------------------------ Set
++
+ // Returns a vector/part with all lanes set to "t".
+ template <size_t N, HWY_IF_LE128(uint8_t, N)>
+ HWY_API Vec128<uint8_t, N> Set(Simd<uint8_t, N> /* tag */, const uint8_t t) {
+@@ -254,7 +296,7 @@ HWY_API uint64_t GetLane(const Vec128<uint64_t, N> v) {
+   Store(v, Simd<uint64_t, N>(), lanes);
+   return lanes[0];
+ #else
+-  return _mm_cvtsi128_si64(v.raw);
++  return static_cast<uint64_t>(_mm_cvtsi128_si64(v.raw));
+ #endif
+ }
+ template <size_t N>
+@@ -350,7 +392,7 @@ HWY_API Vec128<double, N> Xor(const Vec128<double, N> a,
+ template <typename T, size_t N>
+ HWY_API Vec128<T, N> Not(const Vec128<T, N> v) {
+   using TU = MakeUnsigned<T>;
+-#if HWY_TARGET == HWY_AVX3
++#if HWY_TARGET <= HWY_AVX3
+   const __m128i vu = BitCast(Simd<TU, N>(), v).raw;
+   return BitCast(Simd<T, N>(),
+                  Vec128<TU, N>{_mm_ternarylogic_epi32(vu, vu, vu, 0x55)});
+@@ -376,6 +418,96 @@ HWY_API Vec128<T, N> operator^(const Vec128<T, N> a, const Vec128<T, N> b) {
+   return Xor(a, b);
+ }
+ 
++// ------------------------------ PopulationCount
++
++// 8/16 require BITALG, 32/64 require VPOPCNTDQ.
++#if HWY_TARGET == HWY_AVX3_DL
++
++#ifdef HWY_NATIVE_POPCNT
++#undef HWY_NATIVE_POPCNT
++#else
++#define HWY_NATIVE_POPCNT
++#endif
++
++namespace detail {
++
++template <typename T, size_t N>
++HWY_INLINE Vec128<T, N> PopulationCount(hwy::SizeTag<1> /* tag */,
++                                        Vec128<T, N> v) {
++  return Vec128<T, N>{_mm_popcnt_epi8(v.raw)};
++}
++template <typename T, size_t N>
++HWY_INLINE Vec128<T, N> PopulationCount(hwy::SizeTag<2> /* tag */,
++                                        Vec128<T, N> v) {
++  return Vec128<T, N>{_mm_popcnt_epi16(v.raw)};
++}
++template <typename T, size_t N>
++HWY_INLINE Vec128<T, N> PopulationCount(hwy::SizeTag<4> /* tag */,
++                                        Vec128<T, N> v) {
++  return Vec128<T, N>{_mm_popcnt_epi32(v.raw)};
++}
++template <typename T, size_t N>
++HWY_INLINE Vec128<T, N> PopulationCount(hwy::SizeTag<8> /* tag */,
++                                        Vec128<T, N> v) {
++  return Vec128<T, N>{_mm_popcnt_epi64(v.raw)};
++}
++
++}  // namespace detail
++
++template <typename T, size_t N>
++HWY_API Vec128<T, N> PopulationCount(Vec128<T, N> v) {
++  return detail::PopulationCount(hwy::SizeTag<sizeof(T)>(), v);
++}
++
++#endif  // HWY_TARGET == HWY_AVX3_DL
++
++// ================================================== SIGN
++
++// ------------------------------ Neg
++
++template <typename T, size_t N, HWY_IF_FLOAT(T)>
++HWY_API Vec128<T, N> Neg(const Vec128<T, N> v) {
++  return Xor(v, SignBit(Simd<T, N>()));
++}
++
++template <typename T, size_t N, HWY_IF_NOT_FLOAT(T)>
++HWY_API Vec128<T, N> Neg(const Vec128<T, N> v) {
++  return Zero(Simd<T, N>()) - v;
++}
++
++// ------------------------------ Abs
++
++// Returns absolute value, except that LimitsMin() maps to LimitsMax() + 1.
++template <size_t N>
++HWY_API Vec128<int8_t, N> Abs(const Vec128<int8_t, N> v) {
++#if HWY_COMPILER_MSVC
++  // Workaround for incorrect codegen? (reaches breakpoint)
++  const auto zero = Zero(Simd<int8_t, N>());
++  return Vec128<int8_t, N>{_mm_max_epi8(v.raw, (zero - v).raw)};
++#else
++  return Vec128<int8_t, N>{_mm_abs_epi8(v.raw)};
++#endif
++}
++template <size_t N>
++HWY_API Vec128<int16_t, N> Abs(const Vec128<int16_t, N> v) {
++  return Vec128<int16_t, N>{_mm_abs_epi16(v.raw)};
++}
++template <size_t N>
++HWY_API Vec128<int32_t, N> Abs(const Vec128<int32_t, N> v) {
++  return Vec128<int32_t, N>{_mm_abs_epi32(v.raw)};
++}
++// i64 is implemented after BroadcastSignBit.
++template <size_t N>
++HWY_API Vec128<float, N> Abs(const Vec128<float, N> v) {
++  const Vec128<int32_t, N> mask{_mm_set1_epi32(0x7FFFFFFF)};
++  return v & BitCast(Simd<float, N>(), mask);
++}
++template <size_t N>
++HWY_API Vec128<double, N> Abs(const Vec128<double, N> v) {
++  const Vec128<int64_t, N> mask{_mm_set1_epi64x(0x7FFFFFFFFFFFFFFFLL)};
++  return v & BitCast(Simd<double, N>(), mask);
++}
++
+ // ------------------------------ CopySign
+ 
+ template <typename T, size_t N>
+@@ -386,7 +518,7 @@ HWY_API Vec128<T, N> CopySign(const Vec128<T, N> magn,
+   const Simd<T, N> d;
+   const auto msb = SignBit(d);
+ 
+-#if HWY_TARGET == HWY_AVX3
++#if HWY_TARGET <= HWY_AVX3
+   const Rebind<MakeUnsigned<T>, decltype(d)> du;
+   // Truth table for msb, magn, sign | bitwise msb ? sign : mag
+   //                  0    0     0   |  0
+@@ -409,7 +541,7 @@ HWY_API Vec128<T, N> CopySign(const Vec128<T, N> magn,
+ template <typename T, size_t N>
+ HWY_API Vec128<T, N> CopySignToAbs(const Vec128<T, N> abs,
+                                    const Vec128<T, N> sign) {
+-#if HWY_TARGET == HWY_AVX3
++#if HWY_TARGET <= HWY_AVX3
+   // AVX3 can also handle abs < 0, so no extra action needed.
+   return CopySign(abs, sign);
+ #else
+@@ -417,6 +549,8 @@ HWY_API Vec128<T, N> CopySignToAbs(const Vec128<T, N> abs,
+ #endif
+ }
+ 
++// ================================================== MASK
++
+ // ------------------------------ Mask
+ 
+ // Mask and Vec are the same (true = FF..FF).
+@@ -436,6 +570,18 @@ HWY_API Vec128<T, N> VecFromMask(const Simd<T, N> /* tag */,
+   return Vec128<T, N>{v.raw};
+ }
+ 
++#if HWY_TARGET == HWY_SSSE3
++
++// mask ? yes : no
++template <typename T, size_t N>
++HWY_API Vec128<T, N> IfThenElse(Mask128<T, N> mask, Vec128<T, N> yes,
++                                Vec128<T, N> no) {
++  const auto vmask = VecFromMask(Simd<T, N>(), mask);
++  return Or(And(vmask, yes), AndNot(vmask, no));
++}
++
++#else  // HWY_TARGET == HWY_SSSE3
++
+ // mask ? yes : no
+ template <typename T, size_t N>
+ HWY_API Vec128<T, N> IfThenElse(Mask128<T, N> mask, Vec128<T, N> yes,
+@@ -455,6 +601,8 @@ HWY_API Vec128<double, N> IfThenElse(const Mask128<double, N> mask,
+   return Vec128<double, N>{_mm_blendv_pd(no.raw, yes.raw, mask.raw)};
+ }
+ 
++#endif  // HWY_TARGET == HWY_SSSE3
++
+ // mask ? yes : 0
+ template <typename T, size_t N>
+ HWY_API Vec128<T, N> IfThenElseZero(Mask128<T, N> mask, Vec128<T, N> yes) {
+@@ -467,18 +615,11 @@ HWY_API Vec128<T, N> IfThenZeroElse(Mask128<T, N> mask, Vec128<T, N> no) {
+   return AndNot(VecFromMask(Simd<T, N>(), mask), no);
+ }
+ 
+-template <typename T, size_t N, HWY_IF_FLOAT(T)>
+-HWY_API Vec128<T, N> ZeroIfNegative(Vec128<T, N> v) {
+-  const Simd<T, N> d;
+-  return IfThenElse(MaskFromVec(v), Zero(d), v);
+-}
+-
+ // ------------------------------ Mask logical
+ 
+ template <typename T, size_t N>
+ HWY_API Mask128<T, N> Not(const Mask128<T, N> m) {
+-  const Simd<T, N> d;
+-  return MaskFromVec(Not(VecFromMask(d, m)));
++  return MaskFromVec(Not(VecFromMask(Simd<T, N>(), m)));
+ }
+ 
+ template <typename T, size_t N>
+@@ -505,6 +646,84 @@ HWY_API Mask128<T, N> Xor(const Mask128<T, N> a, Mask128<T, N> b) {
+   return MaskFromVec(Xor(VecFromMask(d, a), VecFromMask(d, b)));
+ }
+ 
++// ================================================== SWIZZLE (1)
++
++// ------------------------------ Hard-coded shuffles
++
++// Notation: let Vec128<int32_t> have lanes 3,2,1,0 (0 is least-significant).
++// Shuffle0321 rotates one lane to the right (the previous least-significant
++// lane is now most-significant). These could also be implemented via
++// CombineShiftRightBytes but the shuffle_abcd notation is more convenient.
++
++// Swap 32-bit halves in 64-bit halves.
++template <size_t N>
++HWY_API Vec128<uint32_t, N> Shuffle2301(const Vec128<uint32_t, N> v) {
++  static_assert(N == 2 || N == 4, "Does not make sense for N=1");
++  return Vec128<uint32_t, N>{_mm_shuffle_epi32(v.raw, 0xB1)};
++}
++template <size_t N>
++HWY_API Vec128<int32_t, N> Shuffle2301(const Vec128<int32_t, N> v) {
++  static_assert(N == 2 || N == 4, "Does not make sense for N=1");
++  return Vec128<int32_t, N>{_mm_shuffle_epi32(v.raw, 0xB1)};
++}
++template <size_t N>
++HWY_API Vec128<float, N> Shuffle2301(const Vec128<float, N> v) {
++  static_assert(N == 2 || N == 4, "Does not make sense for N=1");
++  return Vec128<float, N>{_mm_shuffle_ps(v.raw, v.raw, 0xB1)};
++}
++
++// Swap 64-bit halves
++HWY_API Vec128<uint32_t> Shuffle1032(const Vec128<uint32_t> v) {
++  return Vec128<uint32_t>{_mm_shuffle_epi32(v.raw, 0x4E)};
++}
++HWY_API Vec128<int32_t> Shuffle1032(const Vec128<int32_t> v) {
++  return Vec128<int32_t>{_mm_shuffle_epi32(v.raw, 0x4E)};
++}
++HWY_API Vec128<float> Shuffle1032(const Vec128<float> v) {
++  return Vec128<float>{_mm_shuffle_ps(v.raw, v.raw, 0x4E)};
++}
++HWY_API Vec128<uint64_t> Shuffle01(const Vec128<uint64_t> v) {
++  return Vec128<uint64_t>{_mm_shuffle_epi32(v.raw, 0x4E)};
++}
++HWY_API Vec128<int64_t> Shuffle01(const Vec128<int64_t> v) {
++  return Vec128<int64_t>{_mm_shuffle_epi32(v.raw, 0x4E)};
++}
++HWY_API Vec128<double> Shuffle01(const Vec128<double> v) {
++  return Vec128<double>{_mm_shuffle_pd(v.raw, v.raw, 1)};
++}
++
++// Rotate right 32 bits
++HWY_API Vec128<uint32_t> Shuffle0321(const Vec128<uint32_t> v) {
++  return Vec128<uint32_t>{_mm_shuffle_epi32(v.raw, 0x39)};
++}
++HWY_API Vec128<int32_t> Shuffle0321(const Vec128<int32_t> v) {
++  return Vec128<int32_t>{_mm_shuffle_epi32(v.raw, 0x39)};
++}
++HWY_API Vec128<float> Shuffle0321(const Vec128<float> v) {
++  return Vec128<float>{_mm_shuffle_ps(v.raw, v.raw, 0x39)};
++}
++// Rotate left 32 bits
++HWY_API Vec128<uint32_t> Shuffle2103(const Vec128<uint32_t> v) {
++  return Vec128<uint32_t>{_mm_shuffle_epi32(v.raw, 0x93)};
++}
++HWY_API Vec128<int32_t> Shuffle2103(const Vec128<int32_t> v) {
++  return Vec128<int32_t>{_mm_shuffle_epi32(v.raw, 0x93)};
++}
++HWY_API Vec128<float> Shuffle2103(const Vec128<float> v) {
++  return Vec128<float>{_mm_shuffle_ps(v.raw, v.raw, 0x93)};
++}
++
++// Reverse
++HWY_API Vec128<uint32_t> Shuffle0123(const Vec128<uint32_t> v) {
++  return Vec128<uint32_t>{_mm_shuffle_epi32(v.raw, 0x1B)};
++}
++HWY_API Vec128<int32_t> Shuffle0123(const Vec128<int32_t> v) {
++  return Vec128<int32_t>{_mm_shuffle_epi32(v.raw, 0x1B)};
++}
++HWY_API Vec128<float> Shuffle0123(const Vec128<float> v) {
++  return Vec128<float>{_mm_shuffle_ps(v.raw, v.raw, 0x1B)};
++}
++
+ // ================================================== COMPARE
+ 
+ // Comparisons fill a lane with 1-bits if the condition is true, else 0.
+@@ -516,6 +735,12 @@ HWY_API Mask128<TTo, N> RebindMask(Simd<TTo, N> /*tag*/, Mask128<TFrom, N> m) {
+   return MaskFromVec(BitCast(Simd<TTo, N>(), VecFromMask(d, m)));
+ }
+ 
++template <typename T, size_t N>
++HWY_API Mask128<T, N> TestBit(Vec128<T, N> v, Vec128<T, N> bit) {
++  static_assert(!hwy::IsFloat<T>(), "Only integer vectors supported");
++  return (v & bit) == bit;
++}
++
+ // ------------------------------ Equality
+ 
+ // Unsigned
+@@ -537,7 +762,15 @@ HWY_API Mask128<uint32_t, N> operator==(const Vec128<uint32_t, N> a,
+ template <size_t N>
+ HWY_API Mask128<uint64_t, N> operator==(const Vec128<uint64_t, N> a,
+                                         const Vec128<uint64_t, N> b) {
++#if HWY_TARGET == HWY_SSSE3
++  const Simd<uint32_t, N * 2> d32;
++  const Simd<uint64_t, N> d64;
++  const auto cmp32 = VecFromMask(d32, Eq(BitCast(d32, a), BitCast(d32, b)));
++  const auto cmp64 = cmp32 & Shuffle2301(cmp32);
++  return MaskFromVec(BitCast(d64, cmp64));
++#else
+   return Mask128<uint64_t, N>{_mm_cmpeq_epi64(a.raw, b.raw)};
++#endif
+ }
+ 
+ // Signed
+@@ -559,7 +792,9 @@ HWY_API Mask128<int32_t, N> operator==(const Vec128<int32_t, N> a,
+ template <size_t N>
+ HWY_API Mask128<int64_t, N> operator==(const Vec128<int64_t, N> a,
+                                        const Vec128<int64_t, N> b) {
+-  return Mask128<int64_t, N>{_mm_cmpeq_epi64(a.raw, b.raw)};
++  // Same as signed ==; avoid duplicating the SSSE3 version.
++  const Simd<uint64_t, N> du;
++  return RebindMask(Simd<int64_t, N>(), BitCast(du, a) == BitCast(du, b));
+ }
+ 
+ // Float
+@@ -574,10 +809,22 @@ HWY_API Mask128<double, N> operator==(const Vec128<double, N> a,
+   return Mask128<double, N>{_mm_cmpeq_pd(a.raw, b.raw)};
+ }
+ 
+-template <typename T, size_t N>
+-HWY_API Mask128<T, N> TestBit(Vec128<T, N> v, Vec128<T, N> bit) {
+-  static_assert(!hwy::IsFloat<T>(), "Only integer vectors supported");
+-  return (v & bit) == bit;
++// ------------------------------ Inequality
++
++template <typename T, size_t N, HWY_IF_NOT_FLOAT(T)>
++HWY_API Mask128<T, N> operator!=(const Vec128<T, N> a, const Vec128<T, N> b) {
++  return Not(a == b);
++}
++
++template <size_t N>
++HWY_API Mask128<float, N> operator!=(const Vec128<float, N> a,
++                                     const Vec128<float, N> b) {
++  return Mask128<float, N>{_mm_cmpneq_ps(a.raw, b.raw)};
++}
++template <size_t N>
++HWY_API Mask128<double, N> operator!=(const Vec128<double, N> a,
++                                      const Vec128<double, N> b) {
++  return Mask128<double, N>{_mm_cmpneq_pd(a.raw, b.raw)};
+ }
+ 
+ // ------------------------------ Strict inequality
+@@ -639,7 +886,7 @@ HWY_API Mask128<double, N> operator>(const Vec128<double, N> a,
+ template <size_t N>
+ HWY_API Mask128<int64_t, N> operator>(const Vec128<int64_t, N> a,
+                                       const Vec128<int64_t, N> b) {
+-#if HWY_TARGET == HWY_SSE4  // SSE4.1
++#if HWY_TARGET == HWY_SSSE3
+   // If the upper half is less than or greater, this is the answer.
+   const __m128i m_gt = _mm_cmpgt_epi32(a.raw, b.raw);
+ 
+@@ -694,37 +941,230 @@ HWY_API Mask128<T, N> FirstN(const Simd<T, N> d, size_t num) {
+   return RebindMask(d, Iota(di, 0) < Set(di, static_cast<MakeSigned<T>>(num)));
+ }
+ 
+-// ================================================== ARITHMETIC
++// ================================================== MEMORY (1)
+ 
+-// ------------------------------ Addition
++// Clang static analysis claims the memory immediately after a partial vector
++// store is uninitialized, and also flags the input to partial loads (at least
++// for loadl_pd) as "garbage". This is a false alarm because msan does not
++// raise errors. We work around this by using CopyBytes instead of intrinsics,
++// but only for the analyzer to avoid potentially bad code generation.
++// Unfortunately __clang_analyzer__ was not defined for clang-tidy prior to v7.
++#ifndef HWY_SAFE_PARTIAL_LOAD_STORE
++#if defined(__clang_analyzer__) || \
++    (HWY_COMPILER_CLANG != 0 && HWY_COMPILER_CLANG < 700)
++#define HWY_SAFE_PARTIAL_LOAD_STORE 1
++#else
++#define HWY_SAFE_PARTIAL_LOAD_STORE 0
++#endif
++#endif  // HWY_SAFE_PARTIAL_LOAD_STORE
+ 
+-// Unsigned
+-template <size_t N>
+-HWY_API Vec128<uint8_t, N> operator+(const Vec128<uint8_t, N> a,
+-                                     const Vec128<uint8_t, N> b) {
+-  return Vec128<uint8_t, N>{_mm_add_epi8(a.raw, b.raw)};
+-}
+-template <size_t N>
+-HWY_API Vec128<uint16_t, N> operator+(const Vec128<uint16_t, N> a,
+-                                      const Vec128<uint16_t, N> b) {
+-  return Vec128<uint16_t, N>{_mm_add_epi16(a.raw, b.raw)};
++// ------------------------------ Load
++
++template <typename T>
++HWY_API Vec128<T> Load(Full128<T> /* tag */, const T* HWY_RESTRICT aligned) {
++  return Vec128<T>{_mm_load_si128(reinterpret_cast<const __m128i*>(aligned))};
+ }
+-template <size_t N>
+-HWY_API Vec128<uint32_t, N> operator+(const Vec128<uint32_t, N> a,
+-                                      const Vec128<uint32_t, N> b) {
+-  return Vec128<uint32_t, N>{_mm_add_epi32(a.raw, b.raw)};
++HWY_API Vec128<float> Load(Full128<float> /* tag */,
++                           const float* HWY_RESTRICT aligned) {
++  return Vec128<float>{_mm_load_ps(aligned)};
+ }
+-template <size_t N>
+-HWY_API Vec128<uint64_t, N> operator+(const Vec128<uint64_t, N> a,
+-                                      const Vec128<uint64_t, N> b) {
+-  return Vec128<uint64_t, N>{_mm_add_epi64(a.raw, b.raw)};
++HWY_API Vec128<double> Load(Full128<double> /* tag */,
++                            const double* HWY_RESTRICT aligned) {
++  return Vec128<double>{_mm_load_pd(aligned)};
+ }
+ 
+-// Signed
+-template <size_t N>
+-HWY_API Vec128<int8_t, N> operator+(const Vec128<int8_t, N> a,
+-                                    const Vec128<int8_t, N> b) {
+-  return Vec128<int8_t, N>{_mm_add_epi8(a.raw, b.raw)};
++template <typename T>
++HWY_API Vec128<T> LoadU(Full128<T> /* tag */, const T* HWY_RESTRICT p) {
++  return Vec128<T>{_mm_loadu_si128(reinterpret_cast<const __m128i*>(p))};
++}
++HWY_API Vec128<float> LoadU(Full128<float> /* tag */,
++                            const float* HWY_RESTRICT p) {
++  return Vec128<float>{_mm_loadu_ps(p)};
++}
++HWY_API Vec128<double> LoadU(Full128<double> /* tag */,
++                             const double* HWY_RESTRICT p) {
++  return Vec128<double>{_mm_loadu_pd(p)};
++}
++
++template <typename T>
++HWY_API Vec128<T, 8 / sizeof(T)> Load(Simd<T, 8 / sizeof(T)> /* tag */,
++                                      const T* HWY_RESTRICT p) {
++#if HWY_SAFE_PARTIAL_LOAD_STORE
++  __m128i v = _mm_setzero_si128();
++  CopyBytes<8>(p, &v);
++  return Vec128<T, 8 / sizeof(T)>{v};
++#else
++  return Vec128<T, 8 / sizeof(T)>{
++      _mm_loadl_epi64(reinterpret_cast<const __m128i*>(p))};
++#endif
++}
++
++HWY_API Vec128<float, 2> Load(Simd<float, 2> /* tag */,
++                              const float* HWY_RESTRICT p) {
++#if HWY_SAFE_PARTIAL_LOAD_STORE
++  __m128 v = _mm_setzero_ps();
++  CopyBytes<8>(p, &v);
++  return Vec128<float, 2>{v};
++#else
++  const __m128 hi = _mm_setzero_ps();
++  return Vec128<float, 2>{_mm_loadl_pi(hi, reinterpret_cast<const __m64*>(p))};
++#endif
++}
++
++HWY_API Vec128<double, 1> Load(Simd<double, 1> /* tag */,
++                               const double* HWY_RESTRICT p) {
++#if HWY_SAFE_PARTIAL_LOAD_STORE
++  __m128d v = _mm_setzero_pd();
++  CopyBytes<8>(p, &v);
++  return Vec128<double, 1>{v};
++#else
++  return Vec128<double, 1>{_mm_load_sd(p)};
++#endif
++}
++
++HWY_API Vec128<float, 1> Load(Simd<float, 1> /* tag */,
++                              const float* HWY_RESTRICT p) {
++#if HWY_SAFE_PARTIAL_LOAD_STORE
++  __m128 v = _mm_setzero_ps();
++  CopyBytes<4>(p, &v);
++  return Vec128<float, 1>{v};
++#else
++  return Vec128<float, 1>{_mm_load_ss(p)};
++#endif
++}
++
++// Any <= 32 bit except <float, 1>
++template <typename T, size_t N, HWY_IF_LE32(T, N)>
++HWY_API Vec128<T, N> Load(Simd<T, N> /* tag */, const T* HWY_RESTRICT p) {
++  constexpr size_t kSize = sizeof(T) * N;
++#if HWY_SAFE_PARTIAL_LOAD_STORE
++  __m128 v = _mm_setzero_ps();
++  CopyBytes<kSize>(p, &v);
++  return Vec128<T, N>{v};
++#else
++  int32_t bits;
++  CopyBytes<kSize>(p, &bits);
++  return Vec128<T, N>{_mm_cvtsi32_si128(bits)};
++#endif
++}
++
++// For < 128 bit, LoadU == Load.
++template <typename T, size_t N, HWY_IF_LE64(T, N)>
++HWY_API Vec128<T, N> LoadU(Simd<T, N> d, const T* HWY_RESTRICT p) {
++  return Load(d, p);
++}
++
++// 128-bit SIMD => nothing to duplicate, same as an unaligned load.
++template <typename T, size_t N, HWY_IF_LE128(T, N)>
++HWY_API Vec128<T, N> LoadDup128(Simd<T, N> d, const T* HWY_RESTRICT p) {
++  return LoadU(d, p);
++}
++
++// ------------------------------ Store
++
++template <typename T>
++HWY_API void Store(Vec128<T> v, Full128<T> /* tag */, T* HWY_RESTRICT aligned) {
++  _mm_store_si128(reinterpret_cast<__m128i*>(aligned), v.raw);
++}
++HWY_API void Store(const Vec128<float> v, Full128<float> /* tag */,
++                   float* HWY_RESTRICT aligned) {
++  _mm_store_ps(aligned, v.raw);
++}
++HWY_API void Store(const Vec128<double> v, Full128<double> /* tag */,
++                   double* HWY_RESTRICT aligned) {
++  _mm_store_pd(aligned, v.raw);
++}
++
++template <typename T>
++HWY_API void StoreU(Vec128<T> v, Full128<T> /* tag */, T* HWY_RESTRICT p) {
++  _mm_storeu_si128(reinterpret_cast<__m128i*>(p), v.raw);
++}
++HWY_API void StoreU(const Vec128<float> v, Full128<float> /* tag */,
++                    float* HWY_RESTRICT p) {
++  _mm_storeu_ps(p, v.raw);
++}
++HWY_API void StoreU(const Vec128<double> v, Full128<double> /* tag */,
++                    double* HWY_RESTRICT p) {
++  _mm_storeu_pd(p, v.raw);
++}
++
++template <typename T>
++HWY_API void Store(Vec128<T, 8 / sizeof(T)> v, Simd<T, 8 / sizeof(T)> /* tag */,
++                   T* HWY_RESTRICT p) {
++#if HWY_SAFE_PARTIAL_LOAD_STORE
++  CopyBytes<8>(&v, p);
++#else
++  _mm_storel_epi64(reinterpret_cast<__m128i*>(p), v.raw);
++#endif
++}
++HWY_API void Store(const Vec128<float, 2> v, Simd<float, 2> /* tag */,
++                   float* HWY_RESTRICT p) {
++#if HWY_SAFE_PARTIAL_LOAD_STORE
++  CopyBytes<8>(&v, p);
++#else
++  _mm_storel_pi(reinterpret_cast<__m64*>(p), v.raw);
++#endif
++}
++HWY_API void Store(const Vec128<double, 1> v, Simd<double, 1> /* tag */,
++                   double* HWY_RESTRICT p) {
++#if HWY_SAFE_PARTIAL_LOAD_STORE
++  CopyBytes<8>(&v, p);
++#else
++  _mm_storel_pd(p, v.raw);
++#endif
++}
++
++// Any <= 32 bit except <float, 1>
++template <typename T, size_t N, HWY_IF_LE32(T, N)>
++HWY_API void Store(Vec128<T, N> v, Simd<T, N> /* tag */, T* HWY_RESTRICT p) {
++  CopyBytes<sizeof(T) * N>(&v, p);
++}
++HWY_API void Store(const Vec128<float, 1> v, Simd<float, 1> /* tag */,
++                   float* HWY_RESTRICT p) {
++#if HWY_SAFE_PARTIAL_LOAD_STORE
++  CopyBytes<4>(&v, p);
++#else
++  _mm_store_ss(p, v.raw);
++#endif
++}
++
++// For < 128 bit, StoreU == Store.
++template <typename T, size_t N, HWY_IF_LE64(T, N)>
++HWY_API void StoreU(const Vec128<T, N> v, Simd<T, N> d, T* HWY_RESTRICT p) {
++  Store(v, d, p);
++}
++
++// ================================================== ARITHMETIC
++
++// ------------------------------ Addition
++
++// Unsigned
++template <size_t N>
++HWY_API Vec128<uint8_t, N> operator+(const Vec128<uint8_t, N> a,
++                                     const Vec128<uint8_t, N> b) {
++  return Vec128<uint8_t, N>{_mm_add_epi8(a.raw, b.raw)};
++}
++template <size_t N>
++HWY_API Vec128<uint16_t, N> operator+(const Vec128<uint16_t, N> a,
++                                      const Vec128<uint16_t, N> b) {
++  return Vec128<uint16_t, N>{_mm_add_epi16(a.raw, b.raw)};
++}
++template <size_t N>
++HWY_API Vec128<uint32_t, N> operator+(const Vec128<uint32_t, N> a,
++                                      const Vec128<uint32_t, N> b) {
++  return Vec128<uint32_t, N>{_mm_add_epi32(a.raw, b.raw)};
++}
++template <size_t N>
++HWY_API Vec128<uint64_t, N> operator+(const Vec128<uint64_t, N> a,
++                                      const Vec128<uint64_t, N> b) {
++  return Vec128<uint64_t, N>{_mm_add_epi64(a.raw, b.raw)};
++}
++
++// Signed
++template <size_t N>
++HWY_API Vec128<int8_t, N> operator+(const Vec128<int8_t, N> a,
++                                    const Vec128<int8_t, N> b) {
++  return Vec128<int8_t, N>{_mm_add_epi8(a.raw, b.raw)};
+ }
+ template <size_t N>
+ HWY_API Vec128<int16_t, N> operator+(const Vec128<int16_t, N> a,
+@@ -884,64 +1324,18 @@ HWY_API Vec128<uint16_t, N> AverageRound(const Vec128<uint16_t, N> a,
+   return Vec128<uint16_t, N>{_mm_avg_epu16(a.raw, b.raw)};
+ }
+ 
+-// ------------------------------ Abs
+-
+-// Returns absolute value, except that LimitsMin() maps to LimitsMax() + 1.
+-template <size_t N>
+-HWY_API Vec128<int8_t, N> Abs(const Vec128<int8_t, N> v) {
+-#if HWY_COMPILER_MSVC
+-  // Workaround for incorrect codegen? (reaches breakpoint)
+-  const auto zero = Zero(Simd<int8_t, N>());
+-  return Vec128<int8_t, N>{_mm_max_epi8(v.raw, (zero - v).raw)};
+-#else
+-  return Vec128<int8_t, N>{_mm_abs_epi8(v.raw)};
+-#endif
+-}
+-template <size_t N>
+-HWY_API Vec128<int16_t, N> Abs(const Vec128<int16_t, N> v) {
+-  return Vec128<int16_t, N>{_mm_abs_epi16(v.raw)};
+-}
+-template <size_t N>
+-HWY_API Vec128<int32_t, N> Abs(const Vec128<int32_t, N> v) {
+-  return Vec128<int32_t, N>{_mm_abs_epi32(v.raw)};
+-}
+-// i64 is implemented after BroadcastSignBit.
+-template <size_t N>
+-HWY_API Vec128<float, N> Abs(const Vec128<float, N> v) {
+-  const Vec128<int32_t, N> mask{_mm_set1_epi32(0x7FFFFFFF)};
+-  return v & BitCast(Simd<float, N>(), mask);
+-}
+-template <size_t N>
+-HWY_API Vec128<double, N> Abs(const Vec128<double, N> v) {
+-  const Vec128<int64_t, N> mask{_mm_set1_epi64x(0x7FFFFFFFFFFFFFFFLL)};
+-  return v & BitCast(Simd<double, N>(), mask);
+-}
+-
+ // ------------------------------ Integer multiplication
+ 
+-// Unsigned
+ template <size_t N>
+ HWY_API Vec128<uint16_t, N> operator*(const Vec128<uint16_t, N> a,
+                                       const Vec128<uint16_t, N> b) {
+   return Vec128<uint16_t, N>{_mm_mullo_epi16(a.raw, b.raw)};
+ }
+ template <size_t N>
+-HWY_API Vec128<uint32_t, N> operator*(const Vec128<uint32_t, N> a,
+-                                      const Vec128<uint32_t, N> b) {
+-  return Vec128<uint32_t, N>{_mm_mullo_epi32(a.raw, b.raw)};
+-}
+-
+-// Signed
+-template <size_t N>
+ HWY_API Vec128<int16_t, N> operator*(const Vec128<int16_t, N> a,
+                                      const Vec128<int16_t, N> b) {
+   return Vec128<int16_t, N>{_mm_mullo_epi16(a.raw, b.raw)};
+ }
+-template <size_t N>
+-HWY_API Vec128<int32_t, N> operator*(const Vec128<int32_t, N> a,
+-                                     const Vec128<int32_t, N> b) {
+-  return Vec128<int32_t, N>{_mm_mullo_epi32(a.raw, b.raw)};
+-}
+ 
+ // Returns the upper 16 bits of a * b in each lane.
+ template <size_t N>
+@@ -958,36 +1352,93 @@ HWY_API Vec128<int16_t, N> MulHigh(const Vec128<int16_t, N> a,
+ // Multiplies even lanes (0, 2 ..) and places the double-wide result into
+ // even and the upper half into its odd neighbor lane.
+ template <size_t N>
+-HWY_API Vec128<int64_t, (N + 1) / 2> MulEven(const Vec128<int32_t, N> a,
+-                                             const Vec128<int32_t, N> b) {
+-  return Vec128<int64_t, (N + 1) / 2>{_mm_mul_epi32(a.raw, b.raw)};
+-}
+-template <size_t N>
+ HWY_API Vec128<uint64_t, (N + 1) / 2> MulEven(const Vec128<uint32_t, N> a,
+                                               const Vec128<uint32_t, N> b) {
+   return Vec128<uint64_t, (N + 1) / 2>{_mm_mul_epu32(a.raw, b.raw)};
+ }
+ 
+-// ------------------------------ ShiftLeft
++#if HWY_TARGET == HWY_SSSE3
+ 
+-template <int kBits, size_t N>
+-HWY_API Vec128<uint16_t, N> ShiftLeft(const Vec128<uint16_t, N> v) {
+-  return Vec128<uint16_t, N>{_mm_slli_epi16(v.raw, kBits)};
++template <size_t N, HWY_IF_LE64(int32_t, N)>  // N=1 or 2
++HWY_API Vec128<int64_t, (N + 1) / 2> MulEven(const Vec128<int32_t, N> a,
++                                             const Vec128<int32_t, N> b) {
++  return Set(Simd<int64_t, (N + 1) / 2>(), int64_t(GetLane(a)) * GetLane(b));
+ }
+-
+-template <int kBits, size_t N>
+-HWY_API Vec128<uint32_t, N> ShiftLeft(const Vec128<uint32_t, N> v) {
+-  return Vec128<uint32_t, N>{_mm_slli_epi32(v.raw, kBits)};
++HWY_API Vec128<int64_t> MulEven(const Vec128<int32_t> a,
++                                const Vec128<int32_t> b) {
++  alignas(16) int32_t a_lanes[4];
++  alignas(16) int32_t b_lanes[4];
++  const Full128<int32_t> di32;
++  Store(a, di32, a_lanes);
++  Store(b, di32, b_lanes);
++  alignas(16) int64_t mul[2];
++  mul[0] = int64_t(a_lanes[0]) * b_lanes[0];
++  mul[1] = int64_t(a_lanes[2]) * b_lanes[2];
++  return Load(Full128<int64_t>(), mul);
+ }
+ 
+-template <int kBits, size_t N>
+-HWY_API Vec128<uint64_t, N> ShiftLeft(const Vec128<uint64_t, N> v) {
+-  return Vec128<uint64_t, N>{_mm_slli_epi64(v.raw, kBits)};
+-}
++#else  // HWY_TARGET == HWY_SSSE3
+ 
+-template <int kBits, size_t N>
+-HWY_API Vec128<int16_t, N> ShiftLeft(const Vec128<int16_t, N> v) {
+-  return Vec128<int16_t, N>{_mm_slli_epi16(v.raw, kBits)};
++template <size_t N>
++HWY_API Vec128<int64_t, (N + 1) / 2> MulEven(const Vec128<int32_t, N> a,
++                                             const Vec128<int32_t, N> b) {
++  return Vec128<int64_t, (N + 1) / 2>{_mm_mul_epi32(a.raw, b.raw)};
++}
++
++#endif  // HWY_TARGET == HWY_SSSE3
++
++template <size_t N>
++HWY_API Vec128<uint32_t, N> operator*(const Vec128<uint32_t, N> a,
++                                      const Vec128<uint32_t, N> b) {
++#if HWY_TARGET == HWY_SSSE3
++  // Not as inefficient as it looks: _mm_mullo_epi32 has 10 cycle latency.
++  // 64-bit right shift would also work but also needs port 5, so no benefit.
++  // Notation: x=don't care, z=0.
++  const __m128i a_x3x1 = _mm_shuffle_epi32(a.raw, _MM_SHUFFLE(3, 3, 1, 1));
++  const auto mullo_x2x0 = MulEven(a, b);
++  const __m128i b_x3x1 = _mm_shuffle_epi32(b.raw, _MM_SHUFFLE(3, 3, 1, 1));
++  const auto mullo_x3x1 =
++      MulEven(Vec128<uint32_t, N>{a_x3x1}, Vec128<uint32_t, N>{b_x3x1});
++  // We could _mm_slli_epi64 by 32 to get 3z1z and OR with z2z0, but generating
++  // the latter requires one more instruction or a constant.
++  const __m128i mul_20 =
++      _mm_shuffle_epi32(mullo_x2x0.raw, _MM_SHUFFLE(2, 0, 2, 0));
++  const __m128i mul_31 =
++      _mm_shuffle_epi32(mullo_x3x1.raw, _MM_SHUFFLE(2, 0, 2, 0));
++  return Vec128<uint32_t, N>{_mm_unpacklo_epi32(mul_20, mul_31)};
++#else
++  return Vec128<uint32_t, N>{_mm_mullo_epi32(a.raw, b.raw)};
++#endif
++}
++
++template <size_t N>
++HWY_API Vec128<int32_t, N> operator*(const Vec128<int32_t, N> a,
++                                     const Vec128<int32_t, N> b) {
++  // Same as unsigned; avoid duplicating the SSSE3 code.
++  const Simd<uint32_t, N> du;
++  return BitCast(Simd<int32_t, N>(), BitCast(du, a) * BitCast(du, b));
++}
++
++// ------------------------------ ShiftLeft
++
++template <int kBits, size_t N>
++HWY_API Vec128<uint16_t, N> ShiftLeft(const Vec128<uint16_t, N> v) {
++  return Vec128<uint16_t, N>{_mm_slli_epi16(v.raw, kBits)};
++}
++
++template <int kBits, size_t N>
++HWY_API Vec128<uint32_t, N> ShiftLeft(const Vec128<uint32_t, N> v) {
++  return Vec128<uint32_t, N>{_mm_slli_epi32(v.raw, kBits)};
++}
++
++template <int kBits, size_t N>
++HWY_API Vec128<uint64_t, N> ShiftLeft(const Vec128<uint64_t, N> v) {
++  return Vec128<uint64_t, N>{_mm_slli_epi64(v.raw, kBits)};
++}
++
++template <int kBits, size_t N>
++HWY_API Vec128<int16_t, N> ShiftLeft(const Vec128<int16_t, N> v) {
++  return Vec128<int16_t, N>{_mm_slli_epi16(v.raw, kBits)};
+ }
+ template <int kBits, size_t N>
+ HWY_API Vec128<int32_t, N> ShiftLeft(const Vec128<int32_t, N> v) {
+@@ -1071,13 +1522,13 @@ HWY_API Vec128<int32_t, N> BroadcastSignBit(const Vec128<int32_t, N> v) {
+ 
+ template <size_t N>
+ HWY_API Vec128<int64_t, N> BroadcastSignBit(const Vec128<int64_t, N> v) {
+-#if HWY_TARGET == HWY_AVX3
++#if HWY_TARGET <= HWY_AVX3
+   return Vec128<int64_t, N>{_mm_srai_epi64(v.raw, 63)};
+-#elif HWY_TARGET == HWY_AVX2
++#elif HWY_TARGET == HWY_AVX2 || HWY_TARGET == HWY_SSE4
+   return VecFromMask(v < Zero(Simd<int64_t, N>()));
+ #else
+-  // Efficient Gt() requires SSE4.2 but we only have SSE4.1. BLENDVPD requires
+-  // two constants and domain crossing. 32-bit shift avoids generating a zero.
++  // Efficient Lt() requires SSE4.2 and BLENDVPD requires SSE4.1. 32-bit shift
++  // avoids generating a zero.
+   const Simd<int32_t, N * 2> d32;
+   const auto sign = ShiftRight<31>(BitCast(d32, v));
+   return Vec128<int64_t, N>{
+@@ -1087,17 +1538,17 @@ HWY_API Vec128<int64_t, N> BroadcastSignBit(const Vec128<int64_t, N> v) {
+ 
+ template <size_t N>
+ HWY_API Vec128<int64_t, N> Abs(const Vec128<int64_t, N> v) {
+-#if HWY_TARGET == HWY_AVX3
++#if HWY_TARGET <= HWY_AVX3
+   return Vec128<int64_t, N>{_mm_abs_epi64(v.raw)};
+ #else
+-  const auto zero = Zero(Simd<int64_t,N>());
++  const auto zero = Zero(Simd<int64_t, N>());
+   return IfThenElse(MaskFromVec(BroadcastSignBit(v)), zero - v, v);
+ #endif
+ }
+ 
+ template <int kBits, size_t N>
+ HWY_API Vec128<int64_t, N> ShiftRight(const Vec128<int64_t, N> v) {
+-#if HWY_TARGET == HWY_AVX3
++#if HWY_TARGET <= HWY_AVX3
+   return Vec128<int64_t, N>{_mm_srai_epi64(v.raw, kBits)};
+ #else
+   const Simd<int64_t, N> di;
+@@ -1108,6 +1559,19 @@ HWY_API Vec128<int64_t, N> ShiftRight(const Vec128<int64_t, N> v) {
+ #endif
+ }
+ 
++// ------------------------------ ZeroIfNegative (BroadcastSignBit)
++template <typename T, size_t N, HWY_IF_FLOAT(T)>
++HWY_API Vec128<T, N> ZeroIfNegative(Vec128<T, N> v) {
++  const Simd<T, N> d;
++#if HWY_TARGET == HWY_SSSE3
++  const RebindToSigned<decltype(d)> di;
++  const auto mask = MaskFromVec(BitCast(d, BroadcastSignBit(BitCast(di, v))));
++#else
++  const auto mask = MaskFromVec(v);  // MSB is sufficient for BLENDVPS
++#endif
++  return IfThenElse(mask, Zero(d), v);
++}
++
+ // ------------------------------ ShiftLeftSame
+ 
+ template <size_t N>
+@@ -1195,7 +1659,7 @@ HWY_API Vec128<int32_t, N> ShiftRightSame(const Vec128<int32_t, N> v,
+ template <size_t N>
+ HWY_API Vec128<int64_t, N> ShiftRightSame(const Vec128<int64_t, N> v,
+                                           const int bits) {
+-#if HWY_TARGET == HWY_AVX3
++#if HWY_TARGET <= HWY_AVX3
+   return Vec128<int64_t, N>{_mm_sra_epi64(v.raw, _mm_cvtsi32_si128(bits))};
+ #else
+   const Simd<int64_t, N> di;
+@@ -1215,18 +1679,6 @@ HWY_API Vec128<int8_t, N> ShiftRightSame(Vec128<int8_t, N> v, const int bits) {
+   return (shifted ^ shifted_sign) - shifted_sign;
+ }
+ 
+-// ------------------------------ Negate
+-
+-template <typename T, size_t N, HWY_IF_FLOAT(T)>
+-HWY_API Vec128<T, N> Neg(const Vec128<T, N> v) {
+-  return Xor(v, SignBit(Simd<T, N>()));
+-}
+-
+-template <typename T, size_t N, HWY_IF_NOT_FLOAT(T)>
+-HWY_API Vec128<T, N> Neg(const Vec128<T, N> v) {
+-  return Zero(Simd<T, N>()) - v;
+-}
+-
+ // ------------------------------ Floating-point mul / div
+ 
+ template <size_t N>
+@@ -1289,7 +1741,7 @@ template <size_t N>
+ HWY_API Vec128<float, N> MulAdd(const Vec128<float, N> mul,
+                                 const Vec128<float, N> x,
+                                 const Vec128<float, N> add) {
+-#if HWY_TARGET == HWY_SSE4
++#if HWY_TARGET == HWY_SSSE3 || HWY_TARGET == HWY_SSE4
+   return mul * x + add;
+ #else
+   return Vec128<float, N>{_mm_fmadd_ps(mul.raw, x.raw, add.raw)};
+@@ -1299,7 +1751,7 @@ template <size_t N>
+ HWY_API Vec128<double, N> MulAdd(const Vec128<double, N> mul,
+                                  const Vec128<double, N> x,
+                                  const Vec128<double, N> add) {
+-#if HWY_TARGET == HWY_SSE4
++#if HWY_TARGET == HWY_SSSE3 || HWY_TARGET == HWY_SSE4
+   return mul * x + add;
+ #else
+   return Vec128<double, N>{_mm_fmadd_pd(mul.raw, x.raw, add.raw)};
+@@ -1311,7 +1763,7 @@ template <size_t N>
+ HWY_API Vec128<float, N> NegMulAdd(const Vec128<float, N> mul,
+                                    const Vec128<float, N> x,
+                                    const Vec128<float, N> add) {
+-#if HWY_TARGET == HWY_SSE4
++#if HWY_TARGET == HWY_SSSE3 || HWY_TARGET == HWY_SSE4
+   return add - mul * x;
+ #else
+   return Vec128<float, N>{_mm_fnmadd_ps(mul.raw, x.raw, add.raw)};
+@@ -1321,7 +1773,7 @@ template <size_t N>
+ HWY_API Vec128<double, N> NegMulAdd(const Vec128<double, N> mul,
+                                     const Vec128<double, N> x,
+                                     const Vec128<double, N> add) {
+-#if HWY_TARGET == HWY_SSE4
++#if HWY_TARGET == HWY_SSSE3 || HWY_TARGET == HWY_SSE4
+   return add - mul * x;
+ #else
+   return Vec128<double, N>{_mm_fnmadd_pd(mul.raw, x.raw, add.raw)};
+@@ -1333,7 +1785,7 @@ template <size_t N>
+ HWY_API Vec128<float, N> MulSub(const Vec128<float, N> mul,
+                                 const Vec128<float, N> x,
+                                 const Vec128<float, N> sub) {
+-#if HWY_TARGET == HWY_SSE4
++#if HWY_TARGET == HWY_SSSE3 || HWY_TARGET == HWY_SSE4
+   return mul * x - sub;
+ #else
+   return Vec128<float, N>{_mm_fmsub_ps(mul.raw, x.raw, sub.raw)};
+@@ -1343,7 +1795,7 @@ template <size_t N>
+ HWY_API Vec128<double, N> MulSub(const Vec128<double, N> mul,
+                                  const Vec128<double, N> x,
+                                  const Vec128<double, N> sub) {
+-#if HWY_TARGET == HWY_SSE4
++#if HWY_TARGET == HWY_SSSE3 || HWY_TARGET == HWY_SSE4
+   return mul * x - sub;
+ #else
+   return Vec128<double, N>{_mm_fmsub_pd(mul.raw, x.raw, sub.raw)};
+@@ -1355,7 +1807,7 @@ template <size_t N>
+ HWY_API Vec128<float, N> NegMulSub(const Vec128<float, N> mul,
+                                    const Vec128<float, N> x,
+                                    const Vec128<float, N> sub) {
+-#if HWY_TARGET == HWY_SSE4
++#if HWY_TARGET == HWY_SSSE3 || HWY_TARGET == HWY_SSE4
+   return Neg(mul) * x - sub;
+ #else
+   return Vec128<float, N>{_mm_fnmsub_ps(mul.raw, x.raw, sub.raw)};
+@@ -1365,7 +1817,7 @@ template <size_t N>
+ HWY_API Vec128<double, N> NegMulSub(const Vec128<double, N> mul,
+                                     const Vec128<double, N> x,
+                                     const Vec128<double, N> sub) {
+-#if HWY_TARGET == HWY_SSE4
++#if HWY_TARGET == HWY_SSSE3 || HWY_TARGET == HWY_SSE4
+   return Neg(mul) * x - sub;
+ #else
+   return Vec128<double, N>{_mm_fnmsub_pd(mul.raw, x.raw, sub.raw)};
+@@ -1399,57 +1851,21 @@ HWY_API Vec128<float, 1> ApproximateReciprocalSqrt(const Vec128<float, 1> v) {
+   return Vec128<float, 1>{_mm_rsqrt_ss(v.raw)};
+ }
+ 
+-// ------------------------------ Floating-point rounding
+-
+-// Toward nearest integer, ties to even
+-template <size_t N>
+-HWY_API Vec128<float, N> Round(const Vec128<float, N> v) {
+-  return Vec128<float, N>{
+-      _mm_round_ps(v.raw, _MM_FROUND_TO_NEAREST_INT | _MM_FROUND_NO_EXC)};
+-}
+-template <size_t N>
+-HWY_API Vec128<double, N> Round(const Vec128<double, N> v) {
+-  return Vec128<double, N>{
+-      _mm_round_pd(v.raw, _MM_FROUND_TO_NEAREST_INT | _MM_FROUND_NO_EXC)};
+-}
+-
+-// Toward zero, aka truncate
+-template <size_t N>
+-HWY_API Vec128<float, N> Trunc(const Vec128<float, N> v) {
+-  return Vec128<float, N>{
+-      _mm_round_ps(v.raw, _MM_FROUND_TO_ZERO | _MM_FROUND_NO_EXC)};
+-}
+-template <size_t N>
+-HWY_API Vec128<double, N> Trunc(const Vec128<double, N> v) {
+-  return Vec128<double, N>{
+-      _mm_round_pd(v.raw, _MM_FROUND_TO_ZERO | _MM_FROUND_NO_EXC)};
+-}
++// ------------------------------ Min (Gt, IfThenElse)
+ 
+-// Toward +infinity, aka ceiling
+-template <size_t N>
+-HWY_API Vec128<float, N> Ceil(const Vec128<float, N> v) {
+-  return Vec128<float, N>{
+-      _mm_round_ps(v.raw, _MM_FROUND_TO_POS_INF | _MM_FROUND_NO_EXC)};
+-}
+-template <size_t N>
+-HWY_API Vec128<double, N> Ceil(const Vec128<double, N> v) {
+-  return Vec128<double, N>{
+-      _mm_round_pd(v.raw, _MM_FROUND_TO_POS_INF | _MM_FROUND_NO_EXC)};
+-}
++namespace detail {
+ 
+-// Toward -infinity, aka floor
+-template <size_t N>
+-HWY_API Vec128<float, N> Floor(const Vec128<float, N> v) {
+-  return Vec128<float, N>{
+-      _mm_round_ps(v.raw, _MM_FROUND_TO_NEG_INF | _MM_FROUND_NO_EXC)};
+-}
+-template <size_t N>
+-HWY_API Vec128<double, N> Floor(const Vec128<double, N> v) {
+-  return Vec128<double, N>{
+-      _mm_round_pd(v.raw, _MM_FROUND_TO_NEG_INF | _MM_FROUND_NO_EXC)};
++template <typename T, size_t N>
++HWY_INLINE HWY_MAYBE_UNUSED Vec128<T, N> MinU(const Vec128<T, N> a,
++                                              const Vec128<T, N> b) {
++  const Simd<T, N> du;
++  const RebindToSigned<decltype(du)> di;
++  const auto msb = Set(du, static_cast<T>(T(1) << (sizeof(T) * 8 - 1)));
++  const auto gt = RebindMask(du, BitCast(di, a ^ msb) > BitCast(di, b ^ msb));
++  return IfThenElse(gt, b, a);
+ }
+ 
+-// ------------------------------ Min (Gt, IfThenElse)
++}  // namespace detail
+ 
+ // Unsigned
+ template <size_t N>
+@@ -1460,24 +1876,28 @@ HWY_API Vec128<uint8_t, N> Min(const Vec128<uint8_t, N> a,
+ template <size_t N>
+ HWY_API Vec128<uint16_t, N> Min(const Vec128<uint16_t, N> a,
+                                 const Vec128<uint16_t, N> b) {
++#if HWY_TARGET == HWY_SSSE3
++  return detail::MinU(a, b);
++#else
+   return Vec128<uint16_t, N>{_mm_min_epu16(a.raw, b.raw)};
++#endif
+ }
+ template <size_t N>
+ HWY_API Vec128<uint32_t, N> Min(const Vec128<uint32_t, N> a,
+                                 const Vec128<uint32_t, N> b) {
++#if HWY_TARGET == HWY_SSSE3
++  return detail::MinU(a, b);
++#else
+   return Vec128<uint32_t, N>{_mm_min_epu32(a.raw, b.raw)};
++#endif
+ }
+ template <size_t N>
+ HWY_API Vec128<uint64_t, N> Min(const Vec128<uint64_t, N> a,
+                                 const Vec128<uint64_t, N> b) {
+-#if HWY_TARGET == HWY_AVX3
++#if HWY_TARGET <= HWY_AVX3
+   return Vec128<uint64_t, N>{_mm_min_epu64(a.raw, b.raw)};
+ #else
+-  const Simd<uint64_t, N> du;
+-  const Simd<int64_t, N> di;
+-  const auto msb = Set(du, 1ull << 63);
+-  const auto gt = RebindMask(du, BitCast(di, a ^ msb) > BitCast(di, b ^ msb));
+-  return IfThenElse(gt, b, a);
++  return detail::MinU(a, b);
+ #endif
+ }
+ 
+@@ -1485,7 +1905,11 @@ HWY_API Vec128<uint64_t, N> Min(const Vec128<uint64_t, N> a,
+ template <size_t N>
+ HWY_API Vec128<int8_t, N> Min(const Vec128<int8_t, N> a,
+                               const Vec128<int8_t, N> b) {
++#if HWY_TARGET == HWY_SSSE3
++  return IfThenElse(a < b, a, b);
++#else
+   return Vec128<int8_t, N>{_mm_min_epi8(a.raw, b.raw)};
++#endif
+ }
+ template <size_t N>
+ HWY_API Vec128<int16_t, N> Min(const Vec128<int16_t, N> a,
+@@ -1495,12 +1919,16 @@ HWY_API Vec128<int16_t, N> Min(const Vec128<int16_t, N> a,
+ template <size_t N>
+ HWY_API Vec128<int32_t, N> Min(const Vec128<int32_t, N> a,
+                                const Vec128<int32_t, N> b) {
++#if HWY_TARGET == HWY_SSSE3
++  return IfThenElse(a < b, a, b);
++#else
+   return Vec128<int32_t, N>{_mm_min_epi32(a.raw, b.raw)};
++#endif
+ }
+ template <size_t N>
+ HWY_API Vec128<int64_t, N> Min(const Vec128<int64_t, N> a,
+                                const Vec128<int64_t, N> b) {
+-#if HWY_TARGET == HWY_AVX3
++#if HWY_TARGET <= HWY_AVX3
+   return Vec128<int64_t, N>{_mm_min_epi64(a.raw, b.raw)};
+ #else
+   return IfThenElse(a < b, a, b);
+@@ -1521,6 +1949,19 @@ HWY_API Vec128<double, N> Min(const Vec128<double, N> a,
+ 
+ // ------------------------------ Max (Gt, IfThenElse)
+ 
++namespace detail {
++template <typename T, size_t N>
++HWY_INLINE HWY_MAYBE_UNUSED Vec128<T, N> MaxU(const Vec128<T, N> a,
++                                              const Vec128<T, N> b) {
++  const Simd<T, N> du;
++  const RebindToSigned<decltype(du)> di;
++  const auto msb = Set(du, static_cast<T>(T(1) << (sizeof(T) * 8 - 1)));
++  const auto gt = RebindMask(du, BitCast(di, a ^ msb) > BitCast(di, b ^ msb));
++  return IfThenElse(gt, a, b);
++}
++
++}  // namespace detail
++
+ // Unsigned
+ template <size_t N>
+ HWY_API Vec128<uint8_t, N> Max(const Vec128<uint8_t, N> a,
+@@ -1530,24 +1971,28 @@ HWY_API Vec128<uint8_t, N> Max(const Vec128<uint8_t, N> a,
+ template <size_t N>
+ HWY_API Vec128<uint16_t, N> Max(const Vec128<uint16_t, N> a,
+                                 const Vec128<uint16_t, N> b) {
++#if HWY_TARGET == HWY_SSSE3
++  return detail::MaxU(a, b);
++#else
+   return Vec128<uint16_t, N>{_mm_max_epu16(a.raw, b.raw)};
++#endif
+ }
+ template <size_t N>
+ HWY_API Vec128<uint32_t, N> Max(const Vec128<uint32_t, N> a,
+                                 const Vec128<uint32_t, N> b) {
++#if HWY_TARGET == HWY_SSSE3
++  return detail::MaxU(a, b);
++#else
+   return Vec128<uint32_t, N>{_mm_max_epu32(a.raw, b.raw)};
++#endif
+ }
+ template <size_t N>
+ HWY_API Vec128<uint64_t, N> Max(const Vec128<uint64_t, N> a,
+                                 const Vec128<uint64_t, N> b) {
+-#if HWY_TARGET == HWY_AVX3
++#if HWY_TARGET <= HWY_AVX3
+   return Vec128<uint64_t, N>{_mm_max_epu64(a.raw, b.raw)};
+ #else
+-  const Simd<uint64_t, N> du;
+-  const Simd<int64_t, N> di;
+-  const auto msb = Set(du, 1ull << 63);
+-  const auto gt = RebindMask(du, BitCast(di, a ^ msb) > BitCast(di, b ^ msb));
+-  return IfThenElse(gt, a, b);
++  return detail::MaxU(a, b);
+ #endif
+ }
+ 
+@@ -1555,235 +2000,50 @@ HWY_API Vec128<uint64_t, N> Max(const Vec128<uint64_t, N> a,
+ template <size_t N>
+ HWY_API Vec128<int8_t, N> Max(const Vec128<int8_t, N> a,
+                               const Vec128<int8_t, N> b) {
+-  return Vec128<int8_t, N>{_mm_max_epi8(a.raw, b.raw)};
+-}
+-template <size_t N>
+-HWY_API Vec128<int16_t, N> Max(const Vec128<int16_t, N> a,
+-                               const Vec128<int16_t, N> b) {
+-  return Vec128<int16_t, N>{_mm_max_epi16(a.raw, b.raw)};
+-}
+-template <size_t N>
+-HWY_API Vec128<int32_t, N> Max(const Vec128<int32_t, N> a,
+-                               const Vec128<int32_t, N> b) {
+-  return Vec128<int32_t, N>{_mm_max_epi32(a.raw, b.raw)};
+-}
+-template <size_t N>
+-HWY_API Vec128<int64_t, N> Max(const Vec128<int64_t, N> a,
+-                               const Vec128<int64_t, N> b) {
+-#if HWY_TARGET == HWY_AVX3
+-  return Vec128<int64_t, N>{_mm_max_epi64(a.raw, b.raw)};
+-#else
++#if HWY_TARGET == HWY_SSSE3
+   return IfThenElse(a < b, b, a);
+-#endif
+-}
+-
+-// Float
+-template <size_t N>
+-HWY_API Vec128<float, N> Max(const Vec128<float, N> a,
+-                             const Vec128<float, N> b) {
+-  return Vec128<float, N>{_mm_max_ps(a.raw, b.raw)};
+-}
+-template <size_t N>
+-HWY_API Vec128<double, N> Max(const Vec128<double, N> a,
+-                              const Vec128<double, N> b) {
+-  return Vec128<double, N>{_mm_max_pd(a.raw, b.raw)};
+-}
+-
+-
+-// ================================================== MEMORY
+-
+-// Clang static analysis claims the memory immediately after a partial vector
+-// store is uninitialized, and also flags the input to partial loads (at least
+-// for loadl_pd) as "garbage". This is a false alarm because msan does not
+-// raise errors. We work around this by using CopyBytes instead of intrinsics,
+-// but only for the analyzer to avoid potentially bad code generation.
+-// Unfortunately __clang_analyzer__ was not defined for clang-tidy prior to v7.
+-#ifndef HWY_SAFE_PARTIAL_LOAD_STORE
+-#if defined(__clang_analyzer__) || \
+-    (HWY_COMPILER_CLANG != 0 && HWY_COMPILER_CLANG < 700)
+-#define HWY_SAFE_PARTIAL_LOAD_STORE 1
+-#else
+-#define HWY_SAFE_PARTIAL_LOAD_STORE 0
+-#endif
+-#endif  // HWY_SAFE_PARTIAL_LOAD_STORE
+-
+-// ------------------------------ Load
+-
+-template <typename T>
+-HWY_API Vec128<T> Load(Full128<T> /* tag */, const T* HWY_RESTRICT aligned) {
+-  return Vec128<T>{_mm_load_si128(reinterpret_cast<const __m128i*>(aligned))};
+-}
+-HWY_API Vec128<float> Load(Full128<float> /* tag */,
+-                           const float* HWY_RESTRICT aligned) {
+-  return Vec128<float>{_mm_load_ps(aligned)};
+-}
+-HWY_API Vec128<double> Load(Full128<double> /* tag */,
+-                            const double* HWY_RESTRICT aligned) {
+-  return Vec128<double>{_mm_load_pd(aligned)};
+-}
+-
+-template <typename T>
+-HWY_API Vec128<T> LoadU(Full128<T> /* tag */, const T* HWY_RESTRICT p) {
+-  return Vec128<T>{_mm_loadu_si128(reinterpret_cast<const __m128i*>(p))};
+-}
+-HWY_API Vec128<float> LoadU(Full128<float> /* tag */,
+-                            const float* HWY_RESTRICT p) {
+-  return Vec128<float>{_mm_loadu_ps(p)};
+-}
+-HWY_API Vec128<double> LoadU(Full128<double> /* tag */,
+-                             const double* HWY_RESTRICT p) {
+-  return Vec128<double>{_mm_loadu_pd(p)};
+-}
+-
+-template <typename T>
+-HWY_API Vec128<T, 8 / sizeof(T)> Load(Simd<T, 8 / sizeof(T)> /* tag */,
+-                                      const T* HWY_RESTRICT p) {
+-#if HWY_SAFE_PARTIAL_LOAD_STORE
+-  __m128i v = _mm_setzero_si128();
+-  CopyBytes<8>(p, &v);
+-  return Vec128<T, 8 / sizeof(T)>{v};
+-#else
+-  return Vec128<T, 8 / sizeof(T)>{
+-      _mm_loadl_epi64(reinterpret_cast<const __m128i*>(p))};
+-#endif
+-}
+-
+-HWY_API Vec128<float, 2> Load(Simd<float, 2> /* tag */,
+-                              const float* HWY_RESTRICT p) {
+-#if HWY_SAFE_PARTIAL_LOAD_STORE
+-  __m128 v = _mm_setzero_ps();
+-  CopyBytes<8>(p, &v);
+-  return Vec128<float, 2>{v};
+-#else
+-  const __m128 hi = _mm_setzero_ps();
+-  return Vec128<float, 2>{_mm_loadl_pi(hi, reinterpret_cast<const __m64*>(p))};
+-#endif
+-}
+-
+-HWY_API Vec128<double, 1> Load(Simd<double, 1> /* tag */,
+-                               const double* HWY_RESTRICT p) {
+-#if HWY_SAFE_PARTIAL_LOAD_STORE
+-  __m128d v = _mm_setzero_pd();
+-  CopyBytes<8>(p, &v);
+-  return Vec128<double, 1>{v};
+-#else
+-  return Vec128<double, 1>{_mm_load_sd(p)};
+-#endif
+-}
+-
+-HWY_API Vec128<float, 1> Load(Simd<float, 1> /* tag */,
+-                              const float* HWY_RESTRICT p) {
+-#if HWY_SAFE_PARTIAL_LOAD_STORE
+-  __m128 v = _mm_setzero_ps();
+-  CopyBytes<4>(p, &v);
+-  return Vec128<float, 1>{v};
+-#else
+-  return Vec128<float, 1>{_mm_load_ss(p)};
+-#endif
+-}
+-
+-// Any <= 32 bit except <float, 1>
+-template <typename T, size_t N, HWY_IF_LE32(T, N)>
+-HWY_API Vec128<T, N> Load(Simd<T, N> /* tag */, const T* HWY_RESTRICT p) {
+-  constexpr size_t kSize = sizeof(T) * N;
+-#if HWY_SAFE_PARTIAL_LOAD_STORE
+-  __m128 v = _mm_setzero_ps();
+-  CopyBytes<kSize>(p, &v);
+-  return Vec128<T, N>{v};
+-#else
+-  // TODO(janwas): load_ss?
+-  int32_t bits;
+-  CopyBytes<kSize>(p, &bits);
+-  return Vec128<T, N>{_mm_cvtsi32_si128(bits)};
+-#endif
+-}
+-
+-// For < 128 bit, LoadU == Load.
+-template <typename T, size_t N, HWY_IF_LE64(T, N)>
+-HWY_API Vec128<T, N> LoadU(Simd<T, N> d, const T* HWY_RESTRICT p) {
+-  return Load(d, p);
+-}
+-
+-// 128-bit SIMD => nothing to duplicate, same as an unaligned load.
+-template <typename T, size_t N, HWY_IF_LE128(T, N)>
+-HWY_API Vec128<T, N> LoadDup128(Simd<T, N> d, const T* HWY_RESTRICT p) {
+-  return LoadU(d, p);
+-}
+-
+-// ------------------------------ Store
+-
+-template <typename T>
+-HWY_API void Store(Vec128<T> v, Full128<T> /* tag */, T* HWY_RESTRICT aligned) {
+-  _mm_store_si128(reinterpret_cast<__m128i*>(aligned), v.raw);
+-}
+-HWY_API void Store(const Vec128<float> v, Full128<float> /* tag */,
+-                   float* HWY_RESTRICT aligned) {
+-  _mm_store_ps(aligned, v.raw);
+-}
+-HWY_API void Store(const Vec128<double> v, Full128<double> /* tag */,
+-                   double* HWY_RESTRICT aligned) {
+-  _mm_store_pd(aligned, v.raw);
+-}
+-
+-template <typename T>
+-HWY_API void StoreU(Vec128<T> v, Full128<T> /* tag */, T* HWY_RESTRICT p) {
+-  _mm_storeu_si128(reinterpret_cast<__m128i*>(p), v.raw);
+-}
+-HWY_API void StoreU(const Vec128<float> v, Full128<float> /* tag */,
+-                    float* HWY_RESTRICT p) {
+-  _mm_storeu_ps(p, v.raw);
+-}
+-HWY_API void StoreU(const Vec128<double> v, Full128<double> /* tag */,
+-                    double* HWY_RESTRICT p) {
+-  _mm_storeu_pd(p, v.raw);
+-}
+-
+-template <typename T>
+-HWY_API void Store(Vec128<T, 8 / sizeof(T)> v, Simd<T, 8 / sizeof(T)> /* tag */,
+-                   T* HWY_RESTRICT p) {
+-#if HWY_SAFE_PARTIAL_LOAD_STORE
+-  CopyBytes<8>(&v, p);
+-#else
+-  _mm_storel_epi64(reinterpret_cast<__m128i*>(p), v.raw);
+-#endif
+-}
+-HWY_API void Store(const Vec128<float, 2> v, Simd<float, 2> /* tag */,
+-                   float* HWY_RESTRICT p) {
+-#if HWY_SAFE_PARTIAL_LOAD_STORE
+-  CopyBytes<8>(&v, p);
+ #else
+-  _mm_storel_pi(reinterpret_cast<__m64*>(p), v.raw);
++  return Vec128<int8_t, N>{_mm_max_epi8(a.raw, b.raw)};
+ #endif
+ }
+-HWY_API void Store(const Vec128<double, 1> v, Simd<double, 1> /* tag */,
+-                   double* HWY_RESTRICT p) {
+-#if HWY_SAFE_PARTIAL_LOAD_STORE
+-  CopyBytes<8>(&v, p);
++template <size_t N>
++HWY_API Vec128<int16_t, N> Max(const Vec128<int16_t, N> a,
++                               const Vec128<int16_t, N> b) {
++  return Vec128<int16_t, N>{_mm_max_epi16(a.raw, b.raw)};
++}
++template <size_t N>
++HWY_API Vec128<int32_t, N> Max(const Vec128<int32_t, N> a,
++                               const Vec128<int32_t, N> b) {
++#if HWY_TARGET == HWY_SSSE3
++  return IfThenElse(a < b, b, a);
+ #else
+-  _mm_storel_pd(p, v.raw);
++  return Vec128<int32_t, N>{_mm_max_epi32(a.raw, b.raw)};
+ #endif
+ }
+-
+-// Any <= 32 bit except <float, 1>
+-template <typename T, size_t N, HWY_IF_LE32(T, N)>
+-HWY_API void Store(Vec128<T, N> v, Simd<T, N> /* tag */, T* HWY_RESTRICT p) {
+-  CopyBytes<sizeof(T) * N>(&v, p);
+-}
+-HWY_API void Store(const Vec128<float, 1> v, Simd<float, 1> /* tag */,
+-                   float* HWY_RESTRICT p) {
+-#if HWY_SAFE_PARTIAL_LOAD_STORE
+-  CopyBytes<4>(&v, p);
++template <size_t N>
++HWY_API Vec128<int64_t, N> Max(const Vec128<int64_t, N> a,
++                               const Vec128<int64_t, N> b) {
++#if HWY_TARGET <= HWY_AVX3
++  return Vec128<int64_t, N>{_mm_max_epi64(a.raw, b.raw)};
+ #else
+-  _mm_store_ss(p, v.raw);
++  return IfThenElse(a < b, b, a);
+ #endif
+ }
+ 
+-// For < 128 bit, StoreU == Store.
+-template <typename T, size_t N, HWY_IF_LE64(T, N)>
+-HWY_API void StoreU(const Vec128<T, N> v, Simd<T, N> d, T* HWY_RESTRICT p) {
+-  Store(v, d, p);
++// Float
++template <size_t N>
++HWY_API Vec128<float, N> Max(const Vec128<float, N> a,
++                             const Vec128<float, N> b) {
++  return Vec128<float, N>{_mm_max_ps(a.raw, b.raw)};
++}
++template <size_t N>
++HWY_API Vec128<double, N> Max(const Vec128<double, N> a,
++                              const Vec128<double, N> b) {
++  return Vec128<double, N>{_mm_max_pd(a.raw, b.raw)};
+ }
+ 
++// ================================================== MEMORY (2)
++
+ // ------------------------------ Non-temporal stores
+ 
+ // On clang6, we see incorrect code generated for _mm_stream_pi, so
+@@ -1814,13 +2074,13 @@ HWY_DIAGNOSTICS_OFF(disable : 4245 4365, ignored "-Wsign-conversion")
+ using GatherIndex64 = long long int;  // NOLINT(google-runtime-int)
+ static_assert(sizeof(GatherIndex64) == 8, "Must be 64-bit type");
+ 
+-#if HWY_TARGET == HWY_AVX3
++#if HWY_TARGET <= HWY_AVX3
+ namespace detail {
+ 
+ template <typename T, size_t N>
+-HWY_API void ScatterOffset(hwy::SizeTag<4> /* tag */, Vec128<T, N> v,
+-                           Simd<T, N> /* tag */, T* HWY_RESTRICT base,
+-                           const Vec128<int32_t, N> offset) {
++HWY_INLINE void ScatterOffset(hwy::SizeTag<4> /* tag */, Vec128<T, N> v,
++                              Simd<T, N> /* tag */, T* HWY_RESTRICT base,
++                              const Vec128<int32_t, N> offset) {
+   if (N == 4) {
+     _mm_i32scatter_epi32(base, offset.raw, v.raw, 1);
+   } else {
+@@ -1829,9 +2089,9 @@ HWY_API void ScatterOffset(hwy::SizeTag<4> /* tag */, Vec128<T, N> v,
+   }
+ }
+ template <typename T, size_t N>
+-HWY_API void ScatterIndex(hwy::SizeTag<4> /* tag */, Vec128<T, N> v,
+-                          Simd<T, N> /* tag */, T* HWY_RESTRICT base,
+-                          const Vec128<int32_t, N> index) {
++HWY_INLINE void ScatterIndex(hwy::SizeTag<4> /* tag */, Vec128<T, N> v,
++                             Simd<T, N> /* tag */, T* HWY_RESTRICT base,
++                             const Vec128<int32_t, N> index) {
+   if (N == 4) {
+     _mm_i32scatter_epi32(base, index.raw, v.raw, 4);
+   } else {
+@@ -1841,9 +2101,9 @@ HWY_API void ScatterIndex(hwy::SizeTag<4> /* tag */, Vec128<T, N> v,
+ }
+ 
+ template <typename T, size_t N>
+-HWY_API void ScatterOffset(hwy::SizeTag<8> /* tag */, Vec128<T, N> v,
+-                           Simd<T, N> /* tag */, T* HWY_RESTRICT base,
+-                           const Vec128<int64_t, N> offset) {
++HWY_INLINE void ScatterOffset(hwy::SizeTag<8> /* tag */, Vec128<T, N> v,
++                              Simd<T, N> /* tag */, T* HWY_RESTRICT base,
++                              const Vec128<int64_t, N> offset) {
+   if (N == 2) {
+     _mm_i64scatter_epi64(base, offset.raw, v.raw, 1);
+   } else {
+@@ -1852,9 +2112,9 @@ HWY_API void ScatterOffset(hwy::SizeTag<8> /* tag */, Vec128<T, N> v,
+   }
+ }
+ template <typename T, size_t N>
+-HWY_API void ScatterIndex(hwy::SizeTag<8> /* tag */, Vec128<T, N> v,
+-                          Simd<T, N> /* tag */, T* HWY_RESTRICT base,
+-                          const Vec128<int64_t, N> index) {
++HWY_INLINE void ScatterIndex(hwy::SizeTag<8> /* tag */, Vec128<T, N> v,
++                             Simd<T, N> /* tag */, T* HWY_RESTRICT base,
++                             const Vec128<int64_t, N> index) {
+   if (N == 2) {
+     _mm_i64scatter_epi64(base, index.raw, v.raw, 8);
+   } else {
+@@ -1879,9 +2139,9 @@ HWY_API void ScatterIndex(Vec128<T, N> v, Simd<T, N> d, T* HWY_RESTRICT base,
+ }
+ 
+ template <size_t N>
+-HWY_INLINE void ScatterOffset(Vec128<float, N> v, Simd<float, N> /* tag */,
+-                              float* HWY_RESTRICT base,
+-                              const Vec128<int32_t, N> offset) {
++HWY_API void ScatterOffset(Vec128<float, N> v, Simd<float, N> /* tag */,
++                           float* HWY_RESTRICT base,
++                           const Vec128<int32_t, N> offset) {
+   if (N == 4) {
+     _mm_i32scatter_ps(base, offset.raw, v.raw, 1);
+   } else {
+@@ -1890,9 +2150,9 @@ HWY_INLINE void ScatterOffset(Vec128<float, N> v, Simd<float, N> /* tag */,
+   }
+ }
+ template <size_t N>
+-HWY_INLINE void ScatterIndex(Vec128<float, N> v, Simd<float, N> /* tag */,
+-                             float* HWY_RESTRICT base,
+-                             const Vec128<int32_t, N> index) {
++HWY_API void ScatterIndex(Vec128<float, N> v, Simd<float, N> /* tag */,
++                          float* HWY_RESTRICT base,
++                          const Vec128<int32_t, N> index) {
+   if (N == 4) {
+     _mm_i32scatter_ps(base, index.raw, v.raw, 4);
+   } else {
+@@ -1902,9 +2162,9 @@ HWY_INLINE void ScatterIndex(Vec128<float, N> v, Simd<float, N> /* tag */,
+ }
+ 
+ template <size_t N>
+-HWY_INLINE void ScatterOffset(Vec128<double, N> v, Simd<double, N> /* tag */,
+-                              double* HWY_RESTRICT base,
+-                              const Vec128<int64_t, N> offset) {
++HWY_API void ScatterOffset(Vec128<double, N> v, Simd<double, N> /* tag */,
++                           double* HWY_RESTRICT base,
++                           const Vec128<int64_t, N> offset) {
+   if (N == 2) {
+     _mm_i64scatter_pd(base, offset.raw, v.raw, 1);
+   } else {
+@@ -1913,9 +2173,9 @@ HWY_INLINE void ScatterOffset(Vec128<double, N> v, Simd<double, N> /* tag */,
+   }
+ }
+ template <size_t N>
+-HWY_INLINE void ScatterIndex(Vec128<double, N> v, Simd<double, N> /* tag */,
+-                             double* HWY_RESTRICT base,
+-                             const Vec128<int64_t, N> index) {
++HWY_API void ScatterIndex(Vec128<double, N> v, Simd<double, N> /* tag */,
++                          double* HWY_RESTRICT base,
++                          const Vec128<int64_t, N> index) {
+   if (N == 2) {
+     _mm_i64scatter_pd(base, index.raw, v.raw, 8);
+   } else {
+@@ -1923,7 +2183,7 @@ HWY_INLINE void ScatterIndex(Vec128<double, N> v, Simd<double, N> /* tag */,
+     _mm_mask_i64scatter_pd(base, mask, index.raw, v.raw, 8);
+   }
+ }
+-#else  // HWY_TARGET == HWY_AVX3
++#else  // HWY_TARGET <= HWY_AVX3
+ 
+ template <typename T, size_t N, typename Offset, HWY_IF_LE128(T, N)>
+ HWY_API void ScatterOffset(Vec128<T, N> v, Simd<T, N> d, T* HWY_RESTRICT base,
+@@ -1962,7 +2222,7 @@ HWY_API void ScatterIndex(Vec128<T, N> v, Simd<T, N> d, T* HWY_RESTRICT base,
+ 
+ // ------------------------------ Gather (Load/Store)
+ 
+-#if HWY_TARGET == HWY_SSE4
++#if HWY_TARGET == HWY_SSSE3 || HWY_TARGET == HWY_SSE4
+ 
+ template <typename T, size_t N, typename Offset>
+ HWY_API Vec128<T, N> GatherOffset(const Simd<T, N> d,
+@@ -2001,31 +2261,35 @@ HWY_API Vec128<T, N> GatherIndex(const Simd<T, N> d, const T* HWY_RESTRICT base,
+ namespace detail {
+ 
+ template <typename T, size_t N>
+-HWY_API Vec128<T, N> GatherOffset(hwy::SizeTag<4> /* tag */, Simd<T, N> /* d */,
+-                                  const T* HWY_RESTRICT base,
+-                                  const Vec128<int32_t, N> offset) {
++HWY_INLINE Vec128<T, N> GatherOffset(hwy::SizeTag<4> /* tag */,
++                                     Simd<T, N> /* d */,
++                                     const T* HWY_RESTRICT base,
++                                     const Vec128<int32_t, N> offset) {
+   return Vec128<T, N>{_mm_i32gather_epi32(
+       reinterpret_cast<const int32_t*>(base), offset.raw, 1)};
+ }
+ template <typename T, size_t N>
+-HWY_API Vec128<T, N> GatherIndex(hwy::SizeTag<4> /* tag */, Simd<T, N> /* d */,
+-                                 const T* HWY_RESTRICT base,
+-                                 const Vec128<int32_t, N> index) {
++HWY_INLINE Vec128<T, N> GatherIndex(hwy::SizeTag<4> /* tag */,
++                                    Simd<T, N> /* d */,
++                                    const T* HWY_RESTRICT base,
++                                    const Vec128<int32_t, N> index) {
+   return Vec128<T, N>{_mm_i32gather_epi32(
+       reinterpret_cast<const int32_t*>(base), index.raw, 4)};
+ }
+ 
+ template <typename T, size_t N>
+-HWY_API Vec128<T, N> GatherOffset(hwy::SizeTag<8> /* tag */, Simd<T, N> /* d */,
+-                                  const T* HWY_RESTRICT base,
+-                                  const Vec128<int64_t, N> offset) {
++HWY_INLINE Vec128<T, N> GatherOffset(hwy::SizeTag<8> /* tag */,
++                                     Simd<T, N> /* d */,
++                                     const T* HWY_RESTRICT base,
++                                     const Vec128<int64_t, N> offset) {
+   return Vec128<T, N>{_mm_i64gather_epi64(
+       reinterpret_cast<const GatherIndex64*>(base), offset.raw, 1)};
+ }
+ template <typename T, size_t N>
+-HWY_API Vec128<T, N> GatherIndex(hwy::SizeTag<8> /* tag */, Simd<T, N> /* d */,
+-                                 const T* HWY_RESTRICT base,
+-                                 const Vec128<int64_t, N> index) {
++HWY_INLINE Vec128<T, N> GatherIndex(hwy::SizeTag<8> /* tag */,
++                                    Simd<T, N> /* d */,
++                                    const T* HWY_RESTRICT base,
++                                    const Vec128<int64_t, N> index) {
+   return Vec128<T, N>{_mm_i64gather_epi64(
+       reinterpret_cast<const GatherIndex64*>(base), index.raw, 8)};
+ }
+@@ -2069,74 +2333,118 @@ HWY_API Vec128<double, N> GatherIndex(Simd<double, N> /* tag */,
+   return Vec128<double, N>{_mm_i64gather_pd(base, index.raw, 8)};
+ }
+ 
+-#endif  // HWY_TARGET != HWY_SSE4
++#endif  // HWY_TARGET == HWY_SSSE3 || HWY_TARGET == HWY_SSE4
+ 
+ HWY_DIAGNOSTICS(pop)
+ 
+-// ================================================== SWIZZLE
++// ================================================== SWIZZLE (2)
+ 
+-// ------------------------------ Extract half
++// ------------------------------ LowerHalf
+ 
+ // Returns upper/lower half of a vector.
+ template <typename T, size_t N>
+-HWY_API Vec128<T, N / 2> LowerHalf(Vec128<T, N> v) {
++HWY_API Vec128<T, N / 2> LowerHalf(Simd<T, N / 2> /* tag */, Vec128<T, N> v) {
+   return Vec128<T, N / 2>{v.raw};
+ }
+ 
+-// These copy hi into lo (smaller instruction encoding than shifts).
+-template <typename T>
+-HWY_API Vec128<T, 8 / sizeof(T)> UpperHalf(Vec128<T> v) {
+-  return Vec128<T, 8 / sizeof(T)>{_mm_unpackhi_epi64(v.raw, v.raw)};
+-}
+-template <>
+-HWY_INLINE Vec128<float, 2> UpperHalf(Vec128<float> v) {
+-  return Vec128<float, 2>{_mm_movehl_ps(v.raw, v.raw)};
+-}
+-template <>
+-HWY_INLINE Vec128<double, 1> UpperHalf(Vec128<double> v) {
+-  return Vec128<double, 1>{_mm_unpackhi_pd(v.raw, v.raw)};
++template <typename T, size_t N>
++HWY_API Vec128<T, N / 2> LowerHalf(Vec128<T, N> v) {
++  return LowerHalf(Simd<T, N / 2>(), v);
+ }
+ 
+-// ------------------------------ Shift vector by constant #bytes
++// ------------------------------ ShiftLeftBytes
+ 
+-// 0x01..0F, kBytes = 1 => 0x02..0F00
+ template <int kBytes, typename T, size_t N>
+-HWY_API Vec128<T, N> ShiftLeftBytes(const Vec128<T, N> v) {
++HWY_API Vec128<T, N> ShiftLeftBytes(Simd<T, N> /* tag */, Vec128<T, N> v) {
+   static_assert(0 <= kBytes && kBytes <= 16, "Invalid kBytes");
+   return Vec128<T, N>{_mm_slli_si128(v.raw, kBytes)};
+ }
+ 
++template <int kBytes, typename T, size_t N>
++HWY_API Vec128<T, N> ShiftLeftBytes(const Vec128<T, N> v) {
++  return ShiftLeftBytes<kBytes>(Simd<T, N>(), v);
++}
++
++// ------------------------------ ShiftLeftLanes
++
+ template <int kLanes, typename T, size_t N>
+-HWY_API Vec128<T, N> ShiftLeftLanes(const Vec128<T, N> v) {
+-  const Simd<uint8_t, N * sizeof(T)> d8;
+-  const Simd<T, N> d;
++HWY_API Vec128<T, N> ShiftLeftLanes(Simd<T, N> d, const Vec128<T, N> v) {
++  const Repartition<uint8_t, decltype(d)> d8;
+   return BitCast(d, ShiftLeftBytes<kLanes * sizeof(T)>(BitCast(d8, v)));
+ }
+ 
+-// 0x01..0F, kBytes = 1 => 0x0001..0E
++template <int kLanes, typename T, size_t N>
++HWY_API Vec128<T, N> ShiftLeftLanes(const Vec128<T, N> v) {
++  return ShiftLeftLanes<kLanes>(Simd<T, N>(), v);
++}
++
++// ------------------------------ ShiftRightBytes
+ template <int kBytes, typename T, size_t N>
+-HWY_API Vec128<T, N> ShiftRightBytes(const Vec128<T, N> v) {
++HWY_API Vec128<T, N> ShiftRightBytes(Simd<T, N> /* tag */, Vec128<T, N> v) {
+   static_assert(0 <= kBytes && kBytes <= 16, "Invalid kBytes");
++  // For partial vectors, clear upper lanes so we shift in zeros.
++  if (N != 16 / sizeof(T)) {
++    const Vec128<T> vfull{v.raw};
++    v = Vec128<T, N>{IfThenElseZero(FirstN(Full128<T>(), N), vfull).raw};
++  }
+   return Vec128<T, N>{_mm_srli_si128(v.raw, kBytes)};
+ }
+ 
++// ------------------------------ ShiftRightLanes
+ template <int kLanes, typename T, size_t N>
+-HWY_API Vec128<T, N> ShiftRightLanes(const Vec128<T, N> v) {
+-  const Simd<uint8_t, N * sizeof(T)> d8;
+-  const Simd<T, N> d;
++HWY_API Vec128<T, N> ShiftRightLanes(Simd<T, N> d, const Vec128<T, N> v) {
++  const Repartition<uint8_t, decltype(d)> d8;
+   return BitCast(d, ShiftRightBytes<kLanes * sizeof(T)>(BitCast(d8, v)));
+ }
+ 
+-// ------------------------------ Extract from 2x 128-bit at constant offset
++// ------------------------------ UpperHalf (ShiftRightBytes)
++
++// Full input: copy hi into lo (smaller instruction encoding than shifts).
++template <typename T>
++HWY_API Vec128<T, 8 / sizeof(T)> UpperHalf(Half<Full128<T>> /* tag */,
++                                           Vec128<T> v) {
++  return Vec128<T, 8 / sizeof(T)>{_mm_unpackhi_epi64(v.raw, v.raw)};
++}
++HWY_API Vec128<float, 2> UpperHalf(Simd<float, 2> /* tag */, Vec128<float> v) {
++  return Vec128<float, 2>{_mm_movehl_ps(v.raw, v.raw)};
++}
++HWY_API Vec128<double, 1> UpperHalf(Simd<double, 1> /* tag */,
++                                    Vec128<double> v) {
++  return Vec128<double, 1>{_mm_unpackhi_pd(v.raw, v.raw)};
++}
++
++// Partial
++template <typename T, size_t N, HWY_IF_LE64(T, N)>
++HWY_API Vec128<T, (N + 1) / 2> UpperHalf(Half<Simd<T, N>> /* tag */,
++                                         Vec128<T, N> v) {
++  const Simd<T, N> d;
++  const auto vu = BitCast(RebindToUnsigned<decltype(d)>(), v);
++  const auto upper = BitCast(d, ShiftRightBytes<N * sizeof(T) / 2>(vu));
++  return Vec128<T, (N + 1) / 2>{upper.raw};
++}
++
++// ------------------------------ CombineShiftRightBytes
+ 
+-// Extracts 128 bits from <hi, lo> by skipping the least-significant kBytes.
+-template <int kBytes, typename T>
+-HWY_API Vec128<T> CombineShiftRightBytes(const Vec128<T> hi,
+-                                         const Vec128<T> lo) {
+-  const Full128<uint8_t> d8;
+-  const Vec128<uint8_t> extracted_bytes{
+-      _mm_alignr_epi8(BitCast(d8, hi).raw, BitCast(d8, lo).raw, kBytes)};
+-  return BitCast(Full128<T>(), extracted_bytes);
++template <int kBytes, typename T, class V = Vec128<T>>
++HWY_API V CombineShiftRightBytes(Full128<T> d, V hi, V lo) {
++  const Repartition<uint8_t, decltype(d)> d8;
++  return BitCast(d, Vec128<uint8_t>{_mm_alignr_epi8(
++                        BitCast(d8, hi).raw, BitCast(d8, lo).raw, kBytes)});
++}
++
++template <int kBytes, typename T, size_t N, HWY_IF_LE64(T, N),
++          class V = Vec128<T, N>>
++HWY_API V CombineShiftRightBytes(Simd<T, N> d, V hi, V lo) {
++  constexpr size_t kSize = N * sizeof(T);
++  static_assert(0 < kBytes && kBytes < kSize, "kBytes invalid");
++  const Repartition<uint8_t, decltype(d)> d8;
++  const Full128<uint8_t> d_full8;
++  using V8 = VFromD<decltype(d_full8)>;
++  const V8 hi8{BitCast(d8, hi).raw};
++  // Move into most-significant bytes
++  const V8 lo8 = ShiftLeftBytes<16 - kSize>(V8{BitCast(d8, lo).raw});
++  const V8 r = CombineShiftRightBytes<16 - kSize + kBytes>(d_full8, hi8, lo8);
++  return V{BitCast(Full128<T>(), r).raw};
+ }
+ 
+ // ------------------------------ Broadcast/splat any lane
+@@ -2199,84 +2507,18 @@ HWY_API Vec128<double, N> Broadcast(const Vec128<double, N> v) {
+   return Vec128<double, N>{_mm_shuffle_pd(v.raw, v.raw, 3 * kLane)};
+ }
+ 
+-// ------------------------------ Shuffle bytes with variable indices
+-
+-// Returns vector of bytes[from[i]]. "from" is also interpreted as bytes, i.e.
+-// lane indices in [0, 16).
+-template <typename T, size_t N>
+-HWY_API Vec128<T, N> TableLookupBytes(const Vec128<T, N> bytes,
+-                                      const Vec128<T, N> from) {
+-  return Vec128<T, N>{_mm_shuffle_epi8(bytes.raw, from.raw)};
+-}
+-
+-// ------------------------------ Hard-coded shuffles
+-
+-// Notation: let Vec128<int32_t> have lanes 3,2,1,0 (0 is least-significant).
+-// Shuffle0321 rotates one lane to the right (the previous least-significant
+-// lane is now most-significant). These could also be implemented via
+-// CombineShiftRightBytes but the shuffle_abcd notation is more convenient.
+-
+-// Swap 32-bit halves in 64-bit halves.
+-HWY_API Vec128<uint32_t> Shuffle2301(const Vec128<uint32_t> v) {
+-  return Vec128<uint32_t>{_mm_shuffle_epi32(v.raw, 0xB1)};
+-}
+-HWY_API Vec128<int32_t> Shuffle2301(const Vec128<int32_t> v) {
+-  return Vec128<int32_t>{_mm_shuffle_epi32(v.raw, 0xB1)};
+-}
+-HWY_API Vec128<float> Shuffle2301(const Vec128<float> v) {
+-  return Vec128<float>{_mm_shuffle_ps(v.raw, v.raw, 0xB1)};
+-}
+-
+-// Swap 64-bit halves
+-HWY_API Vec128<uint32_t> Shuffle1032(const Vec128<uint32_t> v) {
+-  return Vec128<uint32_t>{_mm_shuffle_epi32(v.raw, 0x4E)};
+-}
+-HWY_API Vec128<int32_t> Shuffle1032(const Vec128<int32_t> v) {
+-  return Vec128<int32_t>{_mm_shuffle_epi32(v.raw, 0x4E)};
+-}
+-HWY_API Vec128<float> Shuffle1032(const Vec128<float> v) {
+-  return Vec128<float>{_mm_shuffle_ps(v.raw, v.raw, 0x4E)};
+-}
+-HWY_API Vec128<uint64_t> Shuffle01(const Vec128<uint64_t> v) {
+-  return Vec128<uint64_t>{_mm_shuffle_epi32(v.raw, 0x4E)};
+-}
+-HWY_API Vec128<int64_t> Shuffle01(const Vec128<int64_t> v) {
+-  return Vec128<int64_t>{_mm_shuffle_epi32(v.raw, 0x4E)};
+-}
+-HWY_API Vec128<double> Shuffle01(const Vec128<double> v) {
+-  return Vec128<double>{_mm_shuffle_pd(v.raw, v.raw, 1)};
+-}
+-
+-// Rotate right 32 bits
+-HWY_API Vec128<uint32_t> Shuffle0321(const Vec128<uint32_t> v) {
+-  return Vec128<uint32_t>{_mm_shuffle_epi32(v.raw, 0x39)};
+-}
+-HWY_API Vec128<int32_t> Shuffle0321(const Vec128<int32_t> v) {
+-  return Vec128<int32_t>{_mm_shuffle_epi32(v.raw, 0x39)};
+-}
+-HWY_API Vec128<float> Shuffle0321(const Vec128<float> v) {
+-  return Vec128<float>{_mm_shuffle_ps(v.raw, v.raw, 0x39)};
+-}
+-// Rotate left 32 bits
+-HWY_API Vec128<uint32_t> Shuffle2103(const Vec128<uint32_t> v) {
+-  return Vec128<uint32_t>{_mm_shuffle_epi32(v.raw, 0x93)};
+-}
+-HWY_API Vec128<int32_t> Shuffle2103(const Vec128<int32_t> v) {
+-  return Vec128<int32_t>{_mm_shuffle_epi32(v.raw, 0x93)};
+-}
+-HWY_API Vec128<float> Shuffle2103(const Vec128<float> v) {
+-  return Vec128<float>{_mm_shuffle_ps(v.raw, v.raw, 0x93)};
++// ------------------------------ TableLookupBytes
++template <typename T, size_t N, typename TI, size_t NI>
++HWY_API Vec128<TI, NI> TableLookupBytes(const Vec128<T, N> bytes,
++                                        const Vec128<TI, NI> from) {
++  return Vec128<TI, NI>{_mm_shuffle_epi8(bytes.raw, from.raw)};
+ }
+ 
+-// Reverse
+-HWY_API Vec128<uint32_t> Shuffle0123(const Vec128<uint32_t> v) {
+-  return Vec128<uint32_t>{_mm_shuffle_epi32(v.raw, 0x1B)};
+-}
+-HWY_API Vec128<int32_t> Shuffle0123(const Vec128<int32_t> v) {
+-  return Vec128<int32_t>{_mm_shuffle_epi32(v.raw, 0x1B)};
+-}
+-HWY_API Vec128<float> Shuffle0123(const Vec128<float> v) {
+-  return Vec128<float>{_mm_shuffle_ps(v.raw, v.raw, 0x1B)};
++// ------------------------------ TableLookupBytesOr0
++// For all vector widths; x86 anyway zeroes if >= 0x80.
++template <class V, class VI>
++HWY_API VI TableLookupBytesOr0(const V bytes, const VI from) {
++  return TableLookupBytes(bytes, from);
+ }
+ 
+ // ------------------------------ TableLookupLanes
+@@ -2325,55 +2567,76 @@ HWY_API Vec128<float, N> TableLookupLanes(const Vec128<float, N> v,
+                  TableLookupBytes(BitCast(di, v), Vec128<int32_t, N>{idx.raw}));
+ }
+ 
+-// ------------------------------ Interleave lanes
++// ------------------------------ InterleaveLower
+ 
+ // Interleaves lanes from halves of the 128-bit blocks of "a" (which provides
+ // the least-significant lane) and "b". To concatenate two half-width integers
+ // into one, use ZipLower/Upper instead (also works with scalar).
+ 
+-HWY_API Vec128<uint8_t> InterleaveLower(const Vec128<uint8_t> a,
+-                                        const Vec128<uint8_t> b) {
+-  return Vec128<uint8_t>{_mm_unpacklo_epi8(a.raw, b.raw)};
++template <size_t N, HWY_IF_LE128(uint8_t, N)>
++HWY_API Vec128<uint8_t, N> InterleaveLower(const Vec128<uint8_t, N> a,
++                                           const Vec128<uint8_t, N> b) {
++  return Vec128<uint8_t, N>{_mm_unpacklo_epi8(a.raw, b.raw)};
+ }
+-HWY_API Vec128<uint16_t> InterleaveLower(const Vec128<uint16_t> a,
+-                                         const Vec128<uint16_t> b) {
+-  return Vec128<uint16_t>{_mm_unpacklo_epi16(a.raw, b.raw)};
++template <size_t N, HWY_IF_LE128(uint16_t, N)>
++HWY_API Vec128<uint16_t, N> InterleaveLower(const Vec128<uint16_t, N> a,
++                                            const Vec128<uint16_t, N> b) {
++  return Vec128<uint16_t, N>{_mm_unpacklo_epi16(a.raw, b.raw)};
+ }
+-HWY_API Vec128<uint32_t> InterleaveLower(const Vec128<uint32_t> a,
+-                                         const Vec128<uint32_t> b) {
+-  return Vec128<uint32_t>{_mm_unpacklo_epi32(a.raw, b.raw)};
++template <size_t N, HWY_IF_LE128(uint32_t, N)>
++HWY_API Vec128<uint32_t, N> InterleaveLower(const Vec128<uint32_t, N> a,
++                                            const Vec128<uint32_t, N> b) {
++  return Vec128<uint32_t, N>{_mm_unpacklo_epi32(a.raw, b.raw)};
+ }
+-HWY_API Vec128<uint64_t> InterleaveLower(const Vec128<uint64_t> a,
+-                                         const Vec128<uint64_t> b) {
+-  return Vec128<uint64_t>{_mm_unpacklo_epi64(a.raw, b.raw)};
++template <size_t N, HWY_IF_LE128(uint64_t, N)>
++HWY_API Vec128<uint64_t, N> InterleaveLower(const Vec128<uint64_t, N> a,
++                                            const Vec128<uint64_t, N> b) {
++  return Vec128<uint64_t, N>{_mm_unpacklo_epi64(a.raw, b.raw)};
+ }
+ 
+-HWY_API Vec128<int8_t> InterleaveLower(const Vec128<int8_t> a,
+-                                       const Vec128<int8_t> b) {
+-  return Vec128<int8_t>{_mm_unpacklo_epi8(a.raw, b.raw)};
++template <size_t N, HWY_IF_LE128(int8_t, N)>
++HWY_API Vec128<int8_t, N> InterleaveLower(const Vec128<int8_t, N> a,
++                                          const Vec128<int8_t, N> b) {
++  return Vec128<int8_t, N>{_mm_unpacklo_epi8(a.raw, b.raw)};
+ }
+-HWY_API Vec128<int16_t> InterleaveLower(const Vec128<int16_t> a,
+-                                        const Vec128<int16_t> b) {
+-  return Vec128<int16_t>{_mm_unpacklo_epi16(a.raw, b.raw)};
++template <size_t N, HWY_IF_LE128(int16_t, N)>
++HWY_API Vec128<int16_t, N> InterleaveLower(const Vec128<int16_t, N> a,
++                                           const Vec128<int16_t, N> b) {
++  return Vec128<int16_t, N>{_mm_unpacklo_epi16(a.raw, b.raw)};
+ }
+-HWY_API Vec128<int32_t> InterleaveLower(const Vec128<int32_t> a,
+-                                        const Vec128<int32_t> b) {
+-  return Vec128<int32_t>{_mm_unpacklo_epi32(a.raw, b.raw)};
++template <size_t N, HWY_IF_LE128(int32_t, N)>
++HWY_API Vec128<int32_t, N> InterleaveLower(const Vec128<int32_t, N> a,
++                                           const Vec128<int32_t, N> b) {
++  return Vec128<int32_t, N>{_mm_unpacklo_epi32(a.raw, b.raw)};
+ }
+-HWY_API Vec128<int64_t> InterleaveLower(const Vec128<int64_t> a,
+-                                        const Vec128<int64_t> b) {
+-  return Vec128<int64_t>{_mm_unpacklo_epi64(a.raw, b.raw)};
++template <size_t N, HWY_IF_LE128(int64_t, N)>
++HWY_API Vec128<int64_t, N> InterleaveLower(const Vec128<int64_t, N> a,
++                                           const Vec128<int64_t, N> b) {
++  return Vec128<int64_t, N>{_mm_unpacklo_epi64(a.raw, b.raw)};
+ }
+ 
+-HWY_API Vec128<float> InterleaveLower(const Vec128<float> a,
+-                                      const Vec128<float> b) {
+-  return Vec128<float>{_mm_unpacklo_ps(a.raw, b.raw)};
++template <size_t N, HWY_IF_LE128(float, N)>
++HWY_API Vec128<float, N> InterleaveLower(const Vec128<float, N> a,
++                                         const Vec128<float, N> b) {
++  return Vec128<float, N>{_mm_unpacklo_ps(a.raw, b.raw)};
+ }
+-HWY_API Vec128<double> InterleaveLower(const Vec128<double> a,
+-                                       const Vec128<double> b) {
+-  return Vec128<double>{_mm_unpacklo_pd(a.raw, b.raw)};
++template <size_t N, HWY_IF_LE128(double, N)>
++HWY_API Vec128<double, N> InterleaveLower(const Vec128<double, N> a,
++                                          const Vec128<double, N> b) {
++  return Vec128<double, N>{_mm_unpacklo_pd(a.raw, b.raw)};
+ }
+ 
++// Additional overload for the optional Simd<> tag.
++template <typename T, size_t N, HWY_IF_LE128(T, N), class V = Vec128<T, N>>
++HWY_API V InterleaveLower(Simd<T, N> /* tag */, V a, V b) {
++  return InterleaveLower(a, b);
++}
++
++// ------------------------------ InterleaveUpper (UpperHalf)
++
++// All functions inside detail lack the required D parameter.
++namespace detail {
++
+ HWY_API Vec128<uint8_t> InterleaveUpper(const Vec128<uint8_t> a,
+                                         const Vec128<uint8_t> b) {
+   return Vec128<uint8_t>{_mm_unpackhi_epi8(a.raw, b.raw)};
+@@ -2417,113 +2680,151 @@ HWY_API Vec128<double> InterleaveUpper(const Vec128<double> a,
+   return Vec128<double>{_mm_unpackhi_pd(a.raw, b.raw)};
+ }
+ 
+-// ------------------------------ Zip lanes
+-
+-// Same as interleave_*, except that the return lanes are double-width integers;
+-// this is necessary because the single-lane scalar cannot return two values.
++}  // namespace detail
+ 
+-template <size_t N>
+-HWY_API Vec128<uint16_t, (N + 1) / 2> ZipLower(const Vec128<uint8_t, N> a,
+-                                               const Vec128<uint8_t, N> b) {
+-  return Vec128<uint16_t, (N + 1) / 2>{_mm_unpacklo_epi8(a.raw, b.raw)};
+-}
+-template <size_t N>
+-HWY_API Vec128<uint32_t, (N + 1) / 2> ZipLower(const Vec128<uint16_t, N> a,
+-                                               const Vec128<uint16_t, N> b) {
+-  return Vec128<uint32_t, (N + 1) / 2>{_mm_unpacklo_epi16(a.raw, b.raw)};
+-}
+-template <size_t N>
+-HWY_API Vec128<uint64_t, (N + 1) / 2> ZipLower(const Vec128<uint32_t, N> a,
+-                                               const Vec128<uint32_t, N> b) {
+-  return Vec128<uint64_t, (N + 1) / 2>{_mm_unpacklo_epi32(a.raw, b.raw)};
++// Full
++template <typename T, class V = Vec128<T>>
++HWY_API V InterleaveUpper(Full128<T> /* tag */, V a, V b) {
++  return detail::InterleaveUpper(a, b);
+ }
+ 
+-template <size_t N>
+-HWY_API Vec128<int16_t, (N + 1) / 2> ZipLower(const Vec128<int8_t, N> a,
+-                                              const Vec128<int8_t, N> b) {
+-  return Vec128<int16_t, (N + 1) / 2>{_mm_unpacklo_epi8(a.raw, b.raw)};
++// Partial
++template <typename T, size_t N, HWY_IF_LE64(T, N), class V = Vec128<T, N>>
++HWY_API V InterleaveUpper(Simd<T, N> d, V a, V b) {
++  const Half<decltype(d)> d2;
++  return InterleaveLower(d, V{UpperHalf(d2, a).raw}, V{UpperHalf(d2, b).raw});
+ }
+-template <size_t N>
+-HWY_API Vec128<int32_t, (N + 1) / 2> ZipLower(const Vec128<int16_t, N> a,
+-                                              const Vec128<int16_t, N> b) {
+-  return Vec128<int32_t, (N + 1) / 2>{_mm_unpacklo_epi16(a.raw, b.raw)};
++
++// ------------------------------ ZipLower/ZipUpper (InterleaveLower)
++
++// Same as Interleave*, except that the return lanes are double-width integers;
++// this is necessary because the single-lane scalar cannot return two values.
++template <typename T, size_t N, class DW = RepartitionToWide<Simd<T, N>>>
++HWY_API VFromD<DW> ZipLower(Vec128<T, N> a, Vec128<T, N> b) {
++  return BitCast(DW(), InterleaveLower(a, b));
+ }
+-template <size_t N>
+-HWY_API Vec128<int64_t, (N + 1) / 2> ZipLower(const Vec128<int32_t, N> a,
+-                                              const Vec128<int32_t, N> b) {
+-  return Vec128<int64_t, (N + 1) / 2>{_mm_unpacklo_epi32(a.raw, b.raw)};
++template <typename T, size_t N, class D = Simd<T, N>,
++          class DW = RepartitionToWide<D>>
++HWY_API VFromD<DW> ZipLower(DW dw, Vec128<T, N> a, Vec128<T, N> b) {
++  return BitCast(dw, InterleaveLower(D(), a, b));
+ }
+ 
+-template <size_t N>
+-HWY_API Vec128<uint16_t, (N + 1) / 2> ZipUpper(const Vec128<uint8_t, N> a,
+-                                               const Vec128<uint8_t, N> b) {
+-  return Vec128<uint16_t, (N + 1) / 2>{_mm_unpackhi_epi8(a.raw, b.raw)};
++template <typename T, size_t N, class D = Simd<T, N>,
++          class DW = RepartitionToWide<D>>
++HWY_API VFromD<DW> ZipUpper(DW dw, Vec128<T, N> a, Vec128<T, N> b) {
++  return BitCast(dw, InterleaveUpper(D(), a, b));
+ }
+-template <size_t N>
+-HWY_API Vec128<uint32_t, (N + 1) / 2> ZipUpper(const Vec128<uint16_t, N> a,
+-                                               const Vec128<uint16_t, N> b) {
+-  return Vec128<uint32_t, (N + 1) / 2>{_mm_unpackhi_epi16(a.raw, b.raw)};
+-}
+-template <size_t N>
+-HWY_API Vec128<uint64_t, (N + 1) / 2> ZipUpper(const Vec128<uint32_t, N> a,
+-                                               const Vec128<uint32_t, N> b) {
+-  return Vec128<uint64_t, (N + 1) / 2>{_mm_unpackhi_epi32(a.raw, b.raw)};
++
++// ================================================== COMBINE
++
++// ------------------------------ Combine (InterleaveLower)
++
++// N = N/2 + N/2 (upper half undefined)
++template <typename T, size_t N, HWY_IF_LE128(T, N)>
++HWY_API Vec128<T, N> Combine(Simd<T, N> d, Vec128<T, N / 2> hi_half,
++                             Vec128<T, N / 2> lo_half) {
++  const Half<decltype(d)> d2;
++  const RebindToUnsigned<decltype(d2)> du2;
++  // Treat half-width input as one lane, and expand to two lanes.
++  using VU = Vec128<UnsignedFromSize<N * sizeof(T) / 2>, 2>;
++  const VU lo{BitCast(du2, lo_half).raw};
++  const VU hi{BitCast(du2, hi_half).raw};
++  return BitCast(d, InterleaveLower(lo, hi));
+ }
+ 
+-template <size_t N>
+-HWY_API Vec128<int16_t, (N + 1) / 2> ZipUpper(const Vec128<int8_t, N> a,
+-                                              const Vec128<int8_t, N> b) {
+-  return Vec128<int16_t, (N + 1) / 2>{_mm_unpackhi_epi8(a.raw, b.raw)};
++// ------------------------------ ZeroExtendVector (Combine, IfThenElseZero)
++
++template <typename T, HWY_IF_NOT_FLOAT(T)>
++HWY_API Vec128<T> ZeroExtendVector(Full128<T> /* tag */,
++                                   Vec128<T, 8 / sizeof(T)> lo) {
++  return Vec128<T>{_mm_move_epi64(lo.raw)};
+ }
+-template <size_t N>
+-HWY_API Vec128<int32_t, (N + 1) / 2> ZipUpper(const Vec128<int16_t, N> a,
+-                                              const Vec128<int16_t, N> b) {
+-  return Vec128<int32_t, (N + 1) / 2>{_mm_unpackhi_epi16(a.raw, b.raw)};
++
++template <typename T, HWY_IF_FLOAT(T)>
++HWY_API Vec128<T> ZeroExtendVector(Full128<T> d, Vec128<T, 8 / sizeof(T)> lo) {
++  const RebindToUnsigned<decltype(d)> du;
++  return BitCast(d, ZeroExtendVector(du, BitCast(Half<decltype(du)>(), lo)));
+ }
+-template <size_t N>
+-HWY_API Vec128<int64_t, (N + 1) / 2> ZipUpper(const Vec128<int32_t, N> a,
+-                                              const Vec128<int32_t, N> b) {
+-  return Vec128<int64_t, (N + 1) / 2>{_mm_unpackhi_epi32(a.raw, b.raw)};
++
++template <typename T, size_t N, HWY_IF_LE64(T, N)>
++HWY_API Vec128<T, N> ZeroExtendVector(Simd<T, N> d, Vec128<T, N / 2> lo) {
++  return IfThenElseZero(FirstN(d, N / 2), Vec128<T, N>{lo.raw});
+ }
+ 
+-// ------------------------------ Blocks
++// ------------------------------ Concat full (InterleaveLower)
+ 
+ // hiH,hiL loH,loL |-> hiL,loL (= lower halves)
+ template <typename T>
+-HWY_API Vec128<T> ConcatLowerLower(const Vec128<T> hi, const Vec128<T> lo) {
+-  const Full128<uint64_t> d64;
+-  return BitCast(Full128<T>(),
+-                 InterleaveLower(BitCast(d64, lo), BitCast(d64, hi)));
++HWY_API Vec128<T> ConcatLowerLower(Full128<T> d, Vec128<T> hi, Vec128<T> lo) {
++  const Repartition<uint64_t, decltype(d)> d64;
++  return BitCast(d, InterleaveLower(BitCast(d64, lo), BitCast(d64, hi)));
+ }
+ 
+ // hiH,hiL loH,loL |-> hiH,loH (= upper halves)
+ template <typename T>
+-HWY_API Vec128<T> ConcatUpperUpper(const Vec128<T> hi, const Vec128<T> lo) {
+-  const Full128<uint64_t> d64;
+-  return BitCast(Full128<T>(),
+-                 InterleaveUpper(BitCast(d64, lo), BitCast(d64, hi)));
++HWY_API Vec128<T> ConcatUpperUpper(Full128<T> d, Vec128<T> hi, Vec128<T> lo) {
++  const Repartition<uint64_t, decltype(d)> d64;
++  return BitCast(d, InterleaveUpper(d64, BitCast(d64, lo), BitCast(d64, hi)));
+ }
+ 
+ // hiH,hiL loH,loL |-> hiL,loH (= inner halves)
+ template <typename T>
+-HWY_API Vec128<T> ConcatLowerUpper(const Vec128<T> hi, const Vec128<T> lo) {
+-  return CombineShiftRightBytes<8>(hi, lo);
++HWY_API Vec128<T> ConcatLowerUpper(Full128<T> d, const Vec128<T> hi,
++                                   const Vec128<T> lo) {
++  return CombineShiftRightBytes<8>(d, hi, lo);
+ }
+ 
+ // hiH,hiL loH,loL |-> hiH,loL (= outer halves)
+ template <typename T>
+-HWY_API Vec128<T> ConcatUpperLower(const Vec128<T> hi, const Vec128<T> lo) {
++HWY_API Vec128<T> ConcatUpperLower(Full128<T> d, Vec128<T> hi, Vec128<T> lo) {
++#if HWY_TARGET == HWY_SSSE3
++  const Full128<double> dd;
++  const __m128d concat = _mm_move_sd(BitCast(dd, hi).raw, BitCast(dd, lo).raw);
++  return BitCast(d, Vec128<double>{concat});
++#else
++  (void)d;
+   return Vec128<T>{_mm_blend_epi16(hi.raw, lo.raw, 0x0F)};
++#endif
+ }
+-template <>
+-HWY_INLINE Vec128<float> ConcatUpperLower(const Vec128<float> hi,
+-                                          const Vec128<float> lo) {
+-  return Vec128<float>{_mm_blend_ps(hi.raw, lo.raw, 3)};
++HWY_API Vec128<float> ConcatUpperLower(Full128<float> /* tag */,
++                                       const Vec128<float> hi,
++                                       const Vec128<float> lo) {
++  return Vec128<float>{_mm_shuffle_ps(lo.raw, hi.raw, _MM_SHUFFLE(3, 2, 1, 0))};
+ }
+-template <>
+-HWY_INLINE Vec128<double> ConcatUpperLower(const Vec128<double> hi,
+-                                           const Vec128<double> lo) {
+-  return Vec128<double>{_mm_blend_pd(hi.raw, lo.raw, 1)};
++HWY_API Vec128<double> ConcatUpperLower(Full128<double> /* tag */,
++                                        const Vec128<double> hi,
++                                        const Vec128<double> lo) {
++  return Vec128<double>{_mm_shuffle_pd(lo.raw, hi.raw, _MM_SHUFFLE2(1, 0))};
++}
++
++// ------------------------------ Concat partial (Combine, LowerHalf)
++
++template <typename T, size_t N, HWY_IF_LE64(T, N)>
++HWY_API Vec128<T, N> ConcatLowerLower(Simd<T, N> d, Vec128<T, N> hi,
++                                      Vec128<T, N> lo) {
++  const Half<decltype(d)> d2;
++  return Combine(LowerHalf(d2, hi), LowerHalf(d2, lo));
++}
++
++template <typename T, size_t N, HWY_IF_LE64(T, N)>
++HWY_API Vec128<T, N> ConcatUpperUpper(Simd<T, N> d, Vec128<T, N> hi,
++                                      Vec128<T, N> lo) {
++  const Half<decltype(d)> d2;
++  return Combine(UpperHalf(d2, hi), UpperHalf(d2, lo));
++}
++
++template <typename T, size_t N, HWY_IF_LE64(T, N)>
++HWY_API Vec128<T, N> ConcatLowerUpper(Simd<T, N> d, const Vec128<T, N> hi,
++                                      const Vec128<T, N> lo) {
++  const Half<decltype(d)> d2;
++  return Combine(LowerHalf(d2, hi), UpperHalf(d2, lo));
++}
++
++template <typename T, size_t N, HWY_IF_LE64(T, N)>
++HWY_API Vec128<T, N> ConcatUpperLower(Simd<T, N> d, Vec128<T, N> hi,
++                                      Vec128<T, N> lo) {
++  const Half<decltype(d)> d2;
++  return Combine(UpperHalf(d2, hi), LowerHalf(d2, lo));
+ }
+ 
+ // ------------------------------ OddEven (IfThenElse)
+@@ -2531,8 +2832,8 @@ HWY_INLINE Vec128<double> ConcatUpperLower(const Vec128<double> hi,
+ namespace detail {
+ 
+ template <typename T, size_t N>
+-HWY_API Vec128<T, N> OddEven(hwy::SizeTag<1> /* tag */, const Vec128<T, N> a,
+-                             const Vec128<T, N> b) {
++HWY_INLINE Vec128<T, N> OddEven(hwy::SizeTag<1> /* tag */, const Vec128<T, N> a,
++                                const Vec128<T, N> b) {
+   const Simd<T, N> d;
+   const Repartition<uint8_t, decltype(d)> d8;
+   alignas(16) constexpr uint8_t mask[16] = {0xFF, 0, 0xFF, 0, 0xFF, 0, 0xFF, 0,
+@@ -2540,19 +2841,39 @@ HWY_API Vec128<T, N> OddEven(hwy::SizeTag<1> /* tag */, const Vec128<T, N> a,
+   return IfThenElse(MaskFromVec(BitCast(d, Load(d8, mask))), b, a);
+ }
+ template <typename T, size_t N>
+-HWY_API Vec128<T, N> OddEven(hwy::SizeTag<2> /* tag */, const Vec128<T, N> a,
+-                             const Vec128<T, N> b) {
++HWY_INLINE Vec128<T, N> OddEven(hwy::SizeTag<2> /* tag */, const Vec128<T, N> a,
++                                const Vec128<T, N> b) {
++#if HWY_TARGET == HWY_SSSE3
++  const Simd<T, N> d;
++  const Repartition<uint8_t, decltype(d)> d8;
++  alignas(16) constexpr uint8_t mask[16] = {0xFF, 0xFF, 0, 0, 0xFF, 0xFF, 0, 0,
++                                            0xFF, 0xFF, 0, 0, 0xFF, 0xFF, 0, 0};
++  return IfThenElse(MaskFromVec(BitCast(d, Load(d8, mask))), b, a);
++#else
+   return Vec128<T, N>{_mm_blend_epi16(a.raw, b.raw, 0x55)};
++#endif
+ }
+ template <typename T, size_t N>
+-HWY_API Vec128<T, N> OddEven(hwy::SizeTag<4> /* tag */, const Vec128<T, N> a,
+-                             const Vec128<T, N> b) {
++HWY_INLINE Vec128<T, N> OddEven(hwy::SizeTag<4> /* tag */, const Vec128<T, N> a,
++                                const Vec128<T, N> b) {
++#if HWY_TARGET == HWY_SSSE3
++  const __m128i odd = _mm_shuffle_epi32(a.raw, _MM_SHUFFLE(3, 1, 3, 1));
++  const __m128i even = _mm_shuffle_epi32(b.raw, _MM_SHUFFLE(2, 0, 2, 0));
++  return Vec128<T, N>{_mm_unpacklo_epi32(even, odd)};
++#else
+   return Vec128<T, N>{_mm_blend_epi16(a.raw, b.raw, 0x33)};
++#endif
+ }
+ template <typename T, size_t N>
+-HWY_API Vec128<T, N> OddEven(hwy::SizeTag<8> /* tag */, const Vec128<T, N> a,
+-                             const Vec128<T, N> b) {
++HWY_INLINE Vec128<T, N> OddEven(hwy::SizeTag<8> /* tag */, const Vec128<T, N> a,
++                                const Vec128<T, N> b) {
++#if HWY_TARGET == HWY_SSSE3
++  const Full128<double> dd;
++  const __m128d concat = _mm_move_sd(BitCast(dd, a).raw, BitCast(dd, b).raw);
++  return BitCast(Full128<T>(), Vec128<double>{concat});
++#else
+   return Vec128<T, N>{_mm_blend_epi16(a.raw, b.raw, 0x0F)};
++#endif
+ }
+ 
+ }  // namespace detail
+@@ -2562,15 +2883,23 @@ HWY_API Vec128<T, N> OddEven(const Vec128<T, N> a, const Vec128<T, N> b) {
+   return detail::OddEven(hwy::SizeTag<sizeof(T)>(), a, b);
+ }
+ template <size_t N>
+-HWY_INLINE Vec128<float, N> OddEven(const Vec128<float, N> a,
+-                                    const Vec128<float, N> b) {
++HWY_API Vec128<float, N> OddEven(const Vec128<float, N> a,
++                                 const Vec128<float, N> b) {
++#if HWY_TARGET == HWY_SSSE3
++  // SHUFPS must fill the lower half of the output from one register, so we
++  // need another shuffle. Unpack avoids another immediate byte.
++  const __m128 odd = _mm_shuffle_ps(a.raw, a.raw, _MM_SHUFFLE(3, 1, 3, 1));
++  const __m128 even = _mm_shuffle_ps(b.raw, b.raw, _MM_SHUFFLE(2, 0, 2, 0));
++  return Vec128<float, N>{_mm_unpacklo_ps(even, odd)};
++#else
+   return Vec128<float, N>{_mm_blend_ps(a.raw, b.raw, 5)};
++#endif
+ }
+ 
+ template <size_t N>
+-HWY_INLINE Vec128<double, N> OddEven(const Vec128<double, N> a,
+-                                     const Vec128<double, N> b) {
+-  return Vec128<double, N>{_mm_blend_pd(a.raw, b.raw, 1)};
++HWY_API Vec128<double, N> OddEven(const Vec128<double, N> a,
++                                  const Vec128<double, N> b) {
++  return Vec128<double>{_mm_shuffle_pd(b.raw, a.raw, _MM_SHUFFLE2(1, 0))};
+ }
+ 
+ // ------------------------------ Shl (ZipLower, Mul)
+@@ -2579,21 +2908,22 @@ HWY_INLINE Vec128<double, N> OddEven(const Vec128<double, N> a,
+ // two from loading float exponents, which is considerably faster (according
+ // to LLVM-MCA) than scalar or testing bits: https://gcc.godbolt.org/z/9G7Y9v.
+ 
+-#if HWY_TARGET != HWY_AVX3
++#if HWY_TARGET > HWY_AVX3  // AVX2 or older
+ namespace detail {
+ 
+ // Returns 2^v for use as per-lane multipliers to emulate 16-bit shifts.
+ template <typename T, size_t N, HWY_IF_LANE_SIZE(T, 2)>
+-HWY_API Vec128<MakeUnsigned<T>, N> Pow2(const Vec128<T, N> v) {
++HWY_INLINE Vec128<MakeUnsigned<T>, N> Pow2(const Vec128<T, N> v) {
+   const Simd<T, N> d;
+-  const Repartition<float, decltype(d)> df;
++  const RepartitionToWide<decltype(d)> dw;
++  const Rebind<float, decltype(dw)> df;
+   const auto zero = Zero(d);
+   // Move into exponent (this u16 will become the upper half of an f32)
+   const auto exp = ShiftLeft<23 - 16>(v);
+   const auto upper = exp + Set(d, 0x3F80);  // upper half of 1.0f
+   // Insert 0 into lower halves for reinterpreting as binary32.
+-  const auto f0 = ZipLower(zero, upper);
+-  const auto f1 = ZipUpper(zero, upper);
++  const auto f0 = ZipLower(dw, zero, upper);
++  const auto f1 = ZipUpper(dw, zero, upper);
+   // See comment below.
+   const Vec128<int32_t, N> bits0{_mm_cvtps_epi32(BitCast(df, f0).raw)};
+   const Vec128<int32_t, N> bits1{_mm_cvtps_epi32(BitCast(df, f1).raw)};
+@@ -2602,7 +2932,7 @@ HWY_API Vec128<MakeUnsigned<T>, N> Pow2(const Vec128<T, N> v) {
+ 
+ // Same, for 32-bit shifts.
+ template <typename T, size_t N, HWY_IF_LANE_SIZE(T, 4)>
+-HWY_API Vec128<MakeUnsigned<T>, N> Pow2(const Vec128<T, N> v) {
++HWY_INLINE Vec128<MakeUnsigned<T>, N> Pow2(const Vec128<T, N> v) {
+   const Simd<T, N> d;
+   const auto exp = ShiftLeft<23>(v);
+   const auto f = exp + Set(d, 0x3F800000);  // 1.0f
+@@ -2613,41 +2943,52 @@ HWY_API Vec128<MakeUnsigned<T>, N> Pow2(const Vec128<T, N> v) {
+ }
+ 
+ }  // namespace detail
+-#endif  // HWY_TARGET != HWY_AVX3
++#endif  // HWY_TARGET > HWY_AVX3
+ 
+ template <size_t N>
+ HWY_API Vec128<uint16_t, N> operator<<(const Vec128<uint16_t, N> v,
+                                        const Vec128<uint16_t, N> bits) {
+-#if HWY_TARGET == HWY_AVX3
++#if HWY_TARGET <= HWY_AVX3
+   return Vec128<uint16_t, N>{_mm_sllv_epi16(v.raw, bits.raw)};
+ #else
+   return v * detail::Pow2(bits);
+ #endif
+ }
++HWY_API Vec128<uint16_t, 1> operator<<(const Vec128<uint16_t, 1> v,
++                                       const Vec128<uint16_t, 1> bits) {
++  return Vec128<uint16_t, 1>{_mm_sll_epi16(v.raw, bits.raw)};
++}
+ 
+ template <size_t N>
+ HWY_API Vec128<uint32_t, N> operator<<(const Vec128<uint32_t, N> v,
+                                        const Vec128<uint32_t, N> bits) {
+-#if HWY_TARGET == HWY_SSE4
++#if HWY_TARGET == HWY_SSSE3 || HWY_TARGET == HWY_SSE4
+   return v * detail::Pow2(bits);
+ #else
+   return Vec128<uint32_t, N>{_mm_sllv_epi32(v.raw, bits.raw)};
+ #endif
+ }
++HWY_API Vec128<uint32_t, 1> operator<<(const Vec128<uint32_t, 1> v,
++                                       const Vec128<uint32_t, 1> bits) {
++  return Vec128<uint32_t, 1>{_mm_sll_epi32(v.raw, bits.raw)};
++}
+ 
+-template <size_t N>
+-HWY_API Vec128<uint64_t, N> operator<<(const Vec128<uint64_t, N> v,
+-                                       const Vec128<uint64_t, N> bits) {
+-#if HWY_TARGET == HWY_SSE4
++HWY_API Vec128<uint64_t> operator<<(const Vec128<uint64_t> v,
++                                    const Vec128<uint64_t> bits) {
++#if HWY_TARGET == HWY_SSSE3 || HWY_TARGET == HWY_SSE4
+   // Individual shifts and combine
+-  const __m128i out0 = _mm_sll_epi64(v.raw, bits.raw);
++  const Vec128<uint64_t> out0{_mm_sll_epi64(v.raw, bits.raw)};
+   const __m128i bits1 = _mm_unpackhi_epi64(bits.raw, bits.raw);
+-  const __m128i out1 = _mm_sll_epi64(v.raw, bits1);
+-  return Vec128<uint64_t, N>{_mm_blend_epi16(out0, out1, 0xF0)};
++  const Vec128<uint64_t> out1{_mm_sll_epi64(v.raw, bits1)};
++  return ConcatUpperLower(Full128<uint64_t>(), out1, out0);
+ #else
+-  return Vec128<uint64_t, N>{_mm_sllv_epi64(v.raw, bits.raw)};
++  return Vec128<uint64_t>{_mm_sllv_epi64(v.raw, bits.raw)};
+ #endif
+ }
++HWY_API Vec128<uint64_t, 1> operator<<(const Vec128<uint64_t, 1> v,
++                                       const Vec128<uint64_t, 1> bits) {
++  return Vec128<uint64_t, 1>{_mm_sll_epi64(v.raw, bits.raw)};
++}
+ 
+ // Signed left shift is the same as unsigned.
+ template <typename T, size_t N, HWY_IF_SIGNED(T)>
+@@ -2659,7 +3000,7 @@ HWY_API Vec128<T, N> operator<<(const Vec128<T, N> v, const Vec128<T, N> bits) {
+ 
+ // ------------------------------ Shr (mul, mask, BroadcastSignBit)
+ 
+-// Use AVX2+ variable shifts except for the SSE4 target or 16-bit. There, we use
++// Use AVX2+ variable shifts except for SSSE3/SSE4 or 16-bit. There, we use
+ // widening multiplication by powers of two obtained by loading float exponents,
+ // followed by a constant right-shift. This is still faster than a scalar or
+ // bit-test approach: https://gcc.godbolt.org/z/9G7Y9v.
+@@ -2667,7 +3008,7 @@ HWY_API Vec128<T, N> operator<<(const Vec128<T, N> v, const Vec128<T, N> bits) {
+ template <size_t N>
+ HWY_API Vec128<uint16_t, N> operator>>(const Vec128<uint16_t, N> in,
+                                        const Vec128<uint16_t, N> bits) {
+-#if HWY_TARGET == HWY_AVX3
++#if HWY_TARGET <= HWY_AVX3
+   return Vec128<uint16_t, N>{_mm_srlv_epi16(in.raw, bits.raw)};
+ #else
+   const Simd<uint16_t, N> d;
+@@ -2677,11 +3018,15 @@ HWY_API Vec128<uint16_t, N> operator>>(const Vec128<uint16_t, N> in,
+   return IfThenElse(bits == Zero(d), in, out);
+ #endif
+ }
++HWY_API Vec128<uint16_t, 1> operator>>(const Vec128<uint16_t, 1> in,
++                                       const Vec128<uint16_t, 1> bits) {
++  return Vec128<uint16_t, 1>{_mm_srl_epi16(in.raw, bits.raw)};
++}
+ 
+ template <size_t N>
+ HWY_API Vec128<uint32_t, N> operator>>(const Vec128<uint32_t, N> in,
+                                        const Vec128<uint32_t, N> bits) {
+-#if HWY_TARGET == HWY_SSE4
++#if HWY_TARGET == HWY_SSSE3 || HWY_TARGET == HWY_SSE4
+   // 32x32 -> 64 bit mul, then shift right by 32.
+   const Simd<uint32_t, N> d32;
+   // Move odd lanes into position for the second mul. Shuffle more gracefully
+@@ -2692,36 +3037,42 @@ HWY_API Vec128<uint32_t, N> operator>>(const Vec128<uint32_t, N> in,
+   const auto out20 = ShiftRight<32>(MulEven(in, mul));  // z 2 z 0
+   const Vec128<uint32_t, N> mul31{_mm_shuffle_epi32(mul.raw, 0x31)};
+   // No need to shift right, already in the correct position.
+-  const auto out31 = MulEven(in31, mul31);  // 3 ? 1 ?
+-  // OddEven is defined below, avoid the dependency.
+-  const Vec128<uint32_t, N> out{_mm_blend_epi16(out31.raw, out20.raw, 0x33)};
++  const auto out31 = BitCast(d32, MulEven(in31, mul31));  // 3 ? 1 ?
++  const Vec128<uint32_t, N> out = OddEven(out31, BitCast(d32, out20));
+   // Replace output with input where bits == 0.
+   return IfThenElse(bits == Zero(d32), in, out);
+ #else
+   return Vec128<uint32_t, N>{_mm_srlv_epi32(in.raw, bits.raw)};
+ #endif
+ }
++HWY_API Vec128<uint32_t, 1> operator>>(const Vec128<uint32_t, 1> in,
++                                       const Vec128<uint32_t, 1> bits) {
++  return Vec128<uint32_t, 1>{_mm_srl_epi32(in.raw, bits.raw)};
++}
+ 
+-template <size_t N>
+-HWY_API Vec128<uint64_t, N> operator>>(const Vec128<uint64_t, N> v,
+-                                       const Vec128<uint64_t, N> bits) {
+-#if HWY_TARGET == HWY_SSE4
++HWY_API Vec128<uint64_t> operator>>(const Vec128<uint64_t> v,
++                                    const Vec128<uint64_t> bits) {
++#if HWY_TARGET == HWY_SSSE3 || HWY_TARGET == HWY_SSE4
+   // Individual shifts and combine
+-  const __m128i out0 = _mm_srl_epi64(v.raw, bits.raw);
++  const Vec128<uint64_t> out0{_mm_srl_epi64(v.raw, bits.raw)};
+   const __m128i bits1 = _mm_unpackhi_epi64(bits.raw, bits.raw);
+-  const __m128i out1 = _mm_srl_epi64(v.raw, bits1);
+-  return Vec128<uint64_t, N>{_mm_blend_epi16(out0, out1, 0xF0)};
++  const Vec128<uint64_t> out1{_mm_srl_epi64(v.raw, bits1)};
++  return ConcatUpperLower(Full128<uint64_t>(), out1, out0);
+ #else
+-  return Vec128<uint64_t, N>{_mm_srlv_epi64(v.raw, bits.raw)};
++  return Vec128<uint64_t>{_mm_srlv_epi64(v.raw, bits.raw)};
+ #endif
+ }
++HWY_API Vec128<uint64_t, 1> operator>>(const Vec128<uint64_t, 1> v,
++                                       const Vec128<uint64_t, 1> bits) {
++  return Vec128<uint64_t, 1>{_mm_srl_epi64(v.raw, bits.raw)};
++}
+ 
+-#if HWY_TARGET != HWY_AVX3
++#if HWY_TARGET > HWY_AVX3  // AVX2 or older
+ namespace detail {
+ 
+ // Also used in x86_256-inl.h.
+ template <class DI, class V>
+-HWY_API V SignedShr(const DI di, const V v, const V count_i) {
++HWY_INLINE V SignedShr(const DI di, const V v, const V count_i) {
+   const RebindToUnsigned<DI> du;
+   const auto count = BitCast(du, count_i);  // same type as value to shift
+   // Clear sign and restore afterwards. This is preferable to shifting the MSB
+@@ -2732,38 +3083,64 @@ HWY_API V SignedShr(const DI di, const V v, const V count_i) {
+ }
+ 
+ }  // namespace detail
+-#endif  // HWY_TARGET != HWY_AVX3
++#endif  // HWY_TARGET > HWY_AVX3
+ 
+ template <size_t N>
+ HWY_API Vec128<int16_t, N> operator>>(const Vec128<int16_t, N> v,
+                                       const Vec128<int16_t, N> bits) {
+-#if HWY_TARGET == HWY_AVX3
++#if HWY_TARGET <= HWY_AVX3
+   return Vec128<int16_t, N>{_mm_srav_epi16(v.raw, bits.raw)};
+ #else
+   return detail::SignedShr(Simd<int16_t, N>(), v, bits);
+ #endif
+ }
++HWY_API Vec128<int16_t, 1> operator>>(const Vec128<int16_t, 1> v,
++                                      const Vec128<int16_t, 1> bits) {
++  return Vec128<int16_t, 1>{_mm_sra_epi16(v.raw, bits.raw)};
++}
+ 
+ template <size_t N>
+ HWY_API Vec128<int32_t, N> operator>>(const Vec128<int32_t, N> v,
+                                       const Vec128<int32_t, N> bits) {
+-#if HWY_TARGET == HWY_AVX3
++#if HWY_TARGET <= HWY_AVX3
+   return Vec128<int32_t, N>{_mm_srav_epi32(v.raw, bits.raw)};
+ #else
+   return detail::SignedShr(Simd<int32_t, N>(), v, bits);
+ #endif
+ }
++HWY_API Vec128<int32_t, 1> operator>>(const Vec128<int32_t, 1> v,
++                                      const Vec128<int32_t, 1> bits) {
++  return Vec128<int32_t, 1>{_mm_sra_epi32(v.raw, bits.raw)};
++}
+ 
+ template <size_t N>
+ HWY_API Vec128<int64_t, N> operator>>(const Vec128<int64_t, N> v,
+                                       const Vec128<int64_t, N> bits) {
+-#if HWY_TARGET == HWY_AVX3
++#if HWY_TARGET <= HWY_AVX3
+   return Vec128<int64_t, N>{_mm_srav_epi64(v.raw, bits.raw)};
+ #else
+   return detail::SignedShr(Simd<int64_t, N>(), v, bits);
+ #endif
+ }
+ 
++// ------------------------------ MulEven/Odd 64x64 (UpperHalf)
++
++HWY_INLINE Vec128<uint64_t> MulEven(const Vec128<uint64_t> a,
++                                    const Vec128<uint64_t> b) {
++  alignas(16) uint64_t mul[2];
++  mul[0] = Mul128(GetLane(a), GetLane(b), &mul[1]);
++  return Load(Full128<uint64_t>(), mul);
++}
++
++HWY_INLINE Vec128<uint64_t> MulOdd(const Vec128<uint64_t> a,
++                                   const Vec128<uint64_t> b) {
++  alignas(16) uint64_t mul[2];
++  const Half<Full128<uint64_t>> d2;
++  mul[0] =
++      Mul128(GetLane(UpperHalf(d2, a)), GetLane(UpperHalf(d2, b)), &mul[1]);
++  return Load(Full128<uint64_t>(), mul);
++}
++
+ // ================================================== CONVERT
+ 
+ // ------------------------------ Promotions (part w/ narrow lanes -> full)
+@@ -2772,59 +3149,98 @@ HWY_API Vec128<int64_t, N> operator>>(const Vec128<int64_t, N> v,
+ template <size_t N>
+ HWY_API Vec128<uint16_t, N> PromoteTo(Simd<uint16_t, N> /* tag */,
+                                       const Vec128<uint8_t, N> v) {
++#if HWY_TARGET == HWY_SSSE3
++  const __m128i zero = _mm_setzero_si128();
++  return Vec128<uint16_t, N>{_mm_unpacklo_epi8(v.raw, zero)};
++#else
+   return Vec128<uint16_t, N>{_mm_cvtepu8_epi16(v.raw)};
++#endif
+ }
+ template <size_t N>
+ HWY_API Vec128<uint32_t, N> PromoteTo(Simd<uint32_t, N> /* tag */,
+-                                      const Vec128<uint8_t, N> v) {
+-  return Vec128<uint32_t, N>{_mm_cvtepu8_epi32(v.raw)};
++                                      const Vec128<uint16_t, N> v) {
++#if HWY_TARGET == HWY_SSSE3
++  return Vec128<uint32_t, N>{_mm_unpacklo_epi16(v.raw, _mm_setzero_si128())};
++#else
++  return Vec128<uint32_t, N>{_mm_cvtepu16_epi32(v.raw)};
++#endif
+ }
+ template <size_t N>
+-HWY_API Vec128<int16_t, N> PromoteTo(Simd<int16_t, N> /* tag */,
+-                                     const Vec128<uint8_t, N> v) {
+-  return Vec128<int16_t, N>{_mm_cvtepu8_epi16(v.raw)};
++HWY_API Vec128<uint64_t, N> PromoteTo(Simd<uint64_t, N> /* tag */,
++                                      const Vec128<uint32_t, N> v) {
++#if HWY_TARGET == HWY_SSSE3
++  return Vec128<uint64_t, N>{_mm_unpacklo_epi32(v.raw, _mm_setzero_si128())};
++#else
++  return Vec128<uint64_t, N>{_mm_cvtepu32_epi64(v.raw)};
++#endif
+ }
+ template <size_t N>
+-HWY_API Vec128<int32_t, N> PromoteTo(Simd<int32_t, N> /* tag */,
+-                                     const Vec128<uint8_t, N> v) {
+-  return Vec128<int32_t, N>{_mm_cvtepu8_epi32(v.raw)};
++HWY_API Vec128<uint32_t, N> PromoteTo(Simd<uint32_t, N> /* tag */,
++                                      const Vec128<uint8_t, N> v) {
++#if HWY_TARGET == HWY_SSSE3
++  const __m128i zero = _mm_setzero_si128();
++  const __m128i u16 = _mm_unpacklo_epi8(v.raw, zero);
++  return Vec128<uint32_t, N>{_mm_unpacklo_epi16(u16, zero)};
++#else
++  return Vec128<uint32_t, N>{_mm_cvtepu8_epi32(v.raw)};
++#endif
+ }
++
++// Unsigned to signed: same plus cast.
+ template <size_t N>
+-HWY_API Vec128<uint32_t, N> PromoteTo(Simd<uint32_t, N> /* tag */,
+-                                      const Vec128<uint16_t, N> v) {
+-  return Vec128<uint32_t, N>{_mm_cvtepu16_epi32(v.raw)};
++HWY_API Vec128<int16_t, N> PromoteTo(Simd<int16_t, N> di,
++                                     const Vec128<uint8_t, N> v) {
++  return BitCast(di, PromoteTo(Simd<uint16_t, N>(), v));
+ }
+ template <size_t N>
+-HWY_API Vec128<int32_t, N> PromoteTo(Simd<int32_t, N> /* tag */,
++HWY_API Vec128<int32_t, N> PromoteTo(Simd<int32_t, N> di,
+                                      const Vec128<uint16_t, N> v) {
+-  return Vec128<int32_t, N>{_mm_cvtepu16_epi32(v.raw)};
++  return BitCast(di, PromoteTo(Simd<uint32_t, N>(), v));
+ }
+ template <size_t N>
+-HWY_API Vec128<uint64_t, N> PromoteTo(Simd<uint64_t, N> /* tag */,
+-                                      const Vec128<uint32_t, N> v) {
+-  return Vec128<uint64_t, N>{_mm_cvtepu32_epi64(v.raw)};
++HWY_API Vec128<int32_t, N> PromoteTo(Simd<int32_t, N> di,
++                                     const Vec128<uint8_t, N> v) {
++  return BitCast(di, PromoteTo(Simd<uint32_t, N>(), v));
+ }
+ 
+ // Signed: replicate sign bit.
+ template <size_t N>
+ HWY_API Vec128<int16_t, N> PromoteTo(Simd<int16_t, N> /* tag */,
+                                      const Vec128<int8_t, N> v) {
++#if HWY_TARGET == HWY_SSSE3
++  return ShiftRight<8>(Vec128<int16_t, N>{_mm_unpacklo_epi8(v.raw, v.raw)});
++#else
+   return Vec128<int16_t, N>{_mm_cvtepi8_epi16(v.raw)};
+-}
+-template <size_t N>
+-HWY_API Vec128<int32_t, N> PromoteTo(Simd<int32_t, N> /* tag */,
+-                                     const Vec128<int8_t, N> v) {
+-  return Vec128<int32_t, N>{_mm_cvtepi8_epi32(v.raw)};
++#endif
+ }
+ template <size_t N>
+ HWY_API Vec128<int32_t, N> PromoteTo(Simd<int32_t, N> /* tag */,
+                                      const Vec128<int16_t, N> v) {
++#if HWY_TARGET == HWY_SSSE3
++  return ShiftRight<16>(Vec128<int32_t, N>{_mm_unpacklo_epi16(v.raw, v.raw)});
++#else
+   return Vec128<int32_t, N>{_mm_cvtepi16_epi32(v.raw)};
++#endif
+ }
+ template <size_t N>
+ HWY_API Vec128<int64_t, N> PromoteTo(Simd<int64_t, N> /* tag */,
+                                      const Vec128<int32_t, N> v) {
++#if HWY_TARGET == HWY_SSSE3
++  return ShiftRight<32>(Vec128<int64_t, N>{_mm_unpacklo_epi32(v.raw, v.raw)});
++#else
+   return Vec128<int64_t, N>{_mm_cvtepi32_epi64(v.raw)};
++#endif
++}
++template <size_t N>
++HWY_API Vec128<int32_t, N> PromoteTo(Simd<int32_t, N> /* tag */,
++                                     const Vec128<int8_t, N> v) {
++#if HWY_TARGET == HWY_SSSE3
++  const __m128i x2 = _mm_unpacklo_epi8(v.raw, v.raw);
++  const __m128i x4 = _mm_unpacklo_epi16(x2, x2);
++  return ShiftRight<24>(Vec128<int32_t, N>{x4});
++#else
++  return Vec128<int32_t, N>{_mm_cvtepi8_epi32(v.raw)};
++#endif
+ }
+ 
+ // Workaround for origin tracking bug in Clang msan prior to 11.0
+@@ -2836,12 +3252,11 @@ HWY_API Vec128<int64_t, N> PromoteTo(Simd<int64_t, N> /* tag */,
+ #define HWY_INLINE_F16 HWY_INLINE
+ #endif
+ template <size_t N>
+-HWY_INLINE_F16 Vec128<float, N> PromoteTo(Simd<float, N> /* tag */,
++HWY_INLINE_F16 Vec128<float, N> PromoteTo(Simd<float, N> df32,
+                                           const Vec128<float16_t, N> v) {
+-#if HWY_TARGET == HWY_SSE4
+-  const Simd<int32_t, N> di32;
+-  const Simd<uint32_t, N> du32;
+-  const Simd<float, N> df32;
++#if HWY_TARGET >= HWY_SSE4 || defined(HWY_DISABLE_F16C)
++  const RebindToSigned<decltype(df32)> di32;
++  const RebindToUnsigned<decltype(df32)> du32;
+   // Expand to u32 so we can shift.
+   const auto bits16 = PromoteTo(du32, Vec128<uint16_t, N>{v.raw});
+   const auto sign = ShiftRight<15>(bits16);
+@@ -2857,6 +3272,7 @@ HWY_INLINE_F16 Vec128<float, N> PromoteTo(Simd<float, N> /* tag */,
+   const auto bits32 = IfThenElse(biased_exp == Zero(du32), subnormal, normal);
+   return BitCast(df32, ShiftLeft<31>(sign) | bits32);
+ #else
++  (void)df32;
+   return Vec128<float, N>{_mm_cvtph_ps(v.raw)};
+ #endif
+ }
+@@ -2878,7 +3294,20 @@ HWY_API Vec128<double, N> PromoteTo(Simd<double, N> /* tag */,
+ template <size_t N>
+ HWY_API Vec128<uint16_t, N> DemoteTo(Simd<uint16_t, N> /* tag */,
+                                      const Vec128<int32_t, N> v) {
++#if HWY_TARGET == HWY_SSSE3
++  const Simd<int32_t, N> di32;
++  const Simd<uint16_t, N * 2> du16;
++  const auto zero_if_neg = AndNot(ShiftRight<31>(v), v);
++  const auto too_big = VecFromMask(di32, Gt(v, Set(di32, 0xFFFF)));
++  const auto clamped = Or(zero_if_neg, too_big);
++  // Lower 2 bytes from each 32-bit lane; same as return type for fewer casts.
++  alignas(16) constexpr uint16_t kLower2Bytes[16] = {
++      0x0100, 0x0504, 0x0908, 0x0D0C, 0x8080, 0x8080, 0x8080, 0x8080};
++  const auto lo2 = Load(du16, kLower2Bytes);
++  return Vec128<uint16_t, N>{TableLookupBytes(BitCast(du16, clamped), lo2).raw};
++#else
+   return Vec128<uint16_t, N>{_mm_packus_epi32(v.raw, v.raw)};
++#endif
+ }
+ 
+ template <size_t N>
+@@ -2890,10 +3319,7 @@ HWY_API Vec128<int16_t, N> DemoteTo(Simd<int16_t, N> /* tag */,
+ template <size_t N>
+ HWY_API Vec128<uint8_t, N> DemoteTo(Simd<uint8_t, N> /* tag */,
+                                     const Vec128<int32_t, N> v) {
+-  const __m128i u16 = _mm_packus_epi32(v.raw, v.raw);
+-  // packus treats the input as signed; we want unsigned. Clear the MSB to get
+-  // unsigned saturation to u8.
+-  const __m128i i16 = _mm_and_si128(u16, _mm_set1_epi16(0x7FFF));
++  const __m128i i16 = _mm_packs_epi32(v.raw, v.raw);
+   return Vec128<uint8_t, N>{_mm_packus_epi16(i16, i16)};
+ }
+ 
+@@ -2917,13 +3343,12 @@ HWY_API Vec128<int8_t, N> DemoteTo(Simd<int8_t, N> /* tag */,
+ }
+ 
+ template <size_t N>
+-HWY_INLINE Vec128<float16_t, N> DemoteTo(Simd<float16_t, N> /* tag */,
+-                                         const Vec128<float, N> v) {
+-#if HWY_TARGET == HWY_SSE4
+-  const Simd<int32_t, N> di;
+-  const Simd<uint32_t, N> du;
+-  const Simd<uint16_t, N> du16;
+-  const Simd<float16_t, N> df16;
++HWY_API Vec128<float16_t, N> DemoteTo(Simd<float16_t, N> df16,
++                                      const Vec128<float, N> v) {
++#if HWY_TARGET >= HWY_SSE4 || defined(HWY_DISABLE_F16C)
++  const RebindToUnsigned<decltype(df16)> du16;
++  const Rebind<uint32_t, decltype(df16)> du;
++  const RebindToSigned<decltype(du)> di;
+   const auto bits32 = BitCast(du, v);
+   const auto sign = ShiftRight<31>(bits32);
+   const auto biased_exp32 = ShiftRight<23>(bits32) & Set(du, 0xFF);
+@@ -2947,13 +3372,14 @@ HWY_INLINE Vec128<float16_t, N> DemoteTo(Simd<float16_t, N> /* tag */,
+   const auto bits16 = IfThenZeroElse(is_tiny, BitCast(di, normal16));
+   return BitCast(df16, DemoteTo(du16, bits16));
+ #else
++  (void)df16;
+   return Vec128<float16_t, N>{_mm_cvtps_ph(v.raw, _MM_FROUND_NO_EXC)};
+ #endif
+ }
+ 
+ template <size_t N>
+-HWY_INLINE Vec128<float, N> DemoteTo(Simd<float, N> /* tag */,
+-                                     const Vec128<double, N> v) {
++HWY_API Vec128<float, N> DemoteTo(Simd<float, N> /* tag */,
++                                  const Vec128<double, N> v) {
+   return Vec128<float, N>{_mm_cvtpd_ps(v.raw)};
+ }
+ 
+@@ -2962,7 +3388,7 @@ namespace detail {
+ // For well-defined float->int demotion in all x86_*-inl.h.
+ 
+ template <size_t N>
+-HWY_API auto ClampF64ToI32Max(Simd<double, N> d, decltype(Zero(d)) v)
++HWY_INLINE auto ClampF64ToI32Max(Simd<double, N> d, decltype(Zero(d)) v)
+     -> decltype(Zero(d)) {
+   // The max can be exactly represented in binary64, so clamping beforehand
+   // prevents x86 conversion from raising an exception and returning 80..00.
+@@ -2973,9 +3399,9 @@ HWY_API auto ClampF64ToI32Max(Simd<double, N> d, decltype(Zero(d)) v)
+ // change the result because the max integer value is not exactly representable.
+ // Instead detect the overflow result after conversion and fix it.
+ template <typename TI, size_t N, class DF = Simd<MakeFloat<TI>, N>>
+-HWY_API auto FixConversionOverflow(Simd<TI, N> di,
+-                                   decltype(Zero(DF())) original,
+-                                   decltype(Zero(di).raw) converted_raw)
++HWY_INLINE auto FixConversionOverflow(Simd<TI, N> di,
++                                      decltype(Zero(DF())) original,
++                                      decltype(Zero(di).raw) converted_raw)
+     -> decltype(Zero(di)) {
+   // Combinations of original and output sign:
+   //   --: normal <0 or -huge_val to 80..00: OK
+@@ -2990,8 +3416,8 @@ HWY_API auto FixConversionOverflow(Simd<TI, N> di,
+ }  // namespace detail
+ 
+ template <size_t N>
+-HWY_INLINE Vec128<int32_t, N> DemoteTo(Simd<int32_t, N> /* tag */,
+-                                       const Vec128<double, N> v) {
++HWY_API Vec128<int32_t, N> DemoteTo(Simd<int32_t, N> /* tag */,
++                                    const Vec128<double, N> v) {
+   const auto clamped = detail::ClampF64ToI32Max(Simd<double, N>(), v);
+   return Vec128<int32_t, N>{_mm_cvttpd_epi32(clamped.raw)};
+ }
+@@ -3019,7 +3445,7 @@ HWY_API Vec128<float, N> ConvertTo(Simd<float, N> /* tag */,
+ template <size_t N>
+ HWY_API Vec128<double, N> ConvertTo(Simd<double, N> dd,
+                                     const Vec128<int64_t, N> v) {
+-#if HWY_TARGET == HWY_AVX3
++#if HWY_TARGET <= HWY_AVX3
+   (void)dd;
+   return Vec128<double, N>{_mm_cvtepi64_pd(v.raw)};
+ #else
+@@ -3047,25 +3473,61 @@ HWY_API Vec128<int32_t, N> ConvertTo(const Simd<int32_t, N> di,
+   return detail::FixConversionOverflow(di, v, _mm_cvttps_epi32(v.raw));
+ }
+ 
+-template <size_t N>
+-HWY_API Vec128<int64_t, N> ConvertTo(Simd<int64_t, N> di,
+-                                     const Vec128<double, N> v) {
+-#if HWY_TARGET == HWY_AVX3
++// Full (partial handled below)
++HWY_API Vec128<int64_t> ConvertTo(Full128<int64_t> di, const Vec128<double> v) {
++#if HWY_TARGET <= HWY_AVX3 && HWY_ARCH_X86_64
+   return detail::FixConversionOverflow(di, v, _mm_cvttpd_epi64(v.raw));
++#elif HWY_ARCH_X86_64
++  const __m128i i0 = _mm_cvtsi64_si128(_mm_cvttsd_si64(v.raw));
++  const Half<Full128<double>> dd2;
++  const __m128i i1 = _mm_cvtsi64_si128(_mm_cvttsd_si64(UpperHalf(dd2, v).raw));
++  return detail::FixConversionOverflow(di, v, _mm_unpacklo_epi64(i0, i1));
+ #else
+-  alignas(16) double lanes_d[2];
+-  Store(v, Simd<double, N>(), lanes_d);
+-  alignas(16) int64_t lanes_i[2];
+-  for (size_t i = 0; i < N; ++i) {
+-    if (lanes_d[i] >= static_cast<double>(LimitsMax<int64_t>())) {
+-      lanes_i[i] = LimitsMax<int64_t>();
+-    } else if (lanes_d[i] <= static_cast<double>(LimitsMin<int64_t>())) {
+-      lanes_i[i] = LimitsMin<int64_t>();
+-    } else {
+-      lanes_i[i] = static_cast<int64_t>(lanes_d[i]);
+-    }
+-  }
+-  return Load(di, lanes_i);
++  using VI = decltype(Zero(di));
++  const VI k0 = Zero(di);
++  const VI k1 = Set(di, 1);
++  const VI k51 = Set(di, 51);
++
++  // Exponent indicates whether the number can be represented as int64_t.
++  const VI biased_exp = ShiftRight<52>(BitCast(di, v)) & Set(di, 0x7FF);
++  const VI exp = biased_exp - Set(di, 0x3FF);
++  const auto in_range = exp < Set(di, 63);
++
++  // If we were to cap the exponent at 51 and add 2^52, the number would be in
++  // [2^52, 2^53) and mantissa bits could be read out directly. We need to
++  // round-to-0 (truncate), but changing rounding mode in MXCSR hits a
++  // compiler reordering bug: https://gcc.godbolt.org/z/4hKj6c6qc . We instead
++  // manually shift the mantissa into place (we already have many of the
++  // inputs anyway).
++  const VI shift_mnt = Max(k51 - exp, k0);
++  const VI shift_int = Max(exp - k51, k0);
++  const VI mantissa = BitCast(di, v) & Set(di, (1ULL << 52) - 1);
++  // Include implicit 1-bit; shift by one more to ensure it's in the mantissa.
++  const VI int52 = (mantissa | Set(di, 1ULL << 52)) >> (shift_mnt + k1);
++  // For inputs larger than 2^52, insert zeros at the bottom.
++  const VI shifted = int52 << shift_int;
++  // Restore the one bit lost when shifting in the implicit 1-bit.
++  const VI restored = shifted | ((mantissa & k1) << (shift_int - k1));
++
++  // Saturate to LimitsMin (unchanged when negating below) or LimitsMax.
++  const VI sign_mask = BroadcastSignBit(BitCast(di, v));
++  const VI limit = Set(di, LimitsMax<int64_t>()) - sign_mask;
++  const VI magnitude = IfThenElse(in_range, restored, limit);
++
++  // If the input was negative, negate the integer (two's complement).
++  return (magnitude ^ sign_mask) - sign_mask;
++#endif
++}
++HWY_API Vec128<int64_t, 1> ConvertTo(Simd<int64_t, 1> di,
++                                     const Vec128<double, 1> v) {
++  // Only need to specialize for non-AVX3, 64-bit (single scalar op)
++#if HWY_TARGET > HWY_AVX3 && HWY_ARCH_X86_64
++  const Vec128<int64_t, 1> i0{_mm_cvtsi64_si128(_mm_cvttsd_si64(v.raw))};
++  return detail::FixConversionOverflow(di, v, i0.raw);
++#else
++  (void)di;
++  const auto full = ConvertTo(Full128<int64_t>(), Vec128<double>{v.raw});
++  return Vec128<int64_t, 1>{full.raw};
+ #endif
+ }
+ 
+@@ -3075,11 +3537,166 @@ HWY_API Vec128<int32_t, N> NearestInt(const Vec128<float, N> v) {
+   return detail::FixConversionOverflow(di, v, _mm_cvtps_epi32(v.raw));
+ }
+ 
++// ------------------------------ Floating-point rounding (ConvertTo)
++
++#if HWY_TARGET == HWY_SSSE3
++
++// Toward nearest integer, ties to even
++template <typename T, size_t N, HWY_IF_FLOAT(T)>
++HWY_API Vec128<T, N> Round(const Vec128<T, N> v) {
++  // Rely on rounding after addition with a large value such that no mantissa
++  // bits remain (assuming the current mode is nearest-even). We may need a
++  // compiler flag for precise floating-point to prevent "optimizing" this out.
++  const Simd<T, N> df;
++  const auto max = Set(df, MantissaEnd<T>());
++  const auto large = CopySignToAbs(max, v);
++  const auto added = large + v;
++  const auto rounded = added - large;
++  // Keep original if NaN or the magnitude is large (already an int).
++  return IfThenElse(Abs(v) < max, rounded, v);
++}
++
++namespace detail {
++
++// Truncating to integer and converting back to float is correct except when the
++// input magnitude is large, in which case the input was already an integer
++// (because mantissa >> exponent is zero).
++template <typename T, size_t N, HWY_IF_FLOAT(T)>
++HWY_INLINE Mask128<T, N> UseInt(const Vec128<T, N> v) {
++  return Abs(v) < Set(Simd<T, N>(), MantissaEnd<T>());
++}
++
++}  // namespace detail
++
++// Toward zero, aka truncate
++template <typename T, size_t N, HWY_IF_FLOAT(T)>
++HWY_API Vec128<T, N> Trunc(const Vec128<T, N> v) {
++  const Simd<T, N> df;
++  const RebindToSigned<decltype(df)> di;
++
++  const auto integer = ConvertTo(di, v);  // round toward 0
++  const auto int_f = ConvertTo(df, integer);
++
++  return IfThenElse(detail::UseInt(v), CopySign(int_f, v), v);
++}
++
++// Toward +infinity, aka ceiling
++template <typename T, size_t N, HWY_IF_FLOAT(T)>
++HWY_API Vec128<T, N> Ceil(const Vec128<T, N> v) {
++  const Simd<T, N> df;
++  const RebindToSigned<decltype(df)> di;
++
++  const auto integer = ConvertTo(di, v);  // round toward 0
++  const auto int_f = ConvertTo(df, integer);
++
++  // Truncating a positive non-integer ends up smaller; if so, add 1.
++  const auto neg1 = ConvertTo(df, VecFromMask(di, RebindMask(di, int_f < v)));
++
++  return IfThenElse(detail::UseInt(v), int_f - neg1, v);
++}
++
++// Toward -infinity, aka floor
++template <typename T, size_t N, HWY_IF_FLOAT(T)>
++HWY_API Vec128<T, N> Floor(const Vec128<T, N> v) {
++  const Simd<T, N> df;
++  const RebindToSigned<decltype(df)> di;
++
++  const auto integer = ConvertTo(di, v);  // round toward 0
++  const auto int_f = ConvertTo(df, integer);
++
++  // Truncating a negative non-integer ends up larger; if so, subtract 1.
++  const auto neg1 = ConvertTo(df, VecFromMask(di, RebindMask(di, int_f > v)));
++
++  return IfThenElse(detail::UseInt(v), int_f + neg1, v);
++}
++
++#else
++
++// Toward nearest integer, ties to even
++template <size_t N>
++HWY_API Vec128<float, N> Round(const Vec128<float, N> v) {
++  return Vec128<float, N>{
++      _mm_round_ps(v.raw, _MM_FROUND_TO_NEAREST_INT | _MM_FROUND_NO_EXC)};
++}
++template <size_t N>
++HWY_API Vec128<double, N> Round(const Vec128<double, N> v) {
++  return Vec128<double, N>{
++      _mm_round_pd(v.raw, _MM_FROUND_TO_NEAREST_INT | _MM_FROUND_NO_EXC)};
++}
++
++// Toward zero, aka truncate
++template <size_t N>
++HWY_API Vec128<float, N> Trunc(const Vec128<float, N> v) {
++  return Vec128<float, N>{
++      _mm_round_ps(v.raw, _MM_FROUND_TO_ZERO | _MM_FROUND_NO_EXC)};
++}
++template <size_t N>
++HWY_API Vec128<double, N> Trunc(const Vec128<double, N> v) {
++  return Vec128<double, N>{
++      _mm_round_pd(v.raw, _MM_FROUND_TO_ZERO | _MM_FROUND_NO_EXC)};
++}
++
++// Toward +infinity, aka ceiling
++template <size_t N>
++HWY_API Vec128<float, N> Ceil(const Vec128<float, N> v) {
++  return Vec128<float, N>{
++      _mm_round_ps(v.raw, _MM_FROUND_TO_POS_INF | _MM_FROUND_NO_EXC)};
++}
++template <size_t N>
++HWY_API Vec128<double, N> Ceil(const Vec128<double, N> v) {
++  return Vec128<double, N>{
++      _mm_round_pd(v.raw, _MM_FROUND_TO_POS_INF | _MM_FROUND_NO_EXC)};
++}
++
++// Toward -infinity, aka floor
++template <size_t N>
++HWY_API Vec128<float, N> Floor(const Vec128<float, N> v) {
++  return Vec128<float, N>{
++      _mm_round_ps(v.raw, _MM_FROUND_TO_NEG_INF | _MM_FROUND_NO_EXC)};
++}
++template <size_t N>
++HWY_API Vec128<double, N> Floor(const Vec128<double, N> v) {
++  return Vec128<double, N>{
++      _mm_round_pd(v.raw, _MM_FROUND_TO_NEG_INF | _MM_FROUND_NO_EXC)};
++}
++
++#endif  // !HWY_SSSE3
++
++// ================================================== CRYPTO
++
++#if !defined(HWY_DISABLE_PCLMUL_AES) && HWY_TARGET != HWY_SSSE3
++
++// Per-target flag to prevent generic_ops-inl.h from defining AESRound.
++#ifdef HWY_NATIVE_AES
++#undef HWY_NATIVE_AES
++#else
++#define HWY_NATIVE_AES
++#endif
++
++HWY_API Vec128<uint8_t> AESRound(Vec128<uint8_t> state,
++                                 Vec128<uint8_t> round_key) {
++  return Vec128<uint8_t>{_mm_aesenc_si128(state.raw, round_key.raw)};
++}
++
++template <size_t N, HWY_IF_LE128(uint64_t, N)>
++HWY_API Vec128<uint64_t, N> CLMulLower(Vec128<uint64_t, N> a,
++                                       Vec128<uint64_t, N> b) {
++  return Vec128<uint64_t, N>{_mm_clmulepi64_si128(a.raw, b.raw, 0x00)};
++}
++
++template <size_t N, HWY_IF_LE128(uint64_t, N)>
++HWY_API Vec128<uint64_t, N> CLMulUpper(Vec128<uint64_t, N> a,
++                                       Vec128<uint64_t, N> b) {
++  return Vec128<uint64_t, N>{_mm_clmulepi64_si128(a.raw, b.raw, 0x11)};
++}
++
++#endif  // !defined(HWY_DISABLE_PCLMUL_AES) && HWY_TARGET != HWY_SSSE3
++
+ // ================================================== MISC
+ 
+ // Returns a vector with lane i=[0, N) set to "first" + i.
+ template <typename T, size_t N, typename T2, HWY_IF_LE128(T, N)>
+-Vec128<T, N> Iota(const Simd<T, N> d, const T2 first) {
++HWY_API Vec128<T, N> Iota(const Simd<T, N> d, const T2 first) {
+   HWY_ALIGN T lanes[16 / sizeof(T)];
+   for (size_t i = 0; i < 16 / sizeof(T); ++i) {
+     lanes[i] = static_cast<T>(first + static_cast<T2>(i));
+@@ -3096,24 +3713,24 @@ constexpr HWY_INLINE uint64_t U64FromInt(int bits) {
+ }
+ 
+ template <typename T, size_t N>
+-HWY_API uint64_t BitsFromMask(hwy::SizeTag<1> /*tag*/,
+-                              const Mask128<T, N> mask) {
++HWY_INLINE uint64_t BitsFromMask(hwy::SizeTag<1> /*tag*/,
++                                 const Mask128<T, N> mask) {
+   const Simd<T, N> d;
+   const auto sign_bits = BitCast(d, VecFromMask(d, mask)).raw;
+   return U64FromInt(_mm_movemask_epi8(sign_bits));
+ }
+ 
+ template <typename T, size_t N>
+-HWY_API uint64_t BitsFromMask(hwy::SizeTag<2> /*tag*/,
+-                              const Mask128<T, N> mask) {
++HWY_INLINE uint64_t BitsFromMask(hwy::SizeTag<2> /*tag*/,
++                                 const Mask128<T, N> mask) {
+   // Remove useless lower half of each u16 while preserving the sign bit.
+   const auto sign_bits = _mm_packs_epi16(mask.raw, _mm_setzero_si128());
+   return U64FromInt(_mm_movemask_epi8(sign_bits));
+ }
+ 
+ template <typename T, size_t N>
+-HWY_API uint64_t BitsFromMask(hwy::SizeTag<4> /*tag*/,
+-                              const Mask128<T, N> mask) {
++HWY_INLINE uint64_t BitsFromMask(hwy::SizeTag<4> /*tag*/,
++                                 const Mask128<T, N> mask) {
+   const Simd<T, N> d;
+   const Simd<float, N> df;
+   const auto sign_bits = BitCast(df, VecFromMask(d, mask));
+@@ -3121,8 +3738,8 @@ HWY_API uint64_t BitsFromMask(hwy::SizeTag<4> /*tag*/,
+ }
+ 
+ template <typename T, size_t N>
+-HWY_API uint64_t BitsFromMask(hwy::SizeTag<8> /*tag*/,
+-                              const Mask128<T, N> mask) {
++HWY_INLINE uint64_t BitsFromMask(hwy::SizeTag<8> /*tag*/,
++                                 const Mask128<T, N> mask) {
+   const Simd<T, N> d;
+   const Simd<double, N> df;
+   const auto sign_bits = BitCast(df, VecFromMask(d, mask));
+@@ -3136,38 +3753,46 @@ constexpr uint64_t OnlyActive(uint64_t bits) {
+ }
+ 
+ template <typename T, size_t N>
+-HWY_API uint64_t BitsFromMask(const Mask128<T, N> mask) {
++HWY_INLINE uint64_t BitsFromMask(const Mask128<T, N> mask) {
+   return OnlyActive<T, N>(BitsFromMask(hwy::SizeTag<sizeof(T)>(), mask));
+ }
+ 
+ }  // namespace detail
+ 
+ template <typename T, size_t N>
+-HWY_INLINE size_t StoreMaskBits(const Mask128<T, N> mask, uint8_t* p) {
++HWY_API size_t StoreMaskBits(const Simd<T, N> /* tag */,
++                             const Mask128<T, N> mask, uint8_t* p) {
+   const uint64_t bits = detail::BitsFromMask(mask);
+-  const size_t kNumBytes = (N + 7)/8;
++  const size_t kNumBytes = (N + 7) / 8;
+   CopyBytes<kNumBytes>(&bits, p);
+   return kNumBytes;
+ }
+ 
+ template <typename T, size_t N>
+-HWY_API bool AllFalse(const Mask128<T, N> mask) {
++HWY_API bool AllFalse(const Simd<T, N> /* tag */, const Mask128<T, N> mask) {
+   // Cheaper than PTEST, which is 2 uop / 3L.
+   return detail::BitsFromMask(mask) == 0;
+ }
+ 
+ template <typename T, size_t N>
+-HWY_API bool AllTrue(const Mask128<T, N> mask) {
++HWY_API bool AllTrue(const Simd<T, N> /* tag */, const Mask128<T, N> mask) {
+   constexpr uint64_t kAllBits =
+       detail::OnlyActive<T, N>((1ull << (16 / sizeof(T))) - 1);
+   return detail::BitsFromMask(mask) == kAllBits;
+ }
+ 
+ template <typename T, size_t N>
+-HWY_API size_t CountTrue(const Mask128<T, N> mask) {
++HWY_API size_t CountTrue(const Simd<T, N> /* tag */, const Mask128<T, N> mask) {
+   return PopCount(detail::BitsFromMask(mask));
+ }
+ 
++template <typename T, size_t N>
++HWY_API intptr_t FindFirstTrue(const Simd<T, N> /* tag */,
++                               const Mask128<T, N> mask) {
++  const uint64_t bits = detail::BitsFromMask(mask);
++  return bits ? Num0BitsBelowLS1Bit_Nonzero64(bits) : -1;
++}
++
+ // ------------------------------ Compress
+ 
+ namespace detail {
+@@ -3356,8 +3981,8 @@ HWY_INLINE Vec128<T, N> Idx64x2FromBits(const uint64_t mask_bits) {
+ // redundant BitsFromMask in the latter.
+ 
+ template <typename T, size_t N>
+-HWY_API Vec128<T, N> Compress(hwy::SizeTag<2> /*tag*/, Vec128<T, N> v,
+-                              const uint64_t mask_bits) {
++HWY_INLINE Vec128<T, N> Compress(hwy::SizeTag<2> /*tag*/, Vec128<T, N> v,
++                                 const uint64_t mask_bits) {
+   const auto idx = detail::Idx16x8FromBits<T, N>(mask_bits);
+   using D = Simd<T, N>;
+   const RebindToSigned<D> di;
+@@ -3365,12 +3990,12 @@ HWY_API Vec128<T, N> Compress(hwy::SizeTag<2> /*tag*/, Vec128<T, N> v,
+ }
+ 
+ template <typename T, size_t N>
+-HWY_API Vec128<T, N> Compress(hwy::SizeTag<4> /*tag*/, Vec128<T, N> v,
+-                              const uint64_t mask_bits) {
++HWY_INLINE Vec128<T, N> Compress(hwy::SizeTag<4> /*tag*/, Vec128<T, N> v,
++                                 const uint64_t mask_bits) {
+   using D = Simd<T, N>;
+   using TI = MakeSigned<T>;
+   const Rebind<TI, D> di;
+-#if HWY_TARGET == HWY_AVX3
++#if HWY_TARGET <= HWY_AVX3
+   return BitCast(D(), Vec128<TI, N>{_mm_maskz_compress_epi32(
+                           mask_bits, BitCast(di, v).raw)});
+ #else
+@@ -3380,12 +4005,12 @@ HWY_API Vec128<T, N> Compress(hwy::SizeTag<4> /*tag*/, Vec128<T, N> v,
+ }
+ 
+ template <typename T, size_t N>
+-HWY_API Vec128<T, N> Compress(hwy::SizeTag<8> /*tag*/, Vec128<T, N> v,
+-                              const uint64_t mask_bits) {
++HWY_INLINE Vec128<T, N> Compress(hwy::SizeTag<8> /*tag*/, Vec128<T, N> v,
++                                 const uint64_t mask_bits) {
+   using D = Simd<T, N>;
+   using TI = MakeSigned<T>;
+   const Rebind<TI, D> di;
+-#if HWY_TARGET == HWY_AVX3
++#if HWY_TARGET <= HWY_AVX3
+   return BitCast(D(), Vec128<TI, N>{_mm_maskz_compress_epi64(
+                           mask_bits, BitCast(di, v).raw)});
+ #else
+@@ -3434,7 +4059,7 @@ HWY_API void StoreInterleaved3(const Vec128<uint8_t> v0,
+       0x80, 2, 0x80, 0x80, 3, 0x80, 0x80, 4, 0x80, 0x80};
+   const auto shuf_r0 = Load(d, tbl_r0);
+   const auto shuf_g0 = Load(d, tbl_g0);  // cannot reuse r0 due to 5 in MSB
+-  const auto shuf_b0 = CombineShiftRightBytes<15>(shuf_g0, shuf_g0);
++  const auto shuf_b0 = CombineShiftRightBytes<15>(d, shuf_g0, shuf_g0);
+   const auto r0 = TableLookupBytes(v0, shuf_r0);  // 5..4..3..2..1..0
+   const auto g0 = TableLookupBytes(v1, shuf_g0);  // ..4..3..2..1..0.
+   const auto b0 = TableLookupBytes(v2, shuf_b0);  // .4..3..2..1..0..
+@@ -3486,7 +4111,7 @@ HWY_API void StoreInterleaved3(const Vec128<uint8_t, 8> v0,
+       0x80, 2, 0x80, 0x80, 3, 0x80, 0x80, 4, 0x80, 0x80};
+   const auto shuf_r0 = Load(d_full, tbl_r0);
+   const auto shuf_g0 = Load(d_full, tbl_g0);  // cannot reuse r0 due to 5 in MSB
+-  const auto shuf_b0 = CombineShiftRightBytes<15>(shuf_g0, shuf_g0);
++  const auto shuf_b0 = CombineShiftRightBytes<15>(d_full, shuf_g0, shuf_g0);
+   const auto r0 = TableLookupBytes(full_a, shuf_r0);  // 5..4..3..2..1..0
+   const auto g0 = TableLookupBytes(full_b, shuf_g0);  // ..4..3..2..1..0.
+   const auto b0 = TableLookupBytes(full_c, shuf_b0);  // .4..3..2..1..0..
+@@ -3524,8 +4149,8 @@ HWY_API void StoreInterleaved3(const Vec128<uint8_t, N> v0,
+       0,    0x80, 0x80, 1,   0x80, 0x80, 2, 0x80, 0x80, 3, 0x80, 0x80,  //
+       0x80, 0x80, 0x80, 0x80};
+   const auto shuf_r0 = Load(d_full, tbl_r0);
+-  const auto shuf_g0 = CombineShiftRightBytes<15>(shuf_r0, shuf_r0);
+-  const auto shuf_b0 = CombineShiftRightBytes<14>(shuf_r0, shuf_r0);
++  const auto shuf_g0 = CombineShiftRightBytes<15>(d_full, shuf_r0, shuf_r0);
++  const auto shuf_b0 = CombineShiftRightBytes<14>(d_full, shuf_r0, shuf_r0);
+   const auto r0 = TableLookupBytes(full_a, shuf_r0);  // ......3..2..1..0
+   const auto g0 = TableLookupBytes(full_b, shuf_g0);  // .....3..2..1..0.
+   const auto b0 = TableLookupBytes(full_c, shuf_b0);  // ....3..2..1..0..
+@@ -3541,21 +4166,23 @@ HWY_API void StoreInterleaved3(const Vec128<uint8_t, N> v0,
+ HWY_API void StoreInterleaved4(const Vec128<uint8_t> v0,
+                                const Vec128<uint8_t> v1,
+                                const Vec128<uint8_t> v2,
+-                               const Vec128<uint8_t> v3, Full128<uint8_t> d,
++                               const Vec128<uint8_t> v3, Full128<uint8_t> d8,
+                                uint8_t* HWY_RESTRICT unaligned) {
++  const RepartitionToWide<decltype(d8)> d16;
++  const RepartitionToWide<decltype(d16)> d32;
+   // let a,b,c,d denote v0..3.
+-  const auto ba0 = ZipLower(v0, v1);  // b7 a7 .. b0 a0
+-  const auto dc0 = ZipLower(v2, v3);  // d7 c7 .. d0 c0
+-  const auto ba8 = ZipUpper(v0, v1);
+-  const auto dc8 = ZipUpper(v2, v3);
+-  const auto dcba_0 = ZipLower(ba0, dc0);  // d..a3 d..a0
+-  const auto dcba_4 = ZipUpper(ba0, dc0);  // d..a7 d..a4
+-  const auto dcba_8 = ZipLower(ba8, dc8);  // d..aB d..a8
+-  const auto dcba_C = ZipUpper(ba8, dc8);  // d..aF d..aC
+-  StoreU(BitCast(d, dcba_0), d, unaligned + 0 * 16);
+-  StoreU(BitCast(d, dcba_4), d, unaligned + 1 * 16);
+-  StoreU(BitCast(d, dcba_8), d, unaligned + 2 * 16);
+-  StoreU(BitCast(d, dcba_C), d, unaligned + 3 * 16);
++  const auto ba0 = ZipLower(d16, v0, v1);  // b7 a7 .. b0 a0
++  const auto dc0 = ZipLower(d16, v2, v3);  // d7 c7 .. d0 c0
++  const auto ba8 = ZipUpper(d16, v0, v1);
++  const auto dc8 = ZipUpper(d16, v2, v3);
++  const auto dcba_0 = ZipLower(d32, ba0, dc0);  // d..a3 d..a0
++  const auto dcba_4 = ZipUpper(d32, ba0, dc0);  // d..a7 d..a4
++  const auto dcba_8 = ZipLower(d32, ba8, dc8);  // d..aB d..a8
++  const auto dcba_C = ZipUpper(d32, ba8, dc8);  // d..aF d..aC
++  StoreU(BitCast(d8, dcba_0), d8, unaligned + 0 * 16);
++  StoreU(BitCast(d8, dcba_4), d8, unaligned + 1 * 16);
++  StoreU(BitCast(d8, dcba_8), d8, unaligned + 2 * 16);
++  StoreU(BitCast(d8, dcba_C), d8, unaligned + 3 * 16);
+ }
+ 
+ // 64 bits
+@@ -3566,18 +4193,20 @@ HWY_API void StoreInterleaved4(const Vec128<uint8_t, 8> in0,
+                                Simd<uint8_t, 8> /*tag*/,
+                                uint8_t* HWY_RESTRICT unaligned) {
+   // Use full vectors to reduce the number of stores.
++  const Full128<uint8_t> d_full8;
++  const RepartitionToWide<decltype(d_full8)> d16;
++  const RepartitionToWide<decltype(d16)> d32;
+   const Vec128<uint8_t> v0{in0.raw};
+   const Vec128<uint8_t> v1{in1.raw};
+   const Vec128<uint8_t> v2{in2.raw};
+   const Vec128<uint8_t> v3{in3.raw};
+   // let a,b,c,d denote v0..3.
+-  const auto ba0 = ZipLower(v0, v1);       // b7 a7 .. b0 a0
+-  const auto dc0 = ZipLower(v2, v3);       // d7 c7 .. d0 c0
+-  const auto dcba_0 = ZipLower(ba0, dc0);  // d..a3 d..a0
+-  const auto dcba_4 = ZipUpper(ba0, dc0);  // d..a7 d..a4
+-  const Full128<uint8_t> d_full;
+-  StoreU(BitCast(d_full, dcba_0), d_full, unaligned + 0 * 16);
+-  StoreU(BitCast(d_full, dcba_4), d_full, unaligned + 1 * 16);
++  const auto ba0 = ZipLower(d16, v0, v1);       // b7 a7 .. b0 a0
++  const auto dc0 = ZipLower(d16, v2, v3);       // d7 c7 .. d0 c0
++  const auto dcba_0 = ZipLower(d32, ba0, dc0);  // d..a3 d..a0
++  const auto dcba_4 = ZipUpper(d32, ba0, dc0);  // d..a7 d..a4
++  StoreU(BitCast(d_full8, dcba_0), d_full8, unaligned + 0 * 16);
++  StoreU(BitCast(d_full8, dcba_4), d_full8, unaligned + 1 * 16);
+ }
+ 
+ // <= 32 bits
+@@ -3589,17 +4218,19 @@ HWY_API void StoreInterleaved4(const Vec128<uint8_t, N> in0,
+                                Simd<uint8_t, N> /*tag*/,
+                                uint8_t* HWY_RESTRICT unaligned) {
+   // Use full vectors to reduce the number of stores.
++  const Full128<uint8_t> d_full8;
++  const RepartitionToWide<decltype(d_full8)> d16;
++  const RepartitionToWide<decltype(d16)> d32;
+   const Vec128<uint8_t> v0{in0.raw};
+   const Vec128<uint8_t> v1{in1.raw};
+   const Vec128<uint8_t> v2{in2.raw};
+   const Vec128<uint8_t> v3{in3.raw};
+   // let a,b,c,d denote v0..3.
+-  const auto ba0 = ZipLower(v0, v1);       // b3 a3 .. b0 a0
+-  const auto dc0 = ZipLower(v2, v3);       // d3 c3 .. d0 c0
+-  const auto dcba_0 = ZipLower(ba0, dc0);  // d..a3 d..a0
++  const auto ba0 = ZipLower(d16, v0, v1);       // b3 a3 .. b0 a0
++  const auto dc0 = ZipLower(d16, v2, v3);       // d3 c3 .. d0 c0
++  const auto dcba_0 = ZipLower(d32, ba0, dc0);  // d..a3 d..a0
+   alignas(16) uint8_t buf[16];
+-  const Full128<uint8_t> d_full;
+-  StoreU(BitCast(d_full, dcba_0), d_full, buf);
++  StoreU(BitCast(d_full8, dcba_0), d_full8, buf);
+   CopyBytes<4 * N>(buf, unaligned);
+ }
+ 
+@@ -3609,18 +4240,18 @@ namespace detail {
+ 
+ // N=1 for any T: no-op
+ template <typename T>
+-HWY_API Vec128<T, 1> SumOfLanes(hwy::SizeTag<sizeof(T)> /* tag */,
+-                                const Vec128<T, 1> v) {
++HWY_INLINE Vec128<T, 1> SumOfLanes(hwy::SizeTag<sizeof(T)> /* tag */,
++                                   const Vec128<T, 1> v) {
+   return v;
+ }
+ template <typename T>
+-HWY_API Vec128<T, 1> MinOfLanes(hwy::SizeTag<sizeof(T)> /* tag */,
+-                                const Vec128<T, 1> v) {
++HWY_INLINE Vec128<T, 1> MinOfLanes(hwy::SizeTag<sizeof(T)> /* tag */,
++                                   const Vec128<T, 1> v) {
+   return v;
+ }
+ template <typename T>
+-HWY_API Vec128<T, 1> MaxOfLanes(hwy::SizeTag<sizeof(T)> /* tag */,
+-                                const Vec128<T, 1> v) {
++HWY_INLINE Vec128<T, 1> MaxOfLanes(hwy::SizeTag<sizeof(T)> /* tag */,
++                                   const Vec128<T, 1> v) {
+   return v;
+ }
+ 
+@@ -3628,38 +4259,41 @@ HWY_API Vec128<T, 1> MaxOfLanes(hwy::SizeTag<sizeof(T)> /* tag */,
+ 
+ // N=2
+ template <typename T>
+-HWY_API Vec128<T, 2> SumOfLanes(hwy::SizeTag<4> /* tag */,
+-                                const Vec128<T, 2> v10) {
+-  return v10 + Vec128<T, 2>{Shuffle2301(Vec128<T>{v10.raw}).raw};
++HWY_INLINE Vec128<T, 2> SumOfLanes(hwy::SizeTag<4> /* tag */,
++                                   const Vec128<T, 2> v10) {
++  return v10 + Shuffle2301(v10);
+ }
+ template <typename T>
+-HWY_API Vec128<T, 2> MinOfLanes(hwy::SizeTag<4> /* tag */,
+-                                const Vec128<T, 2> v10) {
+-  return Min(v10, Vec128<T, 2>{Shuffle2301(Vec128<T>{v10.raw}).raw});
++HWY_INLINE Vec128<T, 2> MinOfLanes(hwy::SizeTag<4> /* tag */,
++                                   const Vec128<T, 2> v10) {
++  return Min(v10, Shuffle2301(v10));
+ }
+ template <typename T>
+-HWY_API Vec128<T, 2> MaxOfLanes(hwy::SizeTag<4> /* tag */,
+-                                const Vec128<T, 2> v10) {
+-  return Max(v10, Vec128<T, 2>{Shuffle2301(Vec128<T>{v10.raw}).raw});
++HWY_INLINE Vec128<T, 2> MaxOfLanes(hwy::SizeTag<4> /* tag */,
++                                   const Vec128<T, 2> v10) {
++  return Max(v10, Shuffle2301(v10));
+ }
+ 
+ // N=4 (full)
+ template <typename T>
+-HWY_API Vec128<T> SumOfLanes(hwy::SizeTag<4> /* tag */, const Vec128<T> v3210) {
++HWY_INLINE Vec128<T> SumOfLanes(hwy::SizeTag<4> /* tag */,
++                                const Vec128<T> v3210) {
+   const Vec128<T> v1032 = Shuffle1032(v3210);
+   const Vec128<T> v31_20_31_20 = v3210 + v1032;
+   const Vec128<T> v20_31_20_31 = Shuffle0321(v31_20_31_20);
+   return v20_31_20_31 + v31_20_31_20;
+ }
+ template <typename T>
+-HWY_API Vec128<T> MinOfLanes(hwy::SizeTag<4> /* tag */, const Vec128<T> v3210) {
++HWY_INLINE Vec128<T> MinOfLanes(hwy::SizeTag<4> /* tag */,
++                                const Vec128<T> v3210) {
+   const Vec128<T> v1032 = Shuffle1032(v3210);
+   const Vec128<T> v31_20_31_20 = Min(v3210, v1032);
+   const Vec128<T> v20_31_20_31 = Shuffle0321(v31_20_31_20);
+   return Min(v20_31_20_31, v31_20_31_20);
+ }
+ template <typename T>
+-HWY_API Vec128<T> MaxOfLanes(hwy::SizeTag<4> /* tag */, const Vec128<T> v3210) {
++HWY_INLINE Vec128<T> MaxOfLanes(hwy::SizeTag<4> /* tag */,
++                                const Vec128<T> v3210) {
+   const Vec128<T> v1032 = Shuffle1032(v3210);
+   const Vec128<T> v31_20_31_20 = Max(v3210, v1032);
+   const Vec128<T> v20_31_20_31 = Shuffle0321(v31_20_31_20);
+@@ -3670,17 +4304,20 @@ HWY_API Vec128<T> MaxOfLanes(hwy::SizeTag<4> /* tag */, const Vec128<T> v3210) {
+ 
+ // N=2 (full)
+ template <typename T>
+-HWY_API Vec128<T> SumOfLanes(hwy::SizeTag<8> /* tag */, const Vec128<T> v10) {
++HWY_INLINE Vec128<T> SumOfLanes(hwy::SizeTag<8> /* tag */,
++                                const Vec128<T> v10) {
+   const Vec128<T> v01 = Shuffle01(v10);
+   return v10 + v01;
+ }
+ template <typename T>
+-HWY_API Vec128<T> MinOfLanes(hwy::SizeTag<8> /* tag */, const Vec128<T> v10) {
++HWY_INLINE Vec128<T> MinOfLanes(hwy::SizeTag<8> /* tag */,
++                                const Vec128<T> v10) {
+   const Vec128<T> v01 = Shuffle01(v10);
+   return Min(v10, v01);
+ }
+ template <typename T>
+-HWY_API Vec128<T> MaxOfLanes(hwy::SizeTag<8> /* tag */, const Vec128<T> v10) {
++HWY_INLINE Vec128<T> MaxOfLanes(hwy::SizeTag<8> /* tag */,
++                                const Vec128<T> v10) {
+   const Vec128<T> v01 = Shuffle01(v10);
+   return Max(v10, v01);
+ }
+@@ -3689,18 +4326,114 @@ HWY_API Vec128<T> MaxOfLanes(hwy::SizeTag<8> /* tag */, const Vec128<T> v10) {
+ 
+ // Supported for u/i/f 32/64. Returns the same value in each lane.
+ template <typename T, size_t N>
+-HWY_API Vec128<T, N> SumOfLanes(const Vec128<T, N> v) {
++HWY_API Vec128<T, N> SumOfLanes(Simd<T, N> /* tag */, const Vec128<T, N> v) {
+   return detail::SumOfLanes(hwy::SizeTag<sizeof(T)>(), v);
+ }
+ template <typename T, size_t N>
+-HWY_API Vec128<T, N> MinOfLanes(const Vec128<T, N> v) {
++HWY_API Vec128<T, N> MinOfLanes(Simd<T, N> /* tag */, const Vec128<T, N> v) {
+   return detail::MinOfLanes(hwy::SizeTag<sizeof(T)>(), v);
+ }
+ template <typename T, size_t N>
+-HWY_API Vec128<T, N> MaxOfLanes(const Vec128<T, N> v) {
++HWY_API Vec128<T, N> MaxOfLanes(Simd<T, N> /* tag */, const Vec128<T, N> v) {
+   return detail::MaxOfLanes(hwy::SizeTag<sizeof(T)>(), v);
+ }
+ 
++// ================================================== DEPRECATED
++
++template <typename T, size_t N>
++HWY_API size_t StoreMaskBits(const Mask128<T, N> mask, uint8_t* p) {
++  return StoreMaskBits(Simd<T, N>(), mask, p);
++}
++
++template <typename T, size_t N>
++HWY_API bool AllTrue(const Mask128<T, N> mask) {
++  return AllTrue(Simd<T, N>(), mask);
++}
++
++template <typename T, size_t N>
++HWY_API bool AllFalse(const Mask128<T, N> mask) {
++  return AllFalse(Simd<T, N>(), mask);
++}
++
++template <typename T, size_t N>
++HWY_API size_t CountTrue(const Mask128<T, N> mask) {
++  return CountTrue(Simd<T, N>(), mask);
++}
++
++template <typename T, size_t N>
++HWY_API Vec128<T, N> SumOfLanes(const Vec128<T, N> v) {
++  return SumOfLanes(Simd<T, N>(), v);
++}
++template <typename T, size_t N>
++HWY_API Vec128<T, N> MinOfLanes(const Vec128<T, N> v) {
++  return MinOfLanes(Simd<T, N>(), v);
++}
++template <typename T, size_t N>
++HWY_API Vec128<T, N> MaxOfLanes(const Vec128<T, N> v) {
++  return MaxOfLanes(Simd<T, N>(), v);
++}
++
++template <typename T, size_t N>
++HWY_API Vec128<T, (N + 1) / 2> UpperHalf(Vec128<T, N> v) {
++  return UpperHalf(Half<Simd<T, N>>(), v);
++}
++
++template <int kBytes, typename T, size_t N>
++HWY_API Vec128<T, N> ShiftRightBytes(const Vec128<T, N> v) {
++  return ShiftRightBytes<kBytes>(Simd<T, N>(), v);
++}
++
++template <int kLanes, typename T, size_t N>
++HWY_API Vec128<T, N> ShiftRightLanes(const Vec128<T, N> v) {
++  return ShiftRightLanes<kLanes>(Simd<T, N>(), v);
++}
++
++template <size_t kBytes, typename T, size_t N>
++HWY_API Vec128<T, N> CombineShiftRightBytes(Vec128<T, N> hi, Vec128<T, N> lo) {
++  return CombineShiftRightBytes<kBytes>(Simd<T, N>(), hi, lo);
++}
++
++template <typename T, size_t N>
++HWY_API Vec128<T, N> InterleaveUpper(Vec128<T, N> a, Vec128<T, N> b) {
++  return InterleaveUpper(Simd<T, N>(), a, b);
++}
++
++template <typename T, size_t N, class D = Simd<T, N>>
++HWY_API VFromD<RepartitionToWide<D>> ZipUpper(Vec128<T, N> a, Vec128<T, N> b) {
++  return InterleaveUpper(RepartitionToWide<D>(), a, b);
++}
++
++template <typename T, size_t N2>
++HWY_API Vec128<T, N2 * 2> Combine(Vec128<T, N2> hi2, Vec128<T, N2> lo2) {
++  return Combine(Simd<T, N2 * 2>(), hi2, lo2);
++}
++
++template <typename T, size_t N2, HWY_IF_LE64(T, N2)>
++HWY_API Vec128<T, N2 * 2> ZeroExtendVector(Vec128<T, N2> lo) {
++  return ZeroExtendVector(Simd<T, N2 * 2>(), lo);
++}
++
++template <typename T, size_t N>
++HWY_API Vec128<T, N> ConcatLowerLower(Vec128<T, N> hi, Vec128<T, N> lo) {
++  return ConcatLowerLower(Simd<T, N>(), hi, lo);
++}
++
++template <typename T, size_t N>
++HWY_API Vec128<T, N> ConcatUpperUpper(Vec128<T, N> hi, Vec128<T, N> lo) {
++  return ConcatUpperUpper(Simd<T, N>(), hi, lo);
++}
++
++template <typename T, size_t N>
++HWY_API Vec128<T, N> ConcatLowerUpper(const Vec128<T, N> hi,
++                                      const Vec128<T, N> lo) {
++  return ConcatLowerUpper(Simd<T, N>(), hi, lo);
++}
++
++template <typename T, size_t N>
++HWY_API Vec128<T, N> ConcatUpperLower(Vec128<T, N> hi, Vec128<T, N> lo) {
++  return ConcatUpperLower(Simd<T, N>(), hi, lo);
++}
++
+ // ================================================== Operator wrapper
+ 
+ // These apply to all x86_*-inl.h because there are no restrictions on V.
+@@ -3737,6 +4470,10 @@ HWY_API auto Eq(V a, V b) -> decltype(a == b) {
+   return a == b;
+ }
+ template <class V>
++HWY_API auto Ne(V a, V b) -> decltype(a == b) {
++  return a != b;
++}
++template <class V>
+ HWY_API auto Lt(V a, V b) -> decltype(a == b) {
+   return a < b;
+ }
+diff --git a/third_party/highway/hwy/ops/x86_256-inl.h b/third_party/highway/hwy/ops/x86_256-inl.h
+index b934140f0c72f..4ff7aabca350a 100644
+--- a/third_party/highway/hwy/ops/x86_256-inl.h
++++ b/third_party/highway/hwy/ops/x86_256-inl.h
+@@ -20,7 +20,6 @@
+ // particular, "Broadcast", pack and zip behavior may be surprising.
+ 
+ #include <immintrin.h>  // AVX2+
+-
+ #if defined(_MSC_VER) && defined(__clang__)
+ // Including <immintrin.h> should be enough, but Clang's headers helpfully skip
+ // including these headers when _MSC_VER is defined, like when using clang-cl.
+@@ -44,6 +43,11 @@ HWY_BEFORE_NAMESPACE();
+ namespace hwy {
+ namespace HWY_NAMESPACE {
+ 
++template <typename T>
++using Full256 = Simd<T, 32 / sizeof(T)>;
++
++namespace detail {
++
+ template <typename T>
+ struct Raw256 {
+   using type = __m256i;
+@@ -57,12 +61,11 @@ struct Raw256<double> {
+   using type = __m256d;
+ };
+ 
+-template <typename T>
+-using Full256 = Simd<T, 32 / sizeof(T)>;
++}  // namespace detail
+ 
+ template <typename T>
+ class Vec256 {
+-  using Raw = typename Raw256<T>::type;
++  using Raw = typename detail::Raw256<T>::type;
+ 
+  public:
+   // Compound assignment. Only usable if there is a corresponding non-member
+@@ -92,25 +95,24 @@ class Vec256 {
+   Raw raw;
+ };
+ 
+-// Integer: FF..FF or 0. Float: MSB, all other bits undefined - see README.
++// FF..FF or 0.
+ template <typename T>
+-class Mask256 {
+-  using Raw = typename Raw256<T>::type;
+-
+- public:
+-  Raw raw;
++struct Mask256 {
++  typename detail::Raw256<T>::type raw;
+ };
+ 
+ // ------------------------------ BitCast
+ 
+ namespace detail {
+ 
+-HWY_API __m256i BitCastToInteger(__m256i v) { return v; }
+-HWY_API __m256i BitCastToInteger(__m256 v) { return _mm256_castps_si256(v); }
+-HWY_API __m256i BitCastToInteger(__m256d v) { return _mm256_castpd_si256(v); }
++HWY_INLINE __m256i BitCastToInteger(__m256i v) { return v; }
++HWY_INLINE __m256i BitCastToInteger(__m256 v) { return _mm256_castps_si256(v); }
++HWY_INLINE __m256i BitCastToInteger(__m256d v) {
++  return _mm256_castpd_si256(v);
++}
+ 
+ template <typename T>
+-HWY_API Vec256<uint8_t> BitCastToByte(Vec256<T> v) {
++HWY_INLINE Vec256<uint8_t> BitCastToByte(Vec256<T> v) {
+   return Vec256<uint8_t>{BitCastToInteger(v.raw)};
+ }
+ 
+@@ -129,7 +131,7 @@ struct BitCastFromInteger256<double> {
+ };
+ 
+ template <typename T>
+-HWY_API Vec256<T> BitCastFromByte(Full256<T> /* tag */, Vec256<uint8_t> v) {
++HWY_INLINE Vec256<T> BitCastFromByte(Full256<T> /* tag */, Vec256<uint8_t> v) {
+   return Vec256<T>{BitCastFromInteger256<T>()(v.raw)};
+ }
+ 
+@@ -272,7 +274,7 @@ HWY_API Vec256<double> Xor(const Vec256<double> a, const Vec256<double> b) {
+ template <typename T>
+ HWY_API Vec256<T> Not(const Vec256<T> v) {
+   using TU = MakeUnsigned<T>;
+-#if HWY_TARGET == HWY_AVX3
++#if HWY_TARGET <= HWY_AVX3
+   const __m256i vu = BitCast(Full256<TU>(), v).raw;
+   return BitCast(Full256<T>(),
+                  Vec256<TU>{_mm256_ternarylogic_epi32(vu, vu, vu, 0x55)});
+@@ -298,6 +300,47 @@ HWY_API Vec256<T> operator^(const Vec256<T> a, const Vec256<T> b) {
+   return Xor(a, b);
+ }
+ 
++// ------------------------------ PopulationCount
++
++// 8/16 require BITALG, 32/64 require VPOPCNTDQ.
++#if HWY_TARGET == HWY_AVX3_DL
++
++#ifdef HWY_NATIVE_POPCNT
++#undef HWY_NATIVE_POPCNT
++#else
++#define HWY_NATIVE_POPCNT
++#endif
++
++namespace detail {
++
++template <typename T>
++HWY_INLINE Vec256<T> PopulationCount(hwy::SizeTag<1> /* tag */, Vec256<T> v) {
++  return Vec256<T>{_mm256_popcnt_epi8(v.raw)};
++}
++template <typename T>
++HWY_INLINE Vec256<T> PopulationCount(hwy::SizeTag<2> /* tag */, Vec256<T> v) {
++  return Vec256<T>{_mm256_popcnt_epi16(v.raw)};
++}
++template <typename T>
++HWY_INLINE Vec256<T> PopulationCount(hwy::SizeTag<4> /* tag */, Vec256<T> v) {
++  return Vec256<T>{_mm256_popcnt_epi32(v.raw)};
++}
++template <typename T>
++HWY_INLINE Vec256<T> PopulationCount(hwy::SizeTag<8> /* tag */, Vec256<T> v) {
++  return Vec256<T>{_mm256_popcnt_epi64(v.raw)};
++}
++
++}  // namespace detail
++
++template <typename T>
++HWY_API Vec256<T> PopulationCount(Vec256<T> v) {
++  return detail::PopulationCount(hwy::SizeTag<sizeof(T)>(), v);
++}
++
++#endif  // HWY_TARGET == HWY_AVX3_DL
++
++// ================================================== SIGN
++
+ // ------------------------------ CopySign
+ 
+ template <typename T>
+@@ -307,7 +350,7 @@ HWY_API Vec256<T> CopySign(const Vec256<T> magn, const Vec256<T> sign) {
+   const Full256<T> d;
+   const auto msb = SignBit(d);
+ 
+-#if HWY_TARGET == HWY_AVX3
++#if HWY_TARGET <= HWY_AVX3
+   const Rebind<MakeUnsigned<T>, decltype(d)> du;
+   // Truth table for msb, magn, sign | bitwise msb ? sign : mag
+   //                  0    0     0   |  0
+@@ -329,7 +372,7 @@ HWY_API Vec256<T> CopySign(const Vec256<T> magn, const Vec256<T> sign) {
+ 
+ template <typename T>
+ HWY_API Vec256<T> CopySignToAbs(const Vec256<T> abs, const Vec256<T> sign) {
+-#if HWY_TARGET == HWY_AVX3
++#if HWY_TARGET <= HWY_AVX3
+   // AVX3 can also handle abs < 0, so no extra action needed.
+   return CopySign(abs, sign);
+ #else
+@@ -337,6 +380,8 @@ HWY_API Vec256<T> CopySignToAbs(const Vec256<T> abs, const Vec256<T> sign) {
+ #endif
+ }
+ 
++// ================================================== MASK
++
+ // ------------------------------ Mask
+ 
+ // Mask and Vec are the same (true = FF..FF).
+@@ -396,8 +441,7 @@ HWY_API Vec256<T> ZeroIfNegative(Vec256<T> v) {
+ 
+ template <typename T>
+ HWY_API Mask256<T> Not(const Mask256<T> m) {
+-  const Full256<T> d;
+-  return MaskFromVec(Not(VecFromMask(d, m)));
++  return MaskFromVec(Not(VecFromMask(Full256<T>(), m)));
+ }
+ 
+ template <typename T>
+@@ -434,6 +478,12 @@ HWY_API Mask256<TTo> RebindMask(Full256<TTo> d_to, Mask256<TFrom> m) {
+   return MaskFromVec(BitCast(d_to, VecFromMask(Full256<TFrom>(), m)));
+ }
+ 
++template <typename T>
++HWY_API Mask256<T> TestBit(const Vec256<T> v, const Vec256<T> bit) {
++  static_assert(!hwy::IsFloat<T>(), "Only integer vectors supported");
++  return (v & bit) == bit;
++}
++
+ // ------------------------------ Equality
+ 
+ // Unsigned
+@@ -482,10 +532,20 @@ HWY_API Mask256<double> operator==(const Vec256<double> a,
+   return Mask256<double>{_mm256_cmp_pd(a.raw, b.raw, _CMP_EQ_OQ)};
+ }
+ 
+-template <typename T>
+-HWY_API Mask256<T> TestBit(const Vec256<T> v, const Vec256<T> bit) {
+-  static_assert(!hwy::IsFloat<T>(), "Only integer vectors supported");
+-  return (v & bit) == bit;
++// ------------------------------ Inequality
++
++template <typename T, HWY_IF_NOT_FLOAT(T)>
++HWY_API Mask256<T> operator!=(const Vec256<T> a, const Vec256<T> b) {
++  return Not(a == b);
++}
++
++HWY_API Mask256<float> operator!=(const Vec256<float> a,
++                                  const Vec256<float> b) {
++  return Mask256<float>{_mm256_cmp_ps(a.raw, b.raw, _CMP_NEQ_OQ)};
++}
++HWY_API Mask256<double> operator!=(const Vec256<double> a,
++                                   const Vec256<double> b) {
++  return Mask256<double>{_mm256_cmp_pd(a.raw, b.raw, _CMP_NEQ_OQ)};
+ }
+ 
+ // ------------------------------ Strict inequality
+@@ -597,7 +657,7 @@ HWY_API Vec256<uint32_t> Min(const Vec256<uint32_t> a,
+ }
+ HWY_API Vec256<uint64_t> Min(const Vec256<uint64_t> a,
+                              const Vec256<uint64_t> b) {
+-#if HWY_TARGET == HWY_AVX3
++#if HWY_TARGET <= HWY_AVX3
+   return Vec256<uint64_t>{_mm256_min_epu64(a.raw, b.raw)};
+ #else
+   const Full256<uint64_t> du;
+@@ -619,7 +679,7 @@ HWY_API Vec256<int32_t> Min(const Vec256<int32_t> a, const Vec256<int32_t> b) {
+   return Vec256<int32_t>{_mm256_min_epi32(a.raw, b.raw)};
+ }
+ HWY_API Vec256<int64_t> Min(const Vec256<int64_t> a, const Vec256<int64_t> b) {
+-#if HWY_TARGET == HWY_AVX3
++#if HWY_TARGET <= HWY_AVX3
+   return Vec256<int64_t>{_mm256_min_epi64(a.raw, b.raw)};
+ #else
+   return IfThenElse(a < b, a, b);
+@@ -650,7 +710,7 @@ HWY_API Vec256<uint32_t> Max(const Vec256<uint32_t> a,
+ }
+ HWY_API Vec256<uint64_t> Max(const Vec256<uint64_t> a,
+                              const Vec256<uint64_t> b) {
+-#if HWY_TARGET == HWY_AVX3
++#if HWY_TARGET <= HWY_AVX3
+   return Vec256<uint64_t>{_mm256_max_epu64(a.raw, b.raw)};
+ #else
+   const Full256<uint64_t> du;
+@@ -672,7 +732,7 @@ HWY_API Vec256<int32_t> Max(const Vec256<int32_t> a, const Vec256<int32_t> b) {
+   return Vec256<int32_t>{_mm256_max_epi32(a.raw, b.raw)};
+ }
+ HWY_API Vec256<int64_t> Max(const Vec256<int64_t> a, const Vec256<int64_t> b) {
+-#if HWY_TARGET == HWY_AVX3
++#if HWY_TARGET <= HWY_AVX3
+   return Vec256<int64_t>{_mm256_max_epi64(a.raw, b.raw)};
+ #else
+   return IfThenElse(a < b, b, a);
+@@ -853,7 +913,7 @@ HWY_API Vec256<uint16_t> AverageRound(const Vec256<uint16_t> a,
+   return Vec256<uint16_t>{_mm256_avg_epu16(a.raw, b.raw)};
+ }
+ 
+-// ------------------------------ Absolute value
++// ------------------------------ Abs (Sub)
+ 
+ // Returns absolute value, except that LimitsMin() maps to LimitsMax() + 1.
+ HWY_API Vec256<int8_t> Abs(const Vec256<int8_t> v) {
+@@ -1037,7 +1097,7 @@ HWY_API Vec256<int64_t> BroadcastSignBit(const Vec256<int64_t> v) {
+ 
+ template <int kBits>
+ HWY_API Vec256<int64_t> ShiftRight(const Vec256<int64_t> v) {
+-#if HWY_TARGET == HWY_AVX3
++#if HWY_TARGET <= HWY_AVX3
+   return Vec256<int64_t>{_mm256_srai_epi64(v.raw, kBits)};
+ #else
+   const Full256<int64_t> di;
+@@ -1049,7 +1109,7 @@ HWY_API Vec256<int64_t> ShiftRight(const Vec256<int64_t> v) {
+ }
+ 
+ HWY_API Vec256<int64_t> Abs(const Vec256<int64_t> v) {
+-#if HWY_TARGET == HWY_AVX3
++#if HWY_TARGET <= HWY_AVX3
+   return Vec256<int64_t>{_mm256_abs_epi64(v.raw)};
+ #else
+   const auto zero = Zero(Full256<int64_t>());
+@@ -1125,7 +1185,7 @@ HWY_API Vec256<int32_t> ShiftRightSame(const Vec256<int32_t> v,
+ }
+ HWY_API Vec256<int64_t> ShiftRightSame(const Vec256<int64_t> v,
+                                        const int bits) {
+-#if HWY_TARGET == HWY_AVX3
++#if HWY_TARGET <= HWY_AVX3
+   return Vec256<int64_t>{_mm256_sra_epi64(v.raw, _mm_cvtsi32_si128(bits))};
+ #else
+   const Full256<int64_t> di;
+@@ -1144,7 +1204,7 @@ HWY_API Vec256<int8_t> ShiftRightSame(Vec256<int8_t> v, const int bits) {
+   return (shifted ^ shifted_sign) - shifted_sign;
+ }
+ 
+-// ------------------------------ Negate
++// ------------------------------ Neg (Xor, Sub)
+ 
+ template <typename T, HWY_IF_FLOAT(T)>
+ HWY_API Vec256<T> Neg(const Vec256<T> v) {
+@@ -1356,6 +1416,13 @@ HWY_API Vec256<T> LoadDup128(Full256<T> /* tag */, const T* HWY_RESTRICT p) {
+   __m256i out;
+   asm("vbroadcasti128 %1, %[reg]" : [ reg ] "=x"(out) : "m"(p[0]));
+   return Vec256<T>{out};
++#elif HWY_COMPILER_MSVC && !HWY_COMPILER_CLANG
++  // Workaround for incorrect results with _mm256_broadcastsi128_si256. Note
++  // that MSVC also lacks _mm256_zextsi128_si256, but cast (which leaves the
++  // upper half undefined) is fine because we're overwriting that anyway.
++  const __m128i v128 = LoadU(Full128<T>(), p).raw;
++  return Vec256<T>{
++      _mm256_inserti128_si256(_mm256_castsi128_si256(v128), v128, 1)};
+ #else
+   return Vec256<T>{_mm256_broadcastsi128_si256(LoadU(Full128<T>(), p).raw)};
+ #endif
+@@ -1366,6 +1433,10 @@ HWY_API Vec256<float> LoadDup128(Full256<float> /* tag */,
+   __m256 out;
+   asm("vbroadcastf128 %1, %[reg]" : [ reg ] "=x"(out) : "m"(p[0]));
+   return Vec256<float>{out};
++#elif HWY_COMPILER_MSVC && !HWY_COMPILER_CLANG
++  const __m128 v128 = LoadU(Full128<float>(), p).raw;
++  return Vec256<float>{
++      _mm256_insertf128_ps(_mm256_castps128_ps256(v128), v128, 1)};
+ #else
+   return Vec256<float>{_mm256_broadcast_ps(reinterpret_cast<const __m128*>(p))};
+ #endif
+@@ -1376,6 +1447,10 @@ HWY_API Vec256<double> LoadDup128(Full256<double> /* tag */,
+   __m256d out;
+   asm("vbroadcastf128 %1, %[reg]" : [ reg ] "=x"(out) : "m"(p[0]));
+   return Vec256<double>{out};
++#elif HWY_COMPILER_MSVC && !HWY_COMPILER_CLANG
++  const __m128d v128 = LoadU(Full128<double>(), p).raw;
++  return Vec256<double>{
++      _mm256_insertf128_pd(_mm256_castpd128_pd256(v128), v128, 1)};
+ #else
+   return Vec256<double>{
+       _mm256_broadcast_pd(reinterpret_cast<const __m128d*>(p))};
+@@ -1432,32 +1507,32 @@ HWY_API void Stream(const Vec256<double> v, Full256<double> /* tag */,
+ HWY_DIAGNOSTICS(push)
+ HWY_DIAGNOSTICS_OFF(disable : 4245 4365, ignored "-Wsign-conversion")
+ 
+-#if HWY_TARGET == HWY_AVX3
++#if HWY_TARGET <= HWY_AVX3
+ namespace detail {
+ 
+ template <typename T>
+-HWY_API void ScatterOffset(hwy::SizeTag<4> /* tag */, Vec256<T> v,
+-                           Full256<T> /* tag */, T* HWY_RESTRICT base,
+-                           const Vec256<int32_t> offset) {
++HWY_INLINE void ScatterOffset(hwy::SizeTag<4> /* tag */, Vec256<T> v,
++                              Full256<T> /* tag */, T* HWY_RESTRICT base,
++                              const Vec256<int32_t> offset) {
+   _mm256_i32scatter_epi32(base, offset.raw, v.raw, 1);
+ }
+ template <typename T>
+-HWY_API void ScatterIndex(hwy::SizeTag<4> /* tag */, Vec256<T> v,
+-                          Full256<T> /* tag */, T* HWY_RESTRICT base,
+-                          const Vec256<int32_t> index) {
++HWY_INLINE void ScatterIndex(hwy::SizeTag<4> /* tag */, Vec256<T> v,
++                             Full256<T> /* tag */, T* HWY_RESTRICT base,
++                             const Vec256<int32_t> index) {
+   _mm256_i32scatter_epi32(base, index.raw, v.raw, 4);
+ }
+ 
+ template <typename T>
+-HWY_API void ScatterOffset(hwy::SizeTag<8> /* tag */, Vec256<T> v,
+-                           Full256<T> /* tag */, T* HWY_RESTRICT base,
+-                           const Vec256<int64_t> offset) {
++HWY_INLINE void ScatterOffset(hwy::SizeTag<8> /* tag */, Vec256<T> v,
++                              Full256<T> /* tag */, T* HWY_RESTRICT base,
++                              const Vec256<int64_t> offset) {
+   _mm256_i64scatter_epi64(base, offset.raw, v.raw, 1);
+ }
+ template <typename T>
+-HWY_API void ScatterIndex(hwy::SizeTag<8> /* tag */, Vec256<T> v,
+-                          Full256<T> /* tag */, T* HWY_RESTRICT base,
+-                          const Vec256<int64_t> index) {
++HWY_INLINE void ScatterIndex(hwy::SizeTag<8> /* tag */, Vec256<T> v,
++                             Full256<T> /* tag */, T* HWY_RESTRICT base,
++                             const Vec256<int64_t> index) {
+   _mm256_i64scatter_epi64(base, index.raw, v.raw, 8);
+ }
+ 
+@@ -1476,31 +1551,25 @@ HWY_API void ScatterIndex(Vec256<T> v, Full256<T> d, T* HWY_RESTRICT base,
+   return detail::ScatterIndex(hwy::SizeTag<sizeof(T)>(), v, d, base, index);
+ }
+ 
+-template <>
+-HWY_INLINE void ScatterOffset<float>(Vec256<float> v, Full256<float> /* tag */,
+-                                     float* HWY_RESTRICT base,
+-                                     const Vec256<int32_t> offset) {
++HWY_API void ScatterOffset(Vec256<float> v, Full256<float> /* tag */,
++                           float* HWY_RESTRICT base,
++                           const Vec256<int32_t> offset) {
+   _mm256_i32scatter_ps(base, offset.raw, v.raw, 1);
+ }
+-template <>
+-HWY_INLINE void ScatterIndex<float>(Vec256<float> v, Full256<float> /* tag */,
+-                                    float* HWY_RESTRICT base,
+-                                    const Vec256<int32_t> index) {
++HWY_API void ScatterIndex(Vec256<float> v, Full256<float> /* tag */,
++                          float* HWY_RESTRICT base,
++                          const Vec256<int32_t> index) {
+   _mm256_i32scatter_ps(base, index.raw, v.raw, 4);
+ }
+ 
+-template <>
+-HWY_INLINE void ScatterOffset<double>(Vec256<double> v,
+-                                      Full256<double> /* tag */,
+-                                      double* HWY_RESTRICT base,
+-                                      const Vec256<int64_t> offset) {
++HWY_API void ScatterOffset(Vec256<double> v, Full256<double> /* tag */,
++                           double* HWY_RESTRICT base,
++                           const Vec256<int64_t> offset) {
+   _mm256_i64scatter_pd(base, offset.raw, v.raw, 1);
+ }
+-template <>
+-HWY_INLINE void ScatterIndex<double>(Vec256<double> v,
+-                                     Full256<double> /* tag */,
+-                                     double* HWY_RESTRICT base,
+-                                     const Vec256<int64_t> index) {
++HWY_API void ScatterIndex(Vec256<double> v, Full256<double> /* tag */,
++                          double* HWY_RESTRICT base,
++                          const Vec256<int64_t> index) {
+   _mm256_i64scatter_pd(base, index.raw, v.raw, 8);
+ }
+ 
+@@ -1548,31 +1617,35 @@ HWY_API void ScatterIndex(Vec256<T> v, Full256<T> d, T* HWY_RESTRICT base,
+ namespace detail {
+ 
+ template <typename T>
+-HWY_API Vec256<T> GatherOffset(hwy::SizeTag<4> /* tag */, Full256<T> /* tag */,
+-                               const T* HWY_RESTRICT base,
+-                               const Vec256<int32_t> offset) {
++HWY_INLINE Vec256<T> GatherOffset(hwy::SizeTag<4> /* tag */,
++                                  Full256<T> /* tag */,
++                                  const T* HWY_RESTRICT base,
++                                  const Vec256<int32_t> offset) {
+   return Vec256<T>{_mm256_i32gather_epi32(
+       reinterpret_cast<const int32_t*>(base), offset.raw, 1)};
+ }
+ template <typename T>
+-HWY_API Vec256<T> GatherIndex(hwy::SizeTag<4> /* tag */, Full256<T> /* tag */,
+-                              const T* HWY_RESTRICT base,
+-                              const Vec256<int32_t> index) {
++HWY_INLINE Vec256<T> GatherIndex(hwy::SizeTag<4> /* tag */,
++                                 Full256<T> /* tag */,
++                                 const T* HWY_RESTRICT base,
++                                 const Vec256<int32_t> index) {
+   return Vec256<T>{_mm256_i32gather_epi32(
+       reinterpret_cast<const int32_t*>(base), index.raw, 4)};
+ }
+ 
+ template <typename T>
+-HWY_API Vec256<T> GatherOffset(hwy::SizeTag<8> /* tag */, Full256<T> /* tag */,
+-                               const T* HWY_RESTRICT base,
+-                               const Vec256<int64_t> offset) {
++HWY_INLINE Vec256<T> GatherOffset(hwy::SizeTag<8> /* tag */,
++                                  Full256<T> /* tag */,
++                                  const T* HWY_RESTRICT base,
++                                  const Vec256<int64_t> offset) {
+   return Vec256<T>{_mm256_i64gather_epi64(
+       reinterpret_cast<const GatherIndex64*>(base), offset.raw, 1)};
+ }
+ template <typename T>
+-HWY_API Vec256<T> GatherIndex(hwy::SizeTag<8> /* tag */, Full256<T> /* tag */,
+-                              const T* HWY_RESTRICT base,
+-                              const Vec256<int64_t> index) {
++HWY_INLINE Vec256<T> GatherIndex(hwy::SizeTag<8> /* tag */,
++                                 Full256<T> /* tag */,
++                                 const T* HWY_RESTRICT base,
++                                 const Vec256<int64_t> index) {
+   return Vec256<T>{_mm256_i64gather_epi64(
+       reinterpret_cast<const GatherIndex64*>(base), index.raw, 8)};
+ }
+@@ -1592,29 +1665,25 @@ HWY_API Vec256<T> GatherIndex(Full256<T> d, const T* HWY_RESTRICT base,
+   return detail::GatherIndex(hwy::SizeTag<sizeof(T)>(), d, base, index);
+ }
+ 
+-template <>
+-HWY_INLINE Vec256<float> GatherOffset<float>(Full256<float> /* tag */,
+-                                             const float* HWY_RESTRICT base,
+-                                             const Vec256<int32_t> offset) {
++HWY_API Vec256<float> GatherOffset(Full256<float> /* tag */,
++                                   const float* HWY_RESTRICT base,
++                                   const Vec256<int32_t> offset) {
+   return Vec256<float>{_mm256_i32gather_ps(base, offset.raw, 1)};
+ }
+-template <>
+-HWY_INLINE Vec256<float> GatherIndex<float>(Full256<float> /* tag */,
+-                                            const float* HWY_RESTRICT base,
+-                                            const Vec256<int32_t> index) {
++HWY_API Vec256<float> GatherIndex(Full256<float> /* tag */,
++                                  const float* HWY_RESTRICT base,
++                                  const Vec256<int32_t> index) {
+   return Vec256<float>{_mm256_i32gather_ps(base, index.raw, 4)};
+ }
+ 
+-template <>
+-HWY_INLINE Vec256<double> GatherOffset<double>(Full256<double> /* tag */,
+-                                               const double* HWY_RESTRICT base,
+-                                               const Vec256<int64_t> offset) {
++HWY_API Vec256<double> GatherOffset(Full256<double> /* tag */,
++                                    const double* HWY_RESTRICT base,
++                                    const Vec256<int64_t> offset) {
+   return Vec256<double>{_mm256_i64gather_pd(base, offset.raw, 1)};
+ }
+-template <>
+-HWY_INLINE Vec256<double> GatherIndex<double>(Full256<double> /* tag */,
+-                                              const double* HWY_RESTRICT base,
+-                                              const Vec256<int64_t> index) {
++HWY_API Vec256<double> GatherIndex(Full256<double> /* tag */,
++                                   const double* HWY_RESTRICT base,
++                                   const Vec256<int64_t> index) {
+   return Vec256<double>{_mm256_i64gather_pd(base, index.raw, 8)};
+ }
+ 
+@@ -1622,39 +1691,43 @@ HWY_DIAGNOSTICS(pop)
+ 
+ // ================================================== SWIZZLE
+ 
+-template <typename T>
+-HWY_API T GetLane(const Vec256<T> v) {
+-  return GetLane(LowerHalf(v));
+-}
+-
+-// ------------------------------ Extract half
++// ------------------------------ LowerHalf
+ 
+ template <typename T>
+-HWY_API Vec128<T> LowerHalf(Vec256<T> v) {
++HWY_API Vec128<T> LowerHalf(Full128<T> /* tag */, Vec256<T> v) {
+   return Vec128<T>{_mm256_castsi256_si128(v.raw)};
+ }
+-template <>
+-HWY_INLINE Vec128<float> LowerHalf(Vec256<float> v) {
++HWY_API Vec128<float> LowerHalf(Full128<float> /* tag */, Vec256<float> v) {
+   return Vec128<float>{_mm256_castps256_ps128(v.raw)};
+ }
+-template <>
+-HWY_INLINE Vec128<double> LowerHalf(Vec256<double> v) {
++HWY_API Vec128<double> LowerHalf(Full128<double> /* tag */, Vec256<double> v) {
+   return Vec128<double>{_mm256_castpd256_pd128(v.raw)};
+ }
+ 
+ template <typename T>
+-HWY_API Vec128<T> UpperHalf(Vec256<T> v) {
++HWY_API Vec128<T> LowerHalf(Vec256<T> v) {
++  return LowerHalf(Full128<T>(), v);
++}
++
++// ------------------------------ UpperHalf
++
++template <typename T>
++HWY_API Vec128<T> UpperHalf(Full128<T> /* tag */, Vec256<T> v) {
+   return Vec128<T>{_mm256_extracti128_si256(v.raw, 1)};
+ }
+-template <>
+-HWY_INLINE Vec128<float> UpperHalf(Vec256<float> v) {
++HWY_API Vec128<float> UpperHalf(Full128<float> /* tag */, Vec256<float> v) {
+   return Vec128<float>{_mm256_extractf128_ps(v.raw, 1)};
+ }
+-template <>
+-HWY_INLINE Vec128<double> UpperHalf(Vec256<double> v) {
++HWY_API Vec128<double> UpperHalf(Full128<double> /* tag */, Vec256<double> v) {
+   return Vec128<double>{_mm256_extractf128_pd(v.raw, 1)};
+ }
+ 
++// ------------------------------ GetLane (LowerHalf)
++template <typename T>
++HWY_API T GetLane(const Vec256<T> v) {
++  return GetLane(LowerHalf(v));
++}
++
+ // ------------------------------ ZeroExtendVector
+ 
+ // Unfortunately the initial _mm256_castsi128_si256 intrinsic leaves the upper
+@@ -1663,29 +1736,29 @@ HWY_INLINE Vec128<double> UpperHalf(Vec256<double> v) {
+ // compiler could decide to optimize out code that relies on this.
+ //
+ // The newer _mm256_zextsi128_si256 intrinsic fixes this by specifying the
+-// zeroing, but it is not available on GCC until 10.1. For older GCC, we can
+-// still obtain the desired code thanks to pattern recognition; note that the
+-// expensive insert instruction is not actually generated, see
++// zeroing, but it is not available on MSVC nor GCC until 10.1. For older GCC,
++// we can still obtain the desired code thanks to pattern recognition; note that
++// the expensive insert instruction is not actually generated, see
+ // https://gcc.godbolt.org/z/1MKGaP.
+ 
+ template <typename T>
+-HWY_API Vec256<T> ZeroExtendVector(Vec128<T> lo) {
++HWY_API Vec256<T> ZeroExtendVector(Full256<T> /* tag */, Vec128<T> lo) {
+ #if !HWY_COMPILER_CLANG && HWY_COMPILER_GCC && (HWY_COMPILER_GCC < 1000)
+   return Vec256<T>{_mm256_inserti128_si256(_mm256_setzero_si256(), lo.raw, 0)};
+ #else
+   return Vec256<T>{_mm256_zextsi128_si256(lo.raw)};
+ #endif
+ }
+-template <>
+-HWY_INLINE Vec256<float> ZeroExtendVector(Vec128<float> lo) {
++HWY_API Vec256<float> ZeroExtendVector(Full256<float> /* tag */,
++                                       Vec128<float> lo) {
+ #if !HWY_COMPILER_CLANG && HWY_COMPILER_GCC && (HWY_COMPILER_GCC < 1000)
+   return Vec256<float>{_mm256_insertf128_ps(_mm256_setzero_ps(), lo.raw, 0)};
+ #else
+   return Vec256<float>{_mm256_zextps128_ps256(lo.raw)};
+ #endif
+ }
+-template <>
+-HWY_INLINE Vec256<double> ZeroExtendVector(Vec128<double> lo) {
++HWY_API Vec256<double> ZeroExtendVector(Full256<double> /* tag */,
++                                        Vec128<double> lo) {
+ #if !HWY_COMPILER_CLANG && HWY_COMPILER_GCC && (HWY_COMPILER_GCC < 1000)
+   return Vec256<double>{_mm256_insertf128_pd(_mm256_setzero_pd(), lo.raw, 0)};
+ #else
+@@ -1696,63 +1769,72 @@ HWY_INLINE Vec256<double> ZeroExtendVector(Vec128<double> lo) {
+ // ------------------------------ Combine
+ 
+ template <typename T>
+-HWY_API Vec256<T> Combine(Vec128<T> hi, Vec128<T> lo) {
+-  const auto lo256 = ZeroExtendVector(lo);
++HWY_API Vec256<T> Combine(Full256<T> d, Vec128<T> hi, Vec128<T> lo) {
++  const auto lo256 = ZeroExtendVector(d, lo);
+   return Vec256<T>{_mm256_inserti128_si256(lo256.raw, hi.raw, 1)};
+ }
+-template <>
+-HWY_INLINE Vec256<float> Combine(Vec128<float> hi, Vec128<float> lo) {
+-  const auto lo256 = ZeroExtendVector(lo);
++HWY_API Vec256<float> Combine(Full256<float> d, Vec128<float> hi,
++                              Vec128<float> lo) {
++  const auto lo256 = ZeroExtendVector(d, lo);
+   return Vec256<float>{_mm256_insertf128_ps(lo256.raw, hi.raw, 1)};
+ }
+-template <>
+-HWY_INLINE Vec256<double> Combine(Vec128<double> hi, Vec128<double> lo) {
+-  const auto lo256 = ZeroExtendVector(lo);
++HWY_API Vec256<double> Combine(Full256<double> d, Vec128<double> hi,
++                               Vec128<double> lo) {
++  const auto lo256 = ZeroExtendVector(d, lo);
+   return Vec256<double>{_mm256_insertf128_pd(lo256.raw, hi.raw, 1)};
+ }
+ 
+-// ------------------------------ Shift vector by constant #bytes
++// ------------------------------ ShiftLeftBytes
+ 
+-// 0x01..0F, kBytes = 1 => 0x02..0F00
+ template <int kBytes, typename T>
+-HWY_API Vec256<T> ShiftLeftBytes(const Vec256<T> v) {
++HWY_API Vec256<T> ShiftLeftBytes(Full256<T> /* tag */, const Vec256<T> v) {
+   static_assert(0 <= kBytes && kBytes <= 16, "Invalid kBytes");
+   // This is the same operation as _mm256_bslli_epi128.
+   return Vec256<T>{_mm256_slli_si256(v.raw, kBytes)};
+ }
+ 
++template <int kBytes, typename T>
++HWY_API Vec256<T> ShiftLeftBytes(const Vec256<T> v) {
++  return ShiftLeftBytes<kBytes>(Full256<T>(), v);
++}
++
++// ------------------------------ ShiftLeftLanes
++
+ template <int kLanes, typename T>
+-HWY_API Vec256<T> ShiftLeftLanes(const Vec256<T> v) {
+-  const Full256<uint8_t> d8;
+-  const Full256<T> d;
++HWY_API Vec256<T> ShiftLeftLanes(Full256<T> d, const Vec256<T> v) {
++  const Repartition<uint8_t, decltype(d)> d8;
+   return BitCast(d, ShiftLeftBytes<kLanes * sizeof(T)>(BitCast(d8, v)));
+ }
+ 
+-// 0x01..0F, kBytes = 1 => 0x0001..0E
++template <int kLanes, typename T>
++HWY_API Vec256<T> ShiftLeftLanes(const Vec256<T> v) {
++  return ShiftLeftLanes<kLanes>(Full256<T>(), v);
++}
++
++// ------------------------------ ShiftRightBytes
++
+ template <int kBytes, typename T>
+-HWY_API Vec256<T> ShiftRightBytes(const Vec256<T> v) {
++HWY_API Vec256<T> ShiftRightBytes(Full256<T> /* tag */, const Vec256<T> v) {
+   static_assert(0 <= kBytes && kBytes <= 16, "Invalid kBytes");
+   // This is the same operation as _mm256_bsrli_epi128.
+   return Vec256<T>{_mm256_srli_si256(v.raw, kBytes)};
+ }
+ 
++// ------------------------------ ShiftRightLanes
+ template <int kLanes, typename T>
+-HWY_API Vec256<T> ShiftRightLanes(const Vec256<T> v) {
+-  const Full256<uint8_t> d8;
+-  const Full256<T> d;
++HWY_API Vec256<T> ShiftRightLanes(Full256<T> d, const Vec256<T> v) {
++  const Repartition<uint8_t, decltype(d)> d8;
+   return BitCast(d, ShiftRightBytes<kLanes * sizeof(T)>(BitCast(d8, v)));
+ }
+ 
+-// ------------------------------ Extract from 2x 128-bit at constant offset
++// ------------------------------ CombineShiftRightBytes
+ 
+ // Extracts 128 bits from <hi, lo> by skipping the least-significant kBytes.
+-template <int kBytes, typename T>
+-HWY_API Vec256<T> CombineShiftRightBytes(const Vec256<T> hi,
+-                                         const Vec256<T> lo) {
+-  const Full256<uint8_t> d8;
+-  const Vec256<uint8_t> extracted_bytes{
+-      _mm256_alignr_epi8(BitCast(d8, hi).raw, BitCast(d8, lo).raw, kBytes)};
+-  return BitCast(Full256<T>(), extracted_bytes);
++template <int kBytes, typename T, class V = Vec256<T>>
++HWY_API V CombineShiftRightBytes(Full256<T> d, V hi, V lo) {
++  const Repartition<uint8_t, decltype(d)> d8;
++  return BitCast(d, Vec256<uint8_t>{_mm256_alignr_epi8(
++                        BitCast(d8, hi).raw, BitCast(d8, lo).raw, kBytes)});
+ }
+ 
+ // ------------------------------ Broadcast/splat any lane
+@@ -1922,7 +2004,7 @@ HWY_API Vec256<float> TableLookupLanes(const Vec256<float> v,
+   return Vec256<float>{_mm256_permutevar8x32_ps(v.raw, idx.raw)};
+ }
+ 
+-// ------------------------------ Interleave lanes
++// ------------------------------ InterleaveLower
+ 
+ // Interleaves lanes from halves of the 128-bit blocks of "a" (which provides
+ // the least-significant lane) and "b". To concatenate two half-width integers
+@@ -1971,6 +2053,17 @@ HWY_API Vec256<double> InterleaveLower(const Vec256<double> a,
+   return Vec256<double>{_mm256_unpacklo_pd(a.raw, b.raw)};
+ }
+ 
++// Additional overload for the optional Simd<> tag.
++template <typename T, class V = Vec256<T>>
++HWY_API V InterleaveLower(Full256<T> /* tag */, V a, V b) {
++  return InterleaveLower(a, b);
++}
++
++// ------------------------------ InterleaveUpper
++
++// All functions inside detail lack the required D parameter.
++namespace detail {
++
+ HWY_API Vec256<uint8_t> InterleaveUpper(const Vec256<uint8_t> a,
+                                         const Vec256<uint8_t> b) {
+   return Vec256<uint8_t>{_mm256_unpackhi_epi8(a.raw, b.raw)};
+@@ -2014,61 +2107,29 @@ HWY_API Vec256<double> InterleaveUpper(const Vec256<double> a,
+   return Vec256<double>{_mm256_unpackhi_pd(a.raw, b.raw)};
+ }
+ 
+-// ------------------------------ Zip lanes
+-
+-// Same as interleave_*, except that the return lanes are double-width integers;
+-// this is necessary because the single-lane scalar cannot return two values.
++}  // namespace detail
+ 
+-HWY_API Vec256<uint16_t> ZipLower(const Vec256<uint8_t> a,
+-                                  const Vec256<uint8_t> b) {
+-  return Vec256<uint16_t>{_mm256_unpacklo_epi8(a.raw, b.raw)};
+-}
+-HWY_API Vec256<uint32_t> ZipLower(const Vec256<uint16_t> a,
+-                                  const Vec256<uint16_t> b) {
+-  return Vec256<uint32_t>{_mm256_unpacklo_epi16(a.raw, b.raw)};
+-}
+-HWY_API Vec256<uint64_t> ZipLower(const Vec256<uint32_t> a,
+-                                  const Vec256<uint32_t> b) {
+-  return Vec256<uint64_t>{_mm256_unpacklo_epi32(a.raw, b.raw)};
++template <typename T, class V = Vec256<T>>
++HWY_API V InterleaveUpper(Full256<T> /* tag */, V a, V b) {
++  return detail::InterleaveUpper(a, b);
+ }
+ 
+-HWY_API Vec256<int16_t> ZipLower(const Vec256<int8_t> a,
+-                                 const Vec256<int8_t> b) {
+-  return Vec256<int16_t>{_mm256_unpacklo_epi8(a.raw, b.raw)};
+-}
+-HWY_API Vec256<int32_t> ZipLower(const Vec256<int16_t> a,
+-                                 const Vec256<int16_t> b) {
+-  return Vec256<int32_t>{_mm256_unpacklo_epi16(a.raw, b.raw)};
+-}
+-HWY_API Vec256<int64_t> ZipLower(const Vec256<int32_t> a,
+-                                 const Vec256<int32_t> b) {
+-  return Vec256<int64_t>{_mm256_unpacklo_epi32(a.raw, b.raw)};
+-}
++// ------------------------------ ZipLower/ZipUpper (InterleaveLower)
+ 
+-HWY_API Vec256<uint16_t> ZipUpper(const Vec256<uint8_t> a,
+-                                  const Vec256<uint8_t> b) {
+-  return Vec256<uint16_t>{_mm256_unpackhi_epi8(a.raw, b.raw)};
+-}
+-HWY_API Vec256<uint32_t> ZipUpper(const Vec256<uint16_t> a,
+-                                  const Vec256<uint16_t> b) {
+-  return Vec256<uint32_t>{_mm256_unpackhi_epi16(a.raw, b.raw)};
++// Same as Interleave*, except that the return lanes are double-width integers;
++// this is necessary because the single-lane scalar cannot return two values.
++template <typename T, typename TW = MakeWide<T>>
++HWY_API Vec256<TW> ZipLower(Vec256<T> a, Vec256<T> b) {
++  return BitCast(Full256<TW>(), InterleaveLower(Full256<T>(), a, b));
+ }
+-HWY_API Vec256<uint64_t> ZipUpper(const Vec256<uint32_t> a,
+-                                  const Vec256<uint32_t> b) {
+-  return Vec256<uint64_t>{_mm256_unpackhi_epi32(a.raw, b.raw)};
++template <typename T, typename TW = MakeWide<T>>
++HWY_API Vec256<TW> ZipLower(Full256<TW> dw, Vec256<T> a, Vec256<T> b) {
++  return BitCast(dw, InterleaveLower(Full256<T>(), a, b));
+ }
+ 
+-HWY_API Vec256<int16_t> ZipUpper(const Vec256<int8_t> a,
+-                                 const Vec256<int8_t> b) {
+-  return Vec256<int16_t>{_mm256_unpackhi_epi8(a.raw, b.raw)};
+-}
+-HWY_API Vec256<int32_t> ZipUpper(const Vec256<int16_t> a,
+-                                 const Vec256<int16_t> b) {
+-  return Vec256<int32_t>{_mm256_unpackhi_epi16(a.raw, b.raw)};
+-}
+-HWY_API Vec256<int64_t> ZipUpper(const Vec256<int32_t> a,
+-                                 const Vec256<int32_t> b) {
+-  return Vec256<int64_t>{_mm256_unpackhi_epi32(a.raw, b.raw)};
++template <typename T, typename TW = MakeWide<T>>
++HWY_API Vec256<TW> ZipUpper(Full256<TW> dw, Vec256<T> a, Vec256<T> b) {
++  return BitCast(dw, InterleaveUpper(Full256<T>(), a, b));
+ }
+ 
+ // ------------------------------ Blocks (LowerHalf, ZeroExtendVector)
+@@ -2079,56 +2140,63 @@ HWY_API Vec256<int64_t> ZipUpper(const Vec256<int32_t> a,
+ 
+ // hiH,hiL loH,loL |-> hiL,loL (= lower halves)
+ template <typename T>
+-HWY_API Vec256<T> ConcatLowerLower(const Vec256<T> hi, const Vec256<T> lo) {
+-  return Vec256<T>{_mm256_inserti128_si256(lo.raw, LowerHalf(hi).raw, 1)};
++HWY_API Vec256<T> ConcatLowerLower(Full256<T> d, const Vec256<T> hi,
++                                   const Vec256<T> lo) {
++  const Half<decltype(d)> d2;
++  return Vec256<T>{_mm256_inserti128_si256(lo.raw, LowerHalf(d2, hi).raw, 1)};
+ }
+-template <>
+-HWY_INLINE Vec256<float> ConcatLowerLower(const Vec256<float> hi,
+-                                          const Vec256<float> lo) {
+-  return Vec256<float>{_mm256_insertf128_ps(lo.raw, LowerHalf(hi).raw, 1)};
++HWY_API Vec256<float> ConcatLowerLower(Full256<float> d, const Vec256<float> hi,
++                                       const Vec256<float> lo) {
++  const Half<decltype(d)> d2;
++  return Vec256<float>{_mm256_insertf128_ps(lo.raw, LowerHalf(d2, hi).raw, 1)};
+ }
+-template <>
+-HWY_INLINE Vec256<double> ConcatLowerLower(const Vec256<double> hi,
+-                                           const Vec256<double> lo) {
+-  return Vec256<double>{_mm256_insertf128_pd(lo.raw, LowerHalf(hi).raw, 1)};
++HWY_API Vec256<double> ConcatLowerLower(Full256<double> d,
++                                        const Vec256<double> hi,
++                                        const Vec256<double> lo) {
++  const Half<decltype(d)> d2;
++  return Vec256<double>{_mm256_insertf128_pd(lo.raw, LowerHalf(d2, hi).raw, 1)};
+ }
+ 
+ // hiH,hiL loH,loL |-> hiL,loH (= inner halves / swap blocks)
+ template <typename T>
+-HWY_API Vec256<T> ConcatLowerUpper(const Vec256<T> hi, const Vec256<T> lo) {
++HWY_API Vec256<T> ConcatLowerUpper(Full256<T> /* tag */, const Vec256<T> hi,
++                                   const Vec256<T> lo) {
+   return Vec256<T>{_mm256_permute2x128_si256(lo.raw, hi.raw, 0x21)};
+ }
+-template <>
+-HWY_INLINE Vec256<float> ConcatLowerUpper(const Vec256<float> hi,
+-                                          const Vec256<float> lo) {
++HWY_API Vec256<float> ConcatLowerUpper(Full256<float> /* tag */,
++                                       const Vec256<float> hi,
++                                       const Vec256<float> lo) {
+   return Vec256<float>{_mm256_permute2f128_ps(lo.raw, hi.raw, 0x21)};
+ }
+-template <>
+-HWY_INLINE Vec256<double> ConcatLowerUpper(const Vec256<double> hi,
+-                                           const Vec256<double> lo) {
++HWY_API Vec256<double> ConcatLowerUpper(Full256<double> /* tag */,
++                                        const Vec256<double> hi,
++                                        const Vec256<double> lo) {
+   return Vec256<double>{_mm256_permute2f128_pd(lo.raw, hi.raw, 0x21)};
+ }
+ 
+ // hiH,hiL loH,loL |-> hiH,loL (= outer halves)
+ template <typename T>
+-HWY_API Vec256<T> ConcatUpperLower(const Vec256<T> hi, const Vec256<T> lo) {
++HWY_API Vec256<T> ConcatUpperLower(Full256<T> /* tag */, const Vec256<T> hi,
++                                   const Vec256<T> lo) {
+   return Vec256<T>{_mm256_blend_epi32(hi.raw, lo.raw, 0x0F)};
+ }
+-template <>
+-HWY_INLINE Vec256<float> ConcatUpperLower(const Vec256<float> hi,
+-                                          const Vec256<float> lo) {
++HWY_API Vec256<float> ConcatUpperLower(Full256<float> /* tag */,
++                                       const Vec256<float> hi,
++                                       const Vec256<float> lo) {
+   return Vec256<float>{_mm256_blend_ps(hi.raw, lo.raw, 0x0F)};
+ }
+-template <>
+-HWY_INLINE Vec256<double> ConcatUpperLower(const Vec256<double> hi,
+-                                           const Vec256<double> lo) {
++HWY_API Vec256<double> ConcatUpperLower(Full256<double> /* tag */,
++                                        const Vec256<double> hi,
++                                        const Vec256<double> lo) {
+   return Vec256<double>{_mm256_blend_pd(hi.raw, lo.raw, 3)};
+ }
+ 
+ // hiH,hiL loH,loL |-> hiH,loH (= upper halves)
+ template <typename T>
+-HWY_API Vec256<T> ConcatUpperUpper(const Vec256<T> hi, const Vec256<T> lo) {
+-  return ConcatUpperLower(hi, ZeroExtendVector(UpperHalf(lo)));
++HWY_API Vec256<T> ConcatUpperUpper(Full256<T> d, const Vec256<T> hi,
++                                   const Vec256<T> lo) {
++  const Half<decltype(d)> d2;
++  return ConcatUpperLower(d, hi, ZeroExtendVector(d, UpperHalf(d2, lo)));
+ }
+ 
+ // ------------------------------ Odd/even lanes
+@@ -2136,8 +2204,8 @@ HWY_API Vec256<T> ConcatUpperUpper(const Vec256<T> hi, const Vec256<T> lo) {
+ namespace detail {
+ 
+ template <typename T>
+-HWY_API Vec256<T> OddEven(hwy::SizeTag<1> /* tag */, const Vec256<T> a,
+-                          const Vec256<T> b) {
++HWY_INLINE Vec256<T> OddEven(hwy::SizeTag<1> /* tag */, const Vec256<T> a,
++                             const Vec256<T> b) {
+   const Full256<T> d;
+   const Full256<uint8_t> d8;
+   alignas(32) constexpr uint8_t mask[16] = {0xFF, 0, 0xFF, 0, 0xFF, 0, 0xFF, 0,
+@@ -2145,18 +2213,18 @@ HWY_API Vec256<T> OddEven(hwy::SizeTag<1> /* tag */, const Vec256<T> a,
+   return IfThenElse(MaskFromVec(BitCast(d, LoadDup128(d8, mask))), b, a);
+ }
+ template <typename T>
+-HWY_API Vec256<T> OddEven(hwy::SizeTag<2> /* tag */, const Vec256<T> a,
+-                          const Vec256<T> b) {
++HWY_INLINE Vec256<T> OddEven(hwy::SizeTag<2> /* tag */, const Vec256<T> a,
++                             const Vec256<T> b) {
+   return Vec256<T>{_mm256_blend_epi16(a.raw, b.raw, 0x55)};
+ }
+ template <typename T>
+-HWY_API Vec256<T> OddEven(hwy::SizeTag<4> /* tag */, const Vec256<T> a,
+-                          const Vec256<T> b) {
++HWY_INLINE Vec256<T> OddEven(hwy::SizeTag<4> /* tag */, const Vec256<T> a,
++                             const Vec256<T> b) {
+   return Vec256<T>{_mm256_blend_epi32(a.raw, b.raw, 0x55)};
+ }
+ template <typename T>
+-HWY_API Vec256<T> OddEven(hwy::SizeTag<8> /* tag */, const Vec256<T> a,
+-                          const Vec256<T> b) {
++HWY_INLINE Vec256<T> OddEven(hwy::SizeTag<8> /* tag */, const Vec256<T> a,
++                             const Vec256<T> b) {
+   return Vec256<T>{_mm256_blend_epi32(a.raw, b.raw, 0x33)};
+ }
+ 
+@@ -2166,45 +2234,63 @@ template <typename T>
+ HWY_API Vec256<T> OddEven(const Vec256<T> a, const Vec256<T> b) {
+   return detail::OddEven(hwy::SizeTag<sizeof(T)>(), a, b);
+ }
+-template <>
+-HWY_INLINE Vec256<float> OddEven<float>(const Vec256<float> a,
+-                                        const Vec256<float> b) {
++HWY_API Vec256<float> OddEven(const Vec256<float> a, const Vec256<float> b) {
+   return Vec256<float>{_mm256_blend_ps(a.raw, b.raw, 0x55)};
+ }
+ 
+-template <>
+-HWY_INLINE Vec256<double> OddEven<double>(const Vec256<double> a,
+-                                          const Vec256<double> b) {
++HWY_API Vec256<double> OddEven(const Vec256<double> a, const Vec256<double> b) {
+   return Vec256<double>{_mm256_blend_pd(a.raw, b.raw, 5)};
+ }
+ 
+-// ------------------------------ Shuffle bytes with variable indices
++// ------------------------------ TableLookupBytes (ZeroExtendVector)
+ 
+-// Returns vector of bytes[from[i]]. "from" is also interpreted as bytes, i.e.
+-// lane indices in [0, 16).
+-template <typename T>
+-HWY_API Vec256<T> TableLookupBytes(const Vec256<T> bytes,
+-                                   const Vec256<T> from) {
+-  return Vec256<T>{_mm256_shuffle_epi8(bytes.raw, from.raw)};
++// Both full
++template <typename T, typename TI>
++HWY_API Vec256<TI> TableLookupBytes(const Vec256<T> bytes,
++                                    const Vec256<TI> from) {
++  return Vec256<TI>{_mm256_shuffle_epi8(bytes.raw, from.raw)};
+ }
+ 
++// Partial index vector
++template <typename T, typename TI, size_t NI>
++HWY_API Vec128<TI, NI> TableLookupBytes(const Vec256<T> bytes,
++                                        const Vec128<TI, NI> from) {
++  // First expand to full 128, then 256.
++  const auto from_256 = ZeroExtendVector(Full256<TI>(), Vec128<TI>{from.raw});
++  const auto tbl_full = TableLookupBytes(bytes, from_256);
++  // Shrink to 128, then partial.
++  return Vec128<TI, NI>{LowerHalf(Full128<TI>(), tbl_full).raw};
++}
++
++// Partial table vector
++template <typename T, size_t N, typename TI>
++HWY_API Vec256<TI> TableLookupBytes(const Vec128<T, N> bytes,
++                                    const Vec256<TI> from) {
++  // First expand to full 128, then 256.
++  const auto bytes_256 = ZeroExtendVector(Full256<T>(), Vec128<T>{bytes.raw});
++  return TableLookupBytes(bytes_256, from);
++}
++
++// Partial both are handled by x86_128.
++
+ // ------------------------------ Shl (Mul, ZipLower)
+ 
+-#if HWY_TARGET != HWY_AVX3
++#if HWY_TARGET > HWY_AVX3  // AVX2 or older
+ namespace detail {
+ 
+ // Returns 2^v for use as per-lane multipliers to emulate 16-bit shifts.
+ template <typename T, HWY_IF_LANE_SIZE(T, 2)>
+-HWY_API Vec256<MakeUnsigned<T>> Pow2(const Vec256<T> v) {
++HWY_INLINE Vec256<MakeUnsigned<T>> Pow2(const Vec256<T> v) {
+   const Full256<T> d;
+-  const Full256<float> df;
++  const RepartitionToWide<decltype(d)> dw;
++  const Rebind<float, decltype(dw)> df;
+   const auto zero = Zero(d);
+   // Move into exponent (this u16 will become the upper half of an f32)
+   const auto exp = ShiftLeft<23 - 16>(v);
+   const auto upper = exp + Set(d, 0x3F80);  // upper half of 1.0f
+   // Insert 0 into lower halves for reinterpreting as binary32.
+-  const auto f0 = ZipLower(zero, upper);
+-  const auto f1 = ZipUpper(zero, upper);
++  const auto f0 = ZipLower(dw, zero, upper);
++  const auto f1 = ZipUpper(dw, zero, upper);
+   // Do not use ConvertTo because it checks for overflow, which is redundant
+   // because we only care about v in [0, 16).
+   const Vec256<int32_t> bits0{_mm256_cvttps_epi32(BitCast(df, f0).raw)};
+@@ -2213,11 +2299,11 @@ HWY_API Vec256<MakeUnsigned<T>> Pow2(const Vec256<T> v) {
+ }
+ 
+ }  // namespace detail
+-#endif  // HWY_TARGET != HWY_AVX3
++#endif  // HWY_TARGET > HWY_AVX3
+ 
+ HWY_API Vec256<uint16_t> operator<<(const Vec256<uint16_t> v,
+                                     const Vec256<uint16_t> bits) {
+-#if HWY_TARGET == HWY_AVX3
++#if HWY_TARGET <= HWY_AVX3
+   return Vec256<uint16_t>{_mm256_sllv_epi16(v.raw, bits.raw)};
+ #else
+   return v * detail::Pow2(bits);
+@@ -2246,7 +2332,7 @@ HWY_API Vec256<T> operator<<(const Vec256<T> v, const Vec256<T> bits) {
+ 
+ HWY_API Vec256<uint16_t> operator>>(const Vec256<uint16_t> v,
+                                     const Vec256<uint16_t> bits) {
+-#if HWY_TARGET == HWY_AVX3
++#if HWY_TARGET <= HWY_AVX3
+   return Vec256<uint16_t>{_mm256_srlv_epi16(v.raw, bits.raw)};
+ #else
+   const Full256<uint16_t> d;
+@@ -2269,7 +2355,7 @@ HWY_API Vec256<uint64_t> operator>>(const Vec256<uint64_t> v,
+ 
+ HWY_API Vec256<int16_t> operator>>(const Vec256<int16_t> v,
+                                    const Vec256<int16_t> bits) {
+-#if HWY_TARGET == HWY_AVX3
++#if HWY_TARGET <= HWY_AVX3
+   return Vec256<int16_t>{_mm256_srav_epi16(v.raw, bits.raw)};
+ #else
+   return detail::SignedShr(Full256<int16_t>(), v, bits);
+@@ -2283,22 +2369,73 @@ HWY_API Vec256<int32_t> operator>>(const Vec256<int32_t> v,
+ 
+ HWY_API Vec256<int64_t> operator>>(const Vec256<int64_t> v,
+                                    const Vec256<int64_t> bits) {
+-#if HWY_TARGET == HWY_AVX3
++#if HWY_TARGET <= HWY_AVX3
+   return Vec256<int64_t>{_mm256_srav_epi64(v.raw, bits.raw)};
+ #else
+   return detail::SignedShr(Full256<int64_t>(), v, bits);
+ #endif
+ }
+ 
+-// ================================================== CONVERT
++HWY_INLINE Vec256<uint64_t> MulEven(const Vec256<uint64_t> a,
++                                    const Vec256<uint64_t> b) {
++  const DFromV<decltype(a)> du64;
++  const RepartitionToNarrow<decltype(du64)> du32;
++  const auto maskL = Set(du64, 0xFFFFFFFFULL);
++  const auto a32 = BitCast(du32, a);
++  const auto b32 = BitCast(du32, b);
++  // Inputs for MulEven: we only need the lower 32 bits
++  const auto aH = Shuffle2301(a32);
++  const auto bH = Shuffle2301(b32);
+ 
+-// ------------------------------ Promotions (part w/ narrow lanes -> full)
++  // Knuth double-word multiplication. We use 32x32 = 64 MulEven and only need
++  // the even (lower 64 bits of every 128-bit block) results. See
++  // https://github.com/hcs0/Hackers-Delight/blob/master/muldwu.c.tat
++  const auto aLbL = MulEven(a32, b32);
++  const auto w3 = aLbL & maskL;
+ 
+-HWY_API Vec256<float> PromoteTo(Full256<float> /* tag */,
+-                                const Vec128<float16_t, 8> v) {
+-  return Vec256<float>{_mm256_cvtph_ps(v.raw)};
++  const auto t2 = MulEven(aH, b32) + ShiftRight<32>(aLbL);
++  const auto w2 = t2 & maskL;
++  const auto w1 = ShiftRight<32>(t2);
++
++  const auto t = MulEven(a32, bH) + w2;
++  const auto k = ShiftRight<32>(t);
++
++  const auto mulH = MulEven(aH, bH) + w1 + k;
++  const auto mulL = ShiftLeft<32>(t) + w3;
++  return InterleaveLower(mulL, mulH);
+ }
+ 
++HWY_INLINE Vec256<uint64_t> MulOdd(const Vec256<uint64_t> a,
++                                   const Vec256<uint64_t> b) {
++  const DFromV<decltype(a)> du64;
++  const RepartitionToNarrow<decltype(du64)> du32;
++  const auto maskL = Set(du64, 0xFFFFFFFFULL);
++  const auto a32 = BitCast(du32, a);
++  const auto b32 = BitCast(du32, b);
++  // Inputs for MulEven: we only need bits [95:64] (= upper half of input)
++  const auto aH = Shuffle2301(a32);
++  const auto bH = Shuffle2301(b32);
++
++  // Same as above, but we're using the odd results (upper 64 bits per block).
++  const auto aLbL = MulEven(a32, b32);
++  const auto w3 = aLbL & maskL;
++
++  const auto t2 = MulEven(aH, b32) + ShiftRight<32>(aLbL);
++  const auto w2 = t2 & maskL;
++  const auto w1 = ShiftRight<32>(t2);
++
++  const auto t = MulEven(a32, bH) + w2;
++  const auto k = ShiftRight<32>(t);
++
++  const auto mulH = MulEven(aH, bH) + w1 + k;
++  const auto mulL = ShiftLeft<32>(t) + w3;
++  return InterleaveUpper(du64, mulL, mulH);
++}
++
++// ================================================== CONVERT
++
++// ------------------------------ Promotions (part w/ narrow lanes -> full)
++
+ HWY_API Vec256<double> PromoteTo(Full256<double> /* tag */,
+                                  const Vec128<float, 4> v) {
+   return Vec256<double>{_mm256_cvtps_pd(v.raw)};
+@@ -2420,9 +2557,38 @@ HWY_API Vec128<int8_t> DemoteTo(Full128<int8_t> /* tag */,
+ HWY_DIAGNOSTICS(push)
+ HWY_DIAGNOSTICS_OFF(disable : 4556, ignored "-Wsign-conversion")
+ 
+-HWY_API Vec128<float16_t> DemoteTo(Full128<float16_t> /* tag */,
++HWY_API Vec128<float16_t> DemoteTo(Full128<float16_t> df16,
+                                    const Vec256<float> v) {
++#ifdef HWY_DISABLE_F16C
++  const RebindToUnsigned<decltype(df16)> du16;
++  const Rebind<uint32_t, decltype(df16)> du;
++  const RebindToSigned<decltype(du)> di;
++  const auto bits32 = BitCast(du, v);
++  const auto sign = ShiftRight<31>(bits32);
++  const auto biased_exp32 = ShiftRight<23>(bits32) & Set(du, 0xFF);
++  const auto mantissa32 = bits32 & Set(du, 0x7FFFFF);
++
++  const auto k15 = Set(di, 15);
++  const auto exp = Min(BitCast(di, biased_exp32) - Set(di, 127), k15);
++  const auto is_tiny = exp < Set(di, -24);
++
++  const auto is_subnormal = exp < Set(di, -14);
++  const auto biased_exp16 =
++      BitCast(du, IfThenZeroElse(is_subnormal, exp + k15));
++  const auto sub_exp = BitCast(du, Set(di, -14) - exp);  // [1, 11)
++  const auto sub_m = (Set(du, 1) << (Set(du, 10) - sub_exp)) +
++                     (mantissa32 >> (Set(du, 13) + sub_exp));
++  const auto mantissa16 = IfThenElse(RebindMask(du, is_subnormal), sub_m,
++                                     ShiftRight<13>(mantissa32));  // <1024
++
++  const auto sign16 = ShiftLeft<15>(sign);
++  const auto normal16 = sign16 | ShiftLeft<10>(biased_exp16) | mantissa16;
++  const auto bits16 = IfThenZeroElse(is_tiny, BitCast(di, normal16));
++  return BitCast(df16, DemoteTo(du16, bits16));
++#else
++  (void)df16;
+   return Vec128<float16_t>{_mm256_cvtps_ph(v.raw, _MM_FROUND_NO_EXC)};
++#endif
+ }
+ 
+ HWY_DIAGNOSTICS(pop)
+@@ -2447,7 +2613,7 @@ HWY_API Vec128<uint8_t, 8> U8FromU32(const Vec256<uint32_t> v) {
+   const auto quad = TableLookupBytes(v, Load(d32, k8From32));
+   // Interleave both quadruplets - OR instead of unpack reduces port5 pressure.
+   const auto lo = LowerHalf(quad);
+-  const auto hi = UpperHalf(quad);
++  const auto hi = UpperHalf(Full128<uint32_t>(), quad);
+   const auto pair = LowerHalf(lo | hi);
+   return BitCast(Simd<uint8_t, 8>(), pair);
+ }
+@@ -2460,7 +2626,7 @@ HWY_API Vec256<float> ConvertTo(Full256<float> /* tag */,
+ }
+ 
+ HWY_API Vec256<double> ConvertTo(Full256<double> dd, const Vec256<int64_t> v) {
+-#if HWY_TARGET == HWY_AVX3
++#if HWY_TARGET <= HWY_AVX3
+   (void)dd;
+   return Vec256<double>{_mm256_cvtepi64_pd(v.raw)};
+ #else
+@@ -2487,22 +2653,42 @@ HWY_API Vec256<int32_t> ConvertTo(Full256<int32_t> d, const Vec256<float> v) {
+ }
+ 
+ HWY_API Vec256<int64_t> ConvertTo(Full256<int64_t> di, const Vec256<double> v) {
+-#if HWY_TARGET == HWY_AVX3
++#if HWY_TARGET <= HWY_AVX3
+   return detail::FixConversionOverflow(di, v, _mm256_cvttpd_epi64(v.raw));
+ #else
+-  alignas(32) double lanes_d[4];
+-  Store(v, Full256<double>(), lanes_d);
+-  alignas(32) int64_t lanes_i[4];
+-  for (size_t i = 0; i < 4; ++i) {
+-    if (lanes_d[i] >= static_cast<double>(LimitsMax<int64_t>())) {
+-      lanes_i[i] = LimitsMax<int64_t>();
+-    } else if (lanes_d[i] <= static_cast<double>(LimitsMin<int64_t>())) {
+-      lanes_i[i] = LimitsMin<int64_t>();
+-    } else {
+-      lanes_i[i] = static_cast<int64_t>(lanes_d[i]);
+-    }
+-  }
+-  return Load(di, lanes_i);
++  using VI = decltype(Zero(di));
++  const VI k0 = Zero(di);
++  const VI k1 = Set(di, 1);
++  const VI k51 = Set(di, 51);
++
++  // Exponent indicates whether the number can be represented as int64_t.
++  const VI biased_exp = ShiftRight<52>(BitCast(di, v)) & Set(di, 0x7FF);
++  const VI exp = biased_exp - Set(di, 0x3FF);
++  const auto in_range = exp < Set(di, 63);
++
++  // If we were to cap the exponent at 51 and add 2^52, the number would be in
++  // [2^52, 2^53) and mantissa bits could be read out directly. We need to
++  // round-to-0 (truncate), but changing rounding mode in MXCSR hits a
++  // compiler reordering bug: https://gcc.godbolt.org/z/4hKj6c6qc . We instead
++  // manually shift the mantissa into place (we already have many of the
++  // inputs anyway).
++  const VI shift_mnt = Max(k51 - exp, k0);
++  const VI shift_int = Max(exp - k51, k0);
++  const VI mantissa = BitCast(di, v) & Set(di, (1ULL << 52) - 1);
++  // Include implicit 1-bit; shift by one more to ensure it's in the mantissa.
++  const VI int52 = (mantissa | Set(di, 1ULL << 52)) >> (shift_mnt + k1);
++  // For inputs larger than 2^52, insert zeros at the bottom.
++  const VI shifted = int52 << shift_int;
++  // Restore the one bit lost when shifting in the implicit 1-bit.
++  const VI restored = shifted | ((mantissa & k1) << (shift_int - k1));
++
++  // Saturate to LimitsMin (unchanged when negating below) or LimitsMax.
++  const VI sign_mask = BroadcastSignBit(BitCast(di, v));
++  const VI limit = Set(di, LimitsMax<int64_t>()) - sign_mask;
++  const VI magnitude = IfThenElse(in_range, restored, limit);
++
++  // If the input was negative, negate the integer (two's complement).
++  return (magnitude ^ sign_mask) - sign_mask;
+ #endif
+ }
+ 
+@@ -2511,11 +2697,84 @@ HWY_API Vec256<int32_t> NearestInt(const Vec256<float> v) {
+   return detail::FixConversionOverflow(di, v, _mm256_cvtps_epi32(v.raw));
+ }
+ 
++
++HWY_API Vec256<float> PromoteTo(Full256<float> df32,
++                                const Vec128<float16_t> v) {
++#ifdef HWY_DISABLE_F16C
++  const RebindToSigned<decltype(df32)> di32;
++  const RebindToUnsigned<decltype(df32)> du32;
++  // Expand to u32 so we can shift.
++  const auto bits16 = PromoteTo(du32, Vec128<uint16_t>{v.raw});
++  const auto sign = ShiftRight<15>(bits16);
++  const auto biased_exp = ShiftRight<10>(bits16) & Set(du32, 0x1F);
++  const auto mantissa = bits16 & Set(du32, 0x3FF);
++  const auto subnormal =
++      BitCast(du32, ConvertTo(df32, BitCast(di32, mantissa)) *
++                        Set(df32, 1.0f / 16384 / 1024));
++
++  const auto biased_exp32 = biased_exp + Set(du32, 127 - 15);
++  const auto mantissa32 = ShiftLeft<23 - 10>(mantissa);
++  const auto normal = ShiftLeft<23>(biased_exp32) | mantissa32;
++  const auto bits32 = IfThenElse(biased_exp == Zero(du32), subnormal, normal);
++  return BitCast(df32, ShiftLeft<31>(sign) | bits32);
++#else
++  (void)df32;
++  return Vec256<float>{_mm256_cvtph_ps(v.raw)};
++#endif
++}
++
++// ================================================== CRYPTO
++
++#if !defined(HWY_DISABLE_PCLMUL_AES)
++
++// Per-target flag to prevent generic_ops-inl.h from defining AESRound.
++#ifdef HWY_NATIVE_AES
++#undef HWY_NATIVE_AES
++#else
++#define HWY_NATIVE_AES
++#endif
++
++HWY_API Vec256<uint8_t> AESRound(Vec256<uint8_t> state,
++                                 Vec256<uint8_t> round_key) {
++#if HWY_TARGET == HWY_AVX3_DL
++  return Vec256<uint8_t>{_mm256_aesenc_epi128(state.raw, round_key.raw)};
++#else
++  const Full256<uint8_t> d;
++  const Half<decltype(d)> d2;
++  return Combine(d, AESRound(UpperHalf(d2, state), UpperHalf(d2, round_key)),
++                 AESRound(LowerHalf(state), LowerHalf(round_key)));
++#endif
++}
++
++HWY_API Vec256<uint64_t> CLMulLower(Vec256<uint64_t> a, Vec256<uint64_t> b) {
++#if HWY_TARGET == HWY_AVX3_DL
++  return Vec256<uint64_t>{_mm256_clmulepi64_epi128(a.raw, b.raw, 0x00)};
++#else
++  const Full256<uint64_t> d;
++  const Half<decltype(d)> d2;
++  return Combine(d, CLMulLower(UpperHalf(d2, a), UpperHalf(d2, b)),
++                 CLMulLower(LowerHalf(a), LowerHalf(b)));
++#endif
++}
++
++HWY_API Vec256<uint64_t> CLMulUpper(Vec256<uint64_t> a, Vec256<uint64_t> b) {
++#if HWY_TARGET == HWY_AVX3_DL
++  return Vec256<uint64_t>{_mm256_clmulepi64_epi128(a.raw, b.raw, 0x11)};
++#else
++  const Full256<uint64_t> d;
++  const Half<decltype(d)> d2;
++  return Combine(d, CLMulUpper(UpperHalf(d2, a), UpperHalf(d2, b)),
++                 CLMulUpper(LowerHalf(a), LowerHalf(b)));
++#endif
++}
++
++#endif  // HWY_DISABLE_PCLMUL_AES
++
+ // ================================================== MISC
+ 
+ // Returns a vector with lane i=[0, N) set to "first" + i.
+ template <typename T, typename T2>
+-Vec256<T> Iota(const Full256<T> d, const T2 first) {
++HWY_API Vec256<T> Iota(const Full256<T> d, const T2 first) {
+   HWY_ALIGN T lanes[32 / sizeof(T)];
+   for (size_t i = 0; i < 32 / sizeof(T); ++i) {
+     lanes[i] = static_cast<T>(first + static_cast<T2>(i));
+@@ -2528,7 +2787,8 @@ Vec256<T> Iota(const Full256<T> d, const T2 first) {
+ namespace detail {
+ 
+ template <typename T>
+-HWY_API uint64_t BitsFromMask(hwy::SizeTag<1> /*tag*/, const Mask256<T> mask) {
++HWY_INLINE uint64_t BitsFromMask(hwy::SizeTag<1> /*tag*/,
++                                 const Mask256<T> mask) {
+   const Full256<T> d;
+   const Full256<uint8_t> d8;
+   const auto sign_bits = BitCast(d8, VecFromMask(d, mask)).raw;
+@@ -2537,7 +2797,8 @@ HWY_API uint64_t BitsFromMask(hwy::SizeTag<1> /*tag*/, const Mask256<T> mask) {
+ }
+ 
+ template <typename T>
+-HWY_API uint64_t BitsFromMask(hwy::SizeTag<2> /*tag*/, const Mask256<T> mask) {
++HWY_INLINE uint64_t BitsFromMask(hwy::SizeTag<2> /*tag*/,
++                                 const Mask256<T> mask) {
+ #if HWY_ARCH_X86_64
+   const uint64_t sign_bits8 = BitsFromMask(hwy::SizeTag<1>(), mask);
+   // Skip the bits from the lower byte of each u16 (better not to use the
+@@ -2556,7 +2817,8 @@ HWY_API uint64_t BitsFromMask(hwy::SizeTag<2> /*tag*/, const Mask256<T> mask) {
+ }
+ 
+ template <typename T>
+-HWY_API uint64_t BitsFromMask(hwy::SizeTag<4> /*tag*/, const Mask256<T> mask) {
++HWY_INLINE uint64_t BitsFromMask(hwy::SizeTag<4> /*tag*/,
++                                 const Mask256<T> mask) {
+   const Full256<T> d;
+   const Full256<float> df;
+   const auto sign_bits = BitCast(df, VecFromMask(d, mask)).raw;
+@@ -2564,7 +2826,8 @@ HWY_API uint64_t BitsFromMask(hwy::SizeTag<4> /*tag*/, const Mask256<T> mask) {
+ }
+ 
+ template <typename T>
+-HWY_API uint64_t BitsFromMask(hwy::SizeTag<8> /*tag*/, const Mask256<T> mask) {
++HWY_INLINE uint64_t BitsFromMask(hwy::SizeTag<8> /*tag*/,
++                                 const Mask256<T> mask) {
+   const Full256<T> d;
+   const Full256<double> df;
+   const auto sign_bits = BitCast(df, VecFromMask(d, mask)).raw;
+@@ -2572,14 +2835,15 @@ HWY_API uint64_t BitsFromMask(hwy::SizeTag<8> /*tag*/, const Mask256<T> mask) {
+ }
+ 
+ template <typename T>
+-HWY_API uint64_t BitsFromMask(const Mask256<T> mask) {
++HWY_INLINE uint64_t BitsFromMask(const Mask256<T> mask) {
+   return BitsFromMask(hwy::SizeTag<sizeof(T)>(), mask);
+ }
+ 
+ }  // namespace detail
+ 
+ template <typename T>
+-HWY_INLINE size_t StoreMaskBits(const Mask256<T> mask, uint8_t* p) {
++HWY_API size_t StoreMaskBits(const Full256<T> /* tag */, const Mask256<T> mask,
++                             uint8_t* p) {
+   const uint64_t bits = detail::BitsFromMask(mask);
+   const size_t kNumBytes = (4 + sizeof(T) - 1) / sizeof(T);
+   CopyBytes<kNumBytes>(&bits, p);
+@@ -2587,22 +2851,29 @@ HWY_INLINE size_t StoreMaskBits(const Mask256<T> mask, uint8_t* p) {
+ }
+ 
+ template <typename T>
+-HWY_API bool AllFalse(const Mask256<T> mask) {
++HWY_API bool AllFalse(const Full256<T> /* tag */, const Mask256<T> mask) {
+   // Cheaper than PTEST, which is 2 uop / 3L.
+   return detail::BitsFromMask(mask) == 0;
+ }
+ 
+ template <typename T>
+-HWY_API bool AllTrue(const Mask256<T> mask) {
++HWY_API bool AllTrue(const Full256<T> /* tag */, const Mask256<T> mask) {
+   constexpr uint64_t kAllBits = (1ull << (32 / sizeof(T))) - 1;
+   return detail::BitsFromMask(mask) == kAllBits;
+ }
+ 
+ template <typename T>
+-HWY_API size_t CountTrue(const Mask256<T> mask) {
++HWY_API size_t CountTrue(const Full256<T> /* tag */, const Mask256<T> mask) {
+   return PopCount(detail::BitsFromMask(mask));
+ }
+ 
++template <typename T>
++HWY_API intptr_t FindFirstTrue(const Full256<T> /* tag */,
++                               const Mask256<T> mask) {
++  const uint64_t bits = detail::BitsFromMask(mask);
++  return bits ? Num0BitsBelowLS1Bit_Nonzero64(bits) : -1;
++}
++
+ // ------------------------------ Compress
+ 
+ namespace detail {
+@@ -2694,10 +2965,10 @@ HWY_INLINE Vec256<uint32_t> Idx64x4FromBits(const uint64_t mask_bits) {
+ // redundant BitsFromMask in the latter.
+ 
+ template <typename T>
+-HWY_API Vec256<T> Compress(hwy::SizeTag<4> /*tag*/, Vec256<T> v,
+-                           const uint64_t mask_bits) {
++HWY_INLINE Vec256<T> Compress(hwy::SizeTag<4> /*tag*/, Vec256<T> v,
++                              const uint64_t mask_bits) {
+   const auto vu = BitCast(Full256<uint32_t>(), v);
+-#if HWY_TARGET == HWY_AVX3
++#if HWY_TARGET <= HWY_AVX3
+   const __m256i ret =
+       _mm256_maskz_compress_epi32(static_cast<__mmask8>(mask_bits), vu.raw);
+ #else
+@@ -2708,10 +2979,10 @@ HWY_API Vec256<T> Compress(hwy::SizeTag<4> /*tag*/, Vec256<T> v,
+ }
+ 
+ template <typename T>
+-HWY_API Vec256<T> Compress(hwy::SizeTag<8> /*tag*/, Vec256<T> v,
+-                           const uint64_t mask_bits) {
++HWY_INLINE Vec256<T> Compress(hwy::SizeTag<8> /*tag*/, Vec256<T> v,
++                              const uint64_t mask_bits) {
+   const auto vu = BitCast(Full256<uint64_t>(), v);
+-#if HWY_TARGET == HWY_AVX3
++#if HWY_TARGET <= HWY_AVX3
+   const __m256i ret =
+       _mm256_maskz_compress_epi64(static_cast<__mmask8>(mask_bits), vu.raw);
+ #else
+@@ -2722,19 +2993,19 @@ HWY_API Vec256<T> Compress(hwy::SizeTag<8> /*tag*/, Vec256<T> v,
+ }
+ 
+ // Otherwise, defined in x86_512-inl.h so it can use wider vectors.
+-#if HWY_TARGET != HWY_AVX3
++#if HWY_TARGET > HWY_AVX3  // AVX2 or older
+ 
+ // LUTs are infeasible for 2^16 possible masks. Promoting to 32-bit and using
+ // the native Compress is probably more efficient than 2 LUTs.
+ template <typename T>
+-HWY_API Vec256<T> Compress(hwy::SizeTag<2> /*tag*/, Vec256<T> v,
+-                           const uint64_t mask_bits) {
++HWY_INLINE Vec256<T> Compress(hwy::SizeTag<2> /*tag*/, Vec256<T> v,
++                              const uint64_t mask_bits) {
+   using D = Full256<T>;
+   const Rebind<uint16_t, D> du;
+   const Repartition<int32_t, D> dw;
+   const auto vu16 = BitCast(du, v);  // (required for float16_t inputs)
+   const auto promoted0 = PromoteTo(dw, LowerHalf(vu16));
+-  const auto promoted1 = PromoteTo(dw, UpperHalf(vu16));
++  const auto promoted1 = PromoteTo(dw, UpperHalf(Half<decltype(du)>(), vu16));
+ 
+   const uint64_t mask_bits0 = mask_bits & 0xFF;
+   const uint64_t mask_bits1 = mask_bits >> 8;
+@@ -2762,7 +3033,8 @@ HWY_API Vec256<T> Compress(hwy::SizeTag<2> /*tag*/, Vec256<T> v,
+       Vec256<uint16_t>{_mm256_alignr_epi8(shift1_multiple4.raw, lo_zz, 14)};
+ 
+   // Make the shift conditional on the lower bit of count0.
+-  const auto m_odd = TestBit(Set(du, count0), Set(du, 1));
++  const auto m_odd =
++      TestBit(Set(du, static_cast<uint16_t>(count0)), Set(du, 1));
+   const auto shifted1 = IfThenElse(m_odd, shift1_multiple2, shift1_multiple4);
+ 
+   // Blend the lower and shifted upper parts.
+@@ -2773,12 +3045,12 @@ HWY_API Vec256<T> Compress(hwy::SizeTag<2> /*tag*/, Vec256<T> v,
+   return BitCast(D(), IfThenElse(m_lower, demoted0, shifted1));
+ }
+ 
+-#endif  // HWY_TARGET != HWY_AVX3
++#endif  // HWY_TARGET > HWY_AVX3
+ 
+ }  // namespace detail
+ 
+ // Otherwise, defined in x86_512-inl.h after detail::Compress.
+-#if HWY_TARGET != HWY_AVX3
++#if HWY_TARGET > HWY_AVX3  // AVX2 or older
+ 
+ template <typename T>
+ HWY_API Vec256<T> Compress(Vec256<T> v, const Mask256<T> mask) {
+@@ -2799,7 +3071,7 @@ HWY_API size_t CompressStore(Vec256<T> v, const Mask256<T> mask, Full256<T> d,
+   return PopCount(mask_bits);
+ }
+ 
+-#endif  // HWY_TARGET != HWY_AVX3
++#endif  // HWY_TARGET > HWY_AVX3
+ 
+ // ------------------------------ StoreInterleaved3 (CombineShiftRightBytes,
+ // TableLookupBytes, ConcatUpperLower)
+@@ -2821,7 +3093,7 @@ HWY_API void StoreInterleaved3(const Vec256<uint8_t> v0,
+       0x80, 2, 0x80, 0x80, 3, 0x80, 0x80, 4, 0x80, 0x80};
+   const auto shuf_r0 = LoadDup128(d, tbl_r0);
+   const auto shuf_g0 = LoadDup128(d, tbl_g0);  // cannot reuse r0 due to 5
+-  const auto shuf_b0 = CombineShiftRightBytes<15>(shuf_g0, shuf_g0);
++  const auto shuf_b0 = CombineShiftRightBytes<15>(d, shuf_g0, shuf_g0);
+   const auto r0 = TableLookupBytes(v0, shuf_r0);  // 5..4..3..2..1..0
+   const auto g0 = TableLookupBytes(v1, shuf_g0);  // ..4..3..2..1..0.
+   const auto b0 = TableLookupBytes(v2, shuf_b0);  // .4..3..2..1..0..
+@@ -2840,7 +3112,7 @@ HWY_API void StoreInterleaved3(const Vec256<uint8_t> v0,
+   // upper halves. We could obtain 10_05 and 15_0A via ConcatUpperLower, but
+   // that would require two ununaligned stores. For the lower halves, we can
+   // merge two 128-bit stores for the same swizzling cost:
+-  const auto out0 = ConcatLowerLower(interleaved_15_05, interleaved_10_00);
++  const auto out0 = ConcatLowerLower(d, interleaved_15_05, interleaved_10_00);
+   StoreU(out0, d, unaligned + 0 * 32);
+ 
+   // Third vector: bgr[15:11], b10
+@@ -2852,10 +3124,10 @@ HWY_API void StoreInterleaved3(const Vec256<uint8_t> v0,
+   const auto b2 = TableLookupBytes(v2, shuf_b2);
+   const auto interleaved_1A_0A = r2 | g2 | b2;
+ 
+-  const auto out1 = ConcatUpperLower(interleaved_10_00, interleaved_1A_0A);
++  const auto out1 = ConcatUpperLower(d, interleaved_10_00, interleaved_1A_0A);
+   StoreU(out1, d, unaligned + 1 * 32);
+ 
+-  const auto out2 = ConcatUpperUpper(interleaved_1A_0A, interleaved_15_05);
++  const auto out2 = ConcatUpperUpper(d, interleaved_1A_0A, interleaved_15_05);
+   StoreU(out2, d, unaligned + 2 * 32);
+ }
+ 
+@@ -2864,27 +3136,29 @@ HWY_API void StoreInterleaved3(const Vec256<uint8_t> v0,
+ HWY_API void StoreInterleaved4(const Vec256<uint8_t> v0,
+                                const Vec256<uint8_t> v1,
+                                const Vec256<uint8_t> v2,
+-                               const Vec256<uint8_t> v3, Full256<uint8_t> d,
++                               const Vec256<uint8_t> v3, Full256<uint8_t> d8,
+                                uint8_t* HWY_RESTRICT unaligned) {
++  const RepartitionToWide<decltype(d8)> d16;
++  const RepartitionToWide<decltype(d16)> d32;
+   // let a,b,c,d denote v0..3.
+-  const auto ba0 = ZipLower(v0, v1);  // b7 a7 .. b0 a0
+-  const auto dc0 = ZipLower(v2, v3);  // d7 c7 .. d0 c0
+-  const auto ba8 = ZipUpper(v0, v1);
+-  const auto dc8 = ZipUpper(v2, v3);
+-  const auto dcba_0 = ZipLower(ba0, dc0);  // d..a13 d..a10 | d..a03 d..a00
+-  const auto dcba_4 = ZipUpper(ba0, dc0);  // d..a17 d..a14 | d..a07 d..a04
+-  const auto dcba_8 = ZipLower(ba8, dc8);  // d..a1B d..a18 | d..a0B d..a08
+-  const auto dcba_C = ZipUpper(ba8, dc8);  // d..a1F d..a1C | d..a0F d..a0C
++  const auto ba0 = ZipLower(d16, v0, v1);  // b7 a7 .. b0 a0
++  const auto dc0 = ZipLower(d16, v2, v3);  // d7 c7 .. d0 c0
++  const auto ba8 = ZipUpper(d16, v0, v1);
++  const auto dc8 = ZipUpper(d16, v2, v3);
++  const auto dcba_0 = ZipLower(d32, ba0, dc0);  // d..a13 d..a10 | d..a03 d..a00
++  const auto dcba_4 = ZipUpper(d32, ba0, dc0);  // d..a17 d..a14 | d..a07 d..a04
++  const auto dcba_8 = ZipLower(d32, ba8, dc8);  // d..a1B d..a18 | d..a0B d..a08
++  const auto dcba_C = ZipUpper(d32, ba8, dc8);  // d..a1F d..a1C | d..a0F d..a0C
+   // Write lower halves, then upper. vperm2i128 is slow on Zen1 but we can
+   // efficiently combine two lower halves into 256 bits:
+-  const auto out0 = BitCast(d, ConcatLowerLower(dcba_4, dcba_0));
+-  const auto out1 = BitCast(d, ConcatLowerLower(dcba_C, dcba_8));
+-  StoreU(out0, d, unaligned + 0 * 32);
+-  StoreU(out1, d, unaligned + 1 * 32);
+-  const auto out2 = BitCast(d, ConcatUpperUpper(dcba_4, dcba_0));
+-  const auto out3 = BitCast(d, ConcatUpperUpper(dcba_C, dcba_8));
+-  StoreU(out2, d, unaligned + 2 * 32);
+-  StoreU(out3, d, unaligned + 3 * 32);
++  const auto out0 = BitCast(d8, ConcatLowerLower(d32, dcba_4, dcba_0));
++  const auto out1 = BitCast(d8, ConcatLowerLower(d32, dcba_C, dcba_8));
++  StoreU(out0, d8, unaligned + 0 * 32);
++  StoreU(out1, d8, unaligned + 1 * 32);
++  const auto out2 = BitCast(d8, ConcatUpperUpper(d32, dcba_4, dcba_0));
++  const auto out3 = BitCast(d8, ConcatUpperUpper(d32, dcba_C, dcba_8));
++  StoreU(out2, d8, unaligned + 2 * 32);
++  StoreU(out3, d8, unaligned + 3 * 32);
+ }
+ 
+ // ------------------------------ Reductions
+@@ -2894,21 +3168,24 @@ namespace detail {
+ // Returns sum{lane[i]} in each lane. "v3210" is a replicated 128-bit block.
+ // Same logic as x86/128.h, but with Vec256 arguments.
+ template <typename T>
+-HWY_API Vec256<T> SumOfLanes(hwy::SizeTag<4> /* tag */, const Vec256<T> v3210) {
++HWY_INLINE Vec256<T> SumOfLanes(hwy::SizeTag<4> /* tag */,
++                                const Vec256<T> v3210) {
+   const auto v1032 = Shuffle1032(v3210);
+   const auto v31_20_31_20 = v3210 + v1032;
+   const auto v20_31_20_31 = Shuffle0321(v31_20_31_20);
+   return v20_31_20_31 + v31_20_31_20;
+ }
+ template <typename T>
+-HWY_API Vec256<T> MinOfLanes(hwy::SizeTag<4> /* tag */, const Vec256<T> v3210) {
++HWY_INLINE Vec256<T> MinOfLanes(hwy::SizeTag<4> /* tag */,
++                                const Vec256<T> v3210) {
+   const auto v1032 = Shuffle1032(v3210);
+   const auto v31_20_31_20 = Min(v3210, v1032);
+   const auto v20_31_20_31 = Shuffle0321(v31_20_31_20);
+   return Min(v20_31_20_31, v31_20_31_20);
+ }
+ template <typename T>
+-HWY_API Vec256<T> MaxOfLanes(hwy::SizeTag<4> /* tag */, const Vec256<T> v3210) {
++HWY_INLINE Vec256<T> MaxOfLanes(hwy::SizeTag<4> /* tag */,
++                                const Vec256<T> v3210) {
+   const auto v1032 = Shuffle1032(v3210);
+   const auto v31_20_31_20 = Max(v3210, v1032);
+   const auto v20_31_20_31 = Shuffle0321(v31_20_31_20);
+@@ -2916,17 +3193,20 @@ HWY_API Vec256<T> MaxOfLanes(hwy::SizeTag<4> /* tag */, const Vec256<T> v3210) {
+ }
+ 
+ template <typename T>
+-HWY_API Vec256<T> SumOfLanes(hwy::SizeTag<8> /* tag */, const Vec256<T> v10) {
++HWY_INLINE Vec256<T> SumOfLanes(hwy::SizeTag<8> /* tag */,
++                                const Vec256<T> v10) {
+   const auto v01 = Shuffle01(v10);
+   return v10 + v01;
+ }
+ template <typename T>
+-HWY_API Vec256<T> MinOfLanes(hwy::SizeTag<8> /* tag */, const Vec256<T> v10) {
++HWY_INLINE Vec256<T> MinOfLanes(hwy::SizeTag<8> /* tag */,
++                                const Vec256<T> v10) {
+   const auto v01 = Shuffle01(v10);
+   return Min(v10, v01);
+ }
+ template <typename T>
+-HWY_API Vec256<T> MaxOfLanes(hwy::SizeTag<8> /* tag */, const Vec256<T> v10) {
++HWY_INLINE Vec256<T> MaxOfLanes(hwy::SizeTag<8> /* tag */,
++                                const Vec256<T> v10) {
+   const auto v01 = Shuffle01(v10);
+   return Max(v10, v01);
+ }
+@@ -2935,21 +3215,116 @@ HWY_API Vec256<T> MaxOfLanes(hwy::SizeTag<8> /* tag */, const Vec256<T> v10) {
+ 
+ // Supported for {uif}32x8, {uif}64x4. Returns the sum in each lane.
+ template <typename T>
+-HWY_API Vec256<T> SumOfLanes(const Vec256<T> vHL) {
+-  const Vec256<T> vLH = ConcatLowerUpper(vHL, vHL);
++HWY_API Vec256<T> SumOfLanes(Full256<T> d, const Vec256<T> vHL) {
++  const Vec256<T> vLH = ConcatLowerUpper(d, vHL, vHL);
+   return detail::SumOfLanes(hwy::SizeTag<sizeof(T)>(), vLH + vHL);
+ }
+ template <typename T>
+-HWY_API Vec256<T> MinOfLanes(const Vec256<T> vHL) {
+-  const Vec256<T> vLH = ConcatLowerUpper(vHL, vHL);
++HWY_API Vec256<T> MinOfLanes(Full256<T> d, const Vec256<T> vHL) {
++  const Vec256<T> vLH = ConcatLowerUpper(d, vHL, vHL);
+   return detail::MinOfLanes(hwy::SizeTag<sizeof(T)>(), Min(vLH, vHL));
+ }
+ template <typename T>
+-HWY_API Vec256<T> MaxOfLanes(const Vec256<T> vHL) {
+-  const Vec256<T> vLH = ConcatLowerUpper(vHL, vHL);
++HWY_API Vec256<T> MaxOfLanes(Full256<T> d, const Vec256<T> vHL) {
++  const Vec256<T> vLH = ConcatLowerUpper(d, vHL, vHL);
+   return detail::MaxOfLanes(hwy::SizeTag<sizeof(T)>(), Max(vLH, vHL));
+ }
+ 
++// ================================================== DEPRECATED
++
++template <typename T>
++HWY_API size_t StoreMaskBits(const Mask256<T> mask, uint8_t* p) {
++  return StoreMaskBits(Full256<T>(), mask, p);
++}
++
++template <typename T>
++HWY_API bool AllTrue(const Mask256<T> mask) {
++  return AllTrue(Full256<T>(), mask);
++}
++
++template <typename T>
++HWY_API bool AllFalse(const Mask256<T> mask) {
++  return AllFalse(Full256<T>(), mask);
++}
++
++template <typename T>
++HWY_API size_t CountTrue(const Mask256<T> mask) {
++  return CountTrue(Full256<T>(), mask);
++}
++
++template <typename T>
++HWY_API Vec256<T> SumOfLanes(const Vec256<T> vHL) {
++  return SumOfLanes(Full256<T>(), vHL);
++}
++template <typename T>
++HWY_API Vec256<T> MinOfLanes(const Vec256<T> vHL) {
++  return MinOfLanes(Full256<T>(), vHL);
++}
++template <typename T>
++HWY_API Vec256<T> MaxOfLanes(const Vec256<T> vHL) {
++  return MaxOfLanes(Full256<T>(), vHL);
++}
++
++template <typename T>
++HWY_API Vec128<T> UpperHalf(Vec256<T> v) {
++  return UpperHalf(Full128<T>(), v);
++}
++
++template <int kBytes, typename T>
++HWY_API Vec256<T> ShiftRightBytes(const Vec256<T> v) {
++  return ShiftRightBytes<kBytes>(Full256<T>(), v);
++}
++
++template <int kLanes, typename T>
++HWY_API Vec256<T> ShiftRightLanes(const Vec256<T> v) {
++  return ShiftRightLanes<kLanes>(Full256<T>(), v);
++}
++
++template <size_t kBytes, typename T>
++HWY_API Vec256<T> CombineShiftRightBytes(Vec256<T> hi, Vec256<T> lo) {
++  return CombineShiftRightBytes<kBytes>(Full256<T>(), hi, lo);
++}
++
++template <typename T>
++HWY_API Vec256<T> InterleaveUpper(Vec256<T> a, Vec256<T> b) {
++  return InterleaveUpper(Full256<T>(), a, b);
++}
++
++template <typename T>
++HWY_API Vec256<MakeWide<T>> ZipUpper(Vec256<T> a, Vec256<T> b) {
++  return InterleaveUpper(Full256<MakeWide<T>>(), a, b);
++}
++
++template <typename T>
++HWY_API Vec256<T> Combine(Vec128<T> hi, Vec128<T> lo) {
++  return Combine(Full256<T>(), hi, lo);
++}
++
++template <typename T>
++HWY_API Vec256<T> ZeroExtendVector(Vec128<T> lo) {
++  return ZeroExtendVector(Full256<T>(), lo);
++}
++
++template <typename T>
++HWY_API Vec256<T> ConcatLowerLower(Vec256<T> hi, Vec256<T> lo) {
++  return ConcatLowerLower(Full256<T>(), hi, lo);
++}
++
++template <typename T>
++HWY_API Vec256<T> ConcatLowerUpper(Vec256<T> hi, Vec256<T> lo) {
++  return ConcatLowerUpper(Full256<T>(), hi, lo);
++}
++
++template <typename T>
++HWY_API Vec256<T> ConcatUpperLower(Vec256<T> hi, Vec256<T> lo) {
++  return ConcatUpperLower(Full256<T>(), hi, lo);
++}
++
++template <typename T>
++HWY_API Vec256<T> ConcatUpperUpper(Vec256<T> hi, Vec256<T> lo) {
++  return ConcatUpperUpper(Full256<T>(), hi, lo);
++}
++
+ // NOLINTNEXTLINE(google-readability-namespace-comments)
+ }  // namespace HWY_NAMESPACE
+ }  // namespace hwy
+diff --git a/third_party/highway/hwy/ops/x86_512-inl.h b/third_party/highway/hwy/ops/x86_512-inl.h
+index fe34146fa972f..3d5792f307e20 100644
+--- a/third_party/highway/hwy/ops/x86_512-inl.h
++++ b/third_party/highway/hwy/ops/x86_512-inl.h
+@@ -23,17 +23,25 @@
+ // Including <immintrin.h> should be enough, but Clang's headers helpfully skip
+ // including these headers when _MSC_VER is defined, like when using clang-cl.
+ // Include these directly here.
++// clang-format off
+ #include <smmintrin.h>
++
+ #include <avxintrin.h>
+ #include <avx2intrin.h>
+ #include <f16cintrin.h>
+ #include <fmaintrin.h>
++
+ #include <avx512fintrin.h>
+ #include <avx512vlintrin.h>
+ #include <avx512bwintrin.h>
+ #include <avx512dqintrin.h>
+ #include <avx512vlbwintrin.h>
+ #include <avx512vldqintrin.h>
++#include <avx512bitalgintrin.h>
++#include <avx512vlbitalgintrin.h>
++#include <avx512vpopcntdqintrin.h>
++#include <avx512vpopcntdqvlintrin.h>
++// clang-format on
+ #endif
+ 
+ #include <stddef.h>
+@@ -46,6 +54,11 @@ HWY_BEFORE_NAMESPACE();
+ namespace hwy {
+ namespace HWY_NAMESPACE {
+ 
++template <typename T>
++using Full512 = Simd<T, 64 / sizeof(T)>;
++
++namespace detail {
++
+ template <typename T>
+ struct Raw512 {
+   using type = __m512i;
+@@ -59,12 +72,31 @@ struct Raw512<double> {
+   using type = __m512d;
+ };
+ 
+-template <typename T>
+-using Full512 = Simd<T, 64 / sizeof(T)>;
++// Template arg: sizeof(lane type)
++template <size_t size>
++struct RawMask512 {};
++template <>
++struct RawMask512<1> {
++  using type = __mmask64;
++};
++template <>
++struct RawMask512<2> {
++  using type = __mmask32;
++};
++template <>
++struct RawMask512<4> {
++  using type = __mmask16;
++};
++template <>
++struct RawMask512<8> {
++  using type = __mmask8;
++};
++
++}  // namespace detail
+ 
+ template <typename T>
+ class Vec512 {
+-  using Raw = typename Raw512<T>::type;
++  using Raw = typename detail::Raw512<T>::type;
+ 
+  public:
+   // Compound assignment. Only usable if there is a corresponding non-member
+@@ -94,44 +126,24 @@ class Vec512 {
+   Raw raw;
+ };
+ 
+-// Template arg: sizeof(lane type)
+-template <size_t size>
+-struct RawMask512 {};
+-template <>
+-struct RawMask512<1> {
+-  using type = __mmask64;
+-};
+-template <>
+-struct RawMask512<2> {
+-  using type = __mmask32;
+-};
+-template <>
+-struct RawMask512<4> {
+-  using type = __mmask16;
+-};
+-template <>
+-struct RawMask512<8> {
+-  using type = __mmask8;
+-};
+-
+ // Mask register: one bit per lane.
+ template <typename T>
+-class Mask512 {
+- public:
+-  using Raw = typename RawMask512<sizeof(T)>::type;
+-  Raw raw;
++struct Mask512 {
++  typename detail::RawMask512<sizeof(T)>::type raw;
+ };
+ 
+ // ------------------------------ BitCast
+ 
+ namespace detail {
+ 
+-HWY_API __m512i BitCastToInteger(__m512i v) { return v; }
+-HWY_API __m512i BitCastToInteger(__m512 v) { return _mm512_castps_si512(v); }
+-HWY_API __m512i BitCastToInteger(__m512d v) { return _mm512_castpd_si512(v); }
++HWY_INLINE __m512i BitCastToInteger(__m512i v) { return v; }
++HWY_INLINE __m512i BitCastToInteger(__m512 v) { return _mm512_castps_si512(v); }
++HWY_INLINE __m512i BitCastToInteger(__m512d v) {
++  return _mm512_castpd_si512(v);
++}
+ 
+ template <typename T>
+-HWY_API Vec512<uint8_t> BitCastToByte(Vec512<T> v) {
++HWY_INLINE Vec512<uint8_t> BitCastToByte(Vec512<T> v) {
+   return Vec512<uint8_t>{BitCastToInteger(v.raw)};
+ }
+ 
+@@ -150,7 +162,7 @@ struct BitCastFromInteger512<double> {
+ };
+ 
+ template <typename T>
+-HWY_API Vec512<T> BitCastFromByte(Full512<T> /* tag */, Vec512<uint8_t> v) {
++HWY_INLINE Vec512<T> BitCastFromByte(Full512<T> /* tag */, Vec512<uint8_t> v) {
+   return Vec512<T>{BitCastFromInteger512<T>()(v.raw)};
+ }
+ 
+@@ -315,6 +327,47 @@ HWY_API Vec512<T> operator^(const Vec512<T> a, const Vec512<T> b) {
+   return Xor(a, b);
+ }
+ 
++// ------------------------------ PopulationCount
++
++// 8/16 require BITALG, 32/64 require VPOPCNTDQ.
++#if HWY_TARGET == HWY_AVX3_DL
++
++#ifdef HWY_NATIVE_POPCNT
++#undef HWY_NATIVE_POPCNT
++#else
++#define HWY_NATIVE_POPCNT
++#endif
++
++namespace detail {
++
++template <typename T>
++HWY_INLINE Vec512<T> PopulationCount(hwy::SizeTag<1> /* tag */, Vec512<T> v) {
++  return Vec512<T>{_mm512_popcnt_epi8(v.raw)};
++}
++template <typename T>
++HWY_INLINE Vec512<T> PopulationCount(hwy::SizeTag<2> /* tag */, Vec512<T> v) {
++  return Vec512<T>{_mm512_popcnt_epi16(v.raw)};
++}
++template <typename T>
++HWY_INLINE Vec512<T> PopulationCount(hwy::SizeTag<4> /* tag */, Vec512<T> v) {
++  return Vec512<T>{_mm512_popcnt_epi32(v.raw)};
++}
++template <typename T>
++HWY_INLINE Vec512<T> PopulationCount(hwy::SizeTag<8> /* tag */, Vec512<T> v) {
++  return Vec512<T>{_mm512_popcnt_epi64(v.raw)};
++}
++
++}  // namespace detail
++
++template <typename T>
++HWY_API Vec512<T> PopulationCount(Vec512<T> v) {
++  return detail::PopulationCount(hwy::SizeTag<sizeof(T)>(), v);
++}
++
++#endif  // HWY_TARGET == HWY_AVX3_DL
++
++// ================================================== SIGN
++
+ // ------------------------------ CopySign
+ 
+ template <typename T>
+@@ -346,6 +399,8 @@ HWY_API Vec512<T> CopySignToAbs(const Vec512<T> abs, const Vec512<T> sign) {
+   return CopySign(abs, sign);
+ }
+ 
++// ================================================== MASK
++
+ // ------------------------------ FirstN
+ 
+ // Possibilities for constructing a bitmask of N ones:
+@@ -360,13 +415,14 @@ namespace detail {
+ 
+ // 32 bit mask is sufficient for lane size >= 2.
+ template <typename T, HWY_IF_NOT_LANE_SIZE(T, 1)>
+-HWY_API Mask512<T> FirstN(size_t n) {
+-  using Bits = typename Mask512<T>::Raw;
+-  return Mask512<T>{static_cast<Bits>(_bzhi_u32(~uint32_t(0), n))};
++HWY_INLINE Mask512<T> FirstN(size_t n) {
++  Mask512<T> m;
++  m.raw = static_cast<decltype(m.raw)>(_bzhi_u32(~uint32_t(0), n));
++  return m;
+ }
+ 
+ template <typename T, HWY_IF_LANE_SIZE(T, 1)>
+-HWY_API Mask512<T> FirstN(size_t n) {
++HWY_INLINE Mask512<T> FirstN(size_t n) {
+   const uint64_t bits = n < 64 ? ((1ULL << n) - 1) : ~uint64_t(0);
+   return Mask512<T>{static_cast<__mmask64>(bits)};
+ }
+@@ -377,8 +433,9 @@ HWY_API Mask512<T> FirstN(size_t n) {
+ template <typename T>
+ HWY_API Mask512<T> FirstN(const Full512<T> /*tag*/, size_t n) {
+ #if HWY_ARCH_X86_64
+-  using Bits = typename Mask512<T>::Raw;
+-  return Mask512<T>{static_cast<Bits>(_bzhi_u64(~uint64_t(0), n))};
++  Mask512<T> m;
++  m.raw = static_cast<decltype(m.raw)>(_bzhi_u64(~uint64_t(0), n));
++  return m;
+ #else
+   return detail::FirstN<T>(n);
+ #endif  // HWY_ARCH_X86_64
+@@ -392,23 +449,27 @@ namespace detail {
+ 
+ // Templates for signed/unsigned integer of a particular size.
+ template <typename T>
+-HWY_API Vec512<T> IfThenElse(hwy::SizeTag<1> /* tag */, const Mask512<T> mask,
+-                             const Vec512<T> yes, const Vec512<T> no) {
++HWY_INLINE Vec512<T> IfThenElse(hwy::SizeTag<1> /* tag */,
++                                const Mask512<T> mask, const Vec512<T> yes,
++                                const Vec512<T> no) {
+   return Vec512<T>{_mm512_mask_mov_epi8(no.raw, mask.raw, yes.raw)};
+ }
+ template <typename T>
+-HWY_API Vec512<T> IfThenElse(hwy::SizeTag<2> /* tag */, const Mask512<T> mask,
+-                             const Vec512<T> yes, const Vec512<T> no) {
++HWY_INLINE Vec512<T> IfThenElse(hwy::SizeTag<2> /* tag */,
++                                const Mask512<T> mask, const Vec512<T> yes,
++                                const Vec512<T> no) {
+   return Vec512<T>{_mm512_mask_mov_epi16(no.raw, mask.raw, yes.raw)};
+ }
+ template <typename T>
+-HWY_API Vec512<T> IfThenElse(hwy::SizeTag<4> /* tag */, const Mask512<T> mask,
+-                             const Vec512<T> yes, const Vec512<T> no) {
++HWY_INLINE Vec512<T> IfThenElse(hwy::SizeTag<4> /* tag */,
++                                const Mask512<T> mask, const Vec512<T> yes,
++                                const Vec512<T> no) {
+   return Vec512<T>{_mm512_mask_mov_epi32(no.raw, mask.raw, yes.raw)};
+ }
+ template <typename T>
+-HWY_API Vec512<T> IfThenElse(hwy::SizeTag<8> /* tag */, const Mask512<T> mask,
+-                             const Vec512<T> yes, const Vec512<T> no) {
++HWY_INLINE Vec512<T> IfThenElse(hwy::SizeTag<8> /* tag */,
++                                const Mask512<T> mask, const Vec512<T> yes,
++                                const Vec512<T> no) {
+   return Vec512<T>{_mm512_mask_mov_epi64(no.raw, mask.raw, yes.raw)};
+ }
+ 
+@@ -419,39 +480,41 @@ HWY_API Vec512<T> IfThenElse(const Mask512<T> mask, const Vec512<T> yes,
+                              const Vec512<T> no) {
+   return detail::IfThenElse(hwy::SizeTag<sizeof(T)>(), mask, yes, no);
+ }
+-template <>
+-HWY_INLINE Vec512<float> IfThenElse(const Mask512<float> mask,
+-                                    const Vec512<float> yes,
+-                                    const Vec512<float> no) {
++HWY_API Vec512<float> IfThenElse(const Mask512<float> mask,
++                                 const Vec512<float> yes,
++                                 const Vec512<float> no) {
+   return Vec512<float>{_mm512_mask_mov_ps(no.raw, mask.raw, yes.raw)};
+ }
+-template <>
+-HWY_INLINE Vec512<double> IfThenElse(const Mask512<double> mask,
+-                                     const Vec512<double> yes,
+-                                     const Vec512<double> no) {
++HWY_API Vec512<double> IfThenElse(const Mask512<double> mask,
++                                  const Vec512<double> yes,
++                                  const Vec512<double> no) {
+   return Vec512<double>{_mm512_mask_mov_pd(no.raw, mask.raw, yes.raw)};
+ }
+ 
+ namespace detail {
+ 
+ template <typename T>
+-HWY_API Vec512<T> IfThenElseZero(hwy::SizeTag<1> /* tag */,
+-                                 const Mask512<T> mask, const Vec512<T> yes) {
++HWY_INLINE Vec512<T> IfThenElseZero(hwy::SizeTag<1> /* tag */,
++                                    const Mask512<T> mask,
++                                    const Vec512<T> yes) {
+   return Vec512<T>{_mm512_maskz_mov_epi8(mask.raw, yes.raw)};
+ }
+ template <typename T>
+-HWY_API Vec512<T> IfThenElseZero(hwy::SizeTag<2> /* tag */,
+-                                 const Mask512<T> mask, const Vec512<T> yes) {
++HWY_INLINE Vec512<T> IfThenElseZero(hwy::SizeTag<2> /* tag */,
++                                    const Mask512<T> mask,
++                                    const Vec512<T> yes) {
+   return Vec512<T>{_mm512_maskz_mov_epi16(mask.raw, yes.raw)};
+ }
+ template <typename T>
+-HWY_API Vec512<T> IfThenElseZero(hwy::SizeTag<4> /* tag */,
+-                                 const Mask512<T> mask, const Vec512<T> yes) {
++HWY_INLINE Vec512<T> IfThenElseZero(hwy::SizeTag<4> /* tag */,
++                                    const Mask512<T> mask,
++                                    const Vec512<T> yes) {
+   return Vec512<T>{_mm512_maskz_mov_epi32(mask.raw, yes.raw)};
+ }
+ template <typename T>
+-HWY_API Vec512<T> IfThenElseZero(hwy::SizeTag<8> /* tag */,
+-                                 const Mask512<T> mask, const Vec512<T> yes) {
++HWY_INLINE Vec512<T> IfThenElseZero(hwy::SizeTag<8> /* tag */,
++                                    const Mask512<T> mask,
++                                    const Vec512<T> yes) {
+   return Vec512<T>{_mm512_maskz_mov_epi64(mask.raw, yes.raw)};
+ }
+ 
+@@ -461,38 +524,36 @@ template <typename T>
+ HWY_API Vec512<T> IfThenElseZero(const Mask512<T> mask, const Vec512<T> yes) {
+   return detail::IfThenElseZero(hwy::SizeTag<sizeof(T)>(), mask, yes);
+ }
+-template <>
+-HWY_INLINE Vec512<float> IfThenElseZero(const Mask512<float> mask,
+-                                        const Vec512<float> yes) {
++HWY_API Vec512<float> IfThenElseZero(const Mask512<float> mask,
++                                     const Vec512<float> yes) {
+   return Vec512<float>{_mm512_maskz_mov_ps(mask.raw, yes.raw)};
+ }
+-template <>
+-HWY_INLINE Vec512<double> IfThenElseZero(const Mask512<double> mask,
+-                                         const Vec512<double> yes) {
++HWY_API Vec512<double> IfThenElseZero(const Mask512<double> mask,
++                                      const Vec512<double> yes) {
+   return Vec512<double>{_mm512_maskz_mov_pd(mask.raw, yes.raw)};
+ }
+ 
+ namespace detail {
+ 
+ template <typename T>
+-HWY_API Vec512<T> IfThenZeroElse(hwy::SizeTag<1> /* tag */,
+-                                 const Mask512<T> mask, const Vec512<T> no) {
++HWY_INLINE Vec512<T> IfThenZeroElse(hwy::SizeTag<1> /* tag */,
++                                    const Mask512<T> mask, const Vec512<T> no) {
+   // xor_epi8/16 are missing, but we have sub, which is just as fast for u8/16.
+   return Vec512<T>{_mm512_mask_sub_epi8(no.raw, mask.raw, no.raw, no.raw)};
+ }
+ template <typename T>
+-HWY_API Vec512<T> IfThenZeroElse(hwy::SizeTag<2> /* tag */,
+-                                 const Mask512<T> mask, const Vec512<T> no) {
++HWY_INLINE Vec512<T> IfThenZeroElse(hwy::SizeTag<2> /* tag */,
++                                    const Mask512<T> mask, const Vec512<T> no) {
+   return Vec512<T>{_mm512_mask_sub_epi16(no.raw, mask.raw, no.raw, no.raw)};
+ }
+ template <typename T>
+-HWY_API Vec512<T> IfThenZeroElse(hwy::SizeTag<4> /* tag */,
+-                                 const Mask512<T> mask, const Vec512<T> no) {
++HWY_INLINE Vec512<T> IfThenZeroElse(hwy::SizeTag<4> /* tag */,
++                                    const Mask512<T> mask, const Vec512<T> no) {
+   return Vec512<T>{_mm512_mask_xor_epi32(no.raw, mask.raw, no.raw, no.raw)};
+ }
+ template <typename T>
+-HWY_API Vec512<T> IfThenZeroElse(hwy::SizeTag<8> /* tag */,
+-                                 const Mask512<T> mask, const Vec512<T> no) {
++HWY_INLINE Vec512<T> IfThenZeroElse(hwy::SizeTag<8> /* tag */,
++                                    const Mask512<T> mask, const Vec512<T> no) {
+   return Vec512<T>{_mm512_mask_xor_epi64(no.raw, mask.raw, no.raw, no.raw)};
+ }
+ 
+@@ -502,14 +563,12 @@ template <typename T>
+ HWY_API Vec512<T> IfThenZeroElse(const Mask512<T> mask, const Vec512<T> no) {
+   return detail::IfThenZeroElse(hwy::SizeTag<sizeof(T)>(), mask, no);
+ }
+-template <>
+-HWY_INLINE Vec512<float> IfThenZeroElse(const Mask512<float> mask,
+-                                        const Vec512<float> no) {
++HWY_API Vec512<float> IfThenZeroElse(const Mask512<float> mask,
++                                     const Vec512<float> no) {
+   return Vec512<float>{_mm512_mask_xor_ps(no.raw, mask.raw, no.raw, no.raw)};
+ }
+-template <>
+-HWY_INLINE Vec512<double> IfThenZeroElse(const Mask512<double> mask,
+-                                         const Vec512<double> no) {
++HWY_API Vec512<double> IfThenZeroElse(const Mask512<double> mask,
++                                      const Vec512<double> no) {
+   return Vec512<double>{_mm512_mask_xor_pd(no.raw, mask.raw, no.raw, no.raw)};
+ }
+ 
+@@ -677,7 +736,7 @@ HWY_API Vec512<uint16_t> AverageRound(const Vec512<uint16_t> a,
+   return Vec512<uint16_t>{_mm512_avg_epu16(a.raw, b.raw)};
+ }
+ 
+-// ------------------------------ Absolute value
++// ------------------------------ Abs (Sub)
+ 
+ // Returns absolute value, except that LimitsMin() maps to LimitsMax() + 1.
+ HWY_API Vec512<int8_t> Abs(const Vec512<int8_t> v) {
+@@ -706,7 +765,6 @@ HWY_API Vec512<float> Abs(const Vec512<float> v) {
+ HWY_API Vec512<double> Abs(const Vec512<double> v) {
+   return Vec512<double>{_mm512_abs_pd(v.raw)};
+ }
+-
+ // ------------------------------ ShiftLeft
+ 
+ template <int kBits>
+@@ -1059,7 +1117,7 @@ HWY_API Vec512<uint64_t> MulEven(const Vec512<uint32_t> a,
+   return Vec512<uint64_t>{_mm512_mul_epu32(a.raw, b.raw)};
+ }
+ 
+-// ------------------------------ Negate
++// ------------------------------ Neg (Sub)
+ 
+ template <typename T, HWY_IF_FLOAT(T)>
+ HWY_API Vec512<T> Neg(const Vec512<T> v) {
+@@ -1219,23 +1277,23 @@ HWY_API Mask512<TTo> RebindMask(Full512<TTo> /*tag*/, Mask512<TFrom> m) {
+ namespace detail {
+ 
+ template <typename T>
+-HWY_API Mask512<T> TestBit(hwy::SizeTag<1> /*tag*/, const Vec512<T> v,
+-                           const Vec512<T> bit) {
++HWY_INLINE Mask512<T> TestBit(hwy::SizeTag<1> /*tag*/, const Vec512<T> v,
++                              const Vec512<T> bit) {
+   return Mask512<T>{_mm512_test_epi8_mask(v.raw, bit.raw)};
+ }
+ template <typename T>
+-HWY_API Mask512<T> TestBit(hwy::SizeTag<2> /*tag*/, const Vec512<T> v,
+-                           const Vec512<T> bit) {
++HWY_INLINE Mask512<T> TestBit(hwy::SizeTag<2> /*tag*/, const Vec512<T> v,
++                              const Vec512<T> bit) {
+   return Mask512<T>{_mm512_test_epi16_mask(v.raw, bit.raw)};
+ }
+ template <typename T>
+-HWY_API Mask512<T> TestBit(hwy::SizeTag<4> /*tag*/, const Vec512<T> v,
+-                           const Vec512<T> bit) {
++HWY_INLINE Mask512<T> TestBit(hwy::SizeTag<4> /*tag*/, const Vec512<T> v,
++                              const Vec512<T> bit) {
+   return Mask512<T>{_mm512_test_epi32_mask(v.raw, bit.raw)};
+ }
+ template <typename T>
+-HWY_API Mask512<T> TestBit(hwy::SizeTag<8> /*tag*/, const Vec512<T> v,
+-                           const Vec512<T> bit) {
++HWY_INLINE Mask512<T> TestBit(hwy::SizeTag<8> /*tag*/, const Vec512<T> v,
++                              const Vec512<T> bit) {
+   return Mask512<T>{_mm512_test_epi64_mask(v.raw, bit.raw)};
+ }
+ 
+@@ -1295,6 +1353,54 @@ HWY_API Mask512<double> operator==(const Vec512<double> a,
+   return Mask512<double>{_mm512_cmp_pd_mask(a.raw, b.raw, _CMP_EQ_OQ)};
+ }
+ 
++// ------------------------------ Inequality
++
++// Unsigned
++HWY_API Mask512<uint8_t> operator!=(const Vec512<uint8_t> a,
++                                    const Vec512<uint8_t> b) {
++  return Mask512<uint8_t>{_mm512_cmpneq_epi8_mask(a.raw, b.raw)};
++}
++HWY_API Mask512<uint16_t> operator!=(const Vec512<uint16_t> a,
++                                     const Vec512<uint16_t> b) {
++  return Mask512<uint16_t>{_mm512_cmpneq_epi16_mask(a.raw, b.raw)};
++}
++HWY_API Mask512<uint32_t> operator!=(const Vec512<uint32_t> a,
++                                     const Vec512<uint32_t> b) {
++  return Mask512<uint32_t>{_mm512_cmpneq_epi32_mask(a.raw, b.raw)};
++}
++HWY_API Mask512<uint64_t> operator!=(const Vec512<uint64_t> a,
++                                     const Vec512<uint64_t> b) {
++  return Mask512<uint64_t>{_mm512_cmpneq_epi64_mask(a.raw, b.raw)};
++}
++
++// Signed
++HWY_API Mask512<int8_t> operator!=(const Vec512<int8_t> a,
++                                   const Vec512<int8_t> b) {
++  return Mask512<int8_t>{_mm512_cmpneq_epi8_mask(a.raw, b.raw)};
++}
++HWY_API Mask512<int16_t> operator!=(const Vec512<int16_t> a,
++                                    const Vec512<int16_t> b) {
++  return Mask512<int16_t>{_mm512_cmpneq_epi16_mask(a.raw, b.raw)};
++}
++HWY_API Mask512<int32_t> operator!=(const Vec512<int32_t> a,
++                                    const Vec512<int32_t> b) {
++  return Mask512<int32_t>{_mm512_cmpneq_epi32_mask(a.raw, b.raw)};
++}
++HWY_API Mask512<int64_t> operator!=(const Vec512<int64_t> a,
++                                    const Vec512<int64_t> b) {
++  return Mask512<int64_t>{_mm512_cmpneq_epi64_mask(a.raw, b.raw)};
++}
++
++// Float
++HWY_API Mask512<float> operator!=(const Vec512<float> a,
++                                  const Vec512<float> b) {
++  return Mask512<float>{_mm512_cmp_ps_mask(a.raw, b.raw, _CMP_NEQ_OQ)};
++}
++HWY_API Mask512<double> operator!=(const Vec512<double> a,
++                                   const Vec512<double> b) {
++  return Mask512<double>{_mm512_cmp_pd_mask(a.raw, b.raw, _CMP_NEQ_OQ)};
++}
++
+ // ------------------------------ Strict inequality
+ 
+ // Signed/float <
+@@ -1372,19 +1478,19 @@ HWY_API Mask512<double> operator>=(const Vec512<double> a,
+ namespace detail {
+ 
+ template <typename T>
+-HWY_API Mask512<T> MaskFromVec(hwy::SizeTag<1> /*tag*/, const Vec512<T> v) {
++HWY_INLINE Mask512<T> MaskFromVec(hwy::SizeTag<1> /*tag*/, const Vec512<T> v) {
+   return Mask512<T>{_mm512_movepi8_mask(v.raw)};
+ }
+ template <typename T>
+-HWY_API Mask512<T> MaskFromVec(hwy::SizeTag<2> /*tag*/, const Vec512<T> v) {
++HWY_INLINE Mask512<T> MaskFromVec(hwy::SizeTag<2> /*tag*/, const Vec512<T> v) {
+   return Mask512<T>{_mm512_movepi16_mask(v.raw)};
+ }
+ template <typename T>
+-HWY_API Mask512<T> MaskFromVec(hwy::SizeTag<4> /*tag*/, const Vec512<T> v) {
++HWY_INLINE Mask512<T> MaskFromVec(hwy::SizeTag<4> /*tag*/, const Vec512<T> v) {
+   return Mask512<T>{_mm512_movepi32_mask(v.raw)};
+ }
+ template <typename T>
+-HWY_API Mask512<T> MaskFromVec(hwy::SizeTag<8> /*tag*/, const Vec512<T> v) {
++HWY_INLINE Mask512<T> MaskFromVec(hwy::SizeTag<8> /*tag*/, const Vec512<T> v) {
+   return Mask512<T>{_mm512_movepi64_mask(v.raw)};
+ }
+ 
+@@ -1455,7 +1561,7 @@ HWY_API Vec512<T> VecFromMask(Full512<T> /* tag */, const Mask512<T> v) {
+ namespace detail {
+ 
+ template <typename T>
+-HWY_API Mask512<T> Not(hwy::SizeTag<1> /*tag*/, const Mask512<T> m) {
++HWY_INLINE Mask512<T> Not(hwy::SizeTag<1> /*tag*/, const Mask512<T> m) {
+ #if HWY_COMPILER_HAS_MASK_INTRINSICS
+   return Mask512<T>{_knot_mask64(m.raw)};
+ #else
+@@ -1463,7 +1569,7 @@ HWY_API Mask512<T> Not(hwy::SizeTag<1> /*tag*/, const Mask512<T> m) {
+ #endif
+ }
+ template <typename T>
+-HWY_API Mask512<T> Not(hwy::SizeTag<2> /*tag*/, const Mask512<T> m) {
++HWY_INLINE Mask512<T> Not(hwy::SizeTag<2> /*tag*/, const Mask512<T> m) {
+ #if HWY_COMPILER_HAS_MASK_INTRINSICS
+   return Mask512<T>{_knot_mask32(m.raw)};
+ #else
+@@ -1471,7 +1577,7 @@ HWY_API Mask512<T> Not(hwy::SizeTag<2> /*tag*/, const Mask512<T> m) {
+ #endif
+ }
+ template <typename T>
+-HWY_API Mask512<T> Not(hwy::SizeTag<4> /*tag*/, const Mask512<T> m) {
++HWY_INLINE Mask512<T> Not(hwy::SizeTag<4> /*tag*/, const Mask512<T> m) {
+ #if HWY_COMPILER_HAS_MASK_INTRINSICS
+   return Mask512<T>{_knot_mask16(m.raw)};
+ #else
+@@ -1479,7 +1585,7 @@ HWY_API Mask512<T> Not(hwy::SizeTag<4> /*tag*/, const Mask512<T> m) {
+ #endif
+ }
+ template <typename T>
+-HWY_API Mask512<T> Not(hwy::SizeTag<8> /*tag*/, const Mask512<T> m) {
++HWY_INLINE Mask512<T> Not(hwy::SizeTag<8> /*tag*/, const Mask512<T> m) {
+ #if HWY_COMPILER_HAS_MASK_INTRINSICS
+   return Mask512<T>{_knot_mask8(m.raw)};
+ #else
+@@ -1488,8 +1594,8 @@ HWY_API Mask512<T> Not(hwy::SizeTag<8> /*tag*/, const Mask512<T> m) {
+ }
+ 
+ template <typename T>
+-HWY_API Mask512<T> And(hwy::SizeTag<1> /*tag*/, const Mask512<T> a,
+-                       const Mask512<T> b) {
++HWY_INLINE Mask512<T> And(hwy::SizeTag<1> /*tag*/, const Mask512<T> a,
++                          const Mask512<T> b) {
+ #if HWY_COMPILER_HAS_MASK_INTRINSICS
+   return Mask512<T>{_kand_mask64(a.raw, b.raw)};
+ #else
+@@ -1497,8 +1603,8 @@ HWY_API Mask512<T> And(hwy::SizeTag<1> /*tag*/, const Mask512<T> a,
+ #endif
+ }
+ template <typename T>
+-HWY_API Mask512<T> And(hwy::SizeTag<2> /*tag*/, const Mask512<T> a,
+-                       const Mask512<T> b) {
++HWY_INLINE Mask512<T> And(hwy::SizeTag<2> /*tag*/, const Mask512<T> a,
++                          const Mask512<T> b) {
+ #if HWY_COMPILER_HAS_MASK_INTRINSICS
+   return Mask512<T>{_kand_mask32(a.raw, b.raw)};
+ #else
+@@ -1506,8 +1612,8 @@ HWY_API Mask512<T> And(hwy::SizeTag<2> /*tag*/, const Mask512<T> a,
+ #endif
+ }
+ template <typename T>
+-HWY_API Mask512<T> And(hwy::SizeTag<4> /*tag*/, const Mask512<T> a,
+-                       const Mask512<T> b) {
++HWY_INLINE Mask512<T> And(hwy::SizeTag<4> /*tag*/, const Mask512<T> a,
++                          const Mask512<T> b) {
+ #if HWY_COMPILER_HAS_MASK_INTRINSICS
+   return Mask512<T>{_kand_mask16(a.raw, b.raw)};
+ #else
+@@ -1515,8 +1621,8 @@ HWY_API Mask512<T> And(hwy::SizeTag<4> /*tag*/, const Mask512<T> a,
+ #endif
+ }
+ template <typename T>
+-HWY_API Mask512<T> And(hwy::SizeTag<8> /*tag*/, const Mask512<T> a,
+-                       const Mask512<T> b) {
++HWY_INLINE Mask512<T> And(hwy::SizeTag<8> /*tag*/, const Mask512<T> a,
++                          const Mask512<T> b) {
+ #if HWY_COMPILER_HAS_MASK_INTRINSICS
+   return Mask512<T>{_kand_mask8(a.raw, b.raw)};
+ #else
+@@ -1525,8 +1631,8 @@ HWY_API Mask512<T> And(hwy::SizeTag<8> /*tag*/, const Mask512<T> a,
+ }
+ 
+ template <typename T>
+-HWY_API Mask512<T> AndNot(hwy::SizeTag<1> /*tag*/, const Mask512<T> a,
+-                          const Mask512<T> b) {
++HWY_INLINE Mask512<T> AndNot(hwy::SizeTag<1> /*tag*/, const Mask512<T> a,
++                             const Mask512<T> b) {
+ #if HWY_COMPILER_HAS_MASK_INTRINSICS
+   return Mask512<T>{_kandn_mask64(a.raw, b.raw)};
+ #else
+@@ -1534,8 +1640,8 @@ HWY_API Mask512<T> AndNot(hwy::SizeTag<1> /*tag*/, const Mask512<T> a,
+ #endif
+ }
+ template <typename T>
+-HWY_API Mask512<T> AndNot(hwy::SizeTag<2> /*tag*/, const Mask512<T> a,
+-                          const Mask512<T> b) {
++HWY_INLINE Mask512<T> AndNot(hwy::SizeTag<2> /*tag*/, const Mask512<T> a,
++                             const Mask512<T> b) {
+ #if HWY_COMPILER_HAS_MASK_INTRINSICS
+   return Mask512<T>{_kandn_mask32(a.raw, b.raw)};
+ #else
+@@ -1543,8 +1649,8 @@ HWY_API Mask512<T> AndNot(hwy::SizeTag<2> /*tag*/, const Mask512<T> a,
+ #endif
+ }
+ template <typename T>
+-HWY_API Mask512<T> AndNot(hwy::SizeTag<4> /*tag*/, const Mask512<T> a,
+-                          const Mask512<T> b) {
++HWY_INLINE Mask512<T> AndNot(hwy::SizeTag<4> /*tag*/, const Mask512<T> a,
++                             const Mask512<T> b) {
+ #if HWY_COMPILER_HAS_MASK_INTRINSICS
+   return Mask512<T>{_kandn_mask16(a.raw, b.raw)};
+ #else
+@@ -1552,8 +1658,8 @@ HWY_API Mask512<T> AndNot(hwy::SizeTag<4> /*tag*/, const Mask512<T> a,
+ #endif
+ }
+ template <typename T>
+-HWY_API Mask512<T> AndNot(hwy::SizeTag<8> /*tag*/, const Mask512<T> a,
+-                          const Mask512<T> b) {
++HWY_INLINE Mask512<T> AndNot(hwy::SizeTag<8> /*tag*/, const Mask512<T> a,
++                             const Mask512<T> b) {
+ #if HWY_COMPILER_HAS_MASK_INTRINSICS
+   return Mask512<T>{_kandn_mask8(a.raw, b.raw)};
+ #else
+@@ -1562,8 +1668,8 @@ HWY_API Mask512<T> AndNot(hwy::SizeTag<8> /*tag*/, const Mask512<T> a,
+ }
+ 
+ template <typename T>
+-HWY_API Mask512<T> Or(hwy::SizeTag<1> /*tag*/, const Mask512<T> a,
+-                      const Mask512<T> b) {
++HWY_INLINE Mask512<T> Or(hwy::SizeTag<1> /*tag*/, const Mask512<T> a,
++                         const Mask512<T> b) {
+ #if HWY_COMPILER_HAS_MASK_INTRINSICS
+   return Mask512<T>{_kor_mask64(a.raw, b.raw)};
+ #else
+@@ -1571,8 +1677,8 @@ HWY_API Mask512<T> Or(hwy::SizeTag<1> /*tag*/, const Mask512<T> a,
+ #endif
+ }
+ template <typename T>
+-HWY_API Mask512<T> Or(hwy::SizeTag<2> /*tag*/, const Mask512<T> a,
+-                      const Mask512<T> b) {
++HWY_INLINE Mask512<T> Or(hwy::SizeTag<2> /*tag*/, const Mask512<T> a,
++                         const Mask512<T> b) {
+ #if HWY_COMPILER_HAS_MASK_INTRINSICS
+   return Mask512<T>{_kor_mask32(a.raw, b.raw)};
+ #else
+@@ -1580,8 +1686,8 @@ HWY_API Mask512<T> Or(hwy::SizeTag<2> /*tag*/, const Mask512<T> a,
+ #endif
+ }
+ template <typename T>
+-HWY_API Mask512<T> Or(hwy::SizeTag<4> /*tag*/, const Mask512<T> a,
+-                      const Mask512<T> b) {
++HWY_INLINE Mask512<T> Or(hwy::SizeTag<4> /*tag*/, const Mask512<T> a,
++                         const Mask512<T> b) {
+ #if HWY_COMPILER_HAS_MASK_INTRINSICS
+   return Mask512<T>{_kor_mask16(a.raw, b.raw)};
+ #else
+@@ -1589,8 +1695,8 @@ HWY_API Mask512<T> Or(hwy::SizeTag<4> /*tag*/, const Mask512<T> a,
+ #endif
+ }
+ template <typename T>
+-HWY_API Mask512<T> Or(hwy::SizeTag<8> /*tag*/, const Mask512<T> a,
+-                      const Mask512<T> b) {
++HWY_INLINE Mask512<T> Or(hwy::SizeTag<8> /*tag*/, const Mask512<T> a,
++                         const Mask512<T> b) {
+ #if HWY_COMPILER_HAS_MASK_INTRINSICS
+   return Mask512<T>{_kor_mask8(a.raw, b.raw)};
+ #else
+@@ -1599,8 +1705,8 @@ HWY_API Mask512<T> Or(hwy::SizeTag<8> /*tag*/, const Mask512<T> a,
+ }
+ 
+ template <typename T>
+-HWY_API Mask512<T> Xor(hwy::SizeTag<1> /*tag*/, const Mask512<T> a,
+-                       const Mask512<T> b) {
++HWY_INLINE Mask512<T> Xor(hwy::SizeTag<1> /*tag*/, const Mask512<T> a,
++                          const Mask512<T> b) {
+ #if HWY_COMPILER_HAS_MASK_INTRINSICS
+   return Mask512<T>{_kxor_mask64(a.raw, b.raw)};
+ #else
+@@ -1608,8 +1714,8 @@ HWY_API Mask512<T> Xor(hwy::SizeTag<1> /*tag*/, const Mask512<T> a,
+ #endif
+ }
+ template <typename T>
+-HWY_API Mask512<T> Xor(hwy::SizeTag<2> /*tag*/, const Mask512<T> a,
+-                       const Mask512<T> b) {
++HWY_INLINE Mask512<T> Xor(hwy::SizeTag<2> /*tag*/, const Mask512<T> a,
++                          const Mask512<T> b) {
+ #if HWY_COMPILER_HAS_MASK_INTRINSICS
+   return Mask512<T>{_kxor_mask32(a.raw, b.raw)};
+ #else
+@@ -1617,8 +1723,8 @@ HWY_API Mask512<T> Xor(hwy::SizeTag<2> /*tag*/, const Mask512<T> a,
+ #endif
+ }
+ template <typename T>
+-HWY_API Mask512<T> Xor(hwy::SizeTag<4> /*tag*/, const Mask512<T> a,
+-                       const Mask512<T> b) {
++HWY_INLINE Mask512<T> Xor(hwy::SizeTag<4> /*tag*/, const Mask512<T> a,
++                          const Mask512<T> b) {
+ #if HWY_COMPILER_HAS_MASK_INTRINSICS
+   return Mask512<T>{_kxor_mask16(a.raw, b.raw)};
+ #else
+@@ -1626,8 +1732,8 @@ HWY_API Mask512<T> Xor(hwy::SizeTag<4> /*tag*/, const Mask512<T> a,
+ #endif
+ }
+ template <typename T>
+-HWY_API Mask512<T> Xor(hwy::SizeTag<8> /*tag*/, const Mask512<T> a,
+-                       const Mask512<T> b) {
++HWY_INLINE Mask512<T> Xor(hwy::SizeTag<8> /*tag*/, const Mask512<T> a,
++                          const Mask512<T> b) {
+ #if HWY_COMPILER_HAS_MASK_INTRINSICS
+   return Mask512<T>{_kxor_mask8(a.raw, b.raw)};
+ #else
+@@ -1807,28 +1913,28 @@ HWY_DIAGNOSTICS_OFF(disable : 4245 4365, ignored "-Wsign-conversion")
+ namespace detail {
+ 
+ template <typename T>
+-HWY_API void ScatterOffset(hwy::SizeTag<4> /* tag */, Vec512<T> v,
+-                           Full512<T> /* tag */, T* HWY_RESTRICT base,
+-                           const Vec512<int32_t> offset) {
++HWY_INLINE void ScatterOffset(hwy::SizeTag<4> /* tag */, Vec512<T> v,
++                              Full512<T> /* tag */, T* HWY_RESTRICT base,
++                              const Vec512<int32_t> offset) {
+   _mm512_i32scatter_epi32(base, offset.raw, v.raw, 1);
+ }
+ template <typename T>
+-HWY_API void ScatterIndex(hwy::SizeTag<4> /* tag */, Vec512<T> v,
+-                          Full512<T> /* tag */, T* HWY_RESTRICT base,
+-                          const Vec512<int32_t> index) {
++HWY_INLINE void ScatterIndex(hwy::SizeTag<4> /* tag */, Vec512<T> v,
++                             Full512<T> /* tag */, T* HWY_RESTRICT base,
++                             const Vec512<int32_t> index) {
+   _mm512_i32scatter_epi32(base, index.raw, v.raw, 4);
+ }
+ 
+ template <typename T>
+-HWY_API void ScatterOffset(hwy::SizeTag<8> /* tag */, Vec512<T> v,
+-                           Full512<T> /* tag */, T* HWY_RESTRICT base,
+-                           const Vec512<int64_t> offset) {
++HWY_INLINE void ScatterOffset(hwy::SizeTag<8> /* tag */, Vec512<T> v,
++                              Full512<T> /* tag */, T* HWY_RESTRICT base,
++                              const Vec512<int64_t> offset) {
+   _mm512_i64scatter_epi64(base, offset.raw, v.raw, 1);
+ }
+ template <typename T>
+-HWY_API void ScatterIndex(hwy::SizeTag<8> /* tag */, Vec512<T> v,
+-                          Full512<T> /* tag */, T* HWY_RESTRICT base,
+-                          const Vec512<int64_t> index) {
++HWY_INLINE void ScatterIndex(hwy::SizeTag<8> /* tag */, Vec512<T> v,
++                             Full512<T> /* tag */, T* HWY_RESTRICT base,
++                             const Vec512<int64_t> index) {
+   _mm512_i64scatter_epi64(base, index.raw, v.raw, 8);
+ }
+ 
+@@ -1847,31 +1953,25 @@ HWY_API void ScatterIndex(Vec512<T> v, Full512<T> d, T* HWY_RESTRICT base,
+   return detail::ScatterIndex(hwy::SizeTag<sizeof(T)>(), v, d, base, index);
+ }
+ 
+-template <>
+-HWY_INLINE void ScatterOffset<float>(Vec512<float> v, Full512<float> /* tag */,
+-                                     float* HWY_RESTRICT base,
+-                                     const Vec512<int32_t> offset) {
++HWY_API void ScatterOffset(Vec512<float> v, Full512<float> /* tag */,
++                           float* HWY_RESTRICT base,
++                           const Vec512<int32_t> offset) {
+   _mm512_i32scatter_ps(base, offset.raw, v.raw, 1);
+ }
+-template <>
+-HWY_INLINE void ScatterIndex<float>(Vec512<float> v, Full512<float> /* tag */,
+-                                    float* HWY_RESTRICT base,
+-                                    const Vec512<int32_t> index) {
++HWY_API void ScatterIndex(Vec512<float> v, Full512<float> /* tag */,
++                          float* HWY_RESTRICT base,
++                          const Vec512<int32_t> index) {
+   _mm512_i32scatter_ps(base, index.raw, v.raw, 4);
+ }
+ 
+-template <>
+-HWY_INLINE void ScatterOffset<double>(Vec512<double> v,
+-                                      Full512<double> /* tag */,
+-                                      double* HWY_RESTRICT base,
+-                                      const Vec512<int64_t> offset) {
++HWY_API void ScatterOffset(Vec512<double> v, Full512<double> /* tag */,
++                           double* HWY_RESTRICT base,
++                           const Vec512<int64_t> offset) {
+   _mm512_i64scatter_pd(base, offset.raw, v.raw, 1);
+ }
+-template <>
+-HWY_INLINE void ScatterIndex<double>(Vec512<double> v,
+-                                     Full512<double> /* tag */,
+-                                     double* HWY_RESTRICT base,
+-                                     const Vec512<int64_t> index) {
++HWY_API void ScatterIndex(Vec512<double> v, Full512<double> /* tag */,
++                          double* HWY_RESTRICT base,
++                          const Vec512<int64_t> index) {
+   _mm512_i64scatter_pd(base, index.raw, v.raw, 8);
+ }
+ 
+@@ -1880,28 +1980,32 @@ HWY_INLINE void ScatterIndex<double>(Vec512<double> v,
+ namespace detail {
+ 
+ template <typename T>
+-HWY_API Vec512<T> GatherOffset(hwy::SizeTag<4> /* tag */, Full512<T> /* tag */,
+-                               const T* HWY_RESTRICT base,
+-                               const Vec512<int32_t> offset) {
++HWY_INLINE Vec512<T> GatherOffset(hwy::SizeTag<4> /* tag */,
++                                  Full512<T> /* tag */,
++                                  const T* HWY_RESTRICT base,
++                                  const Vec512<int32_t> offset) {
+   return Vec512<T>{_mm512_i32gather_epi32(offset.raw, base, 1)};
+ }
+ template <typename T>
+-HWY_API Vec512<T> GatherIndex(hwy::SizeTag<4> /* tag */, Full512<T> /* tag */,
+-                              const T* HWY_RESTRICT base,
+-                              const Vec512<int32_t> index) {
++HWY_INLINE Vec512<T> GatherIndex(hwy::SizeTag<4> /* tag */,
++                                 Full512<T> /* tag */,
++                                 const T* HWY_RESTRICT base,
++                                 const Vec512<int32_t> index) {
+   return Vec512<T>{_mm512_i32gather_epi32(index.raw, base, 4)};
+ }
+ 
+ template <typename T>
+-HWY_API Vec512<T> GatherOffset(hwy::SizeTag<8> /* tag */, Full512<T> /* tag */,
+-                               const T* HWY_RESTRICT base,
+-                               const Vec512<int64_t> offset) {
++HWY_INLINE Vec512<T> GatherOffset(hwy::SizeTag<8> /* tag */,
++                                  Full512<T> /* tag */,
++                                  const T* HWY_RESTRICT base,
++                                  const Vec512<int64_t> offset) {
+   return Vec512<T>{_mm512_i64gather_epi64(offset.raw, base, 1)};
+ }
+ template <typename T>
+-HWY_API Vec512<T> GatherIndex(hwy::SizeTag<8> /* tag */, Full512<T> /* tag */,
+-                              const T* HWY_RESTRICT base,
+-                              const Vec512<int64_t> index) {
++HWY_INLINE Vec512<T> GatherIndex(hwy::SizeTag<8> /* tag */,
++                                 Full512<T> /* tag */,
++                                 const T* HWY_RESTRICT base,
++                                 const Vec512<int64_t> index) {
+   return Vec512<T>{_mm512_i64gather_epi64(index.raw, base, 8)};
+ }
+ 
+@@ -1920,29 +2024,25 @@ HWY_API Vec512<T> GatherIndex(Full512<T> d, const T* HWY_RESTRICT base,
+   return detail::GatherIndex(hwy::SizeTag<sizeof(T)>(), d, base, index);
+ }
+ 
+-template <>
+-HWY_INLINE Vec512<float> GatherOffset<float>(Full512<float> /* tag */,
+-                                             const float* HWY_RESTRICT base,
+-                                             const Vec512<int32_t> offset) {
++HWY_API Vec512<float> GatherOffset(Full512<float> /* tag */,
++                                   const float* HWY_RESTRICT base,
++                                   const Vec512<int32_t> offset) {
+   return Vec512<float>{_mm512_i32gather_ps(offset.raw, base, 1)};
+ }
+-template <>
+-HWY_INLINE Vec512<float> GatherIndex<float>(Full512<float> /* tag */,
+-                                            const float* HWY_RESTRICT base,
+-                                            const Vec512<int32_t> index) {
++HWY_API Vec512<float> GatherIndex(Full512<float> /* tag */,
++                                  const float* HWY_RESTRICT base,
++                                  const Vec512<int32_t> index) {
+   return Vec512<float>{_mm512_i32gather_ps(index.raw, base, 4)};
+ }
+ 
+-template <>
+-HWY_INLINE Vec512<double> GatherOffset<double>(Full512<double> /* tag */,
+-                                               const double* HWY_RESTRICT base,
+-                                               const Vec512<int64_t> offset) {
++HWY_API Vec512<double> GatherOffset(Full512<double> /* tag */,
++                                    const double* HWY_RESTRICT base,
++                                    const Vec512<int64_t> offset) {
+   return Vec512<double>{_mm512_i64gather_pd(offset.raw, base, 1)};
+ }
+-template <>
+-HWY_INLINE Vec512<double> GatherIndex<double>(Full512<double> /* tag */,
+-                                              const double* HWY_RESTRICT base,
+-                                              const Vec512<int64_t> index) {
++HWY_API Vec512<double> GatherIndex(Full512<double> /* tag */,
++                                   const double* HWY_RESTRICT base,
++                                   const Vec512<int64_t> index) {
+   return Vec512<double>{_mm512_i64gather_pd(index.raw, base, 8)};
+ }
+ 
+@@ -1950,39 +2050,43 @@ HWY_DIAGNOSTICS(pop)
+ 
+ // ================================================== SWIZZLE
+ 
+-template <typename T>
+-HWY_API T GetLane(const Vec512<T> v) {
+-  return GetLane(LowerHalf(v));
+-}
+-
+-// ------------------------------ Extract half
++// ------------------------------ LowerHalf
+ 
+ template <typename T>
+-HWY_API Vec256<T> LowerHalf(Vec512<T> v) {
++HWY_API Vec256<T> LowerHalf(Full256<T> /* tag */, Vec512<T> v) {
+   return Vec256<T>{_mm512_castsi512_si256(v.raw)};
+ }
+-template <>
+-HWY_INLINE Vec256<float> LowerHalf(Vec512<float> v) {
++HWY_API Vec256<float> LowerHalf(Full256<float> /* tag */, Vec512<float> v) {
+   return Vec256<float>{_mm512_castps512_ps256(v.raw)};
+ }
+-template <>
+-HWY_INLINE Vec256<double> LowerHalf(Vec512<double> v) {
++HWY_API Vec256<double> LowerHalf(Full256<double> /* tag */, Vec512<double> v) {
+   return Vec256<double>{_mm512_castpd512_pd256(v.raw)};
+ }
+ 
+ template <typename T>
+-HWY_API Vec256<T> UpperHalf(Vec512<T> v) {
++HWY_API Vec256<T> LowerHalf(Vec512<T> v) {
++  return LowerHalf(Full256<T>(), v);
++}
++
++// ------------------------------ UpperHalf
++
++template <typename T>
++HWY_API Vec256<T> UpperHalf(Full256<T> /* tag */, Vec512<T> v) {
+   return Vec256<T>{_mm512_extracti32x8_epi32(v.raw, 1)};
+ }
+-template <>
+-HWY_INLINE Vec256<float> UpperHalf(Vec512<float> v) {
++HWY_API Vec256<float> UpperHalf(Full256<float> /* tag */, Vec512<float> v) {
+   return Vec256<float>{_mm512_extractf32x8_ps(v.raw, 1)};
+ }
+-template <>
+-HWY_INLINE Vec256<double> UpperHalf(Vec512<double> v) {
++HWY_API Vec256<double> UpperHalf(Full256<double> /* tag */, Vec512<double> v) {
+   return Vec256<double>{_mm512_extractf64x4_pd(v.raw, 1)};
+ }
+ 
++// ------------------------------ GetLane (LowerHalf)
++template <typename T>
++HWY_API T GetLane(const Vec512<T> v) {
++  return GetLane(LowerHalf(v));
++}
++
+ // ------------------------------ ZeroExtendVector
+ 
+ // Unfortunately the initial _mm512_castsi256_si512 intrinsic leaves the upper
+@@ -1997,23 +2101,23 @@ HWY_INLINE Vec256<double> UpperHalf(Vec512<double> v) {
+ // https://gcc.godbolt.org/z/1MKGaP.
+ 
+ template <typename T>
+-HWY_API Vec512<T> ZeroExtendVector(Vec256<T> lo) {
++HWY_API Vec512<T> ZeroExtendVector(Full512<T> /* tag */, Vec256<T> lo) {
+ #if !HWY_COMPILER_CLANG && HWY_COMPILER_GCC && (HWY_COMPILER_GCC < 1000)
+   return Vec512<T>{_mm512_inserti32x8(_mm512_setzero_si512(), lo.raw, 0)};
+ #else
+   return Vec512<T>{_mm512_zextsi256_si512(lo.raw)};
+ #endif
+ }
+-template <>
+-HWY_INLINE Vec512<float> ZeroExtendVector(Vec256<float> lo) {
++HWY_API Vec512<float> ZeroExtendVector(Full512<float> /* tag */,
++                                       Vec256<float> lo) {
+ #if !HWY_COMPILER_CLANG && HWY_COMPILER_GCC && (HWY_COMPILER_GCC < 1000)
+   return Vec512<float>{_mm512_insertf32x8(_mm512_setzero_ps(), lo.raw, 0)};
+ #else
+   return Vec512<float>{_mm512_zextps256_ps512(lo.raw)};
+ #endif
+ }
+-template <>
+-HWY_INLINE Vec512<double> ZeroExtendVector(Vec256<double> lo) {
++HWY_API Vec512<double> ZeroExtendVector(Full512<double> /* tag */,
++                                        Vec256<double> lo) {
+ #if !HWY_COMPILER_CLANG && HWY_COMPILER_GCC && (HWY_COMPILER_GCC < 1000)
+   return Vec512<double>{_mm512_insertf64x4(_mm512_setzero_pd(), lo.raw, 0)};
+ #else
+@@ -2024,61 +2128,68 @@ HWY_INLINE Vec512<double> ZeroExtendVector(Vec256<double> lo) {
+ // ------------------------------ Combine
+ 
+ template <typename T>
+-HWY_API Vec512<T> Combine(Vec256<T> hi, Vec256<T> lo) {
+-  const auto lo512 = ZeroExtendVector(lo);
++HWY_API Vec512<T> Combine(Full512<T> d, Vec256<T> hi, Vec256<T> lo) {
++  const auto lo512 = ZeroExtendVector(d, lo);
+   return Vec512<T>{_mm512_inserti32x8(lo512.raw, hi.raw, 1)};
+ }
+-template <>
+-HWY_INLINE Vec512<float> Combine(Vec256<float> hi, Vec256<float> lo) {
+-  const auto lo512 = ZeroExtendVector(lo);
++HWY_API Vec512<float> Combine(Full512<float> d, Vec256<float> hi,
++                              Vec256<float> lo) {
++  const auto lo512 = ZeroExtendVector(d, lo);
+   return Vec512<float>{_mm512_insertf32x8(lo512.raw, hi.raw, 1)};
+ }
+-template <>
+-HWY_INLINE Vec512<double> Combine(Vec256<double> hi, Vec256<double> lo) {
+-  const auto lo512 = ZeroExtendVector(lo);
++HWY_API Vec512<double> Combine(Full512<double> d, Vec256<double> hi,
++                               Vec256<double> lo) {
++  const auto lo512 = ZeroExtendVector(d, lo);
+   return Vec512<double>{_mm512_insertf64x4(lo512.raw, hi.raw, 1)};
+ }
+ 
+-// ------------------------------ Shift vector by constant #bytes
++// ------------------------------ ShiftLeftBytes
+ 
+-// 0x01..0F, kBytes = 1 => 0x02..0F00
+ template <int kBytes, typename T>
+-HWY_API Vec512<T> ShiftLeftBytes(const Vec512<T> v) {
++HWY_API Vec512<T> ShiftLeftBytes(Full512<T> /* tag */, const Vec512<T> v) {
+   static_assert(0 <= kBytes && kBytes <= 16, "Invalid kBytes");
+   return Vec512<T>{_mm512_bslli_epi128(v.raw, kBytes)};
+ }
+ 
++template <int kBytes, typename T>
++HWY_API Vec512<T> ShiftLeftBytes(const Vec512<T> v) {
++  return ShiftLeftBytes<kBytes>(Full512<T>(), v);
++}
++
++// ------------------------------ ShiftLeftLanes
++
+ template <int kLanes, typename T>
+-HWY_API Vec512<T> ShiftLeftLanes(const Vec512<T> v) {
+-  const Full512<uint8_t> d8;
+-  const Full512<T> d;
++HWY_API Vec512<T> ShiftLeftLanes(Full512<T> d, const Vec512<T> v) {
++  const Repartition<uint8_t, decltype(d)> d8;
+   return BitCast(d, ShiftLeftBytes<kLanes * sizeof(T)>(BitCast(d8, v)));
+ }
+ 
+-// 0x01..0F, kBytes = 1 => 0x0001..0E
++template <int kLanes, typename T>
++HWY_API Vec512<T> ShiftLeftLanes(const Vec512<T> v) {
++  return ShiftLeftLanes<kLanes>(Full512<T>(), v);
++}
++
++// ------------------------------ ShiftRightBytes
+ template <int kBytes, typename T>
+-HWY_API Vec512<T> ShiftRightBytes(const Vec512<T> v) {
++HWY_API Vec512<T> ShiftRightBytes(Full512<T> /* tag */, const Vec512<T> v) {
+   static_assert(0 <= kBytes && kBytes <= 16, "Invalid kBytes");
+   return Vec512<T>{_mm512_bsrli_epi128(v.raw, kBytes)};
+ }
+ 
++// ------------------------------ ShiftRightLanes
+ template <int kLanes, typename T>
+-HWY_API Vec512<T> ShiftRightLanes(const Vec512<T> v) {
+-  const Full512<uint8_t> d8;
+-  const Full512<T> d;
++HWY_API Vec512<T> ShiftRightLanes(Full512<T> d, const Vec512<T> v) {
++  const Repartition<uint8_t, decltype(d)> d8;
+   return BitCast(d, ShiftRightBytes<kLanes * sizeof(T)>(BitCast(d8, v)));
+ }
+ 
+-// ------------------------------ Extract from 2x 128-bit at constant offset
++// ------------------------------ CombineShiftRightBytes
+ 
+-// Extracts 128 bits from <hi, lo> by skipping the least-significant kBytes.
+-template <int kBytes, typename T>
+-HWY_API Vec512<T> CombineShiftRightBytes(const Vec512<T> hi,
+-                                         const Vec512<T> lo) {
+-  const Full512<uint8_t> d8;
+-  const Vec512<uint8_t> extracted_bytes{
+-      _mm512_alignr_epi8(BitCast(d8, hi).raw, BitCast(d8, lo).raw, kBytes)};
+-  return BitCast(Full512<T>(), extracted_bytes);
++template <int kBytes, typename T, class V = Vec512<T>>
++HWY_API V CombineShiftRightBytes(Full512<T> d, V hi, V lo) {
++  const Repartition<uint8_t, decltype(d)> d8;
++  return BitCast(d, Vec512<uint8_t>{_mm512_alignr_epi8(
++                        BitCast(d8, hi).raw, BitCast(d8, lo).raw, kBytes)});
+ }
+ 
+ // ------------------------------ Broadcast/splat any lane
+@@ -2254,7 +2365,7 @@ HWY_API Vec512<float> TableLookupLanes(const Vec512<float> v,
+   return Vec512<float>{_mm512_permutexvar_ps(idx.raw, v.raw)};
+ }
+ 
+-// ------------------------------ Interleave lanes
++// ------------------------------ InterleaveLower
+ 
+ // Interleaves lanes from halves of the 128-bit blocks of "a" (which provides
+ // the least-significant lane) and "b". To concatenate two half-width integers
+@@ -2303,6 +2414,17 @@ HWY_API Vec512<double> InterleaveLower(const Vec512<double> a,
+   return Vec512<double>{_mm512_unpacklo_pd(a.raw, b.raw)};
+ }
+ 
++// Additional overload for the optional Simd<> tag.
++template <typename T, class V = Vec512<T>>
++HWY_API V InterleaveLower(Full512<T> /* tag */, V a, V b) {
++  return InterleaveLower(a, b);
++}
++
++// ------------------------------ InterleaveUpper
++
++// All functions inside detail lack the required D parameter.
++namespace detail {
++
+ HWY_API Vec512<uint8_t> InterleaveUpper(const Vec512<uint8_t> a,
+                                         const Vec512<uint8_t> b) {
+   return Vec512<uint8_t>{_mm512_unpackhi_epi8(a.raw, b.raw)};
+@@ -2346,130 +2468,102 @@ HWY_API Vec512<double> InterleaveUpper(const Vec512<double> a,
+   return Vec512<double>{_mm512_unpackhi_pd(a.raw, b.raw)};
+ }
+ 
+-// ------------------------------ Zip lanes
+-
+-// Same as interleave_*, except that the return lanes are double-width integers;
+-// this is necessary because the single-lane scalar cannot return two values.
++}  // namespace detail
+ 
+-HWY_API Vec512<uint16_t> ZipLower(const Vec512<uint8_t> a,
+-                                  const Vec512<uint8_t> b) {
+-  return Vec512<uint16_t>{_mm512_unpacklo_epi8(a.raw, b.raw)};
+-}
+-HWY_API Vec512<uint32_t> ZipLower(const Vec512<uint16_t> a,
+-                                  const Vec512<uint16_t> b) {
+-  return Vec512<uint32_t>{_mm512_unpacklo_epi16(a.raw, b.raw)};
+-}
+-HWY_API Vec512<uint64_t> ZipLower(const Vec512<uint32_t> a,
+-                                  const Vec512<uint32_t> b) {
+-  return Vec512<uint64_t>{_mm512_unpacklo_epi32(a.raw, b.raw)};
++template <typename T, class V = Vec512<T>>
++HWY_API V InterleaveUpper(Full512<T> /* tag */, V a, V b) {
++  return detail::InterleaveUpper(a, b);
+ }
+ 
+-HWY_API Vec512<int16_t> ZipLower(const Vec512<int8_t> a,
+-                                 const Vec512<int8_t> b) {
+-  return Vec512<int16_t>{_mm512_unpacklo_epi8(a.raw, b.raw)};
+-}
+-HWY_API Vec512<int32_t> ZipLower(const Vec512<int16_t> a,
+-                                 const Vec512<int16_t> b) {
+-  return Vec512<int32_t>{_mm512_unpacklo_epi16(a.raw, b.raw)};
+-}
+-HWY_API Vec512<int64_t> ZipLower(const Vec512<int32_t> a,
+-                                 const Vec512<int32_t> b) {
+-  return Vec512<int64_t>{_mm512_unpacklo_epi32(a.raw, b.raw)};
+-}
++// ------------------------------ ZipLower/ZipUpper (InterleaveLower)
+ 
+-HWY_API Vec512<uint16_t> ZipUpper(const Vec512<uint8_t> a,
+-                                  const Vec512<uint8_t> b) {
+-  return Vec512<uint16_t>{_mm512_unpackhi_epi8(a.raw, b.raw)};
+-}
+-HWY_API Vec512<uint32_t> ZipUpper(const Vec512<uint16_t> a,
+-                                  const Vec512<uint16_t> b) {
+-  return Vec512<uint32_t>{_mm512_unpackhi_epi16(a.raw, b.raw)};
++// Same as Interleave*, except that the return lanes are double-width integers;
++// this is necessary because the single-lane scalar cannot return two values.
++template <typename T, typename TW = MakeWide<T>>
++HWY_API Vec512<TW> ZipLower(Vec512<T> a, Vec512<T> b) {
++  return BitCast(Full512<TW>(), InterleaveLower(a, b));
+ }
+-HWY_API Vec512<uint64_t> ZipUpper(const Vec512<uint32_t> a,
+-                                  const Vec512<uint32_t> b) {
+-  return Vec512<uint64_t>{_mm512_unpackhi_epi32(a.raw, b.raw)};
++template <typename T, typename TW = MakeWide<T>>
++HWY_API Vec512<TW> ZipLower(Full512<TW> d, Vec512<T> a, Vec512<T> b) {
++  return BitCast(Full512<TW>(), InterleaveLower(d, a, b));
+ }
+ 
+-HWY_API Vec512<int16_t> ZipUpper(const Vec512<int8_t> a,
+-                                 const Vec512<int8_t> b) {
+-  return Vec512<int16_t>{_mm512_unpackhi_epi8(a.raw, b.raw)};
+-}
+-HWY_API Vec512<int32_t> ZipUpper(const Vec512<int16_t> a,
+-                                 const Vec512<int16_t> b) {
+-  return Vec512<int32_t>{_mm512_unpackhi_epi16(a.raw, b.raw)};
+-}
+-HWY_API Vec512<int64_t> ZipUpper(const Vec512<int32_t> a,
+-                                 const Vec512<int32_t> b) {
+-  return Vec512<int64_t>{_mm512_unpackhi_epi32(a.raw, b.raw)};
++template <typename T, typename TW = MakeWide<T>>
++HWY_API Vec512<TW> ZipUpper(Full512<TW> d, Vec512<T> a, Vec512<T> b) {
++  return BitCast(Full512<TW>(), InterleaveUpper(d, a, b));
+ }
+ 
+ // ------------------------------ Concat* halves
+ 
+ // hiH,hiL loH,loL |-> hiL,loL (= lower halves)
+ template <typename T>
+-HWY_API Vec512<T> ConcatLowerLower(const Vec512<T> hi, const Vec512<T> lo) {
++HWY_API Vec512<T> ConcatLowerLower(Full512<T> /* tag */, const Vec512<T> hi,
++                                   const Vec512<T> lo) {
+   return Vec512<T>{_mm512_shuffle_i32x4(lo.raw, hi.raw, _MM_PERM_BABA)};
+ }
+-template <>
+-HWY_INLINE Vec512<float> ConcatLowerLower(const Vec512<float> hi,
+-                                          const Vec512<float> lo) {
++HWY_API Vec512<float> ConcatLowerLower(Full512<float> /* tag */,
++                                       const Vec512<float> hi,
++                                       const Vec512<float> lo) {
+   return Vec512<float>{_mm512_shuffle_f32x4(lo.raw, hi.raw, _MM_PERM_BABA)};
+ }
+-template <>
+-HWY_INLINE Vec512<double> ConcatLowerLower(const Vec512<double> hi,
+-                                           const Vec512<double> lo) {
++HWY_API Vec512<double> ConcatLowerLower(Full512<double> /* tag */,
++                                        const Vec512<double> hi,
++                                        const Vec512<double> lo) {
+   return Vec512<double>{_mm512_shuffle_f64x2(lo.raw, hi.raw, _MM_PERM_BABA)};
+ }
+ 
+ // hiH,hiL loH,loL |-> hiH,loH (= upper halves)
+ template <typename T>
+-HWY_API Vec512<T> ConcatUpperUpper(const Vec512<T> hi, const Vec512<T> lo) {
++HWY_API Vec512<T> ConcatUpperUpper(Full512<T> /* tag */, const Vec512<T> hi,
++                                   const Vec512<T> lo) {
+   return Vec512<T>{_mm512_shuffle_i32x4(lo.raw, hi.raw, _MM_PERM_DCDC)};
+ }
+-template <>
+-HWY_INLINE Vec512<float> ConcatUpperUpper(const Vec512<float> hi,
+-                                          const Vec512<float> lo) {
++HWY_API Vec512<float> ConcatUpperUpper(Full512<float> /* tag */,
++                                       const Vec512<float> hi,
++                                       const Vec512<float> lo) {
+   return Vec512<float>{_mm512_shuffle_f32x4(lo.raw, hi.raw, _MM_PERM_DCDC)};
+ }
+-template <>
+-HWY_INLINE Vec512<double> ConcatUpperUpper(const Vec512<double> hi,
+-                                           const Vec512<double> lo) {
++HWY_API Vec512<double> ConcatUpperUpper(Full512<double> /* tag */,
++                                        const Vec512<double> hi,
++                                        const Vec512<double> lo) {
+   return Vec512<double>{_mm512_shuffle_f64x2(lo.raw, hi.raw, _MM_PERM_DCDC)};
+ }
+ 
+ // hiH,hiL loH,loL |-> hiL,loH (= inner halves / swap blocks)
+ template <typename T>
+-HWY_API Vec512<T> ConcatLowerUpper(const Vec512<T> hi, const Vec512<T> lo) {
++HWY_API Vec512<T> ConcatLowerUpper(Full512<T> /* tag */, const Vec512<T> hi,
++                                   const Vec512<T> lo) {
+   return Vec512<T>{_mm512_shuffle_i32x4(lo.raw, hi.raw, 0x4E)};
+ }
+-template <>
+-HWY_INLINE Vec512<float> ConcatLowerUpper(const Vec512<float> hi,
+-                                          const Vec512<float> lo) {
++HWY_API Vec512<float> ConcatLowerUpper(Full512<float> /* tag */,
++                                       const Vec512<float> hi,
++                                       const Vec512<float> lo) {
+   return Vec512<float>{_mm512_shuffle_f32x4(lo.raw, hi.raw, 0x4E)};
+ }
+-template <>
+-HWY_INLINE Vec512<double> ConcatLowerUpper(const Vec512<double> hi,
+-                                           const Vec512<double> lo) {
++HWY_API Vec512<double> ConcatLowerUpper(Full512<double> /* tag */,
++                                        const Vec512<double> hi,
++                                        const Vec512<double> lo) {
+   return Vec512<double>{_mm512_shuffle_f64x2(lo.raw, hi.raw, 0x4E)};
+ }
+ 
+ // hiH,hiL loH,loL |-> hiH,loL (= outer halves)
+ template <typename T>
+-HWY_API Vec512<T> ConcatUpperLower(const Vec512<T> hi, const Vec512<T> lo) {
++HWY_API Vec512<T> ConcatUpperLower(Full512<T> /* tag */, const Vec512<T> hi,
++                                   const Vec512<T> lo) {
+   // There are no imm8 blend in AVX512. Use blend16 because 32-bit masks
+   // are efficiently loaded from 32-bit regs.
+   const __mmask32 mask = /*_cvtu32_mask32 */ (0x0000FFFF);
+   return Vec512<T>{_mm512_mask_blend_epi16(mask, hi.raw, lo.raw)};
+ }
+-template <>
+-HWY_INLINE Vec512<float> ConcatUpperLower(const Vec512<float> hi,
+-                                          const Vec512<float> lo) {
++HWY_API Vec512<float> ConcatUpperLower(Full512<float> /* tag */,
++                                       const Vec512<float> hi,
++                                       const Vec512<float> lo) {
+   const __mmask16 mask = /*_cvtu32_mask16 */ (0x00FF);
+   return Vec512<float>{_mm512_mask_blend_ps(mask, hi.raw, lo.raw)};
+ }
+-template <>
+-HWY_INLINE Vec512<double> ConcatUpperLower(const Vec512<double> hi,
+-                                           const Vec512<double> lo) {
++HWY_API Vec512<double> ConcatUpperLower(Full512<double> /* tag */,
++                                        const Vec512<double> hi,
++                                        const Vec512<double> lo) {
+   const __mmask8 mask = /*_cvtu32_mask8 */ (0x0F);
+   return Vec512<double>{_mm512_mask_blend_pd(mask, hi.raw, lo.raw)};
+ }
+@@ -2483,16 +2577,54 @@ HWY_API Vec512<T> OddEven(const Vec512<T> a, const Vec512<T> b) {
+   return IfThenElse(Mask512<T>{0x5555555555555555ull >> shift}, b, a);
+ }
+ 
+-// ------------------------------ Shuffle bytes with variable indices
++// ------------------------------ TableLookupBytes (ZeroExtendVector)
+ 
+-// Returns vector of bytes[from[i]]. "from" is also interpreted as bytes, i.e.
+-// lane indices in [0, 16).
++// Both full
+ template <typename T>
+-HWY_API Vec512<T> TableLookupBytes(const Vec512<T> bytes,
+-                                   const Vec512<T> from) {
++HWY_API Vec512<T> TableLookupBytes(Vec512<T> bytes, Vec512<T> from) {
+   return Vec512<T>{_mm512_shuffle_epi8(bytes.raw, from.raw)};
+ }
+ 
++// Partial index vector
++template <typename T, typename TI, size_t NI>
++HWY_API Vec128<TI, NI> TableLookupBytes(Vec512<T> bytes, Vec128<TI, NI> from) {
++  const Full512<TI> d512;
++  const Half<decltype(d512)> d256;
++  const Half<decltype(d256)> d128;
++  // First expand to full 128, then 256, then 512.
++  const Vec128<TI> from_full{from.raw};
++  const auto from_512 =
++      ZeroExtendVector(d512, ZeroExtendVector(d256, from_full));
++  const auto tbl_full = TableLookupBytes(bytes, from_512);
++  // Shrink to 256, then 128, then partial.
++  return Vec128<TI, NI>{LowerHalf(d128, LowerHalf(d256, tbl_full)).raw};
++}
++template <typename T, typename TI>
++HWY_API Vec256<TI> TableLookupBytes(Vec512<T> bytes, Vec256<TI> from) {
++  const auto from_512 = ZeroExtendVector(Full512<TI>(), from);
++  return LowerHalf(Full256<TI>(), TableLookupBytes(bytes, from_512));
++}
++
++// Partial table vector
++template <typename T, size_t N, typename TI>
++HWY_API Vec512<TI> TableLookupBytes(Vec128<T, N> bytes, Vec512<TI> from) {
++  const Full512<TI> d512;
++  const Half<decltype(d512)> d256;
++  const Half<decltype(d256)> d128;
++  // First expand to full 128, then 256, then 512.
++  const Vec128<T> bytes_full{bytes.raw};
++  const auto bytes_512 =
++      ZeroExtendVector(d512, ZeroExtendVector(d256, bytes_full));
++  return TableLookupBytes(bytes_512, from);
++}
++template <typename T, typename TI>
++HWY_API Vec512<TI> TableLookupBytes(Vec256<T> bytes, Vec512<TI> from) {
++  const auto bytes_512 = ZeroExtendVector(Full512<T>(), bytes);
++  return TableLookupBytes(bytes_512, from);
++}
++
++// Partial both are handled by x86_128/256.
++
+ // ================================================== CONVERT
+ 
+ // ------------------------------ Promotions (part w/ narrow lanes -> full)
+@@ -2696,6 +2828,74 @@ HWY_API Vec512<int32_t> NearestInt(const Vec512<float> v) {
+   return detail::FixConversionOverflow(di, v, _mm512_cvtps_epi32(v.raw));
+ }
+ 
++// ================================================== CRYPTO
++
++#if !defined(HWY_DISABLE_PCLMUL_AES)
++
++// Per-target flag to prevent generic_ops-inl.h from defining AESRound.
++#ifdef HWY_NATIVE_AES
++#undef HWY_NATIVE_AES
++#else
++#define HWY_NATIVE_AES
++#endif
++
++HWY_API Vec512<uint8_t> AESRound(Vec512<uint8_t> state,
++                                 Vec512<uint8_t> round_key) {
++#if HWY_TARGET == HWY_AVX3_DL
++  return Vec512<uint8_t>{_mm512_aesenc_epi128(state.raw, round_key.raw)};
++#else
++  alignas(64) uint8_t a[64];
++  alignas(64) uint8_t b[64];
++  const Full512<uint8_t> d;
++  const Full128<uint8_t> d128;
++  Store(state, d, a);
++  Store(round_key, d, b);
++  for (size_t i = 0; i < 64; i += 16) {
++    const auto enc = AESRound(Load(d128, a + i), Load(d128, b + i));
++    Store(enc, d128, a + i);
++  }
++  return Load(d, a);
++#endif
++}
++
++HWY_API Vec512<uint64_t> CLMulLower(Vec512<uint64_t> va, Vec512<uint64_t> vb) {
++#if HWY_TARGET == HWY_AVX3_DL
++  return Vec512<uint64_t>{_mm512_clmulepi64_epi128(va.raw, vb.raw, 0x00)};
++#else
++  alignas(64) uint64_t a[8];
++  alignas(64) uint64_t b[8];
++  const Full512<uint64_t> d;
++  const Full128<uint64_t> d128;
++  Store(va, d, a);
++  Store(vb, d, b);
++  for (size_t i = 0; i < 8; i += 2) {
++    const auto mul = CLMulLower(Load(d128, a + i), Load(d128, b + i));
++    Store(mul, d128, a + i);
++  }
++  return Load(d, a);
++#endif
++}
++
++HWY_API Vec512<uint64_t> CLMulUpper(Vec512<uint64_t> va, Vec512<uint64_t> vb) {
++#if HWY_TARGET == HWY_AVX3_DL
++  return Vec512<uint64_t>{_mm512_clmulepi64_epi128(va.raw, vb.raw, 0x11)};
++#else
++  alignas(64) uint64_t a[8];
++  alignas(64) uint64_t b[8];
++  const Full512<uint64_t> d;
++  const Full128<uint64_t> d128;
++  Store(va, d, a);
++  Store(vb, d, b);
++  for (size_t i = 0; i < 8; i += 2) {
++    const auto mul = CLMulUpper(Load(d128, a + i), Load(d128, b + i));
++    Store(mul, d128, a + i);
++  }
++  return Load(d, a);
++#endif
++}
++
++#endif  // HWY_DISABLE_PCLMUL_AES
++
+ // ================================================== MISC
+ 
+ // Returns a vector with lane i=[0, N) set to "first" + i.
+@@ -2715,7 +2915,7 @@ Vec512<T> Iota(const Full512<T> d, const T2 first) {
+ namespace detail {
+ 
+ template <typename T>
+-HWY_API bool AllFalse(hwy::SizeTag<1> /*tag*/, const Mask512<T> v) {
++HWY_INLINE bool AllFalse(hwy::SizeTag<1> /*tag*/, const Mask512<T> v) {
+ #if HWY_COMPILER_HAS_MASK_INTRINSICS
+   return _kortestz_mask64_u8(v.raw, v.raw);
+ #else
+@@ -2723,7 +2923,7 @@ HWY_API bool AllFalse(hwy::SizeTag<1> /*tag*/, const Mask512<T> v) {
+ #endif
+ }
+ template <typename T>
+-HWY_API bool AllFalse(hwy::SizeTag<2> /*tag*/, const Mask512<T> v) {
++HWY_INLINE bool AllFalse(hwy::SizeTag<2> /*tag*/, const Mask512<T> v) {
+ #if HWY_COMPILER_HAS_MASK_INTRINSICS
+   return _kortestz_mask32_u8(v.raw, v.raw);
+ #else
+@@ -2731,7 +2931,7 @@ HWY_API bool AllFalse(hwy::SizeTag<2> /*tag*/, const Mask512<T> v) {
+ #endif
+ }
+ template <typename T>
+-HWY_API bool AllFalse(hwy::SizeTag<4> /*tag*/, const Mask512<T> v) {
++HWY_INLINE bool AllFalse(hwy::SizeTag<4> /*tag*/, const Mask512<T> v) {
+ #if HWY_COMPILER_HAS_MASK_INTRINSICS
+   return _kortestz_mask16_u8(v.raw, v.raw);
+ #else
+@@ -2739,7 +2939,7 @@ HWY_API bool AllFalse(hwy::SizeTag<4> /*tag*/, const Mask512<T> v) {
+ #endif
+ }
+ template <typename T>
+-HWY_API bool AllFalse(hwy::SizeTag<8> /*tag*/, const Mask512<T> v) {
++HWY_INLINE bool AllFalse(hwy::SizeTag<8> /*tag*/, const Mask512<T> v) {
+ #if HWY_COMPILER_HAS_MASK_INTRINSICS
+   return _kortestz_mask8_u8(v.raw, v.raw);
+ #else
+@@ -2750,14 +2950,14 @@ HWY_API bool AllFalse(hwy::SizeTag<8> /*tag*/, const Mask512<T> v) {
+ }  // namespace detail
+ 
+ template <typename T>
+-HWY_API bool AllFalse(const Mask512<T> v) {
++HWY_API bool AllFalse(const Full512<T> /* tag */, const Mask512<T> v) {
+   return detail::AllFalse(hwy::SizeTag<sizeof(T)>(), v);
+ }
+ 
+ namespace detail {
+ 
+ template <typename T>
+-HWY_API bool AllTrue(hwy::SizeTag<1> /*tag*/, const Mask512<T> v) {
++HWY_INLINE bool AllTrue(hwy::SizeTag<1> /*tag*/, const Mask512<T> v) {
+ #if HWY_COMPILER_HAS_MASK_INTRINSICS
+   return _kortestc_mask64_u8(v.raw, v.raw);
+ #else
+@@ -2765,7 +2965,7 @@ HWY_API bool AllTrue(hwy::SizeTag<1> /*tag*/, const Mask512<T> v) {
+ #endif
+ }
+ template <typename T>
+-HWY_API bool AllTrue(hwy::SizeTag<2> /*tag*/, const Mask512<T> v) {
++HWY_INLINE bool AllTrue(hwy::SizeTag<2> /*tag*/, const Mask512<T> v) {
+ #if HWY_COMPILER_HAS_MASK_INTRINSICS
+   return _kortestc_mask32_u8(v.raw, v.raw);
+ #else
+@@ -2773,7 +2973,7 @@ HWY_API bool AllTrue(hwy::SizeTag<2> /*tag*/, const Mask512<T> v) {
+ #endif
+ }
+ template <typename T>
+-HWY_API bool AllTrue(hwy::SizeTag<4> /*tag*/, const Mask512<T> v) {
++HWY_INLINE bool AllTrue(hwy::SizeTag<4> /*tag*/, const Mask512<T> v) {
+ #if HWY_COMPILER_HAS_MASK_INTRINSICS
+   return _kortestc_mask16_u8(v.raw, v.raw);
+ #else
+@@ -2781,7 +2981,7 @@ HWY_API bool AllTrue(hwy::SizeTag<4> /*tag*/, const Mask512<T> v) {
+ #endif
+ }
+ template <typename T>
+-HWY_API bool AllTrue(hwy::SizeTag<8> /*tag*/, const Mask512<T> v) {
++HWY_INLINE bool AllTrue(hwy::SizeTag<8> /*tag*/, const Mask512<T> v) {
+ #if HWY_COMPILER_HAS_MASK_INTRINSICS
+   return _kortestc_mask8_u8(v.raw, v.raw);
+ #else
+@@ -2792,22 +2992,35 @@ HWY_API bool AllTrue(hwy::SizeTag<8> /*tag*/, const Mask512<T> v) {
+ }  // namespace detail
+ 
+ template <typename T>
+-HWY_API bool AllTrue(const Mask512<T> v) {
++HWY_API bool AllTrue(const Full512<T> /* tag */, const Mask512<T> v) {
+   return detail::AllTrue(hwy::SizeTag<sizeof(T)>(), v);
+ }
+ 
+ template <typename T>
+-HWY_INLINE size_t StoreMaskBits(const Mask512<T> mask, uint8_t* p) {
++HWY_API size_t StoreMaskBits(const Full512<T> /* tag */, const Mask512<T> mask,
++                             uint8_t* p) {
+   const size_t kNumBytes = 8 / sizeof(T);
+   CopyBytes<kNumBytes>(&mask.raw, p);
+   return kNumBytes;
+ }
+ 
+ template <typename T>
+-HWY_API size_t CountTrue(const Mask512<T> mask) {
++HWY_API size_t CountTrue(const Full512<T> /* tag */, const Mask512<T> mask) {
+   return PopCount(mask.raw);
+ }
+ 
++template <typename T, HWY_IF_NOT_LANE_SIZE(T, 1)>
++HWY_API intptr_t FindFirstTrue(const Full512<T> /* tag */,
++                               const Mask512<T> mask) {
++  return mask.raw ? Num0BitsBelowLS1Bit_Nonzero32(mask.raw) : -1;
++}
++
++template <typename T, HWY_IF_LANE_SIZE(T, 1)>
++HWY_API intptr_t FindFirstTrue(const Full512<T> /* tag */,
++                               const Mask512<T> mask) {
++  return mask.raw ? Num0BitsBelowLS1Bit_Nonzero64(mask.raw) : -1;
++}
++
+ // ------------------------------ Compress
+ 
+ HWY_API Vec512<uint32_t> Compress(Vec512<uint32_t> v,
+@@ -2841,8 +3054,8 @@ namespace detail {
+ // Ignore IDE redefinition error for these two functions: if this header is
+ // included, then the functions weren't actually defined in x86_256-inl.h.
+ template <typename T>
+-HWY_API Vec256<T> Compress(hwy::SizeTag<2> /*tag*/, Vec256<T> v,
+-                           const uint64_t mask_bits) {
++HWY_INLINE Vec256<T> Compress(hwy::SizeTag<2> /*tag*/, Vec256<T> v,
++                              const uint64_t mask_bits) {
+   using D = Full256<T>;
+   const Rebind<uint16_t, D> du;
+   const Rebind<int32_t, D> dw;       // 512-bit, not 256!
+@@ -2867,7 +3080,7 @@ HWY_API Vec512<T> Compress(Vec512<T> v, const Mask512<T> mask) {
+   const Repartition<int32_t, D> dw;
+   const auto vu16 = BitCast(du, v);  // (required for float16_t inputs)
+   const auto promoted0 = PromoteTo(dw, LowerHalf(vu16));
+-  const auto promoted1 = PromoteTo(dw, UpperHalf(vu16));
++  const auto promoted1 = PromoteTo(dw, UpperHalf(Half<decltype(du)>(), vu16));
+ 
+   const Mask512<int32_t> mask0{static_cast<__mmask16>(mask.raw & 0xFFFF)};
+   const Mask512<int32_t> mask1{static_cast<__mmask16>(mask.raw >> 16)};
+@@ -2879,7 +3092,7 @@ HWY_API Vec512<T> Compress(Vec512<T> v, const Mask512<T> mask) {
+   const auto demoted1 = ZeroExtendVector(DemoteTo(dh, compressed1));
+ 
+   // Concatenate into single vector by shifting upper with writemask.
+-  const size_t num0 = CountTrue(mask0);
++  const size_t num0 = CountTrue(dw, mask0);
+   const __mmask32 m_upper = ~((1u << num0) - 1);
+   alignas(64) uint16_t iota[64] = {
+       0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,
+@@ -2908,47 +3121,45 @@ HWY_API size_t CompressStore(Vec512<T> v, const Mask512<T> mask, Full512<T> d,
+   // using StoreU to concatenate the results would cause page faults if
+   // `aligned` is the last valid vector. Instead rely on in-register splicing.
+   Store(Compress(v, mask), d, aligned);
+-  return CountTrue(mask);
++  return CountTrue(d, mask);
+ }
+ 
+ HWY_API size_t CompressStore(Vec512<uint32_t> v, const Mask512<uint32_t> mask,
+-                             Full512<uint32_t> /* tag */,
++                             Full512<uint32_t> d,
+                              uint32_t* HWY_RESTRICT aligned) {
+   _mm512_mask_compressstoreu_epi32(aligned, mask.raw, v.raw);
+-  return CountTrue(mask);
++  return CountTrue(d, mask);
+ }
+ HWY_API size_t CompressStore(Vec512<int32_t> v, const Mask512<int32_t> mask,
+-                             Full512<int32_t> /* tag */,
++                             Full512<int32_t> d,
+                              int32_t* HWY_RESTRICT aligned) {
+   _mm512_mask_compressstoreu_epi32(aligned, mask.raw, v.raw);
+-  return CountTrue(mask);
++  return CountTrue(d, mask);
+ }
+ 
+ HWY_API size_t CompressStore(Vec512<uint64_t> v, const Mask512<uint64_t> mask,
+-                             Full512<uint64_t> /* tag */,
++                             Full512<uint64_t> d,
+                              uint64_t* HWY_RESTRICT aligned) {
+   _mm512_mask_compressstoreu_epi64(aligned, mask.raw, v.raw);
+-  return CountTrue(mask);
++  return CountTrue(d, mask);
+ }
+ HWY_API size_t CompressStore(Vec512<int64_t> v, const Mask512<int64_t> mask,
+-                             Full512<int64_t> /* tag */,
++                             Full512<int64_t> d,
+                              int64_t* HWY_RESTRICT aligned) {
+   _mm512_mask_compressstoreu_epi64(aligned, mask.raw, v.raw);
+-  return CountTrue(mask);
++  return CountTrue(d, mask);
+ }
+ 
+ HWY_API size_t CompressStore(Vec512<float> v, const Mask512<float> mask,
+-                             Full512<float> /* tag */,
+-                             float* HWY_RESTRICT aligned) {
++                             Full512<float> d, float* HWY_RESTRICT aligned) {
+   _mm512_mask_compressstoreu_ps(aligned, mask.raw, v.raw);
+-  return CountTrue(mask);
++  return CountTrue(d, mask);
+ }
+ 
+ HWY_API size_t CompressStore(Vec512<double> v, const Mask512<double> mask,
+-                             Full512<double> /* tag */,
+-                             double* HWY_RESTRICT aligned) {
++                             Full512<double> d, double* HWY_RESTRICT aligned) {
+   _mm512_mask_compressstoreu_pd(aligned, mask.raw, v.raw);
+-  return CountTrue(mask);
++  return CountTrue(d, mask);
+ }
+ 
+ // ------------------------------ StoreInterleaved3 (CombineShiftRightBytes,
+@@ -2970,7 +3181,7 @@ HWY_API void StoreInterleaved3(const Vec512<uint8_t> a, const Vec512<uint8_t> b,
+       0x80, 2, 0x80, 0x80, 3, 0x80, 0x80, 4, 0x80, 0x80};
+   const auto shuf_r0 = LoadDup128(d, tbl_r0);
+   const auto shuf_g0 = LoadDup128(d, tbl_g0);  // cannot reuse r0 due to 5
+-  const auto shuf_b0 = CombineShiftRightBytes<15>(shuf_g0, shuf_g0);
++  const auto shuf_b0 = CombineShiftRightBytes<15>(d, shuf_g0, shuf_g0);
+   const auto r0 = TableLookupBytes(a, shuf_r0);  // 5..4..3..2..1..0
+   const auto g0 = TableLookupBytes(b, shuf_g0);  // ..4..3..2..1..0.
+   const auto b0 = TableLookupBytes(c, shuf_b0);  // .4..3..2..1..0..
+@@ -3015,17 +3226,19 @@ HWY_API void StoreInterleaved3(const Vec512<uint8_t> a, const Vec512<uint8_t> b,
+ HWY_API void StoreInterleaved4(const Vec512<uint8_t> v0,
+                                const Vec512<uint8_t> v1,
+                                const Vec512<uint8_t> v2,
+-                               const Vec512<uint8_t> v3, Full512<uint8_t> d,
++                               const Vec512<uint8_t> v3, Full512<uint8_t> d8,
+                                uint8_t* HWY_RESTRICT unaligned) {
++  const RepartitionToWide<decltype(d8)> d16;
++  const RepartitionToWide<decltype(d16)> d32;
+   // let a,b,c,d denote v0..3.
+-  const auto ba0 = ZipLower(v0, v1);  // b7 a7 .. b0 a0
+-  const auto dc0 = ZipLower(v2, v3);  // d7 c7 .. d0 c0
+-  const auto ba8 = ZipUpper(v0, v1);
+-  const auto dc8 = ZipUpper(v2, v3);
+-  const auto i = ZipLower(ba0, dc0).raw;  // 4x128bit: d..a3 d..a0
+-  const auto j = ZipUpper(ba0, dc0).raw;  // 4x128bit: d..a7 d..a4
+-  const auto k = ZipLower(ba8, dc8).raw;  // 4x128bit: d..aB d..a8
+-  const auto l = ZipUpper(ba8, dc8).raw;  // 4x128bit: d..aF d..aC
++  const auto ba0 = ZipLower(d16, v0, v1);  // b7 a7 .. b0 a0
++  const auto dc0 = ZipLower(d16, v2, v3);  // d7 c7 .. d0 c0
++  const auto ba8 = ZipUpper(d16, v0, v1);
++  const auto dc8 = ZipUpper(d16, v2, v3);
++  const auto i = ZipLower(d32, ba0, dc0).raw;  // 4x128bit: d..a3 d..a0
++  const auto j = ZipUpper(d32, ba0, dc0).raw;  // 4x128bit: d..a7 d..a4
++  const auto k = ZipLower(d32, ba8, dc8).raw;  // 4x128bit: d..aB d..a8
++  const auto l = ZipUpper(d32, ba8, dc8).raw;  // 4x128bit: d..aF d..aC
+   // 128-bit blocks were independent until now; transpose 4x4.
+   const auto j1_j0_i1_i0 = _mm512_shuffle_i64x2(i, j, _MM_SHUFFLE(1, 0, 1, 0));
+   const auto l1_l0_k1_k0 = _mm512_shuffle_i64x2(k, l, _MM_SHUFFLE(1, 0, 1, 0));
+@@ -3037,74 +3250,227 @@ HWY_API void StoreInterleaved4(const Vec512<uint8_t> v0,
+   const auto l1_k1_j1_i1 = _mm512_shuffle_i64x2(j1_j0_i1_i0, l1_l0_k1_k0, k31);
+   const auto l2_k2_j2_i2 = _mm512_shuffle_i64x2(j3_j2_i3_i2, l3_l2_k3_k2, k20);
+   const auto l3_k3_j3_i3 = _mm512_shuffle_i64x2(j3_j2_i3_i2, l3_l2_k3_k2, k31);
+-  StoreU(Vec512<uint8_t>{l0_k0_j0_i0}, d, unaligned + 0 * 64);
+-  StoreU(Vec512<uint8_t>{l1_k1_j1_i1}, d, unaligned + 1 * 64);
+-  StoreU(Vec512<uint8_t>{l2_k2_j2_i2}, d, unaligned + 2 * 64);
+-  StoreU(Vec512<uint8_t>{l3_k3_j3_i3}, d, unaligned + 3 * 64);
++  StoreU(Vec512<uint8_t>{l0_k0_j0_i0}, d8, unaligned + 0 * 64);
++  StoreU(Vec512<uint8_t>{l1_k1_j1_i1}, d8, unaligned + 1 * 64);
++  StoreU(Vec512<uint8_t>{l2_k2_j2_i2}, d8, unaligned + 2 * 64);
++  StoreU(Vec512<uint8_t>{l3_k3_j3_i3}, d8, unaligned + 3 * 64);
++}
++
++// ------------------------------ MulEven/Odd (Shuffle2301, InterleaveLower)
++
++HWY_INLINE Vec512<uint64_t> MulEven(const Vec512<uint64_t> a,
++                                    const Vec512<uint64_t> b) {
++  const DFromV<decltype(a)> du64;
++  const RepartitionToNarrow<decltype(du64)> du32;
++  const auto maskL = Set(du64, 0xFFFFFFFFULL);
++  const auto a32 = BitCast(du32, a);
++  const auto b32 = BitCast(du32, b);
++  // Inputs for MulEven: we only need the lower 32 bits
++  const auto aH = Shuffle2301(a32);
++  const auto bH = Shuffle2301(b32);
++
++  // Knuth double-word multiplication. We use 32x32 = 64 MulEven and only need
++  // the even (lower 64 bits of every 128-bit block) results. See
++  // https://github.com/hcs0/Hackers-Delight/blob/master/muldwu.c.tat
++  const auto aLbL = MulEven(a32, b32);
++  const auto w3 = aLbL & maskL;
++
++  const auto t2 = MulEven(aH, b32) + ShiftRight<32>(aLbL);
++  const auto w2 = t2 & maskL;
++  const auto w1 = ShiftRight<32>(t2);
++
++  const auto t = MulEven(a32, bH) + w2;
++  const auto k = ShiftRight<32>(t);
++
++  const auto mulH = MulEven(aH, bH) + w1 + k;
++  const auto mulL = ShiftLeft<32>(t) + w3;
++  return InterleaveLower(mulL, mulH);
++}
++
++HWY_INLINE Vec512<uint64_t> MulOdd(const Vec512<uint64_t> a,
++                                   const Vec512<uint64_t> b) {
++  const DFromV<decltype(a)> du64;
++  const RepartitionToNarrow<decltype(du64)> du32;
++  const auto maskL = Set(du64, 0xFFFFFFFFULL);
++  const auto a32 = BitCast(du32, a);
++  const auto b32 = BitCast(du32, b);
++  // Inputs for MulEven: we only need bits [95:64] (= upper half of input)
++  const auto aH = Shuffle2301(a32);
++  const auto bH = Shuffle2301(b32);
++
++  // Same as above, but we're using the odd results (upper 64 bits per block).
++  const auto aLbL = MulEven(a32, b32);
++  const auto w3 = aLbL & maskL;
++
++  const auto t2 = MulEven(aH, b32) + ShiftRight<32>(aLbL);
++  const auto w2 = t2 & maskL;
++  const auto w1 = ShiftRight<32>(t2);
++
++  const auto t = MulEven(a32, bH) + w2;
++  const auto k = ShiftRight<32>(t);
++
++  const auto mulH = MulEven(aH, bH) + w1 + k;
++  const auto mulL = ShiftLeft<32>(t) + w3;
++  return InterleaveUpper(du64, mulL, mulH);
+ }
+ 
+ // ------------------------------ Reductions
+ 
+ // Returns the sum in each lane.
+-HWY_API Vec512<int32_t> SumOfLanes(const Vec512<int32_t> v) {
+-  return Set(Full512<int32_t>(), _mm512_reduce_add_epi32(v.raw));
++HWY_API Vec512<int32_t> SumOfLanes(Full512<int32_t> d, Vec512<int32_t> v) {
++  return Set(d, _mm512_reduce_add_epi32(v.raw));
+ }
+-HWY_API Vec512<int64_t> SumOfLanes(const Vec512<int64_t> v) {
+-  return Set(Full512<int64_t>(), _mm512_reduce_add_epi64(v.raw));
++HWY_API Vec512<int64_t> SumOfLanes(Full512<int64_t> d, Vec512<int64_t> v) {
++  return Set(d, _mm512_reduce_add_epi64(v.raw));
+ }
+-HWY_API Vec512<uint32_t> SumOfLanes(const Vec512<uint32_t> v) {
+-  return BitCast(Full512<uint32_t>(),
+-                 SumOfLanes(BitCast(Full512<int32_t>(), v)));
++HWY_API Vec512<uint32_t> SumOfLanes(Full512<uint32_t> d, Vec512<uint32_t> v) {
++  return Set(d, _mm512_reduce_add_epi32(v.raw));
+ }
+-HWY_API Vec512<uint64_t> SumOfLanes(const Vec512<uint64_t> v) {
+-  return BitCast(Full512<uint64_t>(),
+-                 SumOfLanes(BitCast(Full512<int64_t>(), v)));
++HWY_API Vec512<uint64_t> SumOfLanes(Full512<uint64_t> d, Vec512<uint64_t> v) {
++  return Set(d, _mm512_reduce_add_epi64(v.raw));
+ }
+-HWY_API Vec512<float> SumOfLanes(const Vec512<float> v) {
+-  return Set(Full512<float>(), _mm512_reduce_add_ps(v.raw));
++HWY_API Vec512<float> SumOfLanes(Full512<float> d, Vec512<float> v) {
++  return Set(d, _mm512_reduce_add_ps(v.raw));
+ }
+-HWY_API Vec512<double> SumOfLanes(const Vec512<double> v) {
+-  return Set(Full512<double>(), _mm512_reduce_add_pd(v.raw));
++HWY_API Vec512<double> SumOfLanes(Full512<double> d, Vec512<double> v) {
++  return Set(d, _mm512_reduce_add_pd(v.raw));
+ }
+ 
+ // Returns the minimum in each lane.
+-HWY_API Vec512<int32_t> MinOfLanes(const Vec512<int32_t> v) {
+-  return Set(Full512<int32_t>(), _mm512_reduce_min_epi32(v.raw));
++HWY_API Vec512<int32_t> MinOfLanes(Full512<int32_t> d, Vec512<int32_t> v) {
++  return Set(d, _mm512_reduce_min_epi32(v.raw));
+ }
+-HWY_API Vec512<int64_t> MinOfLanes(const Vec512<int64_t> v) {
+-  return Set(Full512<int64_t>(), _mm512_reduce_min_epi64(v.raw));
++HWY_API Vec512<int64_t> MinOfLanes(Full512<int64_t> d, Vec512<int64_t> v) {
++  return Set(d, _mm512_reduce_min_epi64(v.raw));
+ }
+-HWY_API Vec512<uint32_t> MinOfLanes(const Vec512<uint32_t> v) {
+-  return Set(Full512<uint32_t>(), _mm512_reduce_min_epu32(v.raw));
++HWY_API Vec512<uint32_t> MinOfLanes(Full512<uint32_t> d, Vec512<uint32_t> v) {
++  return Set(d, _mm512_reduce_min_epu32(v.raw));
+ }
+-HWY_API Vec512<uint64_t> MinOfLanes(const Vec512<uint64_t> v) {
+-  return Set(Full512<uint64_t>(), _mm512_reduce_min_epu64(v.raw));
++HWY_API Vec512<uint64_t> MinOfLanes(Full512<uint64_t> d, Vec512<uint64_t> v) {
++  return Set(d, _mm512_reduce_min_epu64(v.raw));
+ }
+-HWY_API Vec512<float> MinOfLanes(const Vec512<float> v) {
+-  return Set(Full512<float>(), _mm512_reduce_min_ps(v.raw));
++HWY_API Vec512<float> MinOfLanes(Full512<float> d, Vec512<float> v) {
++  return Set(d, _mm512_reduce_min_ps(v.raw));
+ }
+-HWY_API Vec512<double> MinOfLanes(const Vec512<double> v) {
+-  return Set(Full512<double>(), _mm512_reduce_min_pd(v.raw));
++HWY_API Vec512<double> MinOfLanes(Full512<double> d, Vec512<double> v) {
++  return Set(d, _mm512_reduce_min_pd(v.raw));
+ }
+ 
+ // Returns the maximum in each lane.
+-HWY_API Vec512<int32_t> MaxOfLanes(const Vec512<int32_t> v) {
+-  return Set(Full512<int32_t>(), _mm512_reduce_max_epi32(v.raw));
++HWY_API Vec512<int32_t> MaxOfLanes(Full512<int32_t> d, Vec512<int32_t> v) {
++  return Set(d, _mm512_reduce_max_epi32(v.raw));
++}
++HWY_API Vec512<int64_t> MaxOfLanes(Full512<int64_t> d, Vec512<int64_t> v) {
++  return Set(d, _mm512_reduce_max_epi64(v.raw));
++}
++HWY_API Vec512<uint32_t> MaxOfLanes(Full512<uint32_t> d, Vec512<uint32_t> v) {
++  return Set(d, _mm512_reduce_max_epu32(v.raw));
++}
++HWY_API Vec512<uint64_t> MaxOfLanes(Full512<uint64_t> d, Vec512<uint64_t> v) {
++  return Set(d, _mm512_reduce_max_epu64(v.raw));
++}
++HWY_API Vec512<float> MaxOfLanes(Full512<float> d, Vec512<float> v) {
++  return Set(d, _mm512_reduce_max_ps(v.raw));
++}
++HWY_API Vec512<double> MaxOfLanes(Full512<double> d, Vec512<double> v) {
++  return Set(d, _mm512_reduce_max_pd(v.raw));
++}
++
++// ================================================== DEPRECATED
++
++template <typename T>
++HWY_API size_t StoreMaskBits(const Mask512<T> mask, uint8_t* p) {
++  return StoreMaskBits(Full512<T>(), mask, p);
++}
++
++template <typename T>
++HWY_API bool AllTrue(const Mask512<T> mask) {
++  return AllTrue(Full512<T>(), mask);
++}
++
++template <typename T>
++HWY_API bool AllFalse(const Mask512<T> mask) {
++  return AllFalse(Full512<T>(), mask);
++}
++
++template <typename T>
++HWY_API size_t CountTrue(const Mask512<T> mask) {
++  return CountTrue(Full512<T>(), mask);
++}
++
++template <typename T>
++HWY_API Vec512<T> SumOfLanes(Vec512<T> v) {
++  return SumOfLanes(Full512<T>(), v);
++}
++
++template <typename T>
++HWY_API Vec512<T> MinOfLanes(Vec512<T> v) {
++  return MinOfLanes(Full512<T>(), v);
++}
++
++template <typename T>
++HWY_API Vec512<T> MaxOfLanes(Vec512<T> v) {
++  return MaxOfLanes(Full512<T>(), v);
++}
++
++template <typename T>
++HWY_API Vec256<T> UpperHalf(Vec512<T> v) {
++  return UpperHalf(Full256<T>(), v);
++}
++
++template <int kBytes, typename T>
++HWY_API Vec512<T> ShiftRightBytes(const Vec512<T> v) {
++  return ShiftRightBytes<kBytes>(Full512<T>(), v);
++}
++
++template <int kLanes, typename T>
++HWY_API Vec512<T> ShiftRightLanes(const Vec512<T> v) {
++  return ShiftRightBytes<kLanes>(Full512<T>(), v);
++}
++
++template <size_t kBytes, typename T>
++HWY_API Vec512<T> CombineShiftRightBytes(Vec512<T> hi, Vec512<T> lo) {
++  return CombineShiftRightBytes<kBytes>(Full512<T>(), hi, lo);
++}
++
++template <typename T>
++HWY_API Vec512<T> InterleaveUpper(Vec512<T> a, Vec512<T> b) {
++  return InterleaveUpper(Full512<T>(), a, b);
++}
++
++template <typename T>
++HWY_API Vec512<MakeWide<T>> ZipUpper(Vec512<T> a, Vec512<T> b) {
++  return InterleaveUpper(Full512<MakeWide<T>>(), a, b);
+ }
+-HWY_API Vec512<int64_t> MaxOfLanes(const Vec512<int64_t> v) {
+-  return Set(Full512<int64_t>(), _mm512_reduce_max_epi64(v.raw));
++
++template <typename T>
++HWY_API Vec512<T> Combine(Vec256<T> hi, Vec256<T> lo) {
++  return Combine(Full512<T>(), hi, lo);
++}
++
++template <typename T>
++HWY_API Vec512<T> ZeroExtendVector(Vec256<T> lo) {
++  return ZeroExtendVector(Full512<T>(), lo);
+ }
+-HWY_API Vec512<uint32_t> MaxOfLanes(const Vec512<uint32_t> v) {
+-  return Set(Full512<uint32_t>(), _mm512_reduce_max_epu32(v.raw));
++
++template <typename T>
++HWY_API Vec512<T> ConcatLowerLower(Vec512<T> hi, Vec512<T> lo) {
++  return ConcatLowerLower(Full512<T>(), hi, lo);
+ }
+-HWY_API Vec512<uint64_t> MaxOfLanes(const Vec512<uint64_t> v) {
+-  return Set(Full512<uint64_t>(), _mm512_reduce_max_epu64(v.raw));
++
++template <typename T>
++HWY_API Vec512<T> ConcatLowerUpper(Vec512<T> hi, Vec512<T> lo) {
++  return ConcatLowerUpper(Full512<T>(), hi, lo);
+ }
+-HWY_API Vec512<float> MaxOfLanes(const Vec512<float> v) {
+-  return Set(Full512<float>(), _mm512_reduce_max_ps(v.raw));
++
++template <typename T>
++HWY_API Vec512<T> ConcatUpperLower(Vec512<T> hi, Vec512<T> lo) {
++  return ConcatUpperLower(Full512<T>(), hi, lo);
+ }
+-HWY_API Vec512<double> MaxOfLanes(const Vec512<double> v) {
+-  return Set(Full512<double>(), _mm512_reduce_max_pd(v.raw));
++
++template <typename T>
++HWY_API Vec512<T> ConcatUpperUpper(Vec512<T> hi, Vec512<T> lo) {
++  return ConcatUpperUpper(Full512<T>(), hi, lo);
+ }
+ 
+ // NOLINTNEXTLINE(google-readability-namespace-comments)
+diff --git a/third_party/highway/hwy/targets.cc b/third_party/highway/hwy/targets.cc
+index f910ccd07c552..c0b1c1ac0d562 100644
+--- a/third_party/highway/hwy/targets.cc
++++ b/third_party/highway/hwy/targets.cc
+@@ -100,9 +100,12 @@ constexpr uint32_t kSSE = 1 << 0;
+ constexpr uint32_t kSSE2 = 1 << 1;
+ constexpr uint32_t kSSE3 = 1 << 2;
+ constexpr uint32_t kSSSE3 = 1 << 3;
++constexpr uint32_t kGroupSSSE3 = kSSE | kSSE2 | kSSE3 | kSSSE3;
++
+ constexpr uint32_t kSSE41 = 1 << 4;
+ constexpr uint32_t kSSE42 = 1 << 5;
+-constexpr uint32_t kGroupSSE4 = kSSE | kSSE2 | kSSE3 | kSSSE3 | kSSE41 | kSSE42;
++constexpr uint32_t kCLMUL = 1 << 6;
++constexpr uint32_t kGroupSSE4 = kSSE41 | kSSE42 | kCLMUL | kGroupSSSE3;
+ 
+ constexpr uint32_t kAVX = 1u << 6;
+ constexpr uint32_t kAVX2 = 1u << 7;
+@@ -116,16 +119,26 @@ constexpr uint32_t kBMI2 = 1u << 11;
+ // [https://www.virtualbox.org/ticket/15471]. Thus we provide the option of
+ // avoiding using and requiring these so AVX2 can still be used.
+ #ifdef HWY_DISABLE_BMI2_FMA
+-constexpr uint32_t kGroupAVX2 = kAVX | kAVX2 | kLZCNT;
++constexpr uint32_t kGroupAVX2 = kAVX | kAVX2 | kLZCNT | kGroupSSE4;
+ #else
+-constexpr uint32_t kGroupAVX2 = kAVX | kAVX2 | kFMA | kLZCNT | kBMI | kBMI2;
++constexpr uint32_t kGroupAVX2 =
++    kAVX | kAVX2 | kFMA | kLZCNT | kBMI | kBMI2 | kGroupSSE4;
+ #endif
+ 
+ constexpr uint32_t kAVX512F = 1u << 12;
+ constexpr uint32_t kAVX512VL = 1u << 13;
+ constexpr uint32_t kAVX512DQ = 1u << 14;
+ constexpr uint32_t kAVX512BW = 1u << 15;
+-constexpr uint32_t kGroupAVX3 = kAVX512F | kAVX512VL | kAVX512DQ | kAVX512BW;
++constexpr uint32_t kGroupAVX3 =
++    kAVX512F | kAVX512VL | kAVX512DQ | kAVX512BW | kGroupAVX2;
++
++constexpr uint32_t kVNNI = 1u << 16;
++constexpr uint32_t kVPCLMULQDQ = 1u << 17;
++constexpr uint32_t kVAES = 1u << 18;
++constexpr uint32_t kPOPCNTDQ = 1u << 19;
++constexpr uint32_t kBITALG = 1u << 20;
++constexpr uint32_t kGroupAVX3_DL =
++    kVNNI | kVPCLMULQDQ | kVAES | kPOPCNTDQ | kBITALG | kGroupAVX3;
+ #endif  // HWY_ARCH_X86
+ 
+ }  // namespace
+@@ -193,69 +206,86 @@ uint32_t SupportedTargets() {
+   bits = HWY_SCALAR;
+ 
+ #if HWY_ARCH_X86
+-  uint32_t flags = 0;
+-  uint32_t abcd[4];
+-
+-  Cpuid(0, 0, abcd);
+-  const uint32_t max_level = abcd[0];
+-
+-  // Standard feature flags
+-  Cpuid(1, 0, abcd);
+-  flags |= IsBitSet(abcd[3], 25) ? kSSE : 0;
+-  flags |= IsBitSet(abcd[3], 26) ? kSSE2 : 0;
+-  flags |= IsBitSet(abcd[2], 0) ? kSSE3 : 0;
+-  flags |= IsBitSet(abcd[2], 9) ? kSSSE3 : 0;
+-  flags |= IsBitSet(abcd[2], 19) ? kSSE41 : 0;
+-  flags |= IsBitSet(abcd[2], 20) ? kSSE42 : 0;
+-  flags |= IsBitSet(abcd[2], 12) ? kFMA : 0;
+-  flags |= IsBitSet(abcd[2], 28) ? kAVX : 0;
+-  const bool has_osxsave = IsBitSet(abcd[2], 27);
+-
+-  // Extended feature flags
+-  Cpuid(0x80000001U, 0, abcd);
+-  flags |= IsBitSet(abcd[2], 5) ? kLZCNT : 0;
+-
+-  // Extended features
+-  if (max_level >= 7) {
+-    Cpuid(7, 0, abcd);
+-    flags |= IsBitSet(abcd[1], 3) ? kBMI : 0;
+-    flags |= IsBitSet(abcd[1], 5) ? kAVX2 : 0;
+-    flags |= IsBitSet(abcd[1], 8) ? kBMI2 : 0;
+-
+-    flags |= IsBitSet(abcd[1], 16) ? kAVX512F : 0;
+-    flags |= IsBitSet(abcd[1], 17) ? kAVX512DQ : 0;
+-    flags |= IsBitSet(abcd[1], 30) ? kAVX512BW : 0;
+-    flags |= IsBitSet(abcd[1], 31) ? kAVX512VL : 0;
++  bool has_osxsave = false;
++  {  // ensures we do not accidentally use flags outside this block
++    uint32_t flags = 0;
++    uint32_t abcd[4];
++
++    Cpuid(0, 0, abcd);
++    const uint32_t max_level = abcd[0];
++
++    // Standard feature flags
++    Cpuid(1, 0, abcd);
++    flags |= IsBitSet(abcd[3], 25) ? kSSE : 0;
++    flags |= IsBitSet(abcd[3], 26) ? kSSE2 : 0;
++    flags |= IsBitSet(abcd[2], 0) ? kSSE3 : 0;
++    flags |= IsBitSet(abcd[2], 1) ? kCLMUL : 0;
++    flags |= IsBitSet(abcd[2], 9) ? kSSSE3 : 0;
++    flags |= IsBitSet(abcd[2], 19) ? kSSE41 : 0;
++    flags |= IsBitSet(abcd[2], 20) ? kSSE42 : 0;
++    flags |= IsBitSet(abcd[2], 12) ? kFMA : 0;
++    flags |= IsBitSet(abcd[2], 28) ? kAVX : 0;
++    has_osxsave = IsBitSet(abcd[2], 27);
++
++    // Extended feature flags
++    Cpuid(0x80000001U, 0, abcd);
++    flags |= IsBitSet(abcd[2], 5) ? kLZCNT : 0;
++
++    // Extended features
++    if (max_level >= 7) {
++      Cpuid(7, 0, abcd);
++      flags |= IsBitSet(abcd[1], 3) ? kBMI : 0;
++      flags |= IsBitSet(abcd[1], 5) ? kAVX2 : 0;
++      flags |= IsBitSet(abcd[1], 8) ? kBMI2 : 0;
++
++      flags |= IsBitSet(abcd[1], 16) ? kAVX512F : 0;
++      flags |= IsBitSet(abcd[1], 17) ? kAVX512DQ : 0;
++      flags |= IsBitSet(abcd[1], 30) ? kAVX512BW : 0;
++      flags |= IsBitSet(abcd[1], 31) ? kAVX512VL : 0;
++
++      flags |= IsBitSet(abcd[2], 9) ? kVAES : 0;
++      flags |= IsBitSet(abcd[2], 10) ? kVPCLMULQDQ : 0;
++      flags |= IsBitSet(abcd[2], 11) ? kVNNI : 0;
++      flags |= IsBitSet(abcd[2], 12) ? kBITALG : 0;
++      flags |= IsBitSet(abcd[2], 14) ? kPOPCNTDQ : 0;
++    }
++
++    // Set target bit(s) if all their group's flags are all set.
++    if ((flags & kGroupAVX3_DL) == kGroupAVX3_DL) {
++      bits |= HWY_AVX3_DL;
++    }
++    if ((flags & kGroupAVX3) == kGroupAVX3) {
++      bits |= HWY_AVX3;
++    }
++    if ((flags & kGroupAVX2) == kGroupAVX2) {
++      bits |= HWY_AVX2;
++    }
++    if ((flags & kGroupSSE4) == kGroupSSE4) {
++      bits |= HWY_SSE4;
++    }
++    if ((flags & kGroupSSSE3) == kGroupSSSE3) {
++      bits |= HWY_SSSE3;
++    }
+   }
+ 
+-  // Verify OS support for XSAVE, without which XMM/YMM registers are not
+-  // preserved across context switches and are not safe to use.
++  // Clear bits if the OS does not support XSAVE - otherwise, registers
++  // are not preserved across context switches.
+   if (has_osxsave) {
+     const uint32_t xcr0 = ReadXCR0();
+     // XMM
+     if (!IsBitSet(xcr0, 1)) {
+-      flags = 0;
++      bits &= ~(HWY_SSSE3 | HWY_SSE4 | HWY_AVX2 | HWY_AVX3 | HWY_AVX3_DL);
+     }
+     // YMM
+     if (!IsBitSet(xcr0, 2)) {
+-      flags &= ~kGroupAVX2;
++      bits &= ~(HWY_AVX2 | HWY_AVX3 | HWY_AVX3_DL);
+     }
+     // ZMM + opmask
+     if ((xcr0 & 0x70) != 0x70) {
+-      flags &= ~kGroupAVX3;
++      bits &= ~(HWY_AVX3 | HWY_AVX3_DL);
+     }
+   }
+ 
+-  // Set target bit(s) if all their group's flags are all set.
+-  if ((flags & kGroupAVX3) == kGroupAVX3) {
+-    bits |= HWY_AVX3;
+-  }
+-  if ((flags & kGroupAVX2) == kGroupAVX2) {
+-    bits |= HWY_AVX2;
+-  }
+-  if ((flags & kGroupSSE4) == kGroupSSE4) {
+-    bits |= HWY_SSE4;
+-  }
+ #else
+   // TODO(janwas): detect for other platforms
+   bits = HWY_ENABLED_BASELINE;
+diff --git a/third_party/highway/hwy/targets.h b/third_party/highway/hwy/targets.h
+index 5f0195317d834..b9e8984c2c853 100644
+--- a/third_party/highway/hwy/targets.h
++++ b/third_party/highway/hwy/targets.h
+@@ -21,282 +21,23 @@
+ // generate and call.
+ 
+ #include "hwy/base.h"
+-
+-//------------------------------------------------------------------------------
+-// Optional configuration
+-
+-// See ../quick_reference.md for documentation of these macros.
+-
+-// Uncomment to override the default baseline determined from predefined macros:
+-// #define HWY_BASELINE_TARGETS (HWY_SSE4 | HWY_SCALAR)
+-
+-// Uncomment to override the default blocklist:
+-// #define HWY_BROKEN_TARGETS HWY_AVX3
+-
+-// Uncomment to definitely avoid generating those target(s):
+-// #define HWY_DISABLED_TARGETS HWY_SSE4
+-
+-// Uncomment to avoid emitting BMI/BMI2/FMA instructions (allows generating
+-// AVX2 target for VMs which support AVX2 but not the other instruction sets)
+-// #define HWY_DISABLE_BMI2_FMA
+-
+-//------------------------------------------------------------------------------
+-// Targets
+-
+-// Unique bit value for each target. A lower value is "better" (e.g. more lanes)
+-// than a higher value within the same group/platform - see HWY_STATIC_TARGET.
+-//
+-// All values are unconditionally defined so we can test HWY_TARGETS without
+-// first checking the HWY_ARCH_*.
+-//
+-// The C99 preprocessor evaluates #if expressions using intmax_t types, so we
+-// can use 32-bit literals.
+-
+-// 1,2,4: reserved
+-#define HWY_AVX3 8
+-#define HWY_AVX2 16
+-// 32: reserved for AVX
+-#define HWY_SSE4 64
+-// 0x80, 0x100, 0x200: reserved for SSSE3, SSE3, SSE2
+-
+-// The highest bit in the HWY_TARGETS mask that a x86 target can have. Used for
+-// dynamic dispatch. All x86 target bits must be lower or equal to
+-// (1 << HWY_HIGHEST_TARGET_BIT_X86) and they can only use
+-// HWY_MAX_DYNAMIC_TARGETS in total.
+-#define HWY_HIGHEST_TARGET_BIT_X86 9
+-
+-#define HWY_SVE2 0x400
+-#define HWY_SVE 0x800
+-// 0x1000 reserved for Helium
+-#define HWY_NEON 0x2000
+-
+-#define HWY_HIGHEST_TARGET_BIT_ARM 13
+-
+-// 0x4000, 0x8000 reserved
+-#define HWY_PPC8 0x10000  // v2.07 or 3
+-// 0x20000, 0x40000 reserved for prior VSX/AltiVec
+-
+-#define HWY_HIGHEST_TARGET_BIT_PPC 18
+-
+-// 0x80000 reserved
+-#define HWY_WASM 0x100000
+-
+-#define HWY_HIGHEST_TARGET_BIT_WASM 20
+-
+-// 0x200000, 0x400000, 0x800000 reserved
+-
+-#define HWY_RVV 0x1000000
+-
+-#define HWY_HIGHEST_TARGET_BIT_RVV 24
+-
+-// 0x2000000, 0x4000000, 0x8000000, 0x10000000 reserved
+-
+-#define HWY_SCALAR 0x20000000
+-
+-#define HWY_HIGHEST_TARGET_BIT_SCALAR 29
+-
+-// Cannot use higher values, otherwise HWY_TARGETS computation might overflow.
+-
+-//------------------------------------------------------------------------------
+-// Set default blocklists
+-
+-// Disabled means excluded from enabled at user's request. A separate config
+-// macro allows disabling without deactivating the blocklist below.
+-#ifndef HWY_DISABLED_TARGETS
+-#define HWY_DISABLED_TARGETS 0
+-#endif
+-
+-// Broken means excluded from enabled due to known compiler issues. Allow the
+-// user to override this blocklist without any guarantee of success.
+-#ifndef HWY_BROKEN_TARGETS
+-
+-// x86 clang-6: we saw multiple AVX2/3 compile errors and in one case invalid
+-// SSE4 codegen (possibly only for msan), so disable all those targets.
+-#if HWY_ARCH_X86 && (HWY_COMPILER_CLANG != 0 && HWY_COMPILER_CLANG < 700)
+-#define HWY_BROKEN_TARGETS (HWY_SSE4 | HWY_AVX2 | HWY_AVX3)
+-// This entails a major speed reduction, so warn unless the user explicitly
+-// opts in to scalar-only.
+-#if !defined(HWY_COMPILE_ONLY_SCALAR)
+-#pragma message("x86 Clang <= 6: define HWY_COMPILE_ONLY_SCALAR or upgrade.")
+-#endif
+-
+-// 32-bit may fail to compile AVX2/3.
+-#elif HWY_ARCH_X86_32
+-#define HWY_BROKEN_TARGETS (HWY_AVX2 | HWY_AVX3)
+-
+-// MSVC AVX3 support is buggy: https://github.com/Mysticial/Flops/issues/16
+-#elif HWY_COMPILER_MSVC != 0
+-#define HWY_BROKEN_TARGETS (HWY_AVX3)
+-
+-// armv7be has not been tested and is not yet supported.
+-#elif HWY_ARCH_ARM_V7 && (defined(__ARM_BIG_ENDIAN) || defined(__BIG_ENDIAN))
+-#define HWY_BROKEN_TARGETS (HWY_NEON)
+-
+-#else
+-#define HWY_BROKEN_TARGETS 0
+-#endif
+-
+-#endif  // HWY_BROKEN_TARGETS
+-
+-// Enabled means not disabled nor blocklisted.
+-#define HWY_ENABLED(targets) \
+-  ((targets) & ~((HWY_DISABLED_TARGETS) | (HWY_BROKEN_TARGETS)))
+-
+-//------------------------------------------------------------------------------
+-// Detect baseline targets using predefined macros
+-
+-// Baseline means the targets for which the compiler is allowed to generate
+-// instructions, implying the target CPU would have to support them. Do not use
+-// this directly because it does not take the blocklist into account. Allow the
+-// user to override this without any guarantee of success.
+-#ifndef HWY_BASELINE_TARGETS
+-
+-// Also check HWY_ARCH to ensure that simulating unknown platforms ends up with
+-// HWY_TARGET == HWY_SCALAR.
+-
+-#if HWY_ARCH_WASM && defined(__wasm_simd128__)
+-#define HWY_BASELINE_WASM HWY_WASM
+-#else
+-#define HWY_BASELINE_WASM 0
+-#endif
+-
+-// Avoid choosing the PPC target until we have an implementation.
+-#if HWY_ARCH_PPC && defined(__VSX__) && 0
+-#define HWY_BASELINE_PPC8 HWY_PPC8
+-#else
+-#define HWY_BASELINE_PPC8 0
+-#endif
+-
+-// Avoid choosing the SVE[2] targets the implementation is ready.
+-#if HWY_ARCH_ARM && defined(__ARM_FEATURE_SVE2) && 0
+-#define HWY_BASELINE_SVE2 HWY_SVE2
+-#else
+-#define HWY_BASELINE_SVE2 0
+-#endif
+-
+-#if HWY_ARCH_ARM && defined(__ARM_FEATURE_SVE) && 0
+-#define HWY_BASELINE_SVE HWY_SVE
+-#else
+-#define HWY_BASELINE_SVE 0
+-#endif
+-
+-// GCC 4.5.4 only defines __ARM_NEON__; 5.4 defines both.
+-#if HWY_ARCH_ARM && (defined(__ARM_NEON__) || defined(__ARM_NEON))
+-#define HWY_BASELINE_NEON HWY_NEON
+-#else
+-#define HWY_BASELINE_NEON 0
+-#endif
+-
+-// MSVC does not set SSE4_1, but it does set AVX; checking for the latter means
+-// we at least get SSE4 on machines supporting AVX but not AVX2.
+-// https://stackoverflow.com/questions/18563978/
+-#if HWY_ARCH_X86 && \
+-    (defined(__SSE4_1__) || (HWY_COMPILER_MSVC != 0 && defined(__AVX__)))
+-#define HWY_BASELINE_SSE4 HWY_SSE4
+-#else
+-#define HWY_BASELINE_SSE4 0
+-#endif
+-
+-#if HWY_ARCH_X86 && defined(__AVX2__)
+-#define HWY_BASELINE_AVX2 HWY_AVX2
+-#else
+-#define HWY_BASELINE_AVX2 0
+-#endif
+-
+-#if HWY_ARCH_X86 && defined(__AVX512F__)
+-#define HWY_BASELINE_AVX3 HWY_AVX3
+-#else
+-#define HWY_BASELINE_AVX3 0
+-#endif
+-
+-#if HWY_ARCH_RVV && defined(__riscv_vector)
+-#define HWY_BASELINE_RVV HWY_RVV
+-#else
+-#define HWY_BASELINE_RVV 0
+-#endif
+-
+-#define HWY_BASELINE_TARGETS                                                \
+-  (HWY_SCALAR | HWY_BASELINE_WASM | HWY_BASELINE_PPC8 | HWY_BASELINE_SVE2 | \
+-   HWY_BASELINE_SVE | HWY_BASELINE_NEON | HWY_BASELINE_SSE4 |               \
+-   HWY_BASELINE_AVX2 | HWY_BASELINE_AVX3 | HWY_BASELINE_RVV)
+-
+-#endif  // HWY_BASELINE_TARGETS
+-
+-//------------------------------------------------------------------------------
+-// Choose target for static dispatch
+-
+-#define HWY_ENABLED_BASELINE HWY_ENABLED(HWY_BASELINE_TARGETS)
+-#if HWY_ENABLED_BASELINE == 0
+-#error "At least one baseline target must be defined and enabled"
+-#endif
+-
+-// Best baseline, used for static dispatch. This is the least-significant 1-bit
+-// within HWY_ENABLED_BASELINE and lower bit values imply "better".
+-#define HWY_STATIC_TARGET (HWY_ENABLED_BASELINE & -HWY_ENABLED_BASELINE)
+-
+-// Start by assuming static dispatch. If we later use dynamic dispatch, this
+-// will be defined to other targets during the multiple-inclusion, and finally
+-// return to the initial value. Defining this outside begin/end_target ensures
+-// inl headers successfully compile by themselves (required by Bazel).
+-#define HWY_TARGET HWY_STATIC_TARGET
+-
+-//------------------------------------------------------------------------------
+-// Choose targets for dynamic dispatch according to one of four policies
+-
+-#if (defined(HWY_COMPILE_ONLY_SCALAR) + defined(HWY_COMPILE_ONLY_STATIC) + \
+-     defined(HWY_COMPILE_ALL_ATTAINABLE)) > 1
+-#error "Invalid config: can only define a single policy for targets"
+-#endif
+-
+-// Attainable means enabled and the compiler allows intrinsics (even when not
+-// allowed to autovectorize). Used in 3 and 4.
+-#if HWY_ARCH_X86
+-#define HWY_ATTAINABLE_TARGETS \
+-  HWY_ENABLED(HWY_SCALAR | HWY_SSE4 | HWY_AVX2 | HWY_AVX3)
+-#else
+-#define HWY_ATTAINABLE_TARGETS HWY_ENABLED_BASELINE
+-#endif
+-
+-// 1) For older compilers: disable all SIMD (could also set HWY_DISABLED_TARGETS
+-// to ~HWY_SCALAR, but this is more explicit).
+-#if defined(HWY_COMPILE_ONLY_SCALAR)
+-#undef HWY_STATIC_TARGET
+-#define HWY_STATIC_TARGET HWY_SCALAR  // override baseline
+-#define HWY_TARGETS HWY_SCALAR
+-
+-// 2) For forcing static dispatch without code changes (removing HWY_EXPORT)
+-#elif defined(HWY_COMPILE_ONLY_STATIC)
+-#define HWY_TARGETS HWY_STATIC_TARGET
+-
+-// 3) For tests: include all attainable targets (in particular: scalar)
+-#elif defined(HWY_COMPILE_ALL_ATTAINABLE)
+-#define HWY_TARGETS HWY_ATTAINABLE_TARGETS
+-
+-// 4) Default: attainable WITHOUT non-best baseline. This reduces code size by
+-// excluding superseded targets, in particular scalar.
+-#else
+-
+-#define HWY_TARGETS (HWY_ATTAINABLE_TARGETS & (2 * HWY_STATIC_TARGET - 1))
+-
+-#endif  // target policy
+-
+-// HWY_ONCE and the multiple-inclusion mechanism rely on HWY_STATIC_TARGET being
+-// one of the dynamic targets. This also implies HWY_TARGETS != 0 and
+-// (HWY_TARGETS & HWY_ENABLED_BASELINE) != 0.
+-#if (HWY_TARGETS & HWY_STATIC_TARGET) == 0
+-#error "Logic error: best baseline should be included in dynamic targets"
+-#endif
+-
+-//------------------------------------------------------------------------------
++#include "hwy/detect_targets.h"
+ 
+ namespace hwy {
+ 
+ // Returns (cached) bitfield of enabled targets that are supported on this CPU.
+-// Implemented in supported_targets.cc; unconditionally compiled to support the
+-// use case of binary-only distributions. The HWY_SUPPORTED_TARGETS wrapper may
+-// allow eliding calls to this function.
++// Implemented in targets.cc; unconditionally compiled to support the use case
++// of binary-only distributions. The HWY_SUPPORTED_TARGETS wrapper may allow
++// eliding calls to this function.
+ uint32_t SupportedTargets();
+ 
++// Evaluates to a function call, or literal if there is a single target.
++#if (HWY_TARGETS & (HWY_TARGETS - 1)) == 0
++#define HWY_SUPPORTED_TARGETS HWY_TARGETS
++#else
++#define HWY_SUPPORTED_TARGETS hwy::SupportedTargets()
++#endif
++
+ // Disable from runtime dispatch the mask of compiled in targets. Targets that
+ // were not enabled at compile time are ignored. This function is useful to
+ // disable a target supported by the CPU that is known to have bugs or when a
+@@ -305,17 +46,9 @@ uint32_t SupportedTargets();
+ // returns at least the baseline target.
+ void DisableTargets(uint32_t disabled_targets);
+ 
+-// Single target: reduce code size by eliding the call and conditional branches
+-// inside Choose*() functions.
+-#if (HWY_TARGETS & (HWY_TARGETS - 1)) == 0
+-#define HWY_SUPPORTED_TARGETS HWY_TARGETS
+-#else
+-#define HWY_SUPPORTED_TARGETS hwy::SupportedTargets()
+-#endif
+-
+ // Set the mock mask of CPU supported targets instead of the actual CPU
+ // supported targets computed in SupportedTargets(). The return value of
+-// SupportedTargets() will still be affected by the DisabledTargets() mask
++// SupportedTargets() will still be affected by the DisableTargets() mask
+ // regardless of this mock, to prevent accidentally adding targets that are
+ // known to be buggy in the current CPU. Call with a mask of 0 to disable the
+ // mock and use the actual CPU supported targets instead.
+@@ -341,12 +74,16 @@ HWY_INLINE std::vector<uint32_t> SupportedAndGeneratedTargets() {
+ static inline HWY_MAYBE_UNUSED const char* TargetName(uint32_t target) {
+   switch (target) {
+ #if HWY_ARCH_X86
++    case HWY_SSSE3:
++      return "SSSE3";
+     case HWY_SSE4:
+       return "SSE4";
+     case HWY_AVX2:
+       return "AVX2";
+     case HWY_AVX3:
+       return "AVX3";
++    case HWY_AVX3_DL:
++      return "AVX3_DL";
+ #endif
+ 
+ #if HWY_ARCH_ARM
+@@ -424,17 +161,17 @@ static inline HWY_MAYBE_UNUSED const char* TargetName(uint32_t target) {
+ // HWY_MAX_DYNAMIC_TARGETS) bit. This list must contain exactly
+ // HWY_MAX_DYNAMIC_TARGETS elements and does not include SCALAR. The first entry
+ // corresponds to the best target. Don't include a "," at the end of the list.
+-#define HWY_CHOOSE_TARGET_LIST(func_name)        \
+-  nullptr,                        /* reserved */ \
+-      nullptr,                    /* reserved */ \
+-      nullptr,                    /* reserved */ \
+-      HWY_CHOOSE_AVX3(func_name), /* AVX3 */     \
+-      HWY_CHOOSE_AVX2(func_name), /* AVX2 */     \
+-      nullptr,                    /* AVX */      \
+-      HWY_CHOOSE_SSE4(func_name), /* SSE4 */     \
+-      nullptr,                    /* SSSE3 */    \
+-      nullptr,                    /* SSE3 */     \
+-      nullptr                     /* SSE2 */
++#define HWY_CHOOSE_TARGET_LIST(func_name)           \
++  nullptr,                           /* reserved */ \
++      nullptr,                       /* reserved */ \
++      HWY_CHOOSE_AVX3_DL(func_name), /* AVX3_DL */  \
++      HWY_CHOOSE_AVX3(func_name),    /* AVX3 */     \
++      HWY_CHOOSE_AVX2(func_name),    /* AVX2 */     \
++      nullptr,                       /* AVX */      \
++      HWY_CHOOSE_SSE4(func_name),    /* SSE4 */     \
++      HWY_CHOOSE_SSSE3(func_name),   /* SSSE3 */    \
++      nullptr,                       /* SSE3 */     \
++      nullptr                        /* SSE2 */
+ 
+ #elif HWY_ARCH_ARM
+ // See HWY_ARCH_X86 above for details.
+diff --git a/third_party/highway/hwy/targets_test.cc b/third_party/highway/hwy/targets_test.cc
+index 4cb9291d15f9f..a593fcedef8a4 100644
+--- a/third_party/highway/hwy/targets_test.cc
++++ b/third_party/highway/hwy/targets_test.cc
+@@ -23,10 +23,14 @@ namespace fake {
+     uint32_t FakeFunction(int) { return HWY_##TGT; } \
+   }
+ 
++DECLARE_FUNCTION(AVX3_DL)
+ DECLARE_FUNCTION(AVX3)
+ DECLARE_FUNCTION(AVX2)
+ DECLARE_FUNCTION(SSE4)
++DECLARE_FUNCTION(SSSE3)
+ DECLARE_FUNCTION(NEON)
++DECLARE_FUNCTION(SVE)
++DECLARE_FUNCTION(SVE2)
+ DECLARE_FUNCTION(PPC8)
+ DECLARE_FUNCTION(WASM)
+ DECLARE_FUNCTION(RVV)
+@@ -49,10 +53,14 @@ void CheckFakeFunction() {
+     /* Second call uses the cached value from the previous call. */         \
+     EXPECT_EQ(uint32_t(HWY_##TGT), HWY_DYNAMIC_DISPATCH(FakeFunction)(42)); \
+   }
++  CHECK_ARRAY_ENTRY(AVX3_DL)
+   CHECK_ARRAY_ENTRY(AVX3)
+   CHECK_ARRAY_ENTRY(AVX2)
+   CHECK_ARRAY_ENTRY(SSE4)
++  CHECK_ARRAY_ENTRY(SSSE3)
+   CHECK_ARRAY_ENTRY(NEON)
++  CHECK_ARRAY_ENTRY(SVE)
++  CHECK_ARRAY_ENTRY(SVE2)
+   CHECK_ARRAY_ENTRY(PPC8)
+   CHECK_ARRAY_ENTRY(WASM)
+   CHECK_ARRAY_ENTRY(RVV)
+@@ -100,3 +108,9 @@ TEST_F(HwyTargetsTest, DisabledTargetsTest) {
+ }
+ 
+ }  // namespace hwy
++
++// Ought not to be necessary, but without this, no tests run on RVV.
++int main(int argc, char **argv) {
++  ::testing::InitGoogleTest(&argc, argv);
++  return RUN_ALL_TESTS();
++}
+diff --git a/third_party/highway/hwy/tests/arithmetic_test.cc b/third_party/highway/hwy/tests/arithmetic_test.cc
+index 07086356e6d45..7e2ac26e7e766 100644
+--- a/third_party/highway/hwy/tests/arithmetic_test.cc
++++ b/third_party/highway/hwy/tests/arithmetic_test.cc
+@@ -40,7 +40,7 @@ struct TestPlusMinus {
+     for (size_t i = 0; i < N; ++i) {
+       lanes[i] = static_cast<T>((2 + i) + (3 + i));
+     }
+-    HWY_ASSERT_VEC_EQ(d, lanes.get(), v2 + v3);
++    HWY_ASSERT_VEC_EQ(d, lanes.get(), Add(v2, v3));
+     HWY_ASSERT_VEC_EQ(d, Set(d, 2), Sub(v4, v2));
+ 
+     for (size_t i = 0; i < N; ++i) {
+@@ -376,7 +376,7 @@ class TestSignedRightShifts {
+ 
+     // First test positive values, negative are checked below.
+     const auto v0 = Zero(d);
+-    const auto values = Iota(d, 0) & Set(d, kMax);
++    const auto values = And(Iota(d, 0), Set(d, kMax));
+ 
+     // Shift by 0
+     HWY_ASSERT_VEC_EQ(d, values, ShiftRight<0>(values));
+@@ -616,7 +616,7 @@ struct TestUnsignedMul {
+     for (size_t i = 0; i < N; ++i) {
+       expected[i] = static_cast<T>((1 + i) * (1 + i));
+     }
+-    HWY_ASSERT_VEC_EQ(d, expected.get(), vi * vi);
++    HWY_ASSERT_VEC_EQ(d, expected.get(), Mul(vi, vi));
+ 
+     for (size_t i = 0; i < N; ++i) {
+       expected[i] = static_cast<T>((1 + i) * (3 + i));
+@@ -747,10 +747,52 @@ struct TestMulEven {
+   }
+ };
+ 
++struct TestMulEvenOdd64 {
++  template <typename T, class D>
++  HWY_NOINLINE void operator()(T /*unused*/, D d) {
++#if HWY_TARGET != HWY_SCALAR
++    const auto v0 = Zero(d);
++    HWY_ASSERT_VEC_EQ(d, Zero(d), MulEven(v0, v0));
++    HWY_ASSERT_VEC_EQ(d, Zero(d), MulOdd(v0, v0));
++
++    const size_t N = Lanes(d);
++    if (N == 1) return;
++
++    auto in1 = AllocateAligned<T>(N);
++    auto in2 = AllocateAligned<T>(N);
++    auto expected_even = AllocateAligned<T>(N);
++    auto expected_odd = AllocateAligned<T>(N);
++
++    // Random inputs in each lane
++    RandomState rng;
++    for (size_t rep = 0; rep < 1000; ++rep) {
++      for (size_t i = 0; i < N; ++i) {
++        in1[i] = Random64(&rng);
++        in2[i] = Random64(&rng);
++      }
++
++      for (size_t i = 0; i < N; i += 2) {
++        expected_even[i] = Mul128(in1[i], in2[i], &expected_even[i + 1]);
++        expected_odd[i] = Mul128(in1[i + 1], in2[i + 1], &expected_odd[i + 1]);
++      }
++
++      const auto a = Load(d, in1.get());
++      const auto b = Load(d, in2.get());
++      HWY_ASSERT_VEC_EQ(d, expected_even.get(), MulEven(a, b));
++      HWY_ASSERT_VEC_EQ(d, expected_odd.get(), MulOdd(a, b));
++    }
++#else
++    (void)d;
++#endif  // HWY_TARGET != HWY_SCALAR
++  }
++};
++
+ HWY_NOINLINE void TestAllMulEven() {
+   ForPartialVectors<TestMulEven> test;
+   test(int32_t());
+   test(uint32_t());
++
++  ForGE128Vectors<TestMulEvenOdd64>()(uint64_t());
+ }
+ 
+ struct TestMulAdd {
+@@ -853,7 +895,7 @@ struct TestApproximateReciprocal {
+ 
+     double max_l1 = 0.0;
+     for (size_t i = 0; i < N; ++i) {
+-      max_l1 = std::max<double>(max_l1, std::abs((1.0 / input[i]) - actual[i]));
++      max_l1 = HWY_MAX(max_l1, std::abs((1.0 / input[i]) - actual[i]));
+     }
+     const double max_rel = max_l1 / std::abs(1.0 / input[N - 1]);
+     printf("max err %f\n", max_rel);
+@@ -870,7 +912,7 @@ struct TestSquareRoot {
+   template <typename T, class D>
+   HWY_NOINLINE void operator()(T /*unused*/, D d) {
+     const auto vi = Iota(d, 0);
+-    HWY_ASSERT_VEC_EQ(d, vi, Sqrt(vi * vi));
++    HWY_ASSERT_VEC_EQ(d, vi, Sqrt(Mul(vi, vi)));
+   }
+ };
+ 
+@@ -901,36 +943,53 @@ template <typename T, class D>
+ AlignedFreeUniquePtr<T[]> RoundTestCases(T /*unused*/, D d, size_t& padded) {
+   const T eps = std::numeric_limits<T>::epsilon();
+   const T test_cases[] = {
+-      // +/- 1
+-      T(1), T(-1),
+-      // +/- 0
+-      T(0), T(-0),
+-      // near 0
+-      T(0.4), T(-0.4),
+-      // +/- integer
+-      T(4), T(-32),
+-      // positive near limit
+-      MantissaEnd<T>() - T(1.5), MantissaEnd<T>() + T(1.5),
+-      // negative near limit
+-      -MantissaEnd<T>() - T(1.5), -MantissaEnd<T>() + T(1.5),
+-      // +/- huge (but still fits in float)
+-      T(1E34), T(-1E35),
+-      // positive tiebreak
+-      T(1.5), T(2.5),
+-      // negative tiebreak
+-      T(-1.5), T(-2.5),
+-      // positive +/- delta
+-      T(2.0001), T(3.9999),
+-      // negative +/- delta
+-      T(-999.9999), T(-998.0001),
+-      // positive +/- epsilon
+-      T(1) + eps, T(1) - eps,
+-      // negative +/- epsilon
+-      T(-1) + eps, T(-1) - eps,
+-      // +/- infinity
+-      std::numeric_limits<T>::infinity(), -std::numeric_limits<T>::infinity(),
+-      // qNaN
+-      GetLane(NaN(d))};
++    // +/- 1
++    T(1),
++    T(-1),
++    // +/- 0
++    T(0),
++    T(-0),
++    // near 0
++    T(0.4),
++    T(-0.4),
++    // +/- integer
++    T(4),
++    T(-32),
++    // positive near limit
++    MantissaEnd<T>() - T(1.5),
++    MantissaEnd<T>() + T(1.5),
++    // negative near limit
++    -MantissaEnd<T>() - T(1.5),
++    -MantissaEnd<T>() + T(1.5),
++    // positive tiebreak
++    T(1.5),
++    T(2.5),
++    // negative tiebreak
++    T(-1.5),
++    T(-2.5),
++    // positive +/- delta
++    T(2.0001),
++    T(3.9999),
++    // negative +/- delta
++    T(-999.9999),
++    T(-998.0001),
++    // positive +/- epsilon
++    T(1) + eps,
++    T(1) - eps,
++    // negative +/- epsilon
++    T(-1) + eps,
++    T(-1) - eps,
++#if !defined(HWY_EMULATE_SVE)  // these are not safe to just cast to int
++    // +/- huge (but still fits in float)
++    T(1E34),
++    T(-1E35),
++    // +/- infinity
++    std::numeric_limits<T>::infinity(),
++    -std::numeric_limits<T>::infinity(),
++    // qNaN
++    GetLane(NaN(d))
++#endif
++  };
+   const size_t kNumTestCases = sizeof(test_cases) / sizeof(test_cases[0]);
+   const size_t N = Lanes(d);
+   padded = RoundUpTo(kNumTestCases, N);  // allow loading whole vectors
+@@ -1074,30 +1133,31 @@ struct TestSumOfLanes {
+       in_lanes[i] = i < kBits ? static_cast<T>(1ull << i) : 0;
+       sum += static_cast<double>(in_lanes[i]);
+     }
+-    HWY_ASSERT_VEC_EQ(d, Set(d, T(sum)), SumOfLanes(Load(d, in_lanes.get())));
++    HWY_ASSERT_VEC_EQ(d, Set(d, T(sum)),
++                      SumOfLanes(d, Load(d, in_lanes.get())));
+ 
+     // Lane i = i (iota) to include upper lanes
+     sum = 0.0;
+     for (size_t i = 0; i < N; ++i) {
+       sum += static_cast<double>(i);
+     }
+-    HWY_ASSERT_VEC_EQ(d, Set(d, T(sum)), SumOfLanes(Iota(d, 0)));
++    HWY_ASSERT_VEC_EQ(d, Set(d, T(sum)), SumOfLanes(d, Iota(d, 0)));
+   }
+ };
+ 
+ HWY_NOINLINE void TestAllSumOfLanes() {
+-  const ForPartialVectors<TestSumOfLanes> sum;
++  const ForPartialVectors<TestSumOfLanes> test;
+ 
+   // No u8/u16/i8/i16.
+-  sum(uint32_t());
+-  sum(int32_t());
++  test(uint32_t());
++  test(int32_t());
+ 
+ #if HWY_CAP_INTEGER64
+-  sum(uint64_t());
+-  sum(int64_t());
++  test(uint64_t());
++  test(int64_t());
+ #endif
+ 
+-  ForFloatTypes(sum);
++  ForFloatTypes(test);
+ }
+ 
+ struct TestMinOfLanes {
+@@ -1112,17 +1172,17 @@ struct TestMinOfLanes {
+     constexpr size_t kBits = HWY_MIN(sizeof(T) * 8 - 1, 51);
+     for (size_t i = 0; i < N; ++i) {
+       in_lanes[i] = i < kBits ? static_cast<T>(1ull << i) : 2;
+-      min = std::min(min, in_lanes[i]);
++      min = HWY_MIN(min, in_lanes[i]);
+     }
+-    HWY_ASSERT_VEC_EQ(d, Set(d, min), MinOfLanes(Load(d, in_lanes.get())));
++    HWY_ASSERT_VEC_EQ(d, Set(d, min), MinOfLanes(d, Load(d, in_lanes.get())));
+ 
+     // Lane i = N - i to include upper lanes
+     min = HighestValue<T>();
+     for (size_t i = 0; i < N; ++i) {
+       in_lanes[i] = static_cast<T>(N - i);  // no 8-bit T so no wraparound
+-      min = std::min(min, in_lanes[i]);
++      min = HWY_MIN(min, in_lanes[i]);
+     }
+-    HWY_ASSERT_VEC_EQ(d, Set(d, min), MinOfLanes(Load(d, in_lanes.get())));
++    HWY_ASSERT_VEC_EQ(d, Set(d, min), MinOfLanes(d, Load(d, in_lanes.get())));
+   }
+ };
+ 
+@@ -1137,17 +1197,17 @@ struct TestMaxOfLanes {
+     constexpr size_t kBits = HWY_MIN(sizeof(T) * 8 - 1, 51);
+     for (size_t i = 0; i < N; ++i) {
+       in_lanes[i] = i < kBits ? static_cast<T>(1ull << i) : 0;
+-      max = std::max(max, in_lanes[i]);
++      max = HWY_MAX(max, in_lanes[i]);
+     }
+-    HWY_ASSERT_VEC_EQ(d, Set(d, max), MaxOfLanes(Load(d, in_lanes.get())));
++    HWY_ASSERT_VEC_EQ(d, Set(d, max), MaxOfLanes(d, Load(d, in_lanes.get())));
+ 
+     // Lane i = i to include upper lanes
+     max = LowestValue<T>();
+     for (size_t i = 0; i < N; ++i) {
+       in_lanes[i] = static_cast<T>(i);  // no 8-bit T so no wraparound
+-      max = std::max(max, in_lanes[i]);
++      max = HWY_MAX(max, in_lanes[i]);
+     }
+-    HWY_ASSERT_VEC_EQ(d, Set(d, max), MaxOfLanes(Load(d, in_lanes.get())));
++    HWY_ASSERT_VEC_EQ(d, Set(d, max), MaxOfLanes(d, Load(d, in_lanes.get())));
+   }
+ };
+ 
+@@ -1219,6 +1279,7 @@ HWY_NOINLINE void TestAllNeg() {
+ HWY_AFTER_NAMESPACE();
+ 
+ #if HWY_ONCE
++
+ namespace hwy {
+ HWY_BEFORE_TEST(HwyArithmeticTest);
+ HWY_EXPORT_AND_TEST_P(HwyArithmeticTest, TestAllPlusMinus);
+@@ -1246,4 +1307,11 @@ HWY_EXPORT_AND_TEST_P(HwyArithmeticTest, TestAllFloor);
+ HWY_EXPORT_AND_TEST_P(HwyArithmeticTest, TestAllAbsDiff);
+ HWY_EXPORT_AND_TEST_P(HwyArithmeticTest, TestAllNeg);
+ }  // namespace hwy
++
++// Ought not to be necessary, but without this, no tests run on RVV.
++int main(int argc, char** argv) {
++  ::testing::InitGoogleTest(&argc, argv);
++  return RUN_ALL_TESTS();
++}
++
+ #endif
+diff --git a/third_party/highway/hwy/tests/blockwise_test.cc b/third_party/highway/hwy/tests/blockwise_test.cc
+new file mode 100644
+index 0000000000000..234e606366952
+--- /dev/null
++++ b/third_party/highway/hwy/tests/blockwise_test.cc
+@@ -0,0 +1,655 @@
++// Copyright 2019 Google LLC
++//
++// Licensed under the Apache License, Version 2.0 (the "License");
++// you may not use this file except in compliance with the License.
++// You may obtain a copy of the License at
++//
++//      http://www.apache.org/licenses/LICENSE-2.0
++//
++// Unless required by applicable law or agreed to in writing, software
++// distributed under the License is distributed on an "AS IS" BASIS,
++// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
++// See the License for the specific language governing permissions and
++// limitations under the License.
++
++#include <stddef.h>
++#include <stdint.h>
++#include <string.h>
++
++#undef HWY_TARGET_INCLUDE
++#define HWY_TARGET_INCLUDE "tests/blockwise_test.cc"
++#include "hwy/foreach_target.h"
++#include "hwy/highway.h"
++#include "hwy/tests/test_util-inl.h"
++
++HWY_BEFORE_NAMESPACE();
++namespace hwy {
++namespace HWY_NAMESPACE {
++
++struct TestShiftBytes {
++  template <class T, class D>
++  HWY_NOINLINE void operator()(T /*unused*/, D d) {
++    // Scalar does not define Shift*Bytes.
++#if HWY_TARGET != HWY_SCALAR || HWY_IDE
++    const Repartition<uint8_t, D> du8;
++    const size_t N8 = Lanes(du8);
++
++    // Zero remains zero
++    const auto v0 = Zero(d);
++    HWY_ASSERT_VEC_EQ(d, v0, ShiftLeftBytes<1>(v0));
++    HWY_ASSERT_VEC_EQ(d, v0, ShiftLeftBytes<1>(d, v0));
++    HWY_ASSERT_VEC_EQ(d, v0, ShiftRightBytes<1>(d, v0));
++
++    // Zero after shifting out the high/low byte
++    auto bytes = AllocateAligned<uint8_t>(N8);
++    std::fill(bytes.get(), bytes.get() + N8, 0);
++    bytes[N8 - 1] = 0x7F;
++    const auto vhi = BitCast(d, Load(du8, bytes.get()));
++    bytes[N8 - 1] = 0;
++    bytes[0] = 0x7F;
++    const auto vlo = BitCast(d, Load(du8, bytes.get()));
++    HWY_ASSERT_VEC_EQ(d, v0, ShiftLeftBytes<1>(vhi));
++    HWY_ASSERT_VEC_EQ(d, v0, ShiftLeftBytes<1>(d, vhi));
++    HWY_ASSERT_VEC_EQ(d, v0, ShiftRightBytes<1>(d, vlo));
++
++    // Check expected result with Iota
++    const size_t N = Lanes(d);
++    auto in = AllocateAligned<T>(N);
++    const uint8_t* in_bytes = reinterpret_cast<const uint8_t*>(in.get());
++    const auto v = BitCast(d, Iota(du8, 1));
++    Store(v, d, in.get());
++
++    auto expected = AllocateAligned<T>(N);
++    uint8_t* expected_bytes = reinterpret_cast<uint8_t*>(expected.get());
++
++    const size_t kBlockSize = HWY_MIN(N8, 16);
++    for (size_t block = 0; block < N8; block += kBlockSize) {
++      expected_bytes[block] = 0;
++      memcpy(expected_bytes + block + 1, in_bytes + block, kBlockSize - 1);
++    }
++    HWY_ASSERT_VEC_EQ(d, expected.get(), ShiftLeftBytes<1>(v));
++    HWY_ASSERT_VEC_EQ(d, expected.get(), ShiftLeftBytes<1>(d, v));
++
++    for (size_t block = 0; block < N8; block += kBlockSize) {
++      memcpy(expected_bytes + block, in_bytes + block + 1, kBlockSize - 1);
++      expected_bytes[block + kBlockSize - 1] = 0;
++    }
++    HWY_ASSERT_VEC_EQ(d, expected.get(), ShiftRightBytes<1>(d, v));
++#else
++    (void)d;
++#endif  // #if HWY_TARGET != HWY_SCALAR
++  }
++};
++
++HWY_NOINLINE void TestAllShiftBytes() {
++  ForIntegerTypes(ForPartialVectors<TestShiftBytes>());
++}
++
++struct TestShiftLanes {
++  template <class T, class D>
++  HWY_NOINLINE void operator()(T /*unused*/, D d) {
++    // Scalar does not define Shift*Lanes.
++#if HWY_TARGET != HWY_SCALAR || HWY_IDE
++    const auto v = Iota(d, T(1));
++    const size_t N = Lanes(d);
++    auto expected = AllocateAligned<T>(N);
++
++    HWY_ASSERT_VEC_EQ(d, v, ShiftLeftLanes<0>(v));
++    HWY_ASSERT_VEC_EQ(d, v, ShiftLeftLanes<0>(d, v));
++    HWY_ASSERT_VEC_EQ(d, v, ShiftRightLanes<0>(d, v));
++
++    constexpr size_t kLanesPerBlock = 16 / sizeof(T);
++
++    for (size_t i = 0; i < N; ++i) {
++      expected[i] = (i % kLanesPerBlock) == 0 ? T(0) : T(i);
++    }
++    HWY_ASSERT_VEC_EQ(d, expected.get(), ShiftLeftLanes<1>(v));
++    HWY_ASSERT_VEC_EQ(d, expected.get(), ShiftLeftLanes<1>(d, v));
++
++    for (size_t i = 0; i < N; ++i) {
++      const size_t mod = i % kLanesPerBlock;
++      expected[i] = mod == (kLanesPerBlock - 1) || i >= N - 1 ? T(0) : T(2 + i);
++    }
++    HWY_ASSERT_VEC_EQ(d, expected.get(), ShiftRightLanes<1>(d, v));
++#else
++    (void)d;
++#endif  // #if HWY_TARGET != HWY_SCALAR
++  }
++};
++
++HWY_NOINLINE void TestAllShiftLanes() {
++  ForAllTypes(ForPartialVectors<TestShiftLanes>());
++}
++
++template <typename D, int kLane>
++struct TestBroadcastR {
++  HWY_NOINLINE void operator()() const {
++    using T = typename D::T;
++    const D d;
++    const size_t N = Lanes(d);
++    if (kLane >= N) return;
++    auto in_lanes = AllocateAligned<T>(N);
++    std::fill(in_lanes.get(), in_lanes.get() + N, T(0));
++    const size_t blockN = HWY_MIN(N * sizeof(T), 16) / sizeof(T);
++    // Need to set within each 128-bit block
++    for (size_t block = 0; block < N; block += blockN) {
++      in_lanes[block + kLane] = static_cast<T>(block + 1);
++    }
++    const auto in = Load(d, in_lanes.get());
++    auto expected = AllocateAligned<T>(N);
++    for (size_t block = 0; block < N; block += blockN) {
++      for (size_t i = 0; i < blockN; ++i) {
++        expected[block + i] = T(block + 1);
++      }
++    }
++    HWY_ASSERT_VEC_EQ(d, expected.get(), Broadcast<kLane>(in));
++
++    TestBroadcastR<D, kLane - 1>()();
++  }
++};
++
++template <class D>
++struct TestBroadcastR<D, -1> {
++  void operator()() const {}
++};
++
++struct TestBroadcast {
++  template <class T, class D>
++  HWY_NOINLINE void operator()(T /*unused*/, D d) {
++    TestBroadcastR<D, HWY_MIN(MaxLanes(d), 16 / sizeof(T)) - 1>()();
++  }
++};
++
++HWY_NOINLINE void TestAllBroadcast() {
++  const ForPartialVectors<TestBroadcast> test;
++  // No u8.
++  test(uint16_t());
++  test(uint32_t());
++#if HWY_CAP_INTEGER64
++  test(uint64_t());
++#endif
++
++  // No i8.
++  test(int16_t());
++  test(int32_t());
++#if HWY_CAP_INTEGER64
++  test(int64_t());
++#endif
++
++  ForFloatTypes(test);
++}
++
++template <bool kFull>
++struct ChooseTableSize {
++  template <typename T, typename DIdx>
++  using type = DIdx;
++};
++template <>
++struct ChooseTableSize<true> {
++  template <typename T, typename DIdx>
++  using type = HWY_FULL(T);
++};
++
++template <bool kFull>
++struct TestTableLookupBytes {
++  template <class T, class D>
++  HWY_NOINLINE void operator()(T /*unused*/, D d) {
++#if HWY_TARGET != HWY_SCALAR
++    RandomState rng;
++    const typename ChooseTableSize<kFull>::template type<T, D> d_tbl;
++    const Repartition<uint8_t, decltype(d_tbl)> d_tbl8;
++    const size_t NT8 = Lanes(d_tbl8);
++
++    const Repartition<uint8_t, D> d8;
++    const size_t N = Lanes(d);
++    const size_t N8 = Lanes(d8);
++
++    // Random input bytes
++    auto in_bytes = AllocateAligned<uint8_t>(NT8);
++    for (size_t i = 0; i < NT8; ++i) {
++      in_bytes[i] = Random32(&rng) & 0xFF;
++    }
++    const auto in = BitCast(d_tbl, Load(d_tbl8, in_bytes.get()));
++
++    // Enough test data; for larger vectors, upper lanes will be zero.
++    const uint8_t index_bytes_source[64] = {
++        // Same index as source, multiple outputs from same input,
++        // unused input (9), ascending/descending and nonconsecutive neighbors.
++        0,  2,  1, 2, 15, 12, 13, 14, 6,  7,  8,  5,  4,  3,  10, 11,
++        11, 10, 3, 4, 5,  8,  7,  6,  14, 13, 12, 15, 2,  1,  2,  0,
++        4,  3,  2, 2, 5,  6,  7,  7,  15, 15, 15, 15, 15, 15, 0,  1};
++    auto index_bytes = AllocateAligned<uint8_t>(N8);
++    const size_t max_index = HWY_MIN(N8, 16) - 1;
++    for (size_t i = 0; i < N8; ++i) {
++      index_bytes[i] = (i < 64) ? index_bytes_source[i] : 0;
++      // Avoid asan error for partial vectors.
++      index_bytes[i] = static_cast<uint8_t>(HWY_MIN(index_bytes[i], max_index));
++    }
++    const auto indices = Load(d, reinterpret_cast<const T*>(index_bytes.get()));
++
++    auto expected = AllocateAligned<T>(N);
++    uint8_t* expected_bytes = reinterpret_cast<uint8_t*>(expected.get());
++
++    for (size_t block = 0; block < N8; block += 16) {
++      for (size_t i = 0; i < 16 && (block + i) < N8; ++i) {
++        const uint8_t index = index_bytes[block + i];
++        HWY_ASSERT(block + index < N8);  // indices were already capped to N8.
++        // For large vectors, the lane index may wrap around due to block.
++        expected_bytes[block + i] = in_bytes[(block & 0xFF) + index];
++      }
++    }
++    HWY_ASSERT_VEC_EQ(d, expected.get(), TableLookupBytes(in, indices));
++
++    // Individually test zeroing each byte position.
++    for (size_t i = 0; i < N8; ++i) {
++      const uint8_t prev_expected = expected_bytes[i];
++      const uint8_t prev_index = index_bytes[i];
++      expected_bytes[i] = 0;
++
++      const int idx = 0x80 + ((Random32(&rng) & 7) << 4);
++      HWY_ASSERT(0x80 <= idx && idx < 256);
++      index_bytes[i] = static_cast<uint8_t>(idx);
++
++      const auto indices =
++          Load(d, reinterpret_cast<const T*>(index_bytes.get()));
++      HWY_ASSERT_VEC_EQ(d, expected.get(), TableLookupBytesOr0(in, indices));
++      expected_bytes[i] = prev_expected;
++      index_bytes[i] = prev_index;
++    }
++#else
++    (void)d;
++#endif
++  }
++};
++
++HWY_NOINLINE void TestAllTableLookupBytes() {
++  // Partial index, same-sized table.
++  ForIntegerTypes(ForPartialVectors<TestTableLookupBytes<false>>());
++
++// TODO(janwas): requires LMUL trunc/ext, which is not yet implemented.
++#if HWY_TARGET != HWY_RVV
++  // Partial index, full-size table.
++  ForIntegerTypes(ForPartialVectors<TestTableLookupBytes<true>>());
++#endif
++}
++
++struct TestInterleaveLower {
++  template <class T, class D>
++  HWY_NOINLINE void operator()(T /*unused*/, D d) {
++    using TU = MakeUnsigned<T>;
++    const size_t N = Lanes(d);
++    auto even_lanes = AllocateAligned<T>(N);
++    auto odd_lanes = AllocateAligned<T>(N);
++    auto expected = AllocateAligned<T>(N);
++    for (size_t i = 0; i < N; ++i) {
++      even_lanes[i] = static_cast<T>(2 * i + 0);
++      odd_lanes[i] = static_cast<T>(2 * i + 1);
++    }
++    const auto even = Load(d, even_lanes.get());
++    const auto odd = Load(d, odd_lanes.get());
++
++    const size_t blockN = HWY_MIN(16 / sizeof(T), N);
++    for (size_t i = 0; i < Lanes(d); ++i) {
++      const size_t block = i / blockN;
++      const size_t index = (i % blockN) + block * 2 * blockN;
++      expected[i] = static_cast<T>(index & LimitsMax<TU>());
++    }
++    HWY_ASSERT_VEC_EQ(d, expected.get(), InterleaveLower(even, odd));
++    HWY_ASSERT_VEC_EQ(d, expected.get(), InterleaveLower(d, even, odd));
++  }
++};
++
++struct TestInterleaveUpper {
++  template <class T, class D>
++  HWY_NOINLINE void operator()(T /*unused*/, D d) {
++    const size_t N = Lanes(d);
++    if (N == 1) return;
++    auto even_lanes = AllocateAligned<T>(N);
++    auto odd_lanes = AllocateAligned<T>(N);
++    auto expected = AllocateAligned<T>(N);
++    for (size_t i = 0; i < N; ++i) {
++      even_lanes[i] = static_cast<T>(2 * i + 0);
++      odd_lanes[i] = static_cast<T>(2 * i + 1);
++    }
++    const auto even = Load(d, even_lanes.get());
++    const auto odd = Load(d, odd_lanes.get());
++
++    const size_t blockN = HWY_MIN(16 / sizeof(T), N);
++    for (size_t i = 0; i < Lanes(d); ++i) {
++      const size_t block = i / blockN;
++      expected[i] = T((i % blockN) + block * 2 * blockN + blockN);
++    }
++    HWY_ASSERT_VEC_EQ(d, expected.get(), InterleaveUpper(d, even, odd));
++  }
++};
++
++HWY_NOINLINE void TestAllInterleave() {
++  // Not DemoteVectors because this cannot be supported by HWY_SCALAR.
++  ForAllTypes(ForShrinkableVectors<TestInterleaveLower>());
++  ForAllTypes(ForShrinkableVectors<TestInterleaveUpper>());
++}
++
++struct TestZipLower {
++  template <class T, class D>
++  HWY_NOINLINE void operator()(T /*unused*/, D d) {
++    using WideT = MakeWide<T>;
++    static_assert(sizeof(T) * 2 == sizeof(WideT), "Must be double-width");
++    static_assert(IsSigned<T>() == IsSigned<WideT>(), "Must have same sign");
++    const size_t N = Lanes(d);
++    auto even_lanes = AllocateAligned<T>(N);
++    auto odd_lanes = AllocateAligned<T>(N);
++    for (size_t i = 0; i < N; ++i) {
++      even_lanes[i] = static_cast<T>(2 * i + 0);
++      odd_lanes[i] = static_cast<T>(2 * i + 1);
++    }
++    const auto even = Load(d, even_lanes.get());
++    const auto odd = Load(d, odd_lanes.get());
++
++    const Repartition<WideT, D> dw;
++    const size_t NW = Lanes(dw);
++    auto expected = AllocateAligned<WideT>(NW);
++    const WideT blockN = static_cast<WideT>(HWY_MIN(16 / sizeof(WideT), NW));
++
++    for (size_t i = 0; i < NW; ++i) {
++      const size_t block = i / blockN;
++      // Value of least-significant lane in lo-vector.
++      const WideT lo =
++          static_cast<WideT>(2 * (i % blockN) + 4 * block * blockN);
++      const WideT kBits = static_cast<WideT>(sizeof(T) * 8);
++      expected[i] =
++          static_cast<WideT>((static_cast<WideT>(lo + 1) << kBits) + lo);
++    }
++    HWY_ASSERT_VEC_EQ(dw, expected.get(), ZipLower(even, odd));
++    HWY_ASSERT_VEC_EQ(dw, expected.get(), ZipLower(dw, even, odd));
++  }
++};
++
++struct TestZipUpper {
++  template <class T, class D>
++  HWY_NOINLINE void operator()(T /*unused*/, D d) {
++    using WideT = MakeWide<T>;
++    static_assert(sizeof(T) * 2 == sizeof(WideT), "Must be double-width");
++    static_assert(IsSigned<T>() == IsSigned<WideT>(), "Must have same sign");
++    const size_t N = Lanes(d);
++    if (N < 16 / sizeof(T)) return;
++    auto even_lanes = AllocateAligned<T>(N);
++    auto odd_lanes = AllocateAligned<T>(N);
++    for (size_t i = 0; i < Lanes(d); ++i) {
++      even_lanes[i] = static_cast<T>(2 * i + 0);
++      odd_lanes[i] = static_cast<T>(2 * i + 1);
++    }
++    const auto even = Load(d, even_lanes.get());
++    const auto odd = Load(d, odd_lanes.get());
++
++        const Repartition<WideT, D> dw;
++    const size_t NW = Lanes(dw);
++    auto expected = AllocateAligned<WideT>(NW);
++    const WideT blockN = static_cast<WideT>(HWY_MIN(16 / sizeof(WideT), NW));
++
++    for (size_t i = 0; i < NW; ++i) {
++      const size_t block = i / blockN;
++      const WideT lo =
++          static_cast<WideT>(2 * (i % blockN) + 4 * block * blockN);
++      const WideT kBits = static_cast<WideT>(sizeof(T) * 8);
++      expected[i] = static_cast<WideT>(
++          (static_cast<WideT>(lo + 2 * blockN + 1) << kBits) + lo + 2 * blockN);
++    }
++    HWY_ASSERT_VEC_EQ(dw, expected.get(), ZipUpper(dw, even, odd));
++  }
++};
++
++HWY_NOINLINE void TestAllZip() {
++  const ForDemoteVectors<TestZipLower> lower_unsigned;
++  // TODO(janwas): enable after LowerHalf available
++#if HWY_TARGET != HWY_RVV
++  lower_unsigned(uint8_t());
++#endif
++  lower_unsigned(uint16_t());
++#if HWY_CAP_INTEGER64
++  lower_unsigned(uint32_t());  // generates u64
++#endif
++
++  const ForDemoteVectors<TestZipLower> lower_signed;
++#if HWY_TARGET != HWY_RVV
++  lower_signed(int8_t());
++#endif
++  lower_signed(int16_t());
++#if HWY_CAP_INTEGER64
++  lower_signed(int32_t());  // generates i64
++#endif
++
++  const ForShrinkableVectors<TestZipUpper> upper_unsigned;
++#if HWY_TARGET != HWY_RVV
++  upper_unsigned(uint8_t());
++#endif
++  upper_unsigned(uint16_t());
++#if HWY_CAP_INTEGER64
++  upper_unsigned(uint32_t());  // generates u64
++#endif
++
++  const ForShrinkableVectors<TestZipUpper> upper_signed;
++#if HWY_TARGET != HWY_RVV
++  upper_signed(int8_t());
++#endif
++  upper_signed(int16_t());
++#if HWY_CAP_INTEGER64
++  upper_signed(int32_t());  // generates i64
++#endif
++
++  // No float - concatenating f32 does not result in a f64
++}
++
++template <int kBytes>
++struct TestCombineShiftRightBytesR {
++  template <class T, class D>
++  HWY_NOINLINE void operator()(T t, D d) {
++// Scalar does not define CombineShiftRightBytes.
++#if HWY_TARGET != HWY_SCALAR || HWY_IDE
++    const size_t kBlockSize = 16;
++    static_assert(kBytes < kBlockSize, "Shift count is per block");
++    const Repartition<uint8_t, D> d8;
++    const size_t N8 = Lanes(d8);
++    if (N8 < 16) return;
++    auto hi_bytes = AllocateAligned<uint8_t>(N8);
++    auto lo_bytes = AllocateAligned<uint8_t>(N8);
++    auto expected_bytes = AllocateAligned<uint8_t>(N8);
++    uint8_t combined[2 * kBlockSize];
++
++    // Random inputs in each lane
++    RandomState rng;
++    for (size_t rep = 0; rep < 100; ++rep) {
++      for (size_t i = 0; i < N8; ++i) {
++        hi_bytes[i] = static_cast<uint8_t>(Random64(&rng) & 0xFF);
++        lo_bytes[i] = static_cast<uint8_t>(Random64(&rng) & 0xFF);
++      }
++      for (size_t i = 0; i < N8; i += kBlockSize) {
++        CopyBytes<kBlockSize>(&lo_bytes[i], combined);
++        CopyBytes<kBlockSize>(&hi_bytes[i], combined + kBlockSize);
++        CopyBytes<kBlockSize>(combined + kBytes, &expected_bytes[i]);
++      }
++
++      const auto hi = BitCast(d, Load(d8, hi_bytes.get()));
++      const auto lo = BitCast(d, Load(d8, lo_bytes.get()));
++      const auto expected = BitCast(d, Load(d8, expected_bytes.get()));
++      HWY_ASSERT_VEC_EQ(d, expected, CombineShiftRightBytes<kBytes>(d, hi, lo));
++    }
++
++    TestCombineShiftRightBytesR<kBytes - 1>()(t, d);
++#else
++    (void)t;
++    (void)d;
++#endif  // #if HWY_TARGET != HWY_SCALAR
++  }
++};
++
++template <int kLanes>
++struct TestCombineShiftRightLanesR {
++  template <class T, class D>
++  HWY_NOINLINE void operator()(T t, D d) {
++// Scalar does not define CombineShiftRightBytes (needed for *Lanes).
++#if HWY_TARGET != HWY_SCALAR || HWY_IDE
++    const Repartition<uint8_t, D> d8;
++    const size_t N8 = Lanes(d8);
++    if (N8 < 16) return;
++
++    auto hi_bytes = AllocateAligned<uint8_t>(N8);
++    auto lo_bytes = AllocateAligned<uint8_t>(N8);
++    auto expected_bytes = AllocateAligned<uint8_t>(N8);
++    const size_t kBlockSize = 16;
++    uint8_t combined[2 * kBlockSize];
++
++    // Random inputs in each lane
++    RandomState rng;
++    for (size_t rep = 0; rep < 100; ++rep) {
++      for (size_t i = 0; i < N8; ++i) {
++        hi_bytes[i] = static_cast<uint8_t>(Random64(&rng) & 0xFF);
++        lo_bytes[i] = static_cast<uint8_t>(Random64(&rng) & 0xFF);
++      }
++      for (size_t i = 0; i < N8; i += kBlockSize) {
++        CopyBytes<kBlockSize>(&lo_bytes[i], combined);
++        CopyBytes<kBlockSize>(&hi_bytes[i], combined + kBlockSize);
++        CopyBytes<kBlockSize>(combined + kLanes * sizeof(T),
++                              &expected_bytes[i]);
++      }
++
++      const auto hi = BitCast(d, Load(d8, hi_bytes.get()));
++      const auto lo = BitCast(d, Load(d8, lo_bytes.get()));
++      const auto expected = BitCast(d, Load(d8, expected_bytes.get()));
++      HWY_ASSERT_VEC_EQ(d, expected, CombineShiftRightLanes<kLanes>(d, hi, lo));
++    }
++
++    TestCombineShiftRightLanesR<kLanes - 1>()(t, d);
++#else
++    (void)t;
++    (void)d;
++#endif  // #if HWY_TARGET != HWY_SCALAR
++  }
++};
++
++template <>
++struct TestCombineShiftRightBytesR<0> {
++  template <class T, class D>
++  void operator()(T /*unused*/, D /*unused*/) {}
++};
++
++template <>
++struct TestCombineShiftRightLanesR<0> {
++  template <class T, class D>
++  void operator()(T /*unused*/, D /*unused*/) {}
++};
++
++struct TestCombineShiftRight {
++  template <class T, class D>
++  HWY_NOINLINE void operator()(T t, D d) {
++    constexpr size_t kMaxBytes = HWY_MIN(16, MaxLanes(d) * sizeof(T));
++    TestCombineShiftRightBytesR<kMaxBytes - 1>()(t, d);
++    TestCombineShiftRightLanesR<kMaxBytes / sizeof(T) - 1>()(t, d);
++  }
++};
++
++HWY_NOINLINE void TestAllCombineShiftRight() {
++  // Need at least 2 lanes.
++  ForAllTypes(ForShrinkableVectors<TestCombineShiftRight>());
++}
++
++class TestSpecialShuffle32 {
++ public:
++  template <class T, class D>
++  HWY_NOINLINE void operator()(T /*unused*/, D d) {
++    const auto v = Iota(d, 0);
++    VerifyLanes32(d, Shuffle2301(v), 2, 3, 0, 1, __FILE__, __LINE__);
++    VerifyLanes32(d, Shuffle1032(v), 1, 0, 3, 2, __FILE__, __LINE__);
++    VerifyLanes32(d, Shuffle0321(v), 0, 3, 2, 1, __FILE__, __LINE__);
++    VerifyLanes32(d, Shuffle2103(v), 2, 1, 0, 3, __FILE__, __LINE__);
++    VerifyLanes32(d, Shuffle0123(v), 0, 1, 2, 3, __FILE__, __LINE__);
++  }
++
++ private:
++  template <class D, class V>
++  HWY_NOINLINE void VerifyLanes32(D d, V actual, const int i3, const int i2,
++                                  const int i1, const int i0,
++                                  const char* filename, const int line) {
++    using T = TFromD<D>;
++    constexpr size_t kBlockN = 16 / sizeof(T);
++    const size_t N = Lanes(d);
++    if (N < 4) return;
++    auto expected = AllocateAligned<T>(N);
++    for (size_t block = 0; block < N; block += kBlockN) {
++      expected[block + 3] = static_cast<T>(block + i3);
++      expected[block + 2] = static_cast<T>(block + i2);
++      expected[block + 1] = static_cast<T>(block + i1);
++      expected[block + 0] = static_cast<T>(block + i0);
++    }
++    AssertVecEqual(d, expected.get(), actual, filename, line);
++  }
++};
++
++class TestSpecialShuffle64 {
++ public:
++  template <class T, class D>
++  HWY_NOINLINE void operator()(T /*unused*/, D d) {
++    const auto v = Iota(d, 0);
++    VerifyLanes64(d, Shuffle01(v), 0, 1, __FILE__, __LINE__);
++  }
++
++ private:
++  template <class D, class V>
++  HWY_NOINLINE void VerifyLanes64(D d, V actual, const int i1, const int i0,
++                                  const char* filename, const int line) {
++    using T = TFromD<D>;
++    constexpr size_t kBlockN = 16 / sizeof(T);
++    const size_t N = Lanes(d);
++    if (N < 2) return;
++    auto expected = AllocateAligned<T>(N);
++    for (size_t block = 0; block < N; block += kBlockN) {
++      expected[block + 1] = static_cast<T>(block + i1);
++      expected[block + 0] = static_cast<T>(block + i0);
++    }
++    AssertVecEqual(d, expected.get(), actual, filename, line);
++  }
++};
++
++HWY_NOINLINE void TestAllSpecialShuffles() {
++  const ForGE128Vectors<TestSpecialShuffle32> test32;
++  test32(uint32_t());
++  test32(int32_t());
++  test32(float());
++
++#if HWY_CAP_INTEGER64
++  const ForGE128Vectors<TestSpecialShuffle64> test64;
++  test64(uint64_t());
++  test64(int64_t());
++#endif
++
++#if HWY_CAP_FLOAT64
++  const ForGE128Vectors<TestSpecialShuffle64> test_d;
++  test_d(double());
++#endif
++}
++
++// NOLINTNEXTLINE(google-readability-namespace-comments)
++}  // namespace HWY_NAMESPACE
++}  // namespace hwy
++HWY_AFTER_NAMESPACE();
++
++#if HWY_ONCE
++
++namespace hwy {
++HWY_BEFORE_TEST(HwyBlockwiseTest);
++HWY_EXPORT_AND_TEST_P(HwyBlockwiseTest, TestAllShiftBytes);
++HWY_EXPORT_AND_TEST_P(HwyBlockwiseTest, TestAllShiftLanes);
++HWY_EXPORT_AND_TEST_P(HwyBlockwiseTest, TestAllBroadcast);
++HWY_EXPORT_AND_TEST_P(HwyBlockwiseTest, TestAllTableLookupBytes);
++HWY_EXPORT_AND_TEST_P(HwyBlockwiseTest, TestAllInterleave);
++HWY_EXPORT_AND_TEST_P(HwyBlockwiseTest, TestAllZip);
++HWY_EXPORT_AND_TEST_P(HwyBlockwiseTest, TestAllCombineShiftRight);
++HWY_EXPORT_AND_TEST_P(HwyBlockwiseTest, TestAllSpecialShuffles);
++}  // namespace hwy
++
++// Ought not to be necessary, but without this, no tests run on RVV.
++int main(int argc, char** argv) {
++  ::testing::InitGoogleTest(&argc, argv);
++  return RUN_ALL_TESTS();
++}
++
++#endif
+diff --git a/third_party/highway/hwy/tests/combine_test.cc b/third_party/highway/hwy/tests/combine_test.cc
+index 4f7942f67cc58..bbeb867ce8aed 100644
+--- a/third_party/highway/hwy/tests/combine_test.cc
++++ b/third_party/highway/hwy/tests/combine_test.cc
+@@ -36,16 +36,21 @@ struct TestLowerHalf {
+ 
+     const size_t N = Lanes(d);
+     auto lanes = AllocateAligned<T>(N);
++    auto lanes2 = AllocateAligned<T>(N);
+     std::fill(lanes.get(), lanes.get() + N, T(0));
++    std::fill(lanes2.get(), lanes2.get() + N, T(0));
+     const auto v = Iota(d, 1);
+-    Store(LowerHalf(v), d2, lanes.get());
++    Store(LowerHalf(d2, v), d2, lanes.get());
++    Store(LowerHalf(v), d2, lanes2.get());  // optionally without D
+     size_t i = 0;
+     for (; i < Lanes(d2); ++i) {
+       HWY_ASSERT_EQ(T(1 + i), lanes[i]);
++      HWY_ASSERT_EQ(T(1 + i), lanes2[i]);
+     }
+     // Other half remains unchanged
+     for (; i < N; ++i) {
+       HWY_ASSERT_EQ(T(0), lanes[i]);
++      HWY_ASSERT_EQ(T(0), lanes2[i]);
+     }
+   }
+ };
+@@ -53,29 +58,35 @@ struct TestLowerHalf {
+ struct TestLowerQuarter {
+   template <class T, class D>
+   HWY_NOINLINE void operator()(T /*unused*/, D d) {
+-    const Half<Half<D>> d4;
++    const Half<D> d2;
++    const Half<decltype(d2)> d4;
+ 
+     const size_t N = Lanes(d);
+     auto lanes = AllocateAligned<T>(N);
++    auto lanes2 = AllocateAligned<T>(N);
+     std::fill(lanes.get(), lanes.get() + N, T(0));
++    std::fill(lanes2.get(), lanes2.get() + N, T(0));
+     const auto v = Iota(d, 1);
+-    const auto lo = LowerHalf(LowerHalf(v));
++    const auto lo = LowerHalf(d4, LowerHalf(d2, v));
++    const auto lo2 = LowerHalf(LowerHalf(v));  // optionally without D
+     Store(lo, d4, lanes.get());
++    Store(lo2, d4, lanes2.get());
+     size_t i = 0;
+     for (; i < Lanes(d4); ++i) {
+       HWY_ASSERT_EQ(T(i + 1), lanes[i]);
++      HWY_ASSERT_EQ(T(i + 1), lanes2[i]);
+     }
+     // Upper 3/4 remain unchanged
+     for (; i < N; ++i) {
+       HWY_ASSERT_EQ(T(0), lanes[i]);
++      HWY_ASSERT_EQ(T(0), lanes2[i]);
+     }
+   }
+ };
+ 
+ HWY_NOINLINE void TestAllLowerHalf() {
+-  constexpr size_t kDiv = 1;
+-  ForAllTypes(ForPartialVectors<TestLowerHalf, kDiv, /*kMinLanes=*/2>());
+-  ForAllTypes(ForPartialVectors<TestLowerQuarter, kDiv, /*kMinLanes=*/4>());
++  ForAllTypes(ForDemoteVectors<TestLowerHalf>());
++  ForAllTypes(ForDemoteVectors<TestLowerQuarter, 4>());
+ }
+ 
+ struct TestUpperHalf {
+@@ -90,7 +101,7 @@ struct TestUpperHalf {
+     auto lanes = AllocateAligned<T>(N);
+     std::fill(lanes.get(), lanes.get() + N, T(0));
+ 
+-    Store(UpperHalf(v), d2, lanes.get());
++    Store(UpperHalf(d2, v), d2, lanes.get());
+     size_t i = 0;
+     for (; i < Lanes(d2); ++i) {
+       HWY_ASSERT_EQ(T(Lanes(d2) + 1 + i), lanes[i]);
+@@ -106,22 +117,22 @@ struct TestUpperHalf {
+ };
+ 
+ HWY_NOINLINE void TestAllUpperHalf() {
+-  ForAllTypes(ForGE128Vectors<TestUpperHalf>());
++  ForAllTypes(ForShrinkableVectors<TestUpperHalf>());
+ }
+ 
+ struct TestZeroExtendVector {
+   template <class T, class D>
+   HWY_NOINLINE void operator()(T /*unused*/, D d) {
+-#if HWY_CAP_GE256
+     const Twice<D> d2;
+ 
+     const auto v = Iota(d, 1);
+     const size_t N2 = Lanes(d2);
++    HWY_ASSERT(N2 != 0);
+     auto lanes = AllocateAligned<T>(N2);
+     Store(v, d, &lanes[0]);
+     Store(v, d, &lanes[N2 / 2]);
+ 
+-    const auto ext = ZeroExtendVector(v);
++    const auto ext = ZeroExtendVector(d2, v);
+     Store(ext, d2, lanes.get());
+ 
+     size_t i = 0;
+@@ -133,9 +144,6 @@ struct TestZeroExtendVector {
+     for (; i < N2; ++i) {
+       HWY_ASSERT_EQ(T(0), lanes[i]);
+     }
+-#else
+-    (void)d;
+-#endif
+   }
+ };
+ 
+@@ -146,21 +154,17 @@ HWY_NOINLINE void TestAllZeroExtendVector() {
+ struct TestCombine {
+   template <class T, class D>
+   HWY_NOINLINE void operator()(T /*unused*/, D d) {
+-#if HWY_CAP_GE256
+     const Twice<D> d2;
+     const size_t N2 = Lanes(d2);
+     auto lanes = AllocateAligned<T>(N2);
+ 
+     const auto lo = Iota(d, 1);
+     const auto hi = Iota(d, N2 / 2 + 1);
+-    const auto combined = Combine(hi, lo);
++    const auto combined = Combine(d2, hi, lo);
+     Store(combined, d2, lanes.get());
+ 
+     const auto expected = Iota(d2, 1);
+     HWY_ASSERT_VEC_EQ(d2, expected, combined);
+-#else
+-    (void)d;
+-#endif
+   }
+ };
+ 
+@@ -168,102 +172,60 @@ HWY_NOINLINE void TestAllCombine() {
+   ForAllTypes(ForExtendableVectors<TestCombine>());
+ }
+ 
+-
+-template <int kBytes>
+-struct TestCombineShiftRightBytesR {
+-  template <class T, class D>
+-  HWY_NOINLINE void operator()(T t, D d) {
+-// Scalar does not define CombineShiftRightBytes.
+-#if HWY_TARGET != HWY_SCALAR || HWY_IDE
+-    const Repartition<uint8_t, D> d8;
+-    const size_t N8 = Lanes(d8);
+-    const auto lo = BitCast(d, Iota(d8, 1));
+-    const auto hi = BitCast(d, Iota(d8, 1 + N8));
+-
+-    auto expected = AllocateAligned<T>(Lanes(d));
+-    uint8_t* expected_bytes = reinterpret_cast<uint8_t*>(expected.get());
+-
+-    const size_t kBlockSize = 16;
+-    for (size_t i = 0; i < N8; ++i) {
+-      const size_t block = i / kBlockSize;
+-      const size_t lane = i % kBlockSize;
+-      const size_t first_lo = block * kBlockSize;
+-      const size_t idx = lane + kBytes;
+-      const size_t offset = (idx < kBlockSize) ? 0 : N8 - kBlockSize;
+-      const bool at_end = idx >= 2 * kBlockSize;
+-      expected_bytes[i] =
+-          at_end ? 0 : static_cast<uint8_t>(first_lo + idx + 1 + offset);
+-    }
+-    HWY_ASSERT_VEC_EQ(d, expected.get(),
+-                      CombineShiftRightBytes<kBytes>(hi, lo));
+-
+-    TestCombineShiftRightBytesR<kBytes - 1>()(t, d);
+-#else
+-    (void)t;
+-    (void)d;
+-#endif  // #if HWY_TARGET != HWY_SCALAR
+-  }
+-};
+-
+-template <int kLanes>
+-struct TestCombineShiftRightLanesR {
++struct TestConcat {
+   template <class T, class D>
+-  HWY_NOINLINE void operator()(T t, D d) {
+-// Scalar does not define CombineShiftRightBytes (needed for *Lanes).
+-#if HWY_TARGET != HWY_SCALAR || HWY_IDE
+-    const Repartition<uint8_t, D> d8;
+-    const size_t N8 = Lanes(d8);
+-    const auto lo = BitCast(d, Iota(d8, 1));
+-    const auto hi = BitCast(d, Iota(d8, 1 + N8));
+-
+-    auto expected = AllocateAligned<T>(Lanes(d));
+-
+-    uint8_t* expected_bytes = reinterpret_cast<uint8_t*>(expected.get());
+-
+-    const size_t kBlockSize = 16;
+-    for (size_t i = 0; i < N8; ++i) {
+-      const size_t block = i / kBlockSize;
+-      const size_t lane = i % kBlockSize;
+-      const size_t first_lo = block * kBlockSize;
+-      const size_t idx = lane + kLanes * sizeof(T);
+-      const size_t offset = (idx < kBlockSize) ? 0 : N8 - kBlockSize;
+-      const bool at_end = idx >= 2 * kBlockSize;
+-      expected_bytes[i] =
+-          at_end ? 0 : static_cast<uint8_t>(first_lo + idx + 1 + offset);
++  HWY_NOINLINE void operator()(T /*unused*/, D d) {
++    const size_t N = Lanes(d);
++    if (N == 1) return;
++    const size_t half_bytes = N * sizeof(T) / 2;
++
++    auto hi = AllocateAligned<T>(N);
++    auto lo = AllocateAligned<T>(N);
++    auto expected = AllocateAligned<T>(N);
++    RandomState rng;
++    for (size_t rep = 0; rep < 10; ++rep) {
++      for (size_t i = 0; i < N; ++i) {
++        hi[i] = static_cast<T>(Random64(&rng) & 0xFF);
++        lo[i] = static_cast<T>(Random64(&rng) & 0xFF);
++      }
++
++      {
++        memcpy(&expected[N / 2], &hi[N / 2], half_bytes);
++        memcpy(&expected[0], &lo[0], half_bytes);
++        const auto vhi = Load(d, hi.get());
++        const auto vlo = Load(d, lo.get());
++        HWY_ASSERT_VEC_EQ(d, expected.get(), ConcatUpperLower(d, vhi, vlo));
++      }
++
++      {
++        memcpy(&expected[N / 2], &hi[N / 2], half_bytes);
++        memcpy(&expected[0], &lo[N / 2], half_bytes);
++        const auto vhi = Load(d, hi.get());
++        const auto vlo = Load(d, lo.get());
++        HWY_ASSERT_VEC_EQ(d, expected.get(), ConcatUpperUpper(d, vhi, vlo));
++      }
++
++      {
++        memcpy(&expected[N / 2], &hi[0], half_bytes);
++        memcpy(&expected[0], &lo[N / 2], half_bytes);
++        const auto vhi = Load(d, hi.get());
++        const auto vlo = Load(d, lo.get());
++        HWY_ASSERT_VEC_EQ(d, expected.get(), ConcatLowerUpper(d, vhi, vlo));
++      }
++
++      {
++        memcpy(&expected[N / 2], &hi[0], half_bytes);
++        memcpy(&expected[0], &lo[0], half_bytes);
++        const auto vhi = Load(d, hi.get());
++        const auto vlo = Load(d, lo.get());
++        HWY_ASSERT_VEC_EQ(d, expected.get(), ConcatLowerLower(d, vhi, vlo));
++      }
+     }
+-    HWY_ASSERT_VEC_EQ(d, expected.get(),
+-                      CombineShiftRightLanes<kLanes>(hi, lo));
+-
+-    TestCombineShiftRightBytesR<kLanes - 1>()(t, d);
+-#else
+-    (void)t;
+-    (void)d;
+-#endif  // #if HWY_TARGET != HWY_SCALAR
+-  }
+-};
+-
+-template <>
+-struct TestCombineShiftRightBytesR<0> {
+-  template <class T, class D>
+-  void operator()(T /*unused*/, D /*unused*/) {}
+-};
+-
+-template <>
+-struct TestCombineShiftRightLanesR<0> {
+-  template <class T, class D>
+-  void operator()(T /*unused*/, D /*unused*/) {}
+-};
+-
+-struct TestCombineShiftRight {
+-  template <class T, class D>
+-  HWY_NOINLINE void operator()(T t, D d) {
+-    TestCombineShiftRightBytesR<15>()(t, d);
+-    TestCombineShiftRightLanesR<16 / sizeof(T) - 1>()(t, d);
+   }
+ };
+ 
+-HWY_NOINLINE void TestAllCombineShiftRight() {
+-  ForAllTypes(ForGE128Vectors<TestCombineShiftRight>());
++HWY_NOINLINE void TestAllConcat() {
++  ForAllTypes(ForShrinkableVectors<TestConcat>());
+ }
+ 
+ // NOLINTNEXTLINE(google-readability-namespace-comments)
+@@ -272,15 +234,23 @@ HWY_NOINLINE void TestAllCombineShiftRight() {
+ HWY_AFTER_NAMESPACE();
+ 
+ #if HWY_ONCE
++
+ namespace hwy {
+ HWY_BEFORE_TEST(HwyCombineTest);
+ HWY_EXPORT_AND_TEST_P(HwyCombineTest, TestAllLowerHalf);
+ HWY_EXPORT_AND_TEST_P(HwyCombineTest, TestAllUpperHalf);
+ HWY_EXPORT_AND_TEST_P(HwyCombineTest, TestAllZeroExtendVector);
+ HWY_EXPORT_AND_TEST_P(HwyCombineTest, TestAllCombine);
+-HWY_EXPORT_AND_TEST_P(HwyCombineTest, TestAllCombineShiftRight);
++HWY_EXPORT_AND_TEST_P(HwyCombineTest, TestAllConcat);
+ }  // namespace hwy
+-#endif
++
++// Ought not to be necessary, but without this, no tests run on RVV.
++int main(int argc, char **argv) {
++  ::testing::InitGoogleTest(&argc, argv);
++  return RUN_ALL_TESTS();
++}
++
++#endif  // HWY_ONCE
+ 
+ #else
+ int main(int, char**) { return 0; }
+diff --git a/third_party/highway/hwy/tests/compare_test.cc b/third_party/highway/hwy/tests/compare_test.cc
+index 9e7803b87aca4..cbfcbc38e4924 100644
+--- a/third_party/highway/hwy/tests/compare_test.cc
++++ b/third_party/highway/hwy/tests/compare_test.cc
+@@ -26,25 +26,6 @@ HWY_BEFORE_NAMESPACE();
+ namespace hwy {
+ namespace HWY_NAMESPACE {
+ 
+-// All types.
+-struct TestMask {
+-  template <typename T, class D>
+-  HWY_NOINLINE void operator()(T /*unused*/, D d) {
+-    const size_t N = Lanes(d);
+-    auto lanes = AllocateAligned<T>(N);
+-
+-    std::fill(lanes.get(), lanes.get() + N, T(0));
+-    const auto actual_false = MaskFromVec(Load(d, lanes.get()));
+-    HWY_ASSERT_MASK_EQ(d, MaskFalse(d), actual_false);
+-
+-    memset(lanes.get(), 0xFF, N * sizeof(T));
+-    const auto actual_true = MaskFromVec(Load(d, lanes.get()));
+-    HWY_ASSERT_MASK_EQ(d, MaskTrue(d), actual_true);
+-  }
+-};
+-
+-HWY_NOINLINE void TestAllMask() { ForAllTypes(ForPartialVectors<TestMask>()); }
+-
+ // All types.
+ struct TestEquality {
+   template <typename T, class D>
+@@ -57,8 +38,14 @@ struct TestEquality {
+     const auto mask_true = MaskTrue(d);
+ 
+     HWY_ASSERT_MASK_EQ(d, mask_false, Eq(v2, v3));
++    HWY_ASSERT_MASK_EQ(d, mask_false, Eq(v3, v2));
+     HWY_ASSERT_MASK_EQ(d, mask_true, Eq(v2, v2));
+     HWY_ASSERT_MASK_EQ(d, mask_true, Eq(v2, v2b));
++
++    HWY_ASSERT_MASK_EQ(d, mask_true, Ne(v2, v3));
++    HWY_ASSERT_MASK_EQ(d, mask_true, Ne(v3, v2));
++    HWY_ASSERT_MASK_EQ(d, mask_false, Ne(v2, v2));
++    HWY_ASSERT_MASK_EQ(d, mask_false, Ne(v2, v2b));
+   }
+ };
+ 
+@@ -97,7 +84,7 @@ struct TestStrictInt {
+     const T max = LimitsMax<T>();
+     const auto v0 = Zero(d);
+     const auto v2 = And(Iota(d, T(2)), Set(d, 127));  // 0..127
+-    const auto vn = Neg(v2) - Set(d, 1);              // -1..-128
++    const auto vn = Sub(Neg(v2), Set(d, 1));          // -1..-128
+ 
+     const auto mask_false = MaskFalse(d);
+     const auto mask_true = MaskTrue(d);
+@@ -131,14 +118,14 @@ struct TestStrictInt {
+ };
+ 
+ HWY_NOINLINE void TestAllStrictInt() {
+-  ForSignedTypes(ForExtendableVectors<TestStrictInt>());
++  ForSignedTypes(ForPartialVectors<TestStrictInt>());
+ }
+ 
+ struct TestStrictFloat {
+   template <typename T, class D>
+   HWY_NOINLINE void operator()(T /*unused*/, D d) {
+-    const T huge_neg = -1E35;
+-    const T huge_pos = 1E36;
++    const T huge_neg = T(-1E35);
++    const T huge_pos = T(1E36);
+     const auto v0 = Zero(d);
+     const auto v2 = Iota(d, T(2));
+     const auto vn = Neg(v2);
+@@ -173,13 +160,13 @@ struct TestStrictFloat {
+ };
+ 
+ HWY_NOINLINE void TestAllStrictFloat() {
+-  ForFloatTypes(ForExtendableVectors<TestStrictFloat>());
++  ForFloatTypes(ForPartialVectors<TestStrictFloat>());
+ }
+ 
+ struct TestWeakFloat {
+   template <typename T, class D>
+   HWY_NOINLINE void operator()(T /*unused*/, D d) {
+-    const auto v2 = Iota(d, 2);
++    const auto v2 = Iota(d, T(2));
+     const auto vn = Iota(d, -T(Lanes(d)));
+ 
+     const auto mask_false = MaskFalse(d);
+@@ -206,12 +193,19 @@ HWY_NOINLINE void TestAllWeakFloat() {
+ HWY_AFTER_NAMESPACE();
+ 
+ #if HWY_ONCE
++
+ namespace hwy {
+ HWY_BEFORE_TEST(HwyCompareTest);
+-HWY_EXPORT_AND_TEST_P(HwyCompareTest, TestAllMask);
+ HWY_EXPORT_AND_TEST_P(HwyCompareTest, TestAllEquality);
+ HWY_EXPORT_AND_TEST_P(HwyCompareTest, TestAllStrictInt);
+ HWY_EXPORT_AND_TEST_P(HwyCompareTest, TestAllStrictFloat);
+ HWY_EXPORT_AND_TEST_P(HwyCompareTest, TestAllWeakFloat);
+ }  // namespace hwy
++
++// Ought not to be necessary, but without this, no tests run on RVV.
++int main(int argc, char** argv) {
++  ::testing::InitGoogleTest(&argc, argv);
++  return RUN_ALL_TESTS();
++}
++
+ #endif
+diff --git a/third_party/highway/hwy/tests/convert_test.cc b/third_party/highway/hwy/tests/convert_test.cc
+index 870955fcafbe0..6ec8680e09821 100644
+--- a/third_party/highway/hwy/tests/convert_test.cc
++++ b/third_party/highway/hwy/tests/convert_test.cc
+@@ -34,7 +34,10 @@ struct TestBitCast {
+   template <typename T, class D>
+   HWY_NOINLINE void operator()(T /*unused*/, D d) {
+     const Repartition<ToT, D> dto;
+-    HWY_ASSERT_EQ(Lanes(d) * sizeof(T), Lanes(dto) * sizeof(ToT));
++    const size_t N = Lanes(d);
++    const size_t Nto = Lanes(dto);
++    if (N == 0 || Nto == 0) return;
++    HWY_ASSERT_EQ(N * sizeof(T), Nto * sizeof(ToT));
+     const auto vf = Iota(d, 1);
+     const auto vt = BitCast(dto, vf);
+     // Must return the same bits
+@@ -130,8 +133,10 @@ HWY_NOINLINE void TestAllBitCast() {
+ #endif  // HWY_CAP_INTEGER64
+ #endif  // HWY_CAP_FLOAT64
+ 
++#if HWY_TARGET != HWY_SCALAR
+   // For non-scalar vectors, we can cast all types to all.
+-  ForAllTypes(ForGE128Vectors<TestBitCastFrom>());
++  ForAllTypes(ForGE64Vectors<TestBitCastFrom>());
++#endif
+ }
+ 
+ template <typename ToT>
+@@ -160,39 +165,39 @@ struct TestPromoteTo {
+ };
+ 
+ HWY_NOINLINE void TestAllPromoteTo() {
+-  const ForPartialVectors<TestPromoteTo<uint16_t>, 2> to_u16div2;
++  const ForPromoteVectors<TestPromoteTo<uint16_t>, 2> to_u16div2;
+   to_u16div2(uint8_t());
+ 
+-  const ForPartialVectors<TestPromoteTo<uint32_t>, 4> to_u32div4;
++  const ForPromoteVectors<TestPromoteTo<uint32_t>, 4> to_u32div4;
+   to_u32div4(uint8_t());
+ 
+-  const ForPartialVectors<TestPromoteTo<uint32_t>, 2> to_u32div2;
++  const ForPromoteVectors<TestPromoteTo<uint32_t>, 2> to_u32div2;
+   to_u32div2(uint16_t());
+ 
+-  const ForPartialVectors<TestPromoteTo<int16_t>, 2> to_i16div2;
++  const ForPromoteVectors<TestPromoteTo<int16_t>, 2> to_i16div2;
+   to_i16div2(uint8_t());
+   to_i16div2(int8_t());
+ 
+-  const ForPartialVectors<TestPromoteTo<int32_t>, 2> to_i32div2;
++  const ForPromoteVectors<TestPromoteTo<int32_t>, 2> to_i32div2;
+   to_i32div2(uint16_t());
+   to_i32div2(int16_t());
+ 
+-  const ForPartialVectors<TestPromoteTo<int32_t>, 4> to_i32div4;
++  const ForPromoteVectors<TestPromoteTo<int32_t>, 4> to_i32div4;
+   to_i32div4(uint8_t());
+   to_i32div4(int8_t());
+ 
+   // Must test f16 separately because we can only load/store/convert them.
+ 
+ #if HWY_CAP_INTEGER64
+-  const ForPartialVectors<TestPromoteTo<uint64_t>, 2> to_u64div2;
++  const ForPromoteVectors<TestPromoteTo<uint64_t>, 2> to_u64div2;
+   to_u64div2(uint32_t());
+ 
+-  const ForPartialVectors<TestPromoteTo<int64_t>, 2> to_i64div2;
++  const ForPromoteVectors<TestPromoteTo<int64_t>, 2> to_i64div2;
+   to_i64div2(int32_t());
+ #endif
+ 
+ #if HWY_CAP_FLOAT64
+-  const ForPartialVectors<TestPromoteTo<double>, 2> to_f64div2;
++  const ForPromoteVectors<TestPromoteTo<double>, 2> to_f64div2;
+   to_f64div2(int32_t());
+   to_f64div2(float());
+ #endif
+@@ -224,14 +229,23 @@ struct TestDemoteTo {
+     const T min = LimitsMin<ToT>();
+     const T max = LimitsMax<ToT>();
+ 
++    const auto value_ok = [&](T& value) {
++      if (!IsFinite(value)) return false;
++#if HWY_EMULATE_SVE
++      // farm_sve just casts, which is undefined if the value is out of range.
++      value = HWY_MIN(HWY_MAX(min, value), max);
++#endif
++      return true;
++    };
++
+     RandomState rng;
+     for (size_t rep = 0; rep < 1000; ++rep) {
+       for (size_t i = 0; i < N; ++i) {
+         do {
+           const uint64_t bits = rng();
+           memcpy(&from[i], &bits, sizeof(T));
+-        } while (!IsFinite(from[i]));
+-        expected[i] = static_cast<ToT>(std::min(std::max(min, from[i]), max));
++        } while (!value_ok(from[i]));
++        expected[i] = static_cast<ToT>(HWY_MIN(HWY_MAX(min, from[i]), max));
+       }
+ 
+       HWY_ASSERT_VEC_EQ(to_d, expected.get(),
+@@ -241,22 +255,22 @@ struct TestDemoteTo {
+ };
+ 
+ HWY_NOINLINE void TestAllDemoteToInt() {
+-  ForDemoteVectors<TestDemoteTo<uint8_t>, 2>()(int16_t());
++  ForDemoteVectors<TestDemoteTo<uint8_t>>()(int16_t());
+   ForDemoteVectors<TestDemoteTo<uint8_t>, 4>()(int32_t());
+ 
+-  ForDemoteVectors<TestDemoteTo<int8_t>, 2>()(int16_t());
++  ForDemoteVectors<TestDemoteTo<int8_t>>()(int16_t());
+   ForDemoteVectors<TestDemoteTo<int8_t>, 4>()(int32_t());
+ 
+-  const ForDemoteVectors<TestDemoteTo<uint16_t>, 2> to_u16;
++  const ForDemoteVectors<TestDemoteTo<uint16_t>> to_u16;
+   to_u16(int32_t());
+ 
+-  const ForDemoteVectors<TestDemoteTo<int16_t>, 2> to_i16;
++  const ForDemoteVectors<TestDemoteTo<int16_t>> to_i16;
+   to_i16(int32_t());
+ }
+ 
+ HWY_NOINLINE void TestAllDemoteToMixed() {
+ #if HWY_CAP_FLOAT64
+-  const ForDemoteVectors<TestDemoteTo<int32_t>, 2> to_i32;
++  const ForDemoteVectors<TestDemoteTo<int32_t>> to_i32;
+   to_i32(double());
+ #endif
+ }
+@@ -285,7 +299,7 @@ struct TestDemoteToFloat {
+         const T max_abs = HighestValue<ToT>();
+         // NOTE: std:: version from C++11 cmath is not defined in RVV GCC, see
+         // https://lists.freebsd.org/pipermail/freebsd-current/2014-January/048130.html
+-        const T clipped = copysign(std::min(magn, max_abs), from[i]);
++        const T clipped = copysign(HWY_MIN(magn, max_abs), from[i]);
+         expected[i] = static_cast<ToT>(clipped);
+       }
+ 
+@@ -338,6 +352,7 @@ AlignedFreeUniquePtr<float[]> F16TestCases(D d, size_t& padded) {
+ struct TestF16 {
+   template <typename TF32, class DF32>
+   HWY_NOINLINE void operator()(TF32 /*t*/, DF32 d32) {
++#if HWY_CAP_FLOAT16
+     size_t padded;
+     auto in = F16TestCases(d32, padded);
+     using TF16 = float16_t;
+@@ -350,10 +365,13 @@ struct TestF16 {
+       Store(DemoteTo(d16, loaded), d16, temp16.get());
+       HWY_ASSERT_VEC_EQ(d32, loaded, PromoteTo(d32, Load(d16, temp16.get())));
+     }
++#else
++    (void)d32;
++#endif
+   }
+ };
+ 
+-HWY_NOINLINE void TestAllF16() { ForDemoteVectors<TestF16, 2>()(float()); }
++HWY_NOINLINE void TestAllF16() { ForDemoteVectors<TestF16>()(float()); }
+ 
+ struct TestConvertU8 {
+   template <typename T, class D>
+@@ -376,8 +394,9 @@ struct TestIntFromFloatHuge {
+   template <typename TF, class DF>
+   HWY_NOINLINE void operator()(TF /*unused*/, const DF df) {
+     // Still does not work, although ARMv7 manual says that float->int
+-    // saturates, i.e. chooses the nearest representable value.
+-#if HWY_TARGET != HWY_NEON
++    // saturates, i.e. chooses the nearest representable value. Also causes
++    // out-of-memory for MSVC, and unsafe cast in farm_sve.
++#if HWY_TARGET != HWY_NEON && !HWY_COMPILER_MSVC && !defined(HWY_EMULATE_SVE)
+     using TI = MakeSigned<TF>;
+     const Rebind<TI, DF> di;
+ 
+@@ -395,33 +414,33 @@ struct TestIntFromFloatHuge {
+   }
+ };
+ 
+-struct TestIntFromFloat {
++class TestIntFromFloat {
+   template <typename TF, class DF>
+-  HWY_NOINLINE void operator()(TF /*unused*/, const DF df) {
++  static HWY_NOINLINE void TestPowers(TF /*unused*/, const DF df) {
+     using TI = MakeSigned<TF>;
+     const Rebind<TI, DF> di;
+-    const size_t N = Lanes(df);
+-
+-    // Integer positive
+-    HWY_ASSERT_VEC_EQ(di, Iota(di, TI(4)), ConvertTo(di, Iota(df, TF(4.0))));
+-
+-    // Integer negative
+-    HWY_ASSERT_VEC_EQ(di, Iota(di, -TI(N)), ConvertTo(di, Iota(df, -TF(N))));
+-
+-    // Above positive
+-    HWY_ASSERT_VEC_EQ(di, Iota(di, TI(2)), ConvertTo(di, Iota(df, TF(2.001))));
+-
+-    // Below positive
+-    HWY_ASSERT_VEC_EQ(di, Iota(di, TI(3)), ConvertTo(di, Iota(df, TF(3.9999))));
+-
+-    const TF eps = static_cast<TF>(0.0001);
+-    // Above negative
+-    HWY_ASSERT_VEC_EQ(di, Iota(di, -TI(N)),
+-                      ConvertTo(di, Iota(df, -TF(N + 1) + eps)));
++    constexpr size_t kBits = sizeof(TF) * 8;
++
++    // Powers of two, plus offsets to set some mantissa bits.
++    const uint64_t ofs_table[3] = {0ULL, 3ULL << (kBits / 2),
++                                   1ULL << (kBits - 15)};
++    for (int sign = 0; sign < 2; ++sign) {
++      for (size_t shift = 0; shift < kBits - 1; ++shift) {
++        for (uint64_t ofs : ofs_table) {
++          const int64_t mag = (int64_t(1) << shift) + ofs;
++          const int64_t val = sign ? mag : -mag;
++          HWY_ASSERT_VEC_EQ(di, Set(di, static_cast<TI>(val)),
++                            ConvertTo(di, Set(df, static_cast<TF>(val))));
++        }
++      }
++    }
++  }
+ 
+-    // Below negative
+-    HWY_ASSERT_VEC_EQ(di, Iota(di, -TI(N + 1)),
+-                      ConvertTo(di, Iota(df, -TF(N + 1) - eps)));
++  template <typename TF, class DF>
++  static HWY_NOINLINE void TestRandom(TF /*unused*/, const DF df) {
++    using TI = MakeSigned<TF>;
++    const Rebind<TI, DF> di;
++    const size_t N = Lanes(df);
+ 
+     // TF does not have enough precision to represent TI.
+     const double min = static_cast<double>(LimitsMin<TI>());
+@@ -437,6 +456,10 @@ struct TestIntFromFloat {
+           const uint64_t bits = rng();
+           memcpy(&from[i], &bits, sizeof(TF));
+         } while (!std::isfinite(from[i]));
++#if defined(HWY_EMULATE_SVE)
++        // farm_sve just casts, which is undefined if the value is out of range.
++        from[i] = HWY_MIN(HWY_MAX(min / 2, from[i]), max / 2);
++#endif
+         if (from[i] >= max) {
+           expected[i] = LimitsMax<TI>();
+         } else if (from[i] <= min) {
+@@ -450,6 +473,38 @@ struct TestIntFromFloat {
+                         ConvertTo(di, Load(df, from.get())));
+     }
+   }
++
++ public:
++  template <typename TF, class DF>
++  HWY_NOINLINE void operator()(TF tf, const DF df) {
++    using TI = MakeSigned<TF>;
++    const Rebind<TI, DF> di;
++    const size_t N = Lanes(df);
++
++    // Integer positive
++    HWY_ASSERT_VEC_EQ(di, Iota(di, TI(4)), ConvertTo(di, Iota(df, TF(4.0))));
++
++    // Integer negative
++    HWY_ASSERT_VEC_EQ(di, Iota(di, -TI(N)), ConvertTo(di, Iota(df, -TF(N))));
++
++    // Above positive
++    HWY_ASSERT_VEC_EQ(di, Iota(di, TI(2)), ConvertTo(di, Iota(df, TF(2.001))));
++
++    // Below positive
++    HWY_ASSERT_VEC_EQ(di, Iota(di, TI(3)), ConvertTo(di, Iota(df, TF(3.9999))));
++
++    const TF eps = static_cast<TF>(0.0001);
++    // Above negative
++    HWY_ASSERT_VEC_EQ(di, Iota(di, -TI(N)),
++                      ConvertTo(di, Iota(df, -TF(N + 1) + eps)));
++
++    // Below negative
++    HWY_ASSERT_VEC_EQ(di, Iota(di, -TI(N + 1)),
++                      ConvertTo(di, Iota(df, -TF(N + 1) - eps)));
++
++    TestPowers(tf, df);
++    TestRandom(tf, df);
++  }
+ };
+ 
+ HWY_NOINLINE void TestAllIntFromFloat() {
+@@ -458,10 +513,10 @@ HWY_NOINLINE void TestAllIntFromFloat() {
+ }
+ 
+ struct TestFloatFromInt {
+-  template <typename TI, class DI>
+-  HWY_NOINLINE void operator()(TI /*unused*/, const DI di) {
+-    using TF = MakeFloat<TI>;
+-    const Rebind<TF, DI> df;
++  template <typename TF, class DF>
++  HWY_NOINLINE void operator()(TF /*unused*/, const DF df) {
++    using TI = MakeSigned<TF>;
++    const RebindToSigned<DF> di;
+     const size_t N = Lanes(df);
+ 
+     // Integer positive
+@@ -481,10 +536,7 @@ struct TestFloatFromInt {
+ };
+ 
+ HWY_NOINLINE void TestAllFloatFromInt() {
+-  ForPartialVectors<TestFloatFromInt>()(int32_t());
+-#if HWY_CAP_FLOAT64 && HWY_CAP_INTEGER64
+-  ForPartialVectors<TestFloatFromInt>()(int64_t());
+-#endif
++  ForFloatTypes(ForPartialVectors<TestFloatFromInt>());
+ }
+ 
+ struct TestI32F64 {
+@@ -521,14 +573,6 @@ struct TestI32F64 {
+                       DemoteTo(di, Iota(df, -TF(N + 1) - eps)));
+     HWY_ASSERT_VEC_EQ(df, Iota(df, TF(-2.0)), PromoteTo(df, Iota(di, TI(-2))));
+ 
+-    // Huge positive float
+-    HWY_ASSERT_VEC_EQ(di, Set(di, LimitsMax<TI>()),
+-                      DemoteTo(di, Set(df, TF(1E12))));
+-
+-    // Huge negative float
+-    HWY_ASSERT_VEC_EQ(di, Set(di, LimitsMin<TI>()),
+-                      DemoteTo(di, Set(df, TF(-1E12))));
+-
+     // Max positive int
+     HWY_ASSERT_VEC_EQ(df, Set(df, TF(LimitsMax<TI>())),
+                       PromoteTo(df, Set(di, LimitsMax<TI>())));
+@@ -536,12 +580,23 @@ struct TestI32F64 {
+     // Min negative int
+     HWY_ASSERT_VEC_EQ(df, Set(df, TF(LimitsMin<TI>())),
+                       PromoteTo(df, Set(di, LimitsMin<TI>())));
++
++    // farm_sve just casts, which is undefined if the value is out of range.
++#if !defined(HWY_EMULATE_SVE)
++    // Huge positive float
++    HWY_ASSERT_VEC_EQ(di, Set(di, LimitsMax<TI>()),
++                      DemoteTo(di, Set(df, TF(1E12))));
++
++    // Huge negative float
++    HWY_ASSERT_VEC_EQ(di, Set(di, LimitsMin<TI>()),
++                      DemoteTo(di, Set(df, TF(-1E12))));
++#endif
+   }
+ };
+ 
+ HWY_NOINLINE void TestAllI32F64() {
+ #if HWY_CAP_FLOAT64
+-  ForDemoteVectors<TestI32F64, 2>()(double());
++  ForDemoteVectors<TestI32F64>()(double());
+ #endif
+ }
+ 
+@@ -552,6 +607,7 @@ HWY_NOINLINE void TestAllI32F64() {
+ HWY_AFTER_NAMESPACE();
+ 
+ #if HWY_ONCE
++
+ namespace hwy {
+ HWY_BEFORE_TEST(HwyConvertTest);
+ HWY_EXPORT_AND_TEST_P(HwyConvertTest, TestAllBitCast);
+@@ -565,4 +621,11 @@ HWY_EXPORT_AND_TEST_P(HwyConvertTest, TestAllIntFromFloat);
+ HWY_EXPORT_AND_TEST_P(HwyConvertTest, TestAllFloatFromInt);
+ HWY_EXPORT_AND_TEST_P(HwyConvertTest, TestAllI32F64);
+ }  // namespace hwy
++
++// Ought not to be necessary, but without this, no tests run on RVV.
++int main(int argc, char** argv) {
++  ::testing::InitGoogleTest(&argc, argv);
++  return RUN_ALL_TESTS();
++}
++
+ #endif
+diff --git a/third_party/highway/hwy/tests/crypto_test.cc b/third_party/highway/hwy/tests/crypto_test.cc
+new file mode 100644
+index 0000000000000..c85d63af953eb
+--- /dev/null
++++ b/third_party/highway/hwy/tests/crypto_test.cc
+@@ -0,0 +1,549 @@
++// Copyright 2021 Google LLC
++//
++// Licensed under the Apache License, Version 2.0 (the "License");
++// you may not use this file except in compliance with the License.
++// You may obtain a copy of the License at
++//
++//      http://www.apache.org/licenses/LICENSE-2.0
++//
++// Unless required by applicable law or agreed to in writing, software
++// distributed under the License is distributed on an "AS IS" BASIS,
++// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
++// See the License for the specific language governing permissions and
++// limitations under the License.
++
++#include <stddef.h>
++#include <stdint.h>
++#include <string.h>  // memcpy
++
++#include "hwy/aligned_allocator.h"
++
++#undef HWY_TARGET_INCLUDE
++#define HWY_TARGET_INCLUDE "tests/crypto_test.cc"
++#include "hwy/foreach_target.h"
++#include "hwy/highway.h"
++#include "hwy/tests/test_util-inl.h"
++
++HWY_BEFORE_NAMESPACE();
++namespace hwy {
++namespace HWY_NAMESPACE {
++
++#define HWY_PRINT_CLMUL_GOLDEN 0
++
++#if HWY_TARGET != HWY_SCALAR
++
++class TestAES {
++  template <typename T, class D>
++  HWY_NOINLINE void TestSBox(T /*unused*/, D d) {
++    // The generic implementation of the S-box is difficult to verify by
++    // inspection, so we add a white-box test that verifies it using enumeration
++    // (outputs for 0..255 vs. https://en.wikipedia.org/wiki/Rijndael_S-box).
++    const uint8_t sbox[256] = {
++        0x63, 0x7c, 0x77, 0x7b, 0xf2, 0x6b, 0x6f, 0xc5, 0x30, 0x01, 0x67, 0x2b,
++        0xfe, 0xd7, 0xab, 0x76, 0xca, 0x82, 0xc9, 0x7d, 0xfa, 0x59, 0x47, 0xf0,
++        0xad, 0xd4, 0xa2, 0xaf, 0x9c, 0xa4, 0x72, 0xc0, 0xb7, 0xfd, 0x93, 0x26,
++        0x36, 0x3f, 0xf7, 0xcc, 0x34, 0xa5, 0xe5, 0xf1, 0x71, 0xd8, 0x31, 0x15,
++        0x04, 0xc7, 0x23, 0xc3, 0x18, 0x96, 0x05, 0x9a, 0x07, 0x12, 0x80, 0xe2,
++        0xeb, 0x27, 0xb2, 0x75, 0x09, 0x83, 0x2c, 0x1a, 0x1b, 0x6e, 0x5a, 0xa0,
++        0x52, 0x3b, 0xd6, 0xb3, 0x29, 0xe3, 0x2f, 0x84, 0x53, 0xd1, 0x00, 0xed,
++        0x20, 0xfc, 0xb1, 0x5b, 0x6a, 0xcb, 0xbe, 0x39, 0x4a, 0x4c, 0x58, 0xcf,
++        0xd0, 0xef, 0xaa, 0xfb, 0x43, 0x4d, 0x33, 0x85, 0x45, 0xf9, 0x02, 0x7f,
++        0x50, 0x3c, 0x9f, 0xa8, 0x51, 0xa3, 0x40, 0x8f, 0x92, 0x9d, 0x38, 0xf5,
++        0xbc, 0xb6, 0xda, 0x21, 0x10, 0xff, 0xf3, 0xd2, 0xcd, 0x0c, 0x13, 0xec,
++        0x5f, 0x97, 0x44, 0x17, 0xc4, 0xa7, 0x7e, 0x3d, 0x64, 0x5d, 0x19, 0x73,
++        0x60, 0x81, 0x4f, 0xdc, 0x22, 0x2a, 0x90, 0x88, 0x46, 0xee, 0xb8, 0x14,
++        0xde, 0x5e, 0x0b, 0xdb, 0xe0, 0x32, 0x3a, 0x0a, 0x49, 0x06, 0x24, 0x5c,
++        0xc2, 0xd3, 0xac, 0x62, 0x91, 0x95, 0xe4, 0x79, 0xe7, 0xc8, 0x37, 0x6d,
++        0x8d, 0xd5, 0x4e, 0xa9, 0x6c, 0x56, 0xf4, 0xea, 0x65, 0x7a, 0xae, 0x08,
++        0xba, 0x78, 0x25, 0x2e, 0x1c, 0xa6, 0xb4, 0xc6, 0xe8, 0xdd, 0x74, 0x1f,
++        0x4b, 0xbd, 0x8b, 0x8a, 0x70, 0x3e, 0xb5, 0x66, 0x48, 0x03, 0xf6, 0x0e,
++        0x61, 0x35, 0x57, 0xb9, 0x86, 0xc1, 0x1d, 0x9e, 0xe1, 0xf8, 0x98, 0x11,
++        0x69, 0xd9, 0x8e, 0x94, 0x9b, 0x1e, 0x87, 0xe9, 0xce, 0x55, 0x28, 0xdf,
++        0x8c, 0xa1, 0x89, 0x0d, 0xbf, 0xe6, 0x42, 0x68, 0x41, 0x99, 0x2d, 0x0f,
++        0xb0, 0x54, 0xbb, 0x16};
++
++    // Ensure it's safe to load an entire vector by padding.
++    const size_t N = Lanes(d);
++    const size_t padded = RoundUpTo(256, N);
++    auto expected = AllocateAligned<T>(padded);
++    // Must wrap around to match the input (Iota).
++    for (size_t pos = 0; pos < padded;) {
++      const size_t remaining = HWY_MIN(padded - pos, size_t(256));
++      memcpy(expected.get() + pos, sbox, remaining);
++      pos += remaining;
++    }
++
++    for (size_t i = 0; i < 256; i += N) {
++      const auto in = Iota(d, i);
++      HWY_ASSERT_VEC_EQ(d, expected.get() + i, detail::SubBytes(in));
++    }
++  }
++
++ public:
++  template <typename T, class D>
++  HWY_NOINLINE void operator()(T t, D d) {
++    // Test vector (after first KeyAddition) from
++    // https://csrc.nist.gov/CSRC/media/Projects/Cryptographic-Standards-and-Guidelines/documents/examples/AES_Core128.pdf
++    alignas(16) constexpr uint8_t test_lanes[16] = {
++        0x40, 0xBF, 0xAB, 0xF4, 0x06, 0xEE, 0x4D, 0x30,
++        0x42, 0xCA, 0x6B, 0x99, 0x7A, 0x5C, 0x58, 0x16};
++    const auto test = LoadDup128(d, test_lanes);
++
++    // = MixColumn result
++    alignas(16) constexpr uint8_t expected0_lanes[16] = {
++        0x52, 0x9F, 0x16, 0xC2, 0x97, 0x86, 0x15, 0xCA,
++        0xE0, 0x1A, 0xAE, 0x54, 0xBA, 0x1A, 0x26, 0x59};
++    const auto expected0 = LoadDup128(d, expected0_lanes);
++
++    // = KeyAddition result
++    alignas(16) constexpr uint8_t expected_lanes[16] = {
++        0xF2, 0x65, 0xE8, 0xD5, 0x1F, 0xD2, 0x39, 0x7B,
++        0xC3, 0xB9, 0x97, 0x6D, 0x90, 0x76, 0x50, 0x5C};
++    const auto expected = LoadDup128(d, expected_lanes);
++
++    alignas(16) uint8_t key_lanes[16];
++    for (size_t i = 0; i < 16; ++i) {
++      key_lanes[i] = expected0_lanes[i] ^ expected_lanes[i];
++    }
++    const auto round_key = LoadDup128(d, key_lanes);
++
++    HWY_ASSERT_VEC_EQ(d, expected0, AESRound(test, Zero(d)));
++    HWY_ASSERT_VEC_EQ(d, expected, AESRound(test, round_key));
++
++    TestSBox(t, d);
++  }
++};
++HWY_NOINLINE void TestAllAES() { ForGE128Vectors<TestAES>()(uint8_t()); }
++
++#else
++HWY_NOINLINE void TestAllAES() {}
++#endif  // HWY_TARGET != HWY_SCALAR
++
++struct TestCLMul {
++  template <typename T, class D>
++  HWY_NOINLINE void operator()(T /*unused*/, D d) {
++    // needs 64 bit lanes and 128-bit result
++#if HWY_TARGET != HWY_SCALAR && HWY_CAP_INTEGER64
++    const size_t N = Lanes(d);
++    if (N == 1) return;
++
++    auto in1 = AllocateAligned<T>(N);
++    auto in2 = AllocateAligned<T>(N);
++
++    constexpr size_t kCLMulNum = 512;
++    // Depends on rng!
++    static constexpr uint64_t kCLMulLower[kCLMulNum] = {
++        0x24511d4ce34d6350ULL, 0x4ca582edde1236bbULL, 0x537e58f72dac25a8ULL,
++        0x4e942d5e130b9225ULL, 0x75a906c519257a68ULL, 0x1df9f85126d96c5eULL,
++        0x464e7c13f4ad286aULL, 0x138535ee35dabc40ULL, 0xb2f7477b892664ecULL,
++        0x01557b077167c25dULL, 0xf32682490ee49624ULL, 0x0025bac603b9e140ULL,
++        0xcaa86aca3e3daf40ULL, 0x1fbcfe4af73eb6c4ULL, 0x8ee8064dd0aae5dcULL,
++        0x1248cb547858c213ULL, 0x37a55ee5b10fb34cULL, 0x6eb5c97b958f86e2ULL,
++        0x4b1ab3eb655ea7cdULL, 0x1d66645a85627520ULL, 0xf8728e96daa36748ULL,
++        0x38621043e6ff5e3bULL, 0xd1d28b5da5ffefb4ULL, 0x0a5cd65931546df7ULL,
++        0x2a0639be3d844150ULL, 0x0e2d0f18c8d6f045ULL, 0xfacc770b963326c1ULL,
++        0x19611b31ca2ef141ULL, 0xabea29510dd87518ULL, 0x18a7dc4b205f2768ULL,
++        0x9d3975ea5612dc86ULL, 0x06319c139e374773ULL, 0x6641710400b4c390ULL,
++        0x356c29b6001c3670ULL, 0xe9e04d851e040a00ULL, 0x21febe561222d79aULL,
++        0xc071eaae6e148090ULL, 0x0eed351a0af94f5bULL, 0x04324eedb3c03688ULL,
++        0x39e89b136e0d6ccdULL, 0x07d0fd2777a31600ULL, 0x44b8573827209822ULL,
++        0x6d690229ea177d78ULL, 0x1b9749d960ba9f18ULL, 0x190945271c0fbb94ULL,
++        0x189aea0e07d2c88eULL, 0xf18eab6b65a6beb2ULL, 0x57744b21c13d0d84ULL,
++        0xf63050a613e95c2eULL, 0x12cd20d25f97102fULL, 0x5a5df0678dbcba60ULL,
++        0x0b08fb80948bfafcULL, 0x44cf1cbe7c6fc3c8ULL, 0x166a470ef25da288ULL,
++        0x2c498a609204e48cULL, 0x261b0a22585697ecULL, 0x737750574af7dde4ULL,
++        0x4079959c60b01e0cULL, 0x06ed8aac13f782d6ULL, 0x019d454ba9b5ef20ULL,
++        0xea1edbf96d49e858ULL, 0x17c2f3ebde9ac469ULL, 0x5cf72706e3d6f5e4ULL,
++        0x16e856aa3c841516ULL, 0x256f7e3cef83368eULL, 0x47e17c8eb2774e77ULL,
++        0x9b48ac150a804821ULL, 0x584523f61ccfdf22ULL, 0xedcb6a2a75d9e7f2ULL,
++        0x1fe3d1838e537aa7ULL, 0x778872e9f64549caULL, 0x2f1cea6f0d3faf92ULL,
++        0x0e8c4b6a9343f326ULL, 0x01902d1ba3048954ULL, 0xc5c1fd5269e91dc0ULL,
++        0x0ef8a4707817eb9cULL, 0x1f696f09a5354ca4ULL, 0x369cd9de808b818cULL,
++        0xf6917d1dd43fd784ULL, 0x7f4b76bf40dc166fULL, 0x4ce67698724ace12ULL,
++        0x02c3bf60e6e9cd92ULL, 0xb8229e45b21458e8ULL, 0x415efd41e91adf49ULL,
++        0x5edfcd516bb921cdULL, 0x5ff2c29429fd187eULL, 0x0af666b17103b3e0ULL,
++        0x1f5e4ff8f54c9a5bULL, 0x429253d8a5544ba6ULL, 0x19de2fdf9f4d9dcaULL,
++        0x29bf3d37ddc19a40ULL, 0x04d4513a879552baULL, 0x5cc7476cf71ee155ULL,
++        0x40011f8c238784a5ULL, 0x1a3ae50b0fd2ee2bULL, 0x7db22f432ba462baULL,
++        0x417290b0bee2284aULL, 0x055a6bd5bb853db2ULL, 0xaa667daeed8c2a34ULL,
++        0x0d6b316bda7f3577ULL, 0x72d35598468e3d5dULL, 0x375b594804bfd33aULL,
++        0x16ed3a319b540ae8ULL, 0x093bace4b4695afdULL, 0xc7118754ec2737ceULL,
++        0x0fff361f0505c81aULL, 0x996e9e7291321af0ULL, 0x496b1d9b0b89ba8cULL,
++        0x65a98b2e9181da9cULL, 0x70759c8dd45575dfULL, 0x3446fe727f5e2cbbULL,
++        0x1121ae609d195e74ULL, 0x5ff5d68ce8a21018ULL, 0x0e27eca3825b60d6ULL,
++        0x82f628bceca3d1daULL, 0x2756a0914e344047ULL, 0xa460406c1c708d50ULL,
++        0x63ce32a0c083e491ULL, 0xc883e5a685c480e0ULL, 0x602c951891e600f9ULL,
++        0x02ecb2e3911ca5f8ULL, 0x0d8675f4bb70781aULL, 0x43545cc3c78ea496ULL,
++        0x04164b01d6b011c2ULL, 0x3acbb323dcab2c9bULL, 0x31c5ba4e22793082ULL,
++        0x5a6484af5f7c2d10ULL, 0x1a929b16194e8078ULL, 0x7a6a75d03b313924ULL,
++        0x0553c73a35b1d525ULL, 0xf18628c51142be34ULL, 0x1b51cf80d7efd8f5ULL,
++        0x52e0ca4df63ee258ULL, 0x0e977099160650c9ULL, 0x6be1524e92024f70ULL,
++        0x0ee2152625438b9dULL, 0xfa32af436f6d8eb4ULL, 0x5ecf49c2154287e5ULL,
++        0x6b72f4ae3590569dULL, 0x086c5ee6e87bfb68ULL, 0x737a4f0dc04b6187ULL,
++        0x08c3439280edea41ULL, 0x9547944f01636c5cULL, 0x6acfbfc2571cd71fULL,
++        0x85d7842972449637ULL, 0x252ea5e5a7fad86aULL, 0x4e41468f99ba1632ULL,
++        0x095e0c3ae63b25a2ULL, 0xb005ce88fd1c9425ULL, 0x748e668abbe09f03ULL,
++        0xb2cfdf466b187d18ULL, 0x60b11e633d8fe845ULL, 0x07144c4d246db604ULL,
++        0x139bcaac55e96125ULL, 0x118679b5a6176327ULL, 0x1cebe90fa4d9f83fULL,
++        0x22244f52f0d312acULL, 0x669d4e17c9bfb713ULL, 0x96390e0b834bb0d0ULL,
++        0x01f7f0e82ba08071ULL, 0x2dffeee31ca6d284ULL, 0x1f4738745ef039feULL,
++        0x4ce0dd2b603b6420ULL, 0x0035fc905910a4d5ULL, 0x07df2b533df6fb04ULL,
++        0x1cee2735c9b910ddULL, 0x2bc4af565f7809eaULL, 0x2f876c1f5cb1076cULL,
++        0x33e079524099d056ULL, 0x169e0405d2f9efbaULL, 0x018643ab548a358cULL,
++        0x1bb6fc4331cffe92ULL, 0x05111d3a04e92faaULL, 0x23c27ecf0d638b73ULL,
++        0x1b79071dc1685d68ULL, 0x0662d20aba8e1e0cULL, 0xe7f6440277144c6fULL,
++        0x4ca38b64c22196c0ULL, 0x43c05f6d1936fbeeULL, 0x0654199d4d1faf0fULL,
++        0xf2014054e71c2d04ULL, 0x0a103e47e96b4c84ULL, 0x7986e691dd35b040ULL,
++        0x4e1ebb53c306a341ULL, 0x2775bb3d75d65ba6ULL, 0x0562ab0adeff0f15ULL,
++        0x3c2746ad5eba3eacULL, 0x1facdb5765680c60ULL, 0xb802a60027d81d00ULL,
++        0x1191d0f6366ae3a9ULL, 0x81a97b5ae0ea5d14ULL, 0x06bee05b6178a770ULL,
++        0xc7baeb2fe1d6aeb3ULL, 0x594cb5b867d04fdfULL, 0xf515a80138a4e350ULL,
++        0x646417ad8073cf38ULL, 0x4a229a43373fb8d4ULL, 0x10fa6eafff1ca453ULL,
++        0x9f060700895cc731ULL, 0x00521133d11d11f4ULL, 0xb940a2bb912a7a5cULL,
++        0x3fab180670ad2a3cULL, 0x45a5f0e5b6fdb95dULL, 0x27c1baad6f946b15ULL,
++        0x336c6bdbe527cf58ULL, 0x3b83aa602a5baea3ULL, 0xdf749153f9bcc376ULL,
++        0x1a05513a6c0b4a90ULL, 0xb81e0b570a075c47ULL, 0x471fabb40bdc27ceULL,
++        0x9dec9472f6853f60ULL, 0x361f71b88114193bULL, 0x3b550a8c4feeff00ULL,
++        0x0f6cde5a68bc9bc0ULL, 0x3f50121a925703e0ULL, 0x6967ff66d6d343a9ULL,
++        0xff6b5bd2ce7bc3ccULL, 0x05474cea08bf6cd8ULL, 0xf76eabbfaf108eb0ULL,
++        0x067529be4fc6d981ULL, 0x4d766b137cf8a988ULL, 0x2f09c7395c5cfbbdULL,
++        0x388793712da06228ULL, 0x02c9ff342c8f339aULL, 0x152c734139a860a3ULL,
++        0x35776eb2b270c04dULL, 0x0f8d8b41f11c4608ULL, 0x0c2071665be6b288ULL,
++        0xc034e212b3f71d88ULL, 0x071d961ef3276f99ULL, 0xf98598ee75b60773ULL,
++        0x062062c58c6724e4ULL, 0xd156438e2125572cULL, 0x38552d59a7f0f7c8ULL,
++        0x1a402178206e413cULL, 0x1f1f996c68293b26ULL, 0x8bce3cafe1730f7eULL,
++        0x2d0480a0828f6bf5ULL, 0x6c99cffa171f92f6ULL, 0x0087f842bb0ac681ULL,
++        0x11d7ed06e1e7fd3eULL, 0x07cb1186f2385dc6ULL, 0x5d7763ebff1e170fULL,
++        0x2dacc870231ac292ULL, 0x8486317a9ffb390cULL, 0x1c3a6dd20c959ac6ULL,
++        0x90dc96e3992e06b8ULL, 0x70d60bfa33e72b67ULL, 0x70c9bddd0985ee63ULL,
++        0x012c9767b3673093ULL, 0xfcd3bc5580f6a88aULL, 0x0ac80017ef6308c3ULL,
++        0xdb67d709ef4bba09ULL, 0x4c63e324f0e247ccULL, 0xa15481d3fe219d60ULL,
++        0x094c4279cdccb501ULL, 0x965a28c72575cb82ULL, 0x022869db25e391ebULL,
++        0x37f528c146023910ULL, 0x0c1290636917deceULL, 0x9aee25e96251ca9cULL,
++        0x728ac5ba853b69c2ULL, 0x9f272c93c4be20c8ULL, 0x06c1aa6319d28124ULL,
++        0x4324496b1ca8a4f7ULL, 0x0096ecfe7dfc0189ULL, 0x9e06131b19ae0020ULL,
++        0x15278b15902f4597ULL, 0x2a9fece8c13842d8ULL, 0x1d4e6781f0e1355eULL,
++        0x6855b712d3dbf7c0ULL, 0x06a07fad99be6f46ULL, 0x3ed9d7957e4d1d7cULL,
++        0x0c326f7cbc248bb2ULL, 0xe6363ad2c537cf51ULL, 0x0e12eb1c40723f13ULL,
++        0xf5c6ac850afba803ULL, 0x0322a79d615fa9f0ULL, 0x6116696ed97bd5f8ULL,
++        0x0d438080fbbdc9f1ULL, 0x2e4dc42c38f1e243ULL, 0x64948e9104f3a5bfULL,
++        0x9fd622371bdb5f00ULL, 0x0f12bf082b2a1b6eULL, 0x4b1f8d867d78031cULL,
++        0x134392ea9f5ef832ULL, 0xf3d70472321bc23eULL, 0x05fcbe5e9eea268eULL,
++        0x136dede7175a22cfULL, 0x1308f8baac2cbcccULL, 0xd691026f0915eb64ULL,
++        0x0e49a668345c3a38ULL, 0x24ddbbe8bc96f331ULL, 0x4d2ec9479b640578ULL,
++        0x450f0697327b359cULL, 0x32b45360f4488ee0ULL, 0x4f6d9ecec46a105aULL,
++        0x5500c63401ae8e80ULL, 0x47dea495cf6f98baULL, 0x13dc9a2dfca80babULL,
++        0xe6f8a93f7b24ca92ULL, 0x073f57a6d900a87fULL, 0x9ddb935fd3aa695aULL,
++        0x101e98d24b39e8aaULL, 0x6b8d0eb95a507ddcULL, 0x45a908b3903d209bULL,
++        0x6c96a3e119e617d4ULL, 0x2442787543d3be48ULL, 0xd3bc055c7544b364ULL,
++        0x7693bb042ca8653eULL, 0xb95e3a4ea5d0101eULL, 0x116f0d459bb94a73ULL,
++        0x841244b72cdc5e90ULL, 0x1271acced6cb34d3ULL, 0x07d289106524d638ULL,
++        0x537c9cf49c01b5bbULL, 0x8a8e16706bb7a5daULL, 0x12e50a9c499dc3a9ULL,
++        0x1cade520db2ba830ULL, 0x1add52f000d7db70ULL, 0x12cf15db2ce78e30ULL,
++        0x0657eaf606bfc866ULL, 0x4026816d3b05b1d0ULL, 0x1ba0ebdf90128e4aULL,
++        0xdfd649375996dd6eULL, 0x0f416e906c23d9aeULL, 0x384273cad0582a24ULL,
++        0x2ff27b0378a46189ULL, 0xc4ecd18a2d7a7616ULL, 0x35cef0b5cd51d640ULL,
++        0x7d582363643f48b7ULL, 0x0984ad746ad0ab7cULL, 0x2990a999835f9688ULL,
++        0x2d4df66a97b19e05ULL, 0x592c79720af99aa2ULL, 0x052863c230602cd3ULL,
++        0x5f5e2b15edcf2840ULL, 0x01dff1b694b978b0ULL, 0x14345a48b622025eULL,
++        0x028fab3b6407f715ULL, 0x3455d188e6feca50ULL, 0x1d0d40288fb1b5fdULL,
++        0x4685c5c2b6a1e5aeULL, 0x3a2077b1e5fe5adeULL, 0x1bc55d611445a0d8ULL,
++        0x05480ae95f3f83feULL, 0xbbb59cfcf7e17fb6ULL, 0x13f7f10970bbb990ULL,
++        0x6d00ac169425a352ULL, 0x7da0db397ef2d5d3ULL, 0x5b512a247f8d2479ULL,
++        0x637eaa6a977c3c32ULL, 0x3720f0ae37cba89cULL, 0x443df6e6aa7f525bULL,
++        0x28664c287dcef321ULL, 0x03c267c00cf35e49ULL, 0x690185572d4021deULL,
++        0x2707ff2596e321c2ULL, 0xd865f5af7722c380ULL, 0x1ea285658e33aafbULL,
++        0xc257c5e88755bef4ULL, 0x066f67275cfcc31eULL, 0xb09931945cc0fed0ULL,
++        0x58c1dc38d6e3a03fULL, 0xf99489678fc94ee8ULL, 0x75045bb99be5758aULL,
++        0x6c163bc34b40feefULL, 0x0420063ce7bdd3b4ULL, 0xf86ef10582bf2e28ULL,
++        0x162c3449ca14858cULL, 0x94106aa61dfe3280ULL, 0x4073ae7a4e7e4941ULL,
++        0x32b13fd179c250b4ULL, 0x0178fbb216a7e744ULL, 0xf840ae2f1cf92669ULL,
++        0x18fc709acc80243dULL, 0x20ac2ebd69f4d558ULL, 0x6e580ad9c73ad46aULL,
++        0x76d2b535b541c19dULL, 0x6c7a3fb9dd0ce0afULL, 0xc3481689b9754f28ULL,
++        0x156e813b6557abdbULL, 0x6ee372e31276eb10ULL, 0x19cf37c038c8d381ULL,
++        0x00d4d906c9ae3072ULL, 0x09f03cbb6dfbfd40ULL, 0x461ba31c4125f3cfULL,
++        0x25b29fc63ad9f05bULL, 0x6808c95c2dddede9ULL, 0x0564224337066d9bULL,
++        0xc87eb5f4a4d966f2ULL, 0x66fc66e1701f5847ULL, 0xc553a3559f74da28ULL,
++        0x1dfd841be574df43ULL, 0x3ee2f100c3ebc082ULL, 0x1a2c4f9517b56e89ULL,
++        0x502f65c4b535c8ffULL, 0x1da5663ab6f96ec0ULL, 0xba1f80b73988152cULL,
++        0x364ff12182ac8dc1ULL, 0xe3457a3c4871db31ULL, 0x6ae9cadf92fd7e84ULL,
++        0x9621ba3d6ca15186ULL, 0x00ff5af878c144ceULL, 0x918464dc130101a4ULL,
++        0x036511e6b187efa6ULL, 0x06667d66550ff260ULL, 0x7fd18913f9b51bc1ULL,
++        0x3740e6b27af77aa8ULL, 0x1f546c2fd358ff8aULL, 0x42f1424e3115c891ULL,
++        0x03767db4e3a1bb33ULL, 0xa171a1c564345060ULL, 0x0afcf632fd7b1324ULL,
++        0xb59508d933ffb7d0ULL, 0x57d766c42071be83ULL, 0x659f0447546114a2ULL,
++        0x4070364481c460aeULL, 0xa2b9752280644d52ULL, 0x04ab884bea5771bdULL,
++        0x87cd135602a232b4ULL, 0x15e54cd9a8155313ULL, 0x1e8005efaa3e1047ULL,
++        0x696b93f4ab15d39fULL, 0x0855a8e540de863aULL, 0x0bb11799e79f9426ULL,
++        0xeffa61e5c1b579baULL, 0x1e060a1d11808219ULL, 0x10e219205667c599ULL,
++        0x2f7b206091c49498ULL, 0xb48854c820064860ULL, 0x21c4aaa3bfbe4a38ULL,
++        0x8f4a032a3fa67e9cULL, 0x3146b3823401e2acULL, 0x3afee26f19d88400ULL,
++        0x167087c485791d38ULL, 0xb67a1ed945b0fb4bULL, 0x02436eb17e27f1c0ULL,
++        0xe05afce2ce2d2790ULL, 0x49c536fc6224cfebULL, 0x178865b3b862b856ULL,
++        0x1ce530de26acde5bULL, 0x87312c0b30a06f38ULL, 0x03e653b578558d76ULL,
++        0x4d3663c21d8b3accULL, 0x038003c23626914aULL, 0xd9d5a2c052a09451ULL,
++        0x39b5acfe08a49384ULL, 0x40f349956d5800e4ULL, 0x0968b6950b1bd8feULL,
++        0xd60b2ca030f3779cULL, 0x7c8bc11a23ce18edULL, 0xcc23374e27630bc2ULL,
++        0x2e38fc2a8bb33210ULL, 0xe421357814ee5c44ULL, 0x315fb65ea71ec671ULL,
++        0xfb1b0223f70ed290ULL, 0x30556c9f983eaf07ULL, 0x8dd438c3d0cd625aULL,
++        0x05a8fd0c7ffde71bULL, 0x764d1313b5aeec7aULL, 0x2036af5de9622f47ULL,
++        0x508a5bfadda292feULL, 0x3f77f04ba2830e90ULL, 0x9047cd9c66ca66d2ULL,
++        0x1168b5318a54eb21ULL, 0xc93462d221da2e15ULL, 0x4c2c7cc54abc066eULL,
++        0x767a56fec478240eULL, 0x095de72546595bd3ULL, 0xc9da535865158558ULL,
++        0x1baccf36f33e73fbULL, 0xf3d7dbe64df77f18ULL, 0x1f8ebbb7be4850b8ULL,
++        0x043c5ed77bce25a1ULL, 0x07d401041b2a178aULL, 0x9181ebb8bd8d5618ULL,
++        0x078b935dc3e4034aULL, 0x7b59c08954214300ULL, 0x03570dc2a4f84421ULL,
++        0xdd8715b82f6b4078ULL, 0x2bb49c8bb544163bULL, 0xc9eb125564d59686ULL,
++        0x5fdc7a38f80b810aULL, 0x3a4a6d8fff686544ULL, 0x28360e2418627d3aULL,
++        0x60874244c95ed992ULL, 0x2115cc1dd9c34ed3ULL, 0xfaa3ef61f55e9efcULL,
++        0x27ac9b1ef1adc7e6ULL, 0x95ea00478fec3f54ULL, 0x5aea808b2d99ab43ULL,
++        0xc8f79e51fe43a580ULL, 0x5dbccd714236ce25ULL, 0x783fa76ed0753458ULL,
++        0x48cb290f19d84655ULL, 0xc86a832f7696099aULL, 0x52f30c6fec0e71d3ULL,
++        0x77d4e91e8cdeb886ULL, 0x7169a703c6a79ccdULL, 0x98208145b9596f74ULL,
++        0x0945695c761c0796ULL, 0x0be897830d17bae0ULL, 0x033ad3924caeeeb4ULL,
++        0xedecb6cfa2d303a8ULL, 0x3f86b074818642e7ULL, 0xeefa7c878a8b03f4ULL,
++        0x093c101b80922551ULL, 0xfb3b4e6c26ac0034ULL, 0x162bf87999b94f5eULL,
++        0xeaedae76e975b17cULL, 0x1852aa090effe18eULL};
++
++    static constexpr uint64_t kCLMulUpper[kCLMulNum] = {
++        0xbb41199b1d587c69ULL, 0x514d94d55894ee29ULL, 0xebc6cd4d2efd5d16ULL,
++        0x042044ad2de477fdULL, 0xb865c8b0fcdf4b15ULL, 0x0724d7e551cc40f3ULL,
++        0xb15a16f39edb0bccULL, 0x37d64419ede7a171ULL, 0x2aa01bb80c753401ULL,
++        0x06ff3f8a95fdaf4dULL, 0x79898cc0838546deULL, 0x776acbd1b237c60aULL,
++        0x4c1753be4f4e0064ULL, 0x0ba9243601206ed3ULL, 0xd567c3b1bf3ec557ULL,
++        0x043fac7bcff61fb3ULL, 0x49356232b159fb2fULL, 0x3910c82038102d4dULL,
++        0x30592fef753eb300ULL, 0x7b2660e0c92a9e9aULL, 0x8246c9248d671ef0ULL,
++        0x5a0dcd95147af5faULL, 0x43fde953909cc0eaULL, 0x06147b972cb96e1bULL,
++        0xd84193a6b2411d80ULL, 0x00cd7711b950196fULL, 0x1088f9f4ade7fa64ULL,
++        0x05a13096ec113cfbULL, 0x958d816d53b00edcULL, 0x3846154a7cdba9cbULL,
++        0x8af516db6b27d1e6ULL, 0x1a1d462ab8a33b13ULL, 0x4040b0ac1b2c754cULL,
++        0x05127fe9af2fe1d6ULL, 0x9f96e79374321fa6ULL, 0x06ff64a4d9c326f3ULL,
++        0x28709566e158ac15ULL, 0x301701d7111ca51cULL, 0x31e0445d1b9d9544ULL,
++        0x0a95aff69bf1d03eULL, 0x7c298c8414ecb879ULL, 0x00801499b4143195ULL,
++        0x91521a00dd676a5cULL, 0x2777526a14c2f723ULL, 0xfa26aac6a6357dddULL,
++        0x1d265889b0187a4bULL, 0xcd6e70fa8ed283e4ULL, 0x18a815aa50ea92caULL,
++        0xc01e082694a263c6ULL, 0x4b40163ba53daf25ULL, 0xbc658caff6501673ULL,
++        0x3ba35359586b9652ULL, 0x74f96acc97a4936cULL, 0x3989dfdb0cf1d2cfULL,
++        0x358a01eaa50dda32ULL, 0x01109a5ed8f0802bULL, 0x55b84922e63c2958ULL,
++        0x55b14843d87551d5ULL, 0x1db8ec61b1b578d8ULL, 0x79a2d49ef8c3658fULL,
++        0xa304516816b3fbe0ULL, 0x163ecc09cc7b82f9ULL, 0xab91e8d22aabef00ULL,
++        0x0ed6b09262de8354ULL, 0xcfd47d34cf73f6f2ULL, 0x7dbd1db2390bc6c3ULL,
++        0x5ae789d3875e7b00ULL, 0x1d60fd0e70fe8fa4ULL, 0x690bc15d5ae4f6f5ULL,
++        0x121ef5565104fb44ULL, 0x6e98e89297353b54ULL, 0x42554949249d62edULL,
++        0xd6d6d16b12df78d2ULL, 0x320b33549b74975dULL, 0xd2a0618763d22e00ULL,
++        0x0808deb93cba2017ULL, 0x01bd3b2302a2cc70ULL, 0x0b7b8dd4d71c8dd6ULL,
++        0x34d60a3382a0756cULL, 0x40984584c8219629ULL, 0xf1152cba10093a66ULL,
++        0x068001c6b2159ccbULL, 0x3d70f13c6cda0800ULL, 0x0e6b6746a322b956ULL,
++        0x83a494319d8c770bULL, 0x0faecf64a8553e9aULL, 0xa34919222c39b1bcULL,
++        0x0c63850d89e71c6fULL, 0x585f0bee92e53dc8ULL, 0x10f222b13b4fa5deULL,
++        0x61573114f94252f2ULL, 0x09d59c311fba6c27ULL, 0x014effa7da49ed4eULL,
++        0x4a400a1bc1c31d26ULL, 0xc9091c047b484972ULL, 0x3989f341ec2230ccULL,
++        0xdcb03a98b3aee41eULL, 0x4a54a676a33a95e1ULL, 0xe499b7753951ef7cULL,
++        0x2f43b1d1061d8b48ULL, 0xc3313bdc68ceb146ULL, 0x5159f6bc0e99227fULL,
++        0x98128e6d9c05efcaULL, 0x15ea32b27f77815bULL, 0xe882c054e2654eecULL,
++        0x003d2cdb8faee8c6ULL, 0xb416dd333a9fe1dfULL, 0x73f6746aefcfc98bULL,
++        0x93dc114c10a38d70ULL, 0x05055941657845eaULL, 0x2ed7351347349334ULL,
++        0x26fb1ee2c69ae690ULL, 0xa4575d10dc5b28e0ULL, 0x3395b11295e485ebULL,
++        0xe840f198a224551cULL, 0x78e6e5a431d941d4ULL, 0xa1fee3ceab27f391ULL,
++        0x07d35b3c5698d0dcULL, 0x983c67fca9174a29ULL, 0x2bb6bbae72b5144aULL,
++        0xa7730b8d13ce58efULL, 0x51b5272883de1998ULL, 0xb334e128bb55e260ULL,
++        0x1cacf5fbbe1b9974ULL, 0x71a9df4bb743de60ULL, 0x5176fe545c2d0d7aULL,
++        0xbe592ecf1a16d672ULL, 0x27aa8a30c3efe460ULL, 0x4c78a32f47991e06ULL,
++        0x383459294312f26aULL, 0x97ba789127f1490cULL, 0x51c9aa8a3abd1ef1ULL,
++        0xcc7355188121e50fULL, 0x0ecb3a178ae334c1ULL, 0x84879a5e574b7160ULL,
++        0x0765298f6389e8f3ULL, 0x5c6750435539bb22ULL, 0x11a05cf056c937b5ULL,
++        0xb5dc2172dbfb7662ULL, 0x3ffc17915d9f40e8ULL, 0xbc7904daf3b431b0ULL,
++        0x71f2088490930a7cULL, 0xa89505fd9efb53c4ULL, 0x02e194afd61c5671ULL,
++        0x99a97f4abf35fcecULL, 0x26830aad30fae96fULL, 0x4b2abc16b25cf0b0ULL,
++        0x07ec6fffa1cafbdbULL, 0xf38188fde97a280cULL, 0x121335701afff64dULL,
++        0xea5ef38b4e672a64ULL, 0x477edbcae3eabf03ULL, 0xa32813cc0e0d244dULL,
++        0x13346d2af4972eefULL, 0xcbc18357af1cfa9aULL, 0x561b630316e73fa6ULL,
++        0xe9dfb53249249305ULL, 0x5d2b9dd1479312eeULL, 0x3458008119b56d04ULL,
++        0x50e6790b49801385ULL, 0x5bb9febe2349492bULL, 0x0c2813954299098fULL,
++        0xf747b0c890a071d5ULL, 0x417e8f82cc028d77ULL, 0xa134fee611d804f8ULL,
++        0x24c99ee9a0408761ULL, 0x3ebb224e727137f3ULL, 0x0686022073ceb846ULL,
++        0xa05e901fb82ad7daULL, 0x0ece7dc43ab470fcULL, 0x2d334ecc58f7d6a3ULL,
++        0x23166fadacc54e40ULL, 0x9c3a4472f839556eULL, 0x071717ab5267a4adULL,
++        0xb6600ac351ba3ea0ULL, 0x30ec748313bb63d4ULL, 0xb5374e39287b23ccULL,
++        0x074d75e784238aebULL, 0x77315879243914a4ULL, 0x3bbb1971490865f1ULL,
++        0xa355c21f4fbe02d3ULL, 0x0027f4bb38c8f402ULL, 0xeef8708e652bc5f0ULL,
++        0x7b9aa56cf9440050ULL, 0x113ac03c16cfc924ULL, 0x395db36d3e4bef9fULL,
++        0x5d826fabcaa597aeULL, 0x2a77d3c58786d7e0ULL, 0x85996859a3ba19d4ULL,
++        0x01e7e3c904c2d97fULL, 0x34f90b9b98d51fd0ULL, 0x243aa97fd2e99bb7ULL,
++        0x40a0cebc4f65c1e8ULL, 0x46d3922ed4a5503eULL, 0x446e7ecaf1f9c0a4ULL,
++        0x49dc11558bc2e6aeULL, 0xe7a9f20881793af8ULL, 0x5771cc4bc98103f1ULL,
++        0x2446ea6e718fce90ULL, 0x25d14aca7f7da198ULL, 0x4347af186f9af964ULL,
++        0x10cb44fc9146363aULL, 0x8a35587afce476b4ULL, 0x575144662fee3d3aULL,
++        0x69f41177a6bc7a05ULL, 0x02ff8c38d6b3c898ULL, 0x57c73589a226ca40ULL,
++        0x732f6b5baae66683ULL, 0x00c008bbedd4bb34ULL, 0x7412ff09524d6cadULL,
++        0xb8fd0b5ad8c145a8ULL, 0x74bd9f94b6cdc7dfULL, 0x68233b317ca6c19cULL,
++        0x314b9c2c08b15c54ULL, 0x5bd1ad72072ebd08ULL, 0x6610e6a6c07030e4ULL,
++        0xa4fc38e885ead7ceULL, 0x36975d1ca439e034ULL, 0xa358f0fe358ffb1aULL,
++        0x38e247ad663acf7dULL, 0x77daed3643b5deb8ULL, 0x5507c2aeae1ec3d0ULL,
++        0xfdec226c73acf775ULL, 0x1b87ff5f5033492dULL, 0xa832dee545d9033fULL,
++        0x1cee43a61e41783bULL, 0xdff82b2e2d822f69ULL, 0x2bbc9a376cb38cf2ULL,
++        0x117b1cdaf765dc02ULL, 0x26a407f5682be270ULL, 0x8eb664cf5634af28ULL,
++        0x17cb4513bec68551ULL, 0xb0df6527900cbfd0ULL, 0x335a2dc79c5afdfcULL,
++        0xa2f0ca4cd38dca88ULL, 0x1c370713b81a2de1ULL, 0x849d5df654d1adfcULL,
++        0x2fd1f7675ae14e44ULL, 0x4ff64dfc02247f7bULL, 0x3a2bcf40e395a48dULL,
++        0x436248c821b187c1ULL, 0x29f4337b1c7104c0ULL, 0xfc317c46e6630ec4ULL,
++        0x2774bccc4e3264c7ULL, 0x2d03218d9d5bee23ULL, 0x36a0ed04d659058aULL,
++        0x452484461573cab6ULL, 0x0708edf87ed6272bULL, 0xf07960a1587446cbULL,
++        0x3660167b067d84e0ULL, 0x65990a6993ddf8c4ULL, 0x0b197cd3d0b40b3fULL,
++        0x1dcec4ab619f3a05ULL, 0x722ab223a84f9182ULL, 0x0822d61a81e7c38fULL,
++        0x3d22ad75da563201ULL, 0x93cef6979fd35e0fULL, 0x05c3c25ae598b14cULL,
++        0x1338df97dd496377ULL, 0x15bc324dc9c20acfULL, 0x96397c6127e6e8cfULL,
++        0x004d01069ef2050fULL, 0x2fcf2e27893fdcbcULL, 0x072f77c3e44f4a5cULL,
++        0x5eb1d80b3fe44918ULL, 0x1f59e7c28cc21f22ULL, 0x3390ce5df055c1f8ULL,
++        0x4c0ef11df92cb6bfULL, 0x50f82f9e0848c900ULL, 0x08d0fde3ffc0ae38ULL,
++        0xbd8d0089a3fbfb73ULL, 0x118ba5b0f311ef59ULL, 0x9be9a8407b926a61ULL,
++        0x4ea04fbb21318f63ULL, 0xa1c8e7bb07b871ffULL, 0x1253a7262d5d3b02ULL,
++        0x13e997a0512e5b29ULL, 0x54318460ce9055baULL, 0x4e1d8a4db0054798ULL,
++        0x0b235226e2cade32ULL, 0x2588732c1476b315ULL, 0x16a378750ba8ac68ULL,
++        0xba0b116c04448731ULL, 0x4dd02bd47694c2f1ULL, 0x16d6797b218b6b25ULL,
++        0x769eb3709cfbf936ULL, 0x197746a0ce396f38ULL, 0x7d17ad8465961d6eULL,
++        0xfe58f4998ae19bb4ULL, 0x36df24305233ce69ULL, 0xb88a4eb008f4ee72ULL,
++        0x302b2eb923334787ULL, 0x15a4e3edbe13d448ULL, 0x39a4bf64dd7730ceULL,
++        0xedf25421b31090c4ULL, 0x4d547fc131be3b69ULL, 0x2b316e120ca3b90eULL,
++        0x0faf2357bf18a169ULL, 0x71f34b54ee2c1d62ULL, 0x18eaf6e5c93a3824ULL,
++        0x7e168ba03c1b4c18ULL, 0x1a534dd586d9e871ULL, 0xa2cccd307f5f8c38ULL,
++        0x2999a6fb4dce30f6ULL, 0x8f6d3b02c1d549a6ULL, 0x5cf7f90d817aac5aULL,
++        0xd2a4ceefe66c8170ULL, 0x11560edc4ca959feULL, 0x89e517e6f0dc464dULL,
++        0x75bb8972dddd2085ULL, 0x13859ed1e459d65aULL, 0x057114653326fa84ULL,
++        0xe2e6f465173cc86cULL, 0x0ada4076497d7de4ULL, 0xa856fa10ec6dbf8aULL,
++        0x41505d9a7c25d875ULL, 0x3091b6278382eccdULL, 0x055737185b2c3f13ULL,
++        0x2f4df8ecd6f9c632ULL, 0x0633e89c33552d98ULL, 0xf7673724d16db440ULL,
++        0x7331bd08e636c391ULL, 0x0252f29672fee426ULL, 0x1fc384946b6b9ddeULL,
++        0x03460c12c901443aULL, 0x003a0792e10abcdaULL, 0x8dbec31f624e37d0ULL,
++        0x667420d5bfe4dcbeULL, 0xfbfa30e874ed7641ULL, 0x46d1ae14db7ecef6ULL,
++        0x216bd7e8f5448768ULL, 0x32bcd40d3d69cc88ULL, 0x2e991dbc39b65abeULL,
++        0x0e8fb123a502f553ULL, 0x3d2d486b2c7560c0ULL, 0x09aba1db3079fe03ULL,
++        0xcb540c59398c9bceULL, 0x363970e5339ed600ULL, 0x2caee457c28af00eULL,
++        0x005e7d7ee47f41a0ULL, 0x69fad3eb10f44100ULL, 0x048109388c75beb3ULL,
++        0x253dddf96c7a6fb8ULL, 0x4c47f705b9d47d09ULL, 0x6cec894228b5e978ULL,
++        0x04044bb9f8ff45c2ULL, 0x079e75704d775caeULL, 0x073bd54d2a9e2c33ULL,
++        0xcec7289270a364fbULL, 0x19e7486f19cd9e4eULL, 0xb50ac15b86b76608ULL,
++        0x0620cf81f165c812ULL, 0x63eaaf13be7b11d4ULL, 0x0e0cf831948248c2ULL,
++        0xf0412df8f46e7957ULL, 0x671c1fe752517e3fULL, 0x8841bfb04dd3f540ULL,
++        0x122de4142249f353ULL, 0x40a4959fb0e76870ULL, 0x25cfd3d4b4bbc459ULL,
++        0x78a07c82930c60d0ULL, 0x12c2de24d4cbc969ULL, 0x85d44866096ad7f4ULL,
++        0x1fd917ca66b2007bULL, 0x01fbbb0751764764ULL, 0x3d2a4953c6fe0fdcULL,
++        0xcc1489c5737afd94ULL, 0x1817c5b6a5346f41ULL, 0xe605a6a7e9985644ULL,
++        0x3c50412328ff1946ULL, 0xd8c7fd65817f1291ULL, 0x0bd66975ab66339bULL,
++        0x2baf8fa1c7d10fa9ULL, 0x24abdf06ddef848dULL, 0x14df0c9b2ea4f6c2ULL,
++        0x2be950edfd2cb1f7ULL, 0x21911e21094178b6ULL, 0x0fa54d518a93b379ULL,
++        0xb52508e0ac01ab42ULL, 0x0e035b5fd8cb79beULL, 0x1c1c6d1a3b3c8648ULL,
++        0x286037b42ea9871cULL, 0xfe67bf311e48a340ULL, 0x02324131e932a472ULL,
++        0x2486dc2dd919e2deULL, 0x008aec7f1da1d2ebULL, 0x63269ba0e8d3eb3aULL,
++        0x23c0f11154adb62fULL, 0xc6052393ecd4c018ULL, 0x523585b7d2f5b9fcULL,
++        0xf7e6f8c1e87564c9ULL, 0x09eb9fe5dd32c1a3ULL, 0x4d4f86886e055472ULL,
++        0x67ea17b58a37966bULL, 0x3d3ce8c23b1ed1a8ULL, 0x0df97c5ac48857ceULL,
++        0x9b6992623759eb12ULL, 0x275aa9551ae091f2ULL, 0x08855e19ac5e62e5ULL,
++        0x1155fffe0ae083ccULL, 0xbc9c78db7c570240ULL, 0x074560c447dd2418ULL,
++        0x3bf78d330bcf1e70ULL, 0x49867cd4b7ed134bULL, 0x8e6eee0cb4470accULL,
++        0x1dabafdf59233dd6ULL, 0xea3a50d844fc3fb8ULL, 0x4f03f4454764cb87ULL,
++        0x1f2f41cc36c9e6ecULL, 0x53cba4df42963441ULL, 0x10883b70a88d91fbULL,
++        0x62b1fc77d4eb9481ULL, 0x893d8f2604b362e1ULL, 0x0933b7855368b440ULL,
++        0x9351b545703b2fceULL, 0x59c1d489b9bdd3b4ULL, 0xe72a9c4311417b18ULL,
++        0x5355df77e88eb226ULL, 0xe802c37aa963d7e1ULL, 0x381c3747bd6c3bc3ULL,
++        0x378565573444258cULL, 0x37848b1e52b43c18ULL, 0x5da2cd32bdce12b6ULL,
++        0x13166c5da615f6fdULL, 0xa51ef95efcc66ac8ULL, 0x640c95e473f1e541ULL,
++        0x6ec68def1f217500ULL, 0x49ce3543c76a4079ULL, 0x5fc6fd3cddc706b5ULL,
++        0x05c3c0f0f6a1fb0dULL, 0xe7820c0996ad1bddULL, 0x21f0d752a088f35cULL,
++        0x755405b51d6fc4a0ULL, 0x7ec7649ca4b0e351ULL, 0x3d2b6a46a251f790ULL,
++        0x23e1176b19f418adULL, 0x06056575efe8ac05ULL, 0x0f75981b6966e477ULL,
++        0x06e87ec41ad437e4ULL, 0x43f6c255d5e1cb84ULL, 0xe4e67d1120ceb580ULL,
++        0x2cd67b9e12c26d7bULL, 0xcd00b5ff7fd187f1ULL, 0x3f6cd40accdc4106ULL,
++        0x3e895c835459b330ULL, 0x0814d53a217c0850ULL, 0xc9111fe78bc3a62dULL,
++        0x719967e351473204ULL, 0xe757707d24282aa4ULL, 0x7226b7f5607f98e6ULL,
++        0x7b268ffae3c08d96ULL, 0x16d3917c8b86020eULL, 0x5128bca51c49ea64ULL,
++        0x345ffea02bb1698dULL, 0x9460f5111fe4fbc8ULL, 0x60dd1aa5762852cbULL,
++        0xbb7440ed3c81667cULL, 0x0a4b12affa7f6f5cULL, 0x95cbcb0ae03861b6ULL,
++        0x07ab3b0591db6070ULL, 0xc6476a4c3de78982ULL, 0x204e82e8623ad725ULL,
++        0x569a5b4e8ac2a5ccULL, 0x425a1d77d72ebae2ULL, 0xcdaad5551ab33830ULL,
++        0x0b7c68fd8422939eULL, 0x46d9a01f53ec3020ULL, 0x102871edbb29e852ULL,
++        0x7a8e8084039075a5ULL, 0x40eaede8615e376aULL, 0x4dc67d757a1c751fULL,
++        0x1176ef33063f9145ULL, 0x4ea230285b1c8156ULL, 0x6b2aa46ce0027392ULL,
++        0x32b13230fba1b068ULL, 0x0e69796851bb984fULL, 0xb749f4542db698c0ULL,
++        0x19ad0241ffffd49cULL, 0x2f41e92ef6caff52ULL, 0x4d0b068576747439ULL,
++        0x14d607aef7463e00ULL, 0x1443d00d85fb440eULL, 0x529b43bf68688780ULL,
++        0x21133a6bc3a3e378ULL, 0x865b6436dae0e7e5ULL, 0x6b4fe83dc1d6defcULL,
++        0x03a5858a0ca0be46ULL, 0x1e841b187e67f312ULL, 0x61ee22ef40a66940ULL,
++        0x0494bd2e9e741ef8ULL, 0x4eb59e323010e72cULL, 0x19f2abcfb749810eULL,
++        0xb30f1e4f994ef9bcULL, 0x53cf6cdd51bd2d96ULL, 0x263943036497a514ULL,
++        0x0d4b52170aa2edbaULL, 0x0c4758a1c7b4f758ULL, 0x178dadb1b502b51aULL,
++        0x1ddbb20a602eb57aULL, 0x1fc2e2564a9f27fdULL, 0xd5f8c50a0e3d6f90ULL,
++        0x0081da3bbe72ac09ULL, 0xcf140d002ccdb200ULL, 0x0ae8389f09b017feULL,
++        0x17cc9ffdc03f4440ULL, 0x04eb921d704bcdddULL, 0x139a0ce4cdc521abULL,
++        0x0bfce00c145cb0f0ULL, 0x99925ff132eff707ULL, 0x063f6e5da50c3d35ULL,
++        0xa0c25dea3f0e6e29ULL, 0x0c7a9048cc8e040fULL,
++    };
++
++    const size_t padded = RoundUpTo(kCLMulNum, N);
++    auto expected_lower = AllocateAligned<T>(padded);
++    auto expected_upper = AllocateAligned<T>(padded);
++    memcpy(expected_lower.get(), kCLMulLower, kCLMulNum * sizeof(T));
++    memcpy(expected_upper.get(), kCLMulUpper, kCLMulNum * sizeof(T));
++    const size_t padding_size = (padded - kCLMulNum) * sizeof(T);
++    memset(expected_lower.get() + kCLMulNum, 0, padding_size);
++    memset(expected_upper.get() + kCLMulNum, 0, padding_size);
++
++    // Random inputs in each lane
++    RandomState rng;
++    for (size_t rep = 0; rep < kCLMulNum / N; ++rep) {
++      for (size_t i = 0; i < N; ++i) {
++        in1[i] = Random64(&rng);
++        in2[i] = Random64(&rng);
++      }
++
++      const auto a = Load(d, in1.get());
++      const auto b = Load(d, in2.get());
++#if HWY_PRINT_CLMUL_GOLDEN
++      Store(CLMulLower(a, b), d, expected_lower.get() + rep * N);
++      Store(CLMulUpper(a, b), d, expected_upper.get() + rep * N);
++#else
++      HWY_ASSERT_VEC_EQ(d, expected_lower.get() + rep * N, CLMulLower(a, b));
++      HWY_ASSERT_VEC_EQ(d, expected_upper.get() + rep * N, CLMulUpper(a, b));
++#endif
++    }
++
++#if HWY_PRINT_CLMUL_GOLDEN
++    // RVV lacks PRIu64, so print 32-bit halves.
++    for (size_t i = 0; i < kCLMulNum; ++i) {
++      printf("0x%08x%08xULL,", static_cast<uint32_t>(expected_lower[i] >> 32),
++             static_cast<uint32_t>(expected_lower[i] & 0xFFFFFFFFU));
++    }
++    printf("\n");
++    for (size_t i = 0; i < kCLMulNum; ++i) {
++      printf("0x%08x%08xULL,", static_cast<uint32_t>(expected_upper[i] >> 32),
++             static_cast<uint32_t>(expected_upper[i] & 0xFFFFFFFFU));
++    }
++#endif  // HWY_PRINT_CLMUL_GOLDEN
++#else
++    (void)d;
++#endif
++  }
++};
++
++HWY_NOINLINE void TestAllCLMul() { ForGE128Vectors<TestCLMul>()(uint64_t()); }
++
++// NOLINTNEXTLINE(google-readability-namespace-comments)
++}  // namespace HWY_NAMESPACE
++}  // namespace hwy
++HWY_AFTER_NAMESPACE();
++
++#if HWY_ONCE
++
++namespace hwy {
++HWY_BEFORE_TEST(HwyCryptoTest);
++HWY_EXPORT_AND_TEST_P(HwyCryptoTest, TestAllAES);
++HWY_EXPORT_AND_TEST_P(HwyCryptoTest, TestAllCLMul);
++}  // namespace hwy
++
++// Ought not to be necessary, but without this, no tests run on RVV.
++int main(int argc, char **argv) {
++  ::testing::InitGoogleTest(&argc, argv);
++  return RUN_ALL_TESTS();
++}
++
++#endif
+diff --git a/third_party/highway/hwy/tests/logical_test.cc b/third_party/highway/hwy/tests/logical_test.cc
+index d4a447f6133c1..c7bc1dfdd6921 100644
+--- a/third_party/highway/hwy/tests/logical_test.cc
++++ b/third_party/highway/hwy/tests/logical_test.cc
+@@ -16,12 +16,12 @@
+ #include <stdint.h>
+ #include <string.h>  // memcmp
+ 
++#include "hwy/aligned_allocator.h"
+ #include "hwy/base.h"
+ 
+ #undef HWY_TARGET_INCLUDE
+ #define HWY_TARGET_INCLUDE "tests/logical_test.cc"
+ #include "hwy/foreach_target.h"
+-
+ #include "hwy/highway.h"
+ #include "hwy/tests/test_util-inl.h"
+ 
+@@ -160,302 +160,6 @@ HWY_NOINLINE void TestAllCopySign() {
+   ForFloatTypes(ForPartialVectors<TestCopySign>());
+ }
+ 
+-struct TestFirstN {
+-  template <class T, class D>
+-  HWY_NOINLINE void operator()(T /*unused*/, D d) {
+-    const size_t N = Lanes(d);
+-    auto mask_lanes = AllocateAligned<T>(N);
+-
+-    // NOTE: reverse polarity (mask is true iff mask_lanes[i] == 0) because we
+-    // cannot reliably compare against all bits set (NaN for float types).
+-    const T off = 1;
+-
+-    for (size_t len = 0; len <= N; ++len) {
+-      for (size_t i = 0; i < N; ++i) {
+-        mask_lanes[i] = i < len ? T(0) : off;
+-      }
+-      const auto mask = Eq(Load(d, mask_lanes.get()), Zero(d));
+-      HWY_ASSERT_MASK_EQ(d, mask, FirstN(d, len));
+-    }
+-  }
+-};
+-
+-HWY_NOINLINE void TestAllFirstN() {
+-  ForAllTypes(ForPartialVectors<TestFirstN>());
+-}
+-
+-struct TestIfThenElse {
+-  template <class T, class D>
+-  HWY_NOINLINE void operator()(T /*unused*/, D d) {
+-    RandomState rng;
+-
+-    const size_t N = Lanes(d);
+-    auto in1 = AllocateAligned<T>(N);
+-    auto in2 = AllocateAligned<T>(N);
+-    auto mask_lanes = AllocateAligned<T>(N);
+-    auto expected = AllocateAligned<T>(N);
+-
+-    // NOTE: reverse polarity (mask is true iff lane == 0) because we cannot
+-    // reliably compare against all bits set (NaN for float types).
+-    const T off = 1;
+-
+-    // Each lane should have a chance of having mask=true.
+-    for (size_t rep = 0; rep < 50; ++rep) {
+-      for (size_t i = 0; i < N; ++i) {
+-        in1[i] = static_cast<T>(Random32(&rng));
+-        in2[i] = static_cast<T>(Random32(&rng));
+-        mask_lanes[i] = (Random32(&rng) & 1024) ? off : T(0);
+-      }
+-
+-      const auto v1 = Load(d, in1.get());
+-      const auto v2 = Load(d, in2.get());
+-      const auto mask = Eq(Load(d, mask_lanes.get()), Zero(d));
+-
+-      for (size_t i = 0; i < N; ++i) {
+-        expected[i] = (mask_lanes[i] == off) ? in2[i] : in1[i];
+-      }
+-      HWY_ASSERT_VEC_EQ(d, expected.get(), IfThenElse(mask, v1, v2));
+-
+-      for (size_t i = 0; i < N; ++i) {
+-        expected[i] = mask_lanes[i] ? T(0) : in1[i];
+-      }
+-      HWY_ASSERT_VEC_EQ(d, expected.get(), IfThenElseZero(mask, v1));
+-
+-      for (size_t i = 0; i < N; ++i) {
+-        expected[i] = mask_lanes[i] ? in2[i] : T(0);
+-      }
+-      HWY_ASSERT_VEC_EQ(d, expected.get(), IfThenZeroElse(mask, v2));
+-    }
+-  }
+-};
+-
+-HWY_NOINLINE void TestAllIfThenElse() {
+-  ForAllTypes(ForPartialVectors<TestIfThenElse>());
+-}
+-
+-struct TestMaskVec {
+-  template <class T, class D>
+-  HWY_NOINLINE void operator()(T /*unused*/, D d) {
+-    RandomState rng;
+-
+-    const size_t N = Lanes(d);
+-    auto mask_lanes = AllocateAligned<T>(N);
+-
+-    // Each lane should have a chance of having mask=true.
+-    for (size_t rep = 0; rep < 100; ++rep) {
+-      for (size_t i = 0; i < N; ++i) {
+-        mask_lanes[i] = static_cast<T>(Random32(&rng) & 1);
+-      }
+-
+-      const auto mask = RebindMask(d, Eq(Load(d, mask_lanes.get()), Zero(d)));
+-      HWY_ASSERT_MASK_EQ(d, mask, MaskFromVec(VecFromMask(d, mask)));
+-    }
+-  }
+-};
+-
+-HWY_NOINLINE void TestAllMaskVec() {
+-  const ForPartialVectors<TestMaskVec> test;
+-
+-  test(uint16_t());
+-  test(int16_t());
+-  // TODO(janwas): float16_t - cannot compare yet
+-
+-  test(uint32_t());
+-  test(int32_t());
+-  test(float());
+-
+-#if HWY_CAP_INTEGER64
+-  test(uint64_t());
+-  test(int64_t());
+-#endif
+-#if HWY_CAP_FLOAT64
+-  test(double());
+-#endif
+-}
+-
+-struct TestCompress {
+-  template <class T, class D>
+-  HWY_NOINLINE void operator()(T /*unused*/, D d) {
+-    RandomState rng;
+-
+-    using TU = MakeUnsigned<T>;
+-    const Rebind<TU, D> du;
+-    const size_t N = Lanes(d);
+-    auto in_lanes = AllocateAligned<T>(N);
+-    auto mask_lanes = AllocateAligned<TU>(N);
+-    auto expected = AllocateAligned<T>(N);
+-    auto actual = AllocateAligned<T>(N);
+-
+-    // Each lane should have a chance of having mask=true.
+-    for (size_t rep = 0; rep < 100; ++rep) {
+-      size_t expected_pos = 0;
+-      for (size_t i = 0; i < N; ++i) {
+-        const uint64_t bits = Random32(&rng);
+-        in_lanes[i] = T();  // cannot initialize float16_t directly.
+-        CopyBytes<sizeof(T)>(&bits, &in_lanes[i]);
+-        mask_lanes[i] = static_cast<TU>(Random32(&rng) & 1);
+-        if (mask_lanes[i] == 0) {  // Zero means true (easier to compare)
+-          expected[expected_pos++] = in_lanes[i];
+-        }
+-      }
+-
+-      const auto in = Load(d, in_lanes.get());
+-      const auto mask = RebindMask(d, Eq(Load(du, mask_lanes.get()), Zero(du)));
+-
+-      Store(Compress(in, mask), d, actual.get());
+-      // Upper lanes are undefined.
+-      for (size_t i = 0; i < expected_pos; ++i) {
+-        HWY_ASSERT(memcmp(&actual[i], &expected[i], sizeof(T)) == 0);
+-      }
+-
+-      // Also check CompressStore in the same way.
+-      memset(actual.get(), 0, N * sizeof(T));
+-      const size_t num_written = CompressStore(in, mask, d, actual.get());
+-      HWY_ASSERT_EQ(expected_pos, num_written);
+-      for (size_t i = 0; i < expected_pos; ++i) {
+-        HWY_ASSERT(memcmp(&actual[i], &expected[i], sizeof(T)) == 0);
+-      }
+-    }
+-  }
+-};
+-
+-#if 0
+-namespace detail {  // for code folding
+-void PrintCompress16x8Tables() {
+-  constexpr size_t N = 8;  // 128-bit SIMD
+-  for (uint64_t code = 0; code < 1ull << N; ++code) {
+-    std::array<uint8_t, N> indices{0};
+-    size_t pos = 0;
+-    for (size_t i = 0; i < N; ++i) {
+-      if (code & (1ull << i)) {
+-        indices[pos++] = i;
+-      }
+-    }
+-
+-    // Doubled (for converting lane to byte indices)
+-    for (size_t i = 0; i < N; ++i) {
+-      printf("%d,", 2 * indices[i]);
+-    }
+-  }
+-  printf("\n");
+-}
+-
+-// Compressed to nibbles
+-void PrintCompress32x8Tables() {
+-  constexpr size_t N = 8;  // AVX2
+-  for (uint64_t code = 0; code < 1ull << N; ++code) {
+-    std::array<uint32_t, N> indices{0};
+-    size_t pos = 0;
+-    for (size_t i = 0; i < N; ++i) {
+-      if (code & (1ull << i)) {
+-        indices[pos++] = i;
+-      }
+-    }
+-
+-    // Convert to nibbles
+-    uint64_t packed = 0;
+-    for (size_t i = 0; i < N; ++i) {
+-      HWY_ASSERT(indices[i] < 16);
+-      packed += indices[i] << (i * 4);
+-    }
+-
+-    HWY_ASSERT(packed < (1ull << 32));
+-    printf("0x%08x,", static_cast<uint32_t>(packed));
+-  }
+-  printf("\n");
+-}
+-
+-// Pairs of 32-bit lane indices
+-void PrintCompress64x4Tables() {
+-  constexpr size_t N = 4;  // AVX2
+-  for (uint64_t code = 0; code < 1ull << N; ++code) {
+-    std::array<uint32_t, N> indices{0};
+-    size_t pos = 0;
+-    for (size_t i = 0; i < N; ++i) {
+-      if (code & (1ull << i)) {
+-        indices[pos++] = i;
+-      }
+-    }
+-
+-    for (size_t i = 0; i < N; ++i) {
+-      printf("%d,%d,", 2 * indices[i], 2 * indices[i] + 1);
+-    }
+-  }
+-  printf("\n");
+-}
+-
+-// 4-tuple of byte indices
+-void PrintCompress32x4Tables() {
+-  using T = uint32_t;
+-  constexpr size_t N = 4;  // SSE4
+-  for (uint64_t code = 0; code < 1ull << N; ++code) {
+-    std::array<uint32_t, N> indices{0};
+-    size_t pos = 0;
+-    for (size_t i = 0; i < N; ++i) {
+-      if (code & (1ull << i)) {
+-        indices[pos++] = i;
+-      }
+-    }
+-
+-    for (size_t i = 0; i < N; ++i) {
+-      for (size_t idx_byte = 0; idx_byte < sizeof(T); ++idx_byte) {
+-        printf("%zu,", sizeof(T) * indices[i] + idx_byte);
+-      }
+-    }
+-  }
+-  printf("\n");
+-}
+-
+-// 8-tuple of byte indices
+-void PrintCompress64x2Tables() {
+-  using T = uint64_t;
+-  constexpr size_t N = 2;  // SSE4
+-  for (uint64_t code = 0; code < 1ull << N; ++code) {
+-    std::array<uint32_t, N> indices{0};
+-    size_t pos = 0;
+-    for (size_t i = 0; i < N; ++i) {
+-      if (code & (1ull << i)) {
+-        indices[pos++] = i;
+-      }
+-    }
+-
+-    for (size_t i = 0; i < N; ++i) {
+-      for (size_t idx_byte = 0; idx_byte < sizeof(T); ++idx_byte) {
+-        printf("%zu,", sizeof(T) * indices[i] + idx_byte);
+-      }
+-    }
+-  }
+-  printf("\n");
+-}
+-}  // namespace detail
+-#endif
+-
+-HWY_NOINLINE void TestAllCompress() {
+-  // detail::PrintCompress32x8Tables();
+-  // detail::PrintCompress64x4Tables();
+-  // detail::PrintCompress32x4Tables();
+-  // detail::PrintCompress64x2Tables();
+-  // detail::PrintCompress16x8Tables();
+-
+-  const ForPartialVectors<TestCompress> test;
+-
+-  test(uint16_t());
+-  test(int16_t());
+-  test(float16_t());
+-
+-  test(uint32_t());
+-  test(int32_t());
+-  test(float());
+-
+-#if HWY_CAP_INTEGER64
+-  test(uint64_t());
+-  test(int64_t());
+-#endif
+-#if HWY_CAP_FLOAT64
+-  test(double());
+-#endif
+-}
+-
+ struct TestZeroIfNegative {
+   template <class T, class D>
+   HWY_NOINLINE void operator()(T /*unused*/, D d) {
+@@ -482,7 +186,7 @@ struct TestBroadcastSignBit {
+     const auto s0 = Zero(d);
+     const auto s1 = Set(d, -1);  // all bit set
+     const auto vpos = And(Iota(d, 0), Set(d, LimitsMax<T>()));
+-    const auto vneg = s1 - vpos;
++    const auto vneg = Sub(s1, vpos);
+ 
+     HWY_ASSERT_VEC_EQ(d, s0, BroadcastSignBit(vpos));
+     HWY_ASSERT_VEC_EQ(d, s0, BroadcastSignBit(Set(d, LimitsMax<T>())));
+@@ -507,18 +211,18 @@ struct TestTestBit {
+       const auto bit3 = Set(d, 1ull << ((i + 2) % kNumBits));
+       const auto bits12 = Or(bit1, bit2);
+       const auto bits23 = Or(bit2, bit3);
+-      HWY_ASSERT(AllTrue(TestBit(bit1, bit1)));
+-      HWY_ASSERT(AllTrue(TestBit(bits12, bit1)));
+-      HWY_ASSERT(AllTrue(TestBit(bits12, bit2)));
+-
+-      HWY_ASSERT(AllFalse(TestBit(bits12, bit3)));
+-      HWY_ASSERT(AllFalse(TestBit(bits23, bit1)));
+-      HWY_ASSERT(AllFalse(TestBit(bit1, bit2)));
+-      HWY_ASSERT(AllFalse(TestBit(bit2, bit1)));
+-      HWY_ASSERT(AllFalse(TestBit(bit1, bit3)));
+-      HWY_ASSERT(AllFalse(TestBit(bit3, bit1)));
+-      HWY_ASSERT(AllFalse(TestBit(bit2, bit3)));
+-      HWY_ASSERT(AllFalse(TestBit(bit3, bit2)));
++      HWY_ASSERT(AllTrue(d, TestBit(bit1, bit1)));
++      HWY_ASSERT(AllTrue(d, TestBit(bits12, bit1)));
++      HWY_ASSERT(AllTrue(d, TestBit(bits12, bit2)));
++
++      HWY_ASSERT(AllFalse(d, TestBit(bits12, bit3)));
++      HWY_ASSERT(AllFalse(d, TestBit(bits23, bit1)));
++      HWY_ASSERT(AllFalse(d, TestBit(bit1, bit2)));
++      HWY_ASSERT(AllFalse(d, TestBit(bit2, bit1)));
++      HWY_ASSERT(AllFalse(d, TestBit(bit1, bit3)));
++      HWY_ASSERT(AllFalse(d, TestBit(bit3, bit1)));
++      HWY_ASSERT(AllFalse(d, TestBit(bit2, bit3)));
++      HWY_ASSERT(AllFalse(d, TestBit(bit3, bit2)));
+     }
+   }
+ };
+@@ -527,198 +231,54 @@ HWY_NOINLINE void TestAllTestBit() {
+   ForIntegerTypes(ForPartialVectors<TestTestBit>());
+ }
+ 
+-struct TestAllTrueFalse {
++struct TestPopulationCount {
+   template <class T, class D>
+   HWY_NOINLINE void operator()(T /*unused*/, D d) {
+-    const auto zero = Zero(d);
+-    auto v = zero;
+-
+-    const size_t N = Lanes(d);
+-    auto lanes = AllocateAligned<T>(N);
+-    std::fill(lanes.get(), lanes.get() + N, T(0));
+-
+-    auto mask_lanes = AllocateAligned<T>(N);
+-
+-    HWY_ASSERT(AllTrue(Eq(v, zero)));
+-    HWY_ASSERT(!AllFalse(Eq(v, zero)));
+-
+-    // Single lane implies AllFalse = !AllTrue. Otherwise, there are multiple
+-    // lanes and one is nonzero.
+-    const bool expected_all_false = (N != 1);
+-
+-    // Set each lane to nonzero and back to zero
+-    for (size_t i = 0; i < N; ++i) {
+-      lanes[i] = T(1);
+-      v = Load(d, lanes.get());
+-
+-      // GCC 10.2.1 workaround: AllTrue(Eq(v, zero)) is true but should not be.
+-      // Assigning to an lvalue is insufficient but storing to memory prevents
+-      // the bug; so does Print of VecFromMask(d, Eq(v, zero)).
+-      Store(VecFromMask(d, Eq(v, zero)), d, mask_lanes.get());
+-      HWY_ASSERT(!AllTrue(MaskFromVec(Load(d, mask_lanes.get()))));
+-
+-      HWY_ASSERT(expected_all_false ^ AllFalse(Eq(v, zero)));
+-
+-      lanes[i] = T(-1);
+-      v = Load(d, lanes.get());
+-      HWY_ASSERT(!AllTrue(Eq(v, zero)));
+-      HWY_ASSERT(expected_all_false ^ AllFalse(Eq(v, zero)));
+-
+-      // Reset to all zero
+-      lanes[i] = T(0);
+-      v = Load(d, lanes.get());
+-      HWY_ASSERT(AllTrue(Eq(v, zero)));
+-      HWY_ASSERT(!AllFalse(Eq(v, zero)));
+-    }
+-  }
+-};
+-
+-HWY_NOINLINE void TestAllAllTrueFalse() {
+-  ForAllTypes(ForPartialVectors<TestAllTrueFalse>());
+-}
+-
+-class TestStoreMaskBits {
+- public:
+-  template <class T, class D>
+-  HWY_NOINLINE void operator()(T /*t*/, D d) {
+-    // TODO(janwas): remove once implemented (cast or vse1)
+-#if HWY_TARGET != HWY_RVV
+-    RandomState rng;
+-    const size_t N = Lanes(d);
+-    auto lanes = AllocateAligned<T>(N);
+-    const size_t expected_bytes = (N + 7) / 8;
+-    auto bits = AllocateAligned<uint8_t>(expected_bytes);
+-
+-    for (size_t rep = 0; rep < 100; ++rep) {
+-      // Generate random mask pattern.
+-      for (size_t i = 0; i < N; ++i) {
+-        lanes[i] = static_cast<T>((rng() & 1024) ? 1 : 0);
+-      }
+-      const auto mask = Load(d, lanes.get()) == Zero(d);
+-
+-      const size_t bytes_written = StoreMaskBits(mask, bits.get());
+-
+-      HWY_ASSERT_EQ(expected_bytes, bytes_written);
+-      size_t i = 0;
+-      // Stored bits must match original mask
+-      for (; i < N; ++i) {
+-        const bool bit = (bits[i / 8] & (1 << (i % 8))) != 0;
+-        HWY_ASSERT_EQ(bit, lanes[i] == 0);
+-      }
+-      // Any partial bits in the last byte must be zero
+-      for (; i < 8 * bytes_written; ++i) {
+-        const int bit = (bits[i / 8] & (1 << (i % 8)));
+-        HWY_ASSERT_EQ(bit, 0);
+-      }
+-    }
++#if HWY_TARGET != HWY_RVV && defined(NDEBUG)
++    constexpr size_t kNumTests = 1 << 20;
+ #else
+-    (void)d;
++    constexpr size_t kNumTests = 1 << 14;
+ #endif
+-  }
+-};
+-
+-HWY_NOINLINE void TestAllStoreMaskBits() {
+-  ForAllTypes(ForPartialVectors<TestStoreMaskBits>());
+-}
+-
+-struct TestCountTrue {
+-  template <class T, class D>
+-  HWY_NOINLINE void operator()(T /*unused*/, D d) {
+-    const size_t N = Lanes(d);
+-    // For all combinations of zero/nonzero state of subset of lanes:
+-    const size_t max_lanes = std::min(N, size_t(10));
+-
+-    auto lanes = AllocateAligned<T>(N);
+-    std::fill(lanes.get(), lanes.get() + N, T(1));
+-
+-    for (size_t code = 0; code < (1ull << max_lanes); ++code) {
+-      // Number of zeros written = number of mask lanes that are true.
+-      size_t expected = 0;
+-      for (size_t i = 0; i < max_lanes; ++i) {
+-        lanes[i] = T(1);
+-        if (code & (1ull << i)) {
+-          ++expected;
+-          lanes[i] = T(0);
+-        }
++    RandomState rng;
++    size_t N = Lanes(d);
++    auto data = AllocateAligned<T>(N);
++    auto popcnt = AllocateAligned<T>(N);
++    for (size_t i = 0; i < kNumTests / N; i++) {
++      for (size_t i = 0; i < N; i++) {
++        data[i] = static_cast<T>(rng());
++        popcnt[i] = static_cast<T>(PopCount(data[i]));
+       }
+-
+-      const auto mask = Eq(Load(d, lanes.get()), Zero(d));
+-      const size_t actual = CountTrue(mask);
+-      HWY_ASSERT_EQ(expected, actual);
++      HWY_ASSERT_VEC_EQ(d, popcnt.get(), PopulationCount(Load(d, data.get())));
+     }
+   }
+ };
+ 
+-HWY_NOINLINE void TestAllCountTrue() {
+-  ForAllTypes(ForPartialVectors<TestCountTrue>());
++HWY_NOINLINE void TestAllPopulationCount() {
++  ForUnsignedTypes(ForPartialVectors<TestPopulationCount>());
+ }
+ 
+-struct TestLogicalMask {
+-  template <class T, class D>
+-  HWY_NOINLINE void operator()(T /*unused*/, D d) {
+-    const auto m0 = MaskFalse(d);
+-    const auto m_all = MaskTrue(d);
+-
+-    const size_t N = Lanes(d);
+-    auto lanes = AllocateAligned<T>(N);
+-    std::fill(lanes.get(), lanes.get() + N, T(1));
+-
+-    HWY_ASSERT_MASK_EQ(d, m0, Not(m_all));
+-    HWY_ASSERT_MASK_EQ(d, m_all, Not(m0));
+-
+-    // For all combinations of zero/nonzero state of subset of lanes:
+-    const size_t max_lanes = std::min(N, size_t(6));
+-    for (size_t code = 0; code < (1ull << max_lanes); ++code) {
+-      for (size_t i = 0; i < max_lanes; ++i) {
+-        lanes[i] = T(1);
+-        if (code & (1ull << i)) {
+-          lanes[i] = T(0);
+-        }
+-      }
+-
+-      const auto m = Eq(Load(d, lanes.get()), Zero(d));
+-
+-      HWY_ASSERT_MASK_EQ(d, m0, Xor(m, m));
+-      HWY_ASSERT_MASK_EQ(d, m0, AndNot(m, m));
+-      HWY_ASSERT_MASK_EQ(d, m0, AndNot(m_all, m));
+-
+-      HWY_ASSERT_MASK_EQ(d, m, Or(m, m));
+-      HWY_ASSERT_MASK_EQ(d, m, Or(m0, m));
+-      HWY_ASSERT_MASK_EQ(d, m, Or(m, m0));
+-      HWY_ASSERT_MASK_EQ(d, m, Xor(m0, m));
+-      HWY_ASSERT_MASK_EQ(d, m, Xor(m, m0));
+-      HWY_ASSERT_MASK_EQ(d, m, And(m, m));
+-      HWY_ASSERT_MASK_EQ(d, m, And(m_all, m));
+-      HWY_ASSERT_MASK_EQ(d, m, And(m, m_all));
+-      HWY_ASSERT_MASK_EQ(d, m, AndNot(m0, m));
+-    }
+-  }
+-};
+-
+-HWY_NOINLINE void TestAllLogicalMask() {
+-  ForAllTypes(ForPartialVectors<TestLogicalMask>());
+-}
+ // NOLINTNEXTLINE(google-readability-namespace-comments)
+ }  // namespace HWY_NAMESPACE
+ }  // namespace hwy
+ HWY_AFTER_NAMESPACE();
+ 
+ #if HWY_ONCE
++
+ namespace hwy {
+ HWY_BEFORE_TEST(HwyLogicalTest);
+ HWY_EXPORT_AND_TEST_P(HwyLogicalTest, TestAllLogicalInteger);
+ HWY_EXPORT_AND_TEST_P(HwyLogicalTest, TestAllLogicalFloat);
+ HWY_EXPORT_AND_TEST_P(HwyLogicalTest, TestAllCopySign);
+-HWY_EXPORT_AND_TEST_P(HwyLogicalTest, TestAllFirstN);
+-HWY_EXPORT_AND_TEST_P(HwyLogicalTest, TestAllIfThenElse);
+-HWY_EXPORT_AND_TEST_P(HwyLogicalTest, TestAllMaskVec);
+-HWY_EXPORT_AND_TEST_P(HwyLogicalTest, TestAllCompress);
+ HWY_EXPORT_AND_TEST_P(HwyLogicalTest, TestAllZeroIfNegative);
+ HWY_EXPORT_AND_TEST_P(HwyLogicalTest, TestAllBroadcastSignBit);
+ HWY_EXPORT_AND_TEST_P(HwyLogicalTest, TestAllTestBit);
+-HWY_EXPORT_AND_TEST_P(HwyLogicalTest, TestAllAllTrueFalse);
+-HWY_EXPORT_AND_TEST_P(HwyLogicalTest, TestAllStoreMaskBits);
+-HWY_EXPORT_AND_TEST_P(HwyLogicalTest, TestAllCountTrue);
+-HWY_EXPORT_AND_TEST_P(HwyLogicalTest, TestAllLogicalMask);
++HWY_EXPORT_AND_TEST_P(HwyLogicalTest, TestAllPopulationCount);
+ }  // namespace hwy
++
++// Ought not to be necessary, but without this, no tests run on RVV.
++int main(int argc, char **argv) {
++  ::testing::InitGoogleTest(&argc, argv);
++  return RUN_ALL_TESTS();
++}
++
+ #endif
+diff --git a/third_party/highway/hwy/tests/mask_test.cc b/third_party/highway/hwy/tests/mask_test.cc
+new file mode 100644
+index 0000000000000..b9ab5203e8358
+--- /dev/null
++++ b/third_party/highway/hwy/tests/mask_test.cc
+@@ -0,0 +1,435 @@
++// Copyright 2019 Google LLC
++//
++// Licensed under the Apache License, Version 2.0 (the "License");
++// you may not use this file except in compliance with the License.
++// You may obtain a copy of the License at
++//
++//      http://www.apache.org/licenses/LICENSE-2.0
++//
++// Unless required by applicable law or agreed to in writing, software
++// distributed under the License is distributed on an "AS IS" BASIS,
++// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
++// See the License for the specific language governing permissions and
++// limitations under the License.
++
++#include <stddef.h>
++#include <stdint.h>
++#include <string.h>  // memcmp
++
++#include "hwy/base.h"
++
++#undef HWY_TARGET_INCLUDE
++#define HWY_TARGET_INCLUDE "tests/mask_test.cc"
++#include "hwy/foreach_target.h"
++
++#include "hwy/highway.h"
++#include "hwy/tests/test_util-inl.h"
++
++HWY_BEFORE_NAMESPACE();
++namespace hwy {
++namespace HWY_NAMESPACE {
++
++// All types.
++struct TestFromVec {
++  template <typename T, class D>
++  HWY_NOINLINE void operator()(T /*unused*/, D d) {
++    const size_t N = Lanes(d);
++    auto lanes = AllocateAligned<T>(N);
++
++    memset(lanes.get(), 0, N * sizeof(T));
++    const auto actual_false = MaskFromVec(Load(d, lanes.get()));
++    HWY_ASSERT_MASK_EQ(d, MaskFalse(d), actual_false);
++
++    memset(lanes.get(), 0xFF, N * sizeof(T));
++    const auto actual_true = MaskFromVec(Load(d, lanes.get()));
++    HWY_ASSERT_MASK_EQ(d, MaskTrue(d), actual_true);
++  }
++};
++
++HWY_NOINLINE void TestAllFromVec() {
++  ForAllTypes(ForPartialVectors<TestFromVec>());
++}
++
++struct TestFirstN {
++  template <class T, class D>
++  HWY_NOINLINE void operator()(T /*unused*/, D d) {
++    const size_t N = Lanes(d);
++    auto mask_lanes = AllocateAligned<T>(N);
++
++    // GCC workaround: we previously used zero to indicate true because we can
++    // safely compare with that value. However, that hits an ICE for u64x1 on
++    // GCC 8.3 but not 8.4, even if the implementation of operator== is
++    // simplified to return zero. Using MaskFromVec avoids this, and requires
++    // FF..FF and 0 constants.
++    T on;
++    memset(&on, 0xFF, sizeof(on));
++    const T off = 0;
++
++    for (size_t len = 0; len <= N; ++len) {
++      for (size_t i = 0; i < N; ++i) {
++        mask_lanes[i] = i < len ? on : off;
++      }
++      const auto mask_vals = Load(d, mask_lanes.get());
++      const auto mask = MaskFromVec(mask_vals);
++      HWY_ASSERT_MASK_EQ(d, mask, FirstN(d, len));
++    }
++  }
++};
++
++HWY_NOINLINE void TestAllFirstN() {
++  ForAllTypes(ForPartialVectors<TestFirstN>());
++}
++
++struct TestIfThenElse {
++  template <class T, class D>
++  HWY_NOINLINE void operator()(T /*unused*/, D d) {
++    RandomState rng;
++
++    using TI = MakeSigned<T>;  // For mask > 0 comparison
++    const Rebind<TI, D> di;
++    const size_t N = Lanes(d);
++    auto in1 = AllocateAligned<T>(N);
++    auto in2 = AllocateAligned<T>(N);
++    auto bool_lanes = AllocateAligned<TI>(N);
++    auto expected = AllocateAligned<T>(N);
++
++    // Each lane should have a chance of having mask=true.
++    for (size_t rep = 0; rep < 50; ++rep) {
++      for (size_t i = 0; i < N; ++i) {
++        in1[i] = static_cast<T>(Random32(&rng));
++        in2[i] = static_cast<T>(Random32(&rng));
++        bool_lanes[i] = (Random32(&rng) & 16) ? TI(1) : TI(0);
++      }
++
++      const auto v1 = Load(d, in1.get());
++      const auto v2 = Load(d, in2.get());
++      const auto mask = RebindMask(d, Gt(Load(di, bool_lanes.get()), Zero(di)));
++
++      for (size_t i = 0; i < N; ++i) {
++        expected[i] = bool_lanes[i] ? in1[i] : in2[i];
++      }
++      HWY_ASSERT_VEC_EQ(d, expected.get(), IfThenElse(mask, v1, v2));
++
++      for (size_t i = 0; i < N; ++i) {
++        expected[i] = bool_lanes[i] ? in1[i] : T(0);
++      }
++      HWY_ASSERT_VEC_EQ(d, expected.get(), IfThenElseZero(mask, v1));
++
++      for (size_t i = 0; i < N; ++i) {
++        expected[i] = bool_lanes[i] ? T(0) : in2[i];
++      }
++      HWY_ASSERT_VEC_EQ(d, expected.get(), IfThenZeroElse(mask, v2));
++    }
++  }
++};
++
++HWY_NOINLINE void TestAllIfThenElse() {
++  ForAllTypes(ForPartialVectors<TestIfThenElse>());
++}
++
++struct TestMaskVec {
++  template <class T, class D>
++  HWY_NOINLINE void operator()(T /*unused*/, D d) {
++    RandomState rng;
++
++    using TI = MakeSigned<T>;  // For mask > 0 comparison
++    const Rebind<TI, D> di;
++    const size_t N = Lanes(d);
++    auto bool_lanes = AllocateAligned<TI>(N);
++
++    // Each lane should have a chance of having mask=true.
++    for (size_t rep = 0; rep < 100; ++rep) {
++      for (size_t i = 0; i < N; ++i) {
++        bool_lanes[i] = (Random32(&rng) & 1024) ? TI(1) : TI(0);
++      }
++
++      const auto mask = RebindMask(d, Gt(Load(di, bool_lanes.get()), Zero(di)));
++      HWY_ASSERT_MASK_EQ(d, mask, MaskFromVec(VecFromMask(d, mask)));
++    }
++  }
++};
++
++HWY_NOINLINE void TestAllMaskVec() {
++  const ForPartialVectors<TestMaskVec> test;
++
++  test(uint16_t());
++  test(int16_t());
++  // TODO(janwas): float16_t - cannot compare yet
++
++  test(uint32_t());
++  test(int32_t());
++  test(float());
++
++#if HWY_CAP_INTEGER64
++  test(uint64_t());
++  test(int64_t());
++#endif
++#if HWY_CAP_FLOAT64
++  test(double());
++#endif
++}
++
++struct TestAllTrueFalse {
++  template <class T, class D>
++  HWY_NOINLINE void operator()(T /*unused*/, D d) {
++    const auto zero = Zero(d);
++    auto v = zero;
++
++    const size_t N = Lanes(d);
++    auto lanes = AllocateAligned<T>(N);
++    std::fill(lanes.get(), lanes.get() + N, T(0));
++
++    auto mask_lanes = AllocateAligned<T>(N);
++
++    HWY_ASSERT(AllTrue(d, Eq(v, zero)));
++    HWY_ASSERT(!AllFalse(d, Eq(v, zero)));
++
++    // Single lane implies AllFalse = !AllTrue. Otherwise, there are multiple
++    // lanes and one is nonzero.
++    const bool expected_all_false = (N != 1);
++
++    // Set each lane to nonzero and back to zero
++    for (size_t i = 0; i < N; ++i) {
++      lanes[i] = T(1);
++      v = Load(d, lanes.get());
++
++      // GCC 10.2.1 workaround: AllTrue(Eq(v, zero)) is true but should not be.
++      // Assigning to an lvalue is insufficient but storing to memory prevents
++      // the bug; so does Print of VecFromMask(d, Eq(v, zero)).
++      Store(VecFromMask(d, Eq(v, zero)), d, mask_lanes.get());
++      HWY_ASSERT(!AllTrue(d, MaskFromVec(Load(d, mask_lanes.get()))));
++
++      HWY_ASSERT(expected_all_false ^ AllFalse(d, Eq(v, zero)));
++
++      lanes[i] = T(-1);
++      v = Load(d, lanes.get());
++      HWY_ASSERT(!AllTrue(d, Eq(v, zero)));
++      HWY_ASSERT(expected_all_false ^ AllFalse(d, Eq(v, zero)));
++
++      // Reset to all zero
++      lanes[i] = T(0);
++      v = Load(d, lanes.get());
++      HWY_ASSERT(AllTrue(d, Eq(v, zero)));
++      HWY_ASSERT(!AllFalse(d, Eq(v, zero)));
++    }
++  }
++};
++
++HWY_NOINLINE void TestAllAllTrueFalse() {
++  ForAllTypes(ForPartialVectors<TestAllTrueFalse>());
++}
++
++class TestStoreMaskBits {
++ public:
++  template <class T, class D>
++  HWY_NOINLINE void operator()(T /*t*/, D /*d*/) {
++    // TODO(janwas): remove once implemented (cast or vse1)
++#if HWY_TARGET != HWY_RVV
++    RandomState rng;
++    using TI = MakeSigned<T>;  // For mask > 0 comparison
++    const Rebind<TI, D> di;
++    const size_t N = Lanes(di);
++    auto bool_lanes = AllocateAligned<TI>(N);
++
++    const Half<Half<Half<HWY_FULL(uint8_t)>>> d_bits;
++    const size_t expected_num_bytes = (N + 7) / 8;
++    auto expected = AllocateAligned<uint8_t>(expected_num_bytes);
++    auto actual = AllocateAligned<uint8_t>(expected_num_bytes);
++
++    for (size_t rep = 0; rep < 100; ++rep) {
++      // Generate random mask pattern.
++      for (size_t i = 0; i < N; ++i) {
++        bool_lanes[i] = static_cast<TI>((rng() & 1024) ? 1 : 0);
++      }
++      const auto bools = Load(di, bool_lanes.get());
++      const auto mask = Gt(bools, Zero(di));
++
++      const size_t bytes_written = StoreMaskBits(di, mask, actual.get());
++      if (bytes_written != expected_num_bytes) {
++        fprintf(stderr, "%s expected %zu bytes, actual %zu\n",
++                TypeName(T(), N).c_str(), expected_num_bytes, bytes_written);
++
++        HWY_ASSERT(false);
++      }
++
++      memset(expected.get(), 0, expected_num_bytes);
++      for (size_t i = 0; i < N; ++i) {
++        expected[i / 8] |= bool_lanes[i] << (i % 8);
++      }
++
++      size_t i = 0;
++      // Stored bits must match original mask
++      for (; i < N; ++i) {
++        const TI is_set = (actual[i / 8] & (1 << (i % 8))) ? 1 : 0;
++        if (is_set != bool_lanes[i]) {
++          fprintf(stderr, "%s lane %zu: expected %d, actual %d\n",
++                  TypeName(T(), N).c_str(), i, int(bool_lanes[i]), int(is_set));
++          Print(di, "bools", bools, 0, N);
++          Print(d_bits, "expected bytes", Load(d_bits, expected.get()), 0,
++                expected_num_bytes);
++          Print(d_bits, "actual bytes", Load(d_bits, actual.get()), 0,
++                expected_num_bytes);
++
++          HWY_ASSERT(false);
++        }
++      }
++      // Any partial bits in the last byte must be zero
++      for (; i < 8 * bytes_written; ++i) {
++        const int bit = (actual[i / 8] & (1 << (i % 8)));
++        if (bit != 0) {
++          fprintf(stderr, "%s: bit #%zu should be zero\n",
++                  TypeName(T(), N).c_str(), i);
++          Print(di, "bools", bools, 0, N);
++          Print(d_bits, "expected bytes", Load(d_bits, expected.get()), 0,
++                expected_num_bytes);
++          Print(d_bits, "actual bytes", Load(d_bits, actual.get()), 0,
++                expected_num_bytes);
++
++          HWY_ASSERT(false);
++        }
++      }
++    }
++#endif
++  }
++};
++
++HWY_NOINLINE void TestAllStoreMaskBits() {
++  ForAllTypes(ForPartialVectors<TestStoreMaskBits>());
++}
++
++struct TestCountTrue {
++  template <class T, class D>
++  HWY_NOINLINE void operator()(T /*unused*/, D d) {
++    using TI = MakeSigned<T>;  // For mask > 0 comparison
++    const Rebind<TI, D> di;
++    const size_t N = Lanes(di);
++    auto bool_lanes = AllocateAligned<TI>(N);
++    memset(bool_lanes.get(), 0, N * sizeof(TI));
++
++    // For all combinations of zero/nonzero state of subset of lanes:
++    const size_t max_lanes = HWY_MIN(N, size_t(10));
++
++    for (size_t code = 0; code < (1ull << max_lanes); ++code) {
++      // Number of zeros written = number of mask lanes that are true.
++      size_t expected = 0;
++      for (size_t i = 0; i < max_lanes; ++i) {
++        const bool is_true = (code & (1ull << i)) != 0;
++        bool_lanes[i] = is_true ? TI(1) : TI(0);
++        expected += is_true;
++      }
++
++      const auto mask = RebindMask(d, Gt(Load(di, bool_lanes.get()), Zero(di)));
++      const size_t actual = CountTrue(d, mask);
++      HWY_ASSERT_EQ(expected, actual);
++    }
++  }
++};
++
++HWY_NOINLINE void TestAllCountTrue() {
++  ForAllTypes(ForPartialVectors<TestCountTrue>());
++}
++
++struct TestFindFirstTrue {
++  template <class T, class D>
++  HWY_NOINLINE void operator()(T /*unused*/, D d) {
++    using TI = MakeSigned<T>;  // For mask > 0 comparison
++    const Rebind<TI, D> di;
++    const size_t N = Lanes(di);
++    auto bool_lanes = AllocateAligned<TI>(N);
++    memset(bool_lanes.get(), 0, N * sizeof(TI));
++
++    // For all combinations of zero/nonzero state of subset of lanes:
++    const size_t max_lanes = HWY_MIN(N, size_t(10));
++
++    HWY_ASSERT_EQ(intptr_t(-1), FindFirstTrue(d, MaskFalse(d)));
++    HWY_ASSERT_EQ(intptr_t(0), FindFirstTrue(d, MaskTrue(d)));
++
++    for (size_t code = 1; code < (1ull << max_lanes); ++code) {
++      for (size_t i = 0; i < max_lanes; ++i) {
++        bool_lanes[i] = (code & (1ull << i)) ? TI(1) : TI(0);
++      }
++
++      const intptr_t expected =
++          static_cast<intptr_t>(Num0BitsBelowLS1Bit_Nonzero32(code));
++      const auto mask = RebindMask(d, Gt(Load(di, bool_lanes.get()), Zero(di)));
++      const intptr_t actual = FindFirstTrue(d, mask);
++      HWY_ASSERT_EQ(expected, actual);
++    }
++  }
++};
++
++HWY_NOINLINE void TestAllFindFirstTrue() {
++  ForAllTypes(ForPartialVectors<TestFindFirstTrue>());
++}
++
++struct TestLogicalMask {
++  template <class T, class D>
++  HWY_NOINLINE void operator()(T /*unused*/, D d) {
++    const auto m0 = MaskFalse(d);
++    const auto m_all = MaskTrue(d);
++
++    using TI = MakeSigned<T>;  // For mask > 0 comparison
++    const Rebind<TI, D> di;
++    const size_t N = Lanes(di);
++    auto bool_lanes = AllocateAligned<TI>(N);
++    memset(bool_lanes.get(), 0, N * sizeof(TI));
++
++    HWY_ASSERT_MASK_EQ(d, m0, Not(m_all));
++    HWY_ASSERT_MASK_EQ(d, m_all, Not(m0));
++
++    // For all combinations of zero/nonzero state of subset of lanes:
++    const size_t max_lanes = HWY_MIN(N, size_t(6));
++    for (size_t code = 0; code < (1ull << max_lanes); ++code) {
++      for (size_t i = 0; i < max_lanes; ++i) {
++        bool_lanes[i] = (code & (1ull << i)) ? TI(1) : TI(0);
++      }
++
++      const auto m = RebindMask(d, Gt(Load(di, bool_lanes.get()), Zero(di)));
++
++      HWY_ASSERT_MASK_EQ(d, m0, Xor(m, m));
++      HWY_ASSERT_MASK_EQ(d, m0, AndNot(m, m));
++      HWY_ASSERT_MASK_EQ(d, m0, AndNot(m_all, m));
++
++      HWY_ASSERT_MASK_EQ(d, m, Or(m, m));
++      HWY_ASSERT_MASK_EQ(d, m, Or(m0, m));
++      HWY_ASSERT_MASK_EQ(d, m, Or(m, m0));
++      HWY_ASSERT_MASK_EQ(d, m, Xor(m0, m));
++      HWY_ASSERT_MASK_EQ(d, m, Xor(m, m0));
++      HWY_ASSERT_MASK_EQ(d, m, And(m, m));
++      HWY_ASSERT_MASK_EQ(d, m, And(m_all, m));
++      HWY_ASSERT_MASK_EQ(d, m, And(m, m_all));
++      HWY_ASSERT_MASK_EQ(d, m, AndNot(m0, m));
++    }
++  }
++};
++
++HWY_NOINLINE void TestAllLogicalMask() {
++  ForAllTypes(ForPartialVectors<TestLogicalMask>());
++}
++// NOLINTNEXTLINE(google-readability-namespace-comments)
++}  // namespace HWY_NAMESPACE
++}  // namespace hwy
++HWY_AFTER_NAMESPACE();
++
++#if HWY_ONCE
++
++namespace hwy {
++HWY_BEFORE_TEST(HwyMaskTest);
++HWY_EXPORT_AND_TEST_P(HwyMaskTest, TestAllFromVec);
++HWY_EXPORT_AND_TEST_P(HwyMaskTest, TestAllFirstN);
++HWY_EXPORT_AND_TEST_P(HwyMaskTest, TestAllIfThenElse);
++HWY_EXPORT_AND_TEST_P(HwyMaskTest, TestAllMaskVec);
++HWY_EXPORT_AND_TEST_P(HwyMaskTest, TestAllAllTrueFalse);
++HWY_EXPORT_AND_TEST_P(HwyMaskTest, TestAllStoreMaskBits);
++HWY_EXPORT_AND_TEST_P(HwyMaskTest, TestAllCountTrue);
++HWY_EXPORT_AND_TEST_P(HwyMaskTest, TestAllFindFirstTrue);
++HWY_EXPORT_AND_TEST_P(HwyMaskTest, TestAllLogicalMask);
++}  // namespace hwy
++
++// Ought not to be necessary, but without this, no tests run on RVV.
++int main(int argc, char **argv) {
++  ::testing::InitGoogleTest(&argc, argv);
++  return RUN_ALL_TESTS();
++}
++
++#endif
+diff --git a/third_party/highway/hwy/tests/memory_test.cc b/third_party/highway/hwy/tests/memory_test.cc
+index 31bd7dbce52d1..f20e48d108b6c 100644
+--- a/third_party/highway/hwy/tests/memory_test.cc
++++ b/third_party/highway/hwy/tests/memory_test.cc
+@@ -12,6 +12,12 @@
+ // See the License for the specific language governing permissions and
+ // limitations under the License.
+ 
++// Ensure incompabilities with Windows macros (e.g. #define StoreFence) are
++// detected. Must come before Highway headers.
++#if defined(_WIN32) || defined(_WIN64)
++#include <windows.h>
++#endif
++
+ #include <stddef.h>
+ #include <stdint.h>
+ 
+@@ -281,8 +287,8 @@ struct TestScatter {
+       std::fill(expected.get(), expected.get() + range, T(0));
+       std::fill(actual.get(), actual.get() + range, T(0));
+       for (size_t i = 0; i < N; ++i) {
+-        offsets[i] =
+-            static_cast<Offset>(Random32(&rng) % (max_bytes - sizeof(T)));
++        // Must be aligned
++        offsets[i] = static_cast<Offset>((Random32(&rng) % range) * sizeof(T));
+         CopyBytes<sizeof(T)>(
+             bytes.get() + i * sizeof(T),
+             reinterpret_cast<uint8_t*>(expected.get()) + offsets[i]);
+@@ -334,11 +340,12 @@ struct TestGather {
+     using Offset = MakeSigned<T>;
+ 
+     const size_t N = Lanes(d);
++    const size_t range = 4 * N;                  // number of items to gather
++    const size_t max_bytes = range * sizeof(T);  // upper bound on offset
+ 
+     RandomState rng;
+ 
+     // Data to be gathered from
+-    const size_t max_bytes = 4 * N * sizeof(T);  // upper bound on offset
+     auto bytes = AllocateAligned<uint8_t>(max_bytes);
+     for (size_t i = 0; i < max_bytes; ++i) {
+       bytes[i] = static_cast<uint8_t>(Random32(&rng) & 0xFF);
+@@ -351,8 +358,8 @@ struct TestGather {
+     for (size_t rep = 0; rep < 100; ++rep) {
+       // Offsets
+       for (size_t i = 0; i < N; ++i) {
+-        offsets[i] =
+-            static_cast<Offset>(Random32(&rng) % (max_bytes - sizeof(T)));
++        // Must be aligned
++        offsets[i] = static_cast<Offset>((Random32(&rng) % range) * sizeof(T));
+         CopyBytes<sizeof(T)>(bytes.get() + offsets[i], &expected[i]);
+       }
+ 
+@@ -401,6 +408,7 @@ HWY_NOINLINE void TestAllCache() {
+ HWY_AFTER_NAMESPACE();
+ 
+ #if HWY_ONCE
++
+ namespace hwy {
+ HWY_BEFORE_TEST(HwyMemoryTest);
+ HWY_EXPORT_AND_TEST_P(HwyMemoryTest, TestAllLoadStore);
+@@ -412,4 +420,11 @@ HWY_EXPORT_AND_TEST_P(HwyMemoryTest, TestAllScatter);
+ HWY_EXPORT_AND_TEST_P(HwyMemoryTest, TestAllGather);
+ HWY_EXPORT_AND_TEST_P(HwyMemoryTest, TestAllCache);
+ }  // namespace hwy
++
++// Ought not to be necessary, but without this, no tests run on RVV.
++int main(int argc, char** argv) {
++  ::testing::InitGoogleTest(&argc, argv);
++  return RUN_ALL_TESTS();
++}
++
+ #endif
+diff --git a/third_party/highway/hwy/tests/swizzle_test.cc b/third_party/highway/hwy/tests/swizzle_test.cc
+index 565dc115e4abf..f7ebbc59407d9 100644
+--- a/third_party/highway/hwy/tests/swizzle_test.cc
++++ b/third_party/highway/hwy/tests/swizzle_test.cc
+@@ -26,202 +26,34 @@ HWY_BEFORE_NAMESPACE();
+ namespace hwy {
+ namespace HWY_NAMESPACE {
+ 
+-struct TestShiftBytes {
++struct TestGetLane {
+   template <class T, class D>
+   HWY_NOINLINE void operator()(T /*unused*/, D d) {
+-    // Scalar does not define Shift*Bytes.
+-#if HWY_TARGET != HWY_SCALAR || HWY_IDE
+-    const Repartition<uint8_t, D> du8;
+-    const size_t N8 = Lanes(du8);
+-
+-    // Zero remains zero
+-    const auto v0 = Zero(d);
+-    HWY_ASSERT_VEC_EQ(d, v0, ShiftLeftBytes<1>(v0));
+-    HWY_ASSERT_VEC_EQ(d, v0, ShiftRightBytes<1>(v0));
+-
+-    // Zero after shifting out the high/low byte
+-    auto bytes = AllocateAligned<uint8_t>(N8);
+-    std::fill(bytes.get(), bytes.get() + N8, 0);
+-    bytes[N8 - 1] = 0x7F;
+-    const auto vhi = BitCast(d, Load(du8, bytes.get()));
+-    bytes[N8 - 1] = 0;
+-    bytes[0] = 0x7F;
+-    const auto vlo = BitCast(d, Load(du8, bytes.get()));
+-    HWY_ASSERT_VEC_EQ(d, v0, ShiftLeftBytes<1>(vhi));
+-    HWY_ASSERT_VEC_EQ(d, v0, ShiftRightBytes<1>(vlo));
+-
+-    // Check expected result with Iota
+-    const size_t N = Lanes(d);
+-    auto in = AllocateAligned<T>(N);
+-    const uint8_t* in_bytes = reinterpret_cast<const uint8_t*>(in.get());
+-    const auto v = BitCast(d, Iota(du8, 1));
+-    Store(v, d, in.get());
+-
+-    auto expected = AllocateAligned<T>(N);
+-    uint8_t* expected_bytes = reinterpret_cast<uint8_t*>(expected.get());
+-
+-    const size_t kBlockSize = HWY_MIN(N8, 16);
+-    for (size_t block = 0; block < N8; block += kBlockSize) {
+-      expected_bytes[block] = 0;
+-      memcpy(expected_bytes + block + 1, in_bytes + block, kBlockSize - 1);
+-    }
+-    HWY_ASSERT_VEC_EQ(d, expected.get(), ShiftLeftBytes<1>(v));
+-
+-    for (size_t block = 0; block < N8; block += kBlockSize) {
+-      memcpy(expected_bytes + block, in_bytes + block + 1, kBlockSize - 1);
+-      expected_bytes[block + kBlockSize - 1] = 0;
+-    }
+-    HWY_ASSERT_VEC_EQ(d, expected.get(), ShiftRightBytes<1>(v));
+-#else
+-    (void)d;
+-#endif  // #if HWY_TARGET != HWY_SCALAR
+-  }
+-};
+-
+-HWY_NOINLINE void TestAllShiftBytes() {
+-  ForIntegerTypes(ForGE128Vectors<TestShiftBytes>());
+-}
+-
+-struct TestShiftLanes {
+-  template <class T, class D>
+-  HWY_NOINLINE void operator()(T /*unused*/, D d) {
+-    // Scalar does not define Shift*Lanes.
+-#if HWY_TARGET != HWY_SCALAR || HWY_IDE
+     const auto v = Iota(d, T(1));
+-    const size_t N = Lanes(d);
+-    auto expected = AllocateAligned<T>(N);
+-
+-    HWY_ASSERT_VEC_EQ(d, v, ShiftLeftLanes<0>(v));
+-    HWY_ASSERT_VEC_EQ(d, v, ShiftRightLanes<0>(v));
+-
+-    constexpr size_t kLanesPerBlock = 16 / sizeof(T);
+-
+-    for (size_t i = 0; i < N; ++i) {
+-      expected[i] = (i % kLanesPerBlock) == 0 ? T(0) : T(i);
+-    }
+-    HWY_ASSERT_VEC_EQ(d, expected.get(), ShiftLeftLanes<1>(v));
+-
+-    for (size_t i = 0; i < N; ++i) {
+-      expected[i] =
+-          (i % kLanesPerBlock) == (kLanesPerBlock - 1) ? T(0) : T(2 + i);
+-    }
+-    HWY_ASSERT_VEC_EQ(d, expected.get(), ShiftRightLanes<1>(v));
+-#else
+-    (void)d;
+-#endif  // #if HWY_TARGET != HWY_SCALAR
++    HWY_ASSERT_EQ(T(1), GetLane(v));
+   }
+ };
+ 
+-HWY_NOINLINE void TestAllShiftLanes() {
+-  ForAllTypes(ForGE128Vectors<TestShiftLanes>());
+-}
+-
+-template <typename D, int kLane>
+-struct TestBroadcastR {
+-  HWY_NOINLINE void operator()() const {
+-// TODO(janwas): fix failure
+-#if HWY_TARGET != HWY_WASM
+-    using T = typename D::T;
+-    const D d;
+-    const size_t N = Lanes(d);
+-    auto in_lanes = AllocateAligned<T>(N);
+-    std::fill(in_lanes.get(), in_lanes.get() + N, T(0));
+-    const size_t blockN = HWY_MIN(N * sizeof(T), 16) / sizeof(T);
+-    // Need to set within each 128-bit block
+-    for (size_t block = 0; block < N; block += blockN) {
+-      in_lanes[block + kLane] = static_cast<T>(block + 1);
+-    }
+-    const auto in = Load(d, in_lanes.get());
+-    auto expected = AllocateAligned<T>(N);
+-    for (size_t block = 0; block < N; block += blockN) {
+-      for (size_t i = 0; i < blockN; ++i) {
+-        expected[block + i] = T(block + 1);
+-      }
+-    }
+-    HWY_ASSERT_VEC_EQ(d, expected.get(), Broadcast<kLane>(in));
+-
+-    TestBroadcastR<D, kLane - 1>()();
+-#endif
+-  }
+-};
+-
+-template <class D>
+-struct TestBroadcastR<D, -1> {
+-  void operator()() const {}
+-};
+-
+-struct TestBroadcast {
+-  template <class T, class D>
+-  HWY_NOINLINE void operator()(T /*unused*/, D d) {
+-    TestBroadcastR<D, HWY_MIN(MaxLanes(d), 16 / sizeof(T)) - 1>()();
+-  }
+-};
+-
+-HWY_NOINLINE void TestAllBroadcast() {
+-  const ForPartialVectors<TestBroadcast> test;
+-  // No u8.
+-  test(uint16_t());
+-  test(uint32_t());
+-#if HWY_CAP_INTEGER64
+-  test(uint64_t());
+-#endif
+-
+-  // No i8.
+-  test(int16_t());
+-  test(int32_t());
+-#if HWY_CAP_INTEGER64
+-  test(int64_t());
+-#endif
+-
+-  ForFloatTypes(test);
++HWY_NOINLINE void TestAllGetLane() {
++  ForAllTypes(ForPartialVectors<TestGetLane>());
+ }
+ 
+-struct TestTableLookupBytes {
++struct TestOddEven {
+   template <class T, class D>
+   HWY_NOINLINE void operator()(T /*unused*/, D d) {
+-    RandomState rng;
+     const size_t N = Lanes(d);
+-    const size_t N8 = Lanes(Repartition<uint8_t, D>());
+-    auto in_bytes = AllocateAligned<uint8_t>(N8);
+-    for (size_t i = 0; i < N8; ++i) {
+-      in_bytes[i] = Random32(&rng) & 0xFF;
+-    }
+-    const auto in =
+-        BitCast(d, Load(d, reinterpret_cast<const T*>(in_bytes.get())));
+-
+-    // Enough test data; for larger vectors, upper lanes will be zero.
+-    const uint8_t index_bytes_source[64] = {
+-        // Same index as source, multiple outputs from same input,
+-        // unused input (9), ascending/descending and nonconsecutive neighbors.
+-        0,  2,  1, 2, 15, 12, 13, 14, 6,  7,  8,  5,  4,  3,  10, 11,
+-        11, 10, 3, 4, 5,  8,  7,  6,  14, 13, 12, 15, 2,  1,  2,  0,
+-        4,  3,  2, 2, 5,  6,  7,  7,  15, 15, 15, 15, 15, 15, 0,  1};
+-    auto index_bytes = AllocateAligned<uint8_t>(N8);
+-    for (size_t i = 0; i < N8; ++i) {
+-      index_bytes[i] = (i < 64) ? index_bytes_source[i] : 0;
+-      // Avoid undefined results / asan error for scalar by capping indices.
+-      if (index_bytes[i] >= N * sizeof(T)) {
+-        index_bytes[i] = static_cast<uint8_t>(N * sizeof(T) - 1);
+-      }
+-    }
+-    const auto indices = Load(d, reinterpret_cast<const T*>(index_bytes.get()));
++    const auto even = Iota(d, 1);
++    const auto odd = Iota(d, 1 + N);
+     auto expected = AllocateAligned<T>(N);
+-    uint8_t* expected_bytes = reinterpret_cast<uint8_t*>(expected.get());
+-
+-    // Byte indices wrap around
+-    const size_t mod = HWY_MIN(N8, 256);
+-    for (size_t block = 0; block < N8; block += 16) {
+-      for (size_t i = 0; i < 16 && (block + i) < N8; ++i) {
+-        const uint8_t index = index_bytes[block + i];
+-        expected_bytes[block + i] = in_bytes[(block + index) % mod];
+-      }
++    for (size_t i = 0; i < N; ++i) {
++      expected[i] = static_cast<T>(1 + i + ((i & 1) ? N : 0));
+     }
+-    HWY_ASSERT_VEC_EQ(d, expected.get(), TableLookupBytes(in, indices));
++    HWY_ASSERT_VEC_EQ(d, expected.get(), OddEven(odd, even));
+   }
+ };
+ 
+-HWY_NOINLINE void TestAllTableLookupBytes() {
+-  ForIntegerTypes(ForPartialVectors<TestTableLookupBytes>());
++HWY_NOINLINE void TestAllOddEven() {
++  ForAllTypes(ForShrinkableVectors<TestOddEven>());
+ }
+ 
+ struct TestTableLookupLanes {
+@@ -294,330 +126,204 @@ HWY_NOINLINE void TestAllTableLookupLanes() {
+   test(float());
+ }
+ 
+-struct TestInterleave {
++struct TestCompress {
+   template <class T, class D>
+   HWY_NOINLINE void operator()(T /*unused*/, D d) {
+-    using TU = MakeUnsigned<T>;
++    RandomState rng;
++
++    using TI = MakeSigned<T>;  // For mask > 0 comparison
++    const Rebind<TI, D> di;
+     const size_t N = Lanes(d);
+-    auto even_lanes = AllocateAligned<T>(N);
+-    auto odd_lanes = AllocateAligned<T>(N);
++    auto in_lanes = AllocateAligned<T>(N);
++    auto mask_lanes = AllocateAligned<TI>(N);
+     auto expected = AllocateAligned<T>(N);
+-    for (size_t i = 0; i < N; ++i) {
+-      even_lanes[i] = static_cast<T>(2 * i + 0);
+-      odd_lanes[i] = static_cast<T>(2 * i + 1);
+-    }
+-    const auto even = Load(d, even_lanes.get());
+-    const auto odd = Load(d, odd_lanes.get());
+-
+-    const size_t blockN = 16 / sizeof(T);
+-    for (size_t i = 0; i < Lanes(d); ++i) {
+-      const size_t block = i / blockN;
+-      const size_t index = (i % blockN) + block * 2 * blockN;
+-      expected[i] = static_cast<T>(index & LimitsMax<TU>());
+-    }
+-    HWY_ASSERT_VEC_EQ(d, expected.get(), InterleaveLower(even, odd));
++    auto actual = AllocateAligned<T>(N);
+ 
+-    for (size_t i = 0; i < Lanes(d); ++i) {
+-      const size_t block = i / blockN;
+-      expected[i] = T((i % blockN) + block * 2 * blockN + blockN);
+-    }
+-    HWY_ASSERT_VEC_EQ(d, expected.get(), InterleaveUpper(even, odd));
+-  }
+-};
++    // Each lane should have a chance of having mask=true.
++    for (size_t rep = 0; rep < 100; ++rep) {
++      size_t expected_pos = 0;
++      for (size_t i = 0; i < N; ++i) {
++        const uint64_t bits = Random32(&rng);
++        in_lanes[i] = T();  // cannot initialize float16_t directly.
++        CopyBytes<sizeof(T)>(&bits, &in_lanes[i]);
++        mask_lanes[i] = (Random32(&rng) & 1024) ? TI(1) : TI(0);
++        if (mask_lanes[i] > 0) {
++          expected[expected_pos++] = in_lanes[i];
++        }
++      }
+ 
+-HWY_NOINLINE void TestAllInterleave() {
+-  // Not supported by HWY_SCALAR: Interleave(f32, f32) would return f32x2.
+-  ForAllTypes(ForGE128Vectors<TestInterleave>());
+-}
++      const auto in = Load(d, in_lanes.get());
++      const auto mask = RebindMask(d, Gt(Load(di, mask_lanes.get()), Zero(di)));
++
++      Store(Compress(in, mask), d, actual.get());
++      // Upper lanes are undefined. Modified from AssertVecEqual.
++      for (size_t i = 0; i < expected_pos; ++i) {
++        if (!IsEqual(expected[i], actual[i])) {
++          fprintf(stderr, "Mismatch at i=%zu of %zu:\n\n", i, expected_pos);
++          Print(di, "mask", Load(di, mask_lanes.get()), 0, N);
++          Print(d, "in", in, 0, N);
++          Print(d, "expect", Load(d, expected.get()), 0, N);
++          Print(d, "actual", Load(d, actual.get()), 0, N);
++          HWY_ASSERT(false);
++        }
++      }
+ 
+-struct TestZipLower {
+-  template <class T, class D>
+-  HWY_NOINLINE void operator()(T /*unused*/, D d) {
+-    using WideT = MakeWide<T>;
+-    static_assert(sizeof(T) * 2 == sizeof(WideT), "Must be double-width");
+-    static_assert(IsSigned<T>() == IsSigned<WideT>(), "Must have same sign");
+-    const size_t N = Lanes(d);
+-    auto even_lanes = AllocateAligned<T>(N);
+-    auto odd_lanes = AllocateAligned<T>(N);
+-    for (size_t i = 0; i < N; ++i) {
+-      even_lanes[i] = static_cast<T>(2 * i + 0);
+-      odd_lanes[i] = static_cast<T>(2 * i + 1);
+-    }
+-    const auto even = Load(d, even_lanes.get());
+-    const auto odd = Load(d, odd_lanes.get());
+-
+-    const Repartition<WideT, D> dw;
+-    auto expected = AllocateAligned<WideT>(Lanes(dw));
+-    const WideT blockN = static_cast<WideT>(16 / sizeof(WideT));
+-    for (size_t i = 0; i < Lanes(dw); ++i) {
+-      const size_t block = i / blockN;
+-      // Value of least-significant lane in lo-vector.
+-      const WideT lo =
+-          static_cast<WideT>(2 * (i % blockN) + 4 * block * blockN);
+-      const WideT kBits = static_cast<WideT>(sizeof(T) * 8);
+-      expected[i] =
+-          static_cast<WideT>((static_cast<WideT>(lo + 1) << kBits) + lo);
++      // Also check CompressStore in the same way.
++      memset(actual.get(), 0, N * sizeof(T));
++      const size_t num_written = CompressStore(in, mask, d, actual.get());
++      HWY_ASSERT_EQ(expected_pos, num_written);
++      for (size_t i = 0; i < expected_pos; ++i) {
++        if (!IsEqual(expected[i], actual[i])) {
++          fprintf(stderr, "Mismatch at i=%zu of %zu:\n\n", i, expected_pos);
++          Print(di, "mask", Load(di, mask_lanes.get()), 0, N);
++          Print(d, "in", in, 0, N);
++          Print(d, "expect", Load(d, expected.get()), 0, N);
++          Print(d, "actual", Load(d, actual.get()), 0, N);
++          HWY_ASSERT(false);
++        }
++      }
+     }
+-    HWY_ASSERT_VEC_EQ(dw, expected.get(), ZipLower(even, odd));
+   }
+ };
+ 
+-struct TestZipUpper {
+-  template <class T, class D>
+-  HWY_NOINLINE void operator()(T /*unused*/, D d) {
+-    using WideT = MakeWide<T>;
+-    static_assert(sizeof(T) * 2 == sizeof(WideT), "Must be double-width");
+-    static_assert(IsSigned<T>() == IsSigned<WideT>(), "Must have same sign");
+-    const size_t N = Lanes(d);
+-    auto even_lanes = AllocateAligned<T>(N);
+-    auto odd_lanes = AllocateAligned<T>(N);
+-    for (size_t i = 0; i < Lanes(d); ++i) {
+-      even_lanes[i] = static_cast<T>(2 * i + 0);
+-      odd_lanes[i] = static_cast<T>(2 * i + 1);
++// For regenerating tables used in the implementation
++#if 0
++namespace detail {  // for code folding
++void PrintCompress16x8Tables() {
++  constexpr size_t N = 8;  // 128-bit SIMD
++  for (uint64_t code = 0; code < 1ull << N; ++code) {
++    std::array<uint8_t, N> indices{0};
++    size_t pos = 0;
++    for (size_t i = 0; i < N; ++i) {
++      if (code & (1ull << i)) {
++        indices[pos++] = i;
++      }
+     }
+-    const auto even = Load(d, even_lanes.get());
+-    const auto odd = Load(d, odd_lanes.get());
+-
+-    const Repartition<WideT, D> dw;
+-    auto expected = AllocateAligned<WideT>(Lanes(dw));
+-
+-    constexpr WideT blockN = static_cast<WideT>(16 / sizeof(WideT));
+-    for (size_t i = 0; i < Lanes(dw); ++i) {
+-      const size_t block = i / blockN;
+-      const WideT lo =
+-          static_cast<WideT>(2 * (i % blockN) + 4 * block * blockN);
+-      const WideT kBits = static_cast<WideT>(sizeof(T) * 8);
+-      expected[i] = static_cast<WideT>(
+-          (static_cast<WideT>(lo + 2 * blockN + 1) << kBits) + lo + 2 * blockN);
++
++    // Doubled (for converting lane to byte indices)
++    for (size_t i = 0; i < N; ++i) {
++      printf("%d,", 2 * indices[i]);
+     }
+-    HWY_ASSERT_VEC_EQ(dw, expected.get(), ZipUpper(even, odd));
+   }
+-};
+-
+-HWY_NOINLINE void TestAllZip() {
+-  const ForPartialVectors<TestZipLower, 2> lower_unsigned;
+-  // TODO(janwas): fix
+-#if HWY_TARGET != HWY_RVV
+-  lower_unsigned(uint8_t());
+-#endif
+-  lower_unsigned(uint16_t());
+-#if HWY_CAP_INTEGER64
+-  lower_unsigned(uint32_t());  // generates u64
+-#endif
+-
+-  const ForPartialVectors<TestZipLower, 2> lower_signed;
+-#if HWY_TARGET != HWY_RVV
+-  lower_signed(int8_t());
+-#endif
+-  lower_signed(int16_t());
+-#if HWY_CAP_INTEGER64
+-  lower_signed(int32_t());  // generates i64
+-#endif
+-
+-  const ForGE128Vectors<TestZipUpper> upper_unsigned;
+-#if HWY_TARGET != HWY_RVV
+-  upper_unsigned(uint8_t());
+-#endif
+-  upper_unsigned(uint16_t());
+-#if HWY_CAP_INTEGER64
+-  upper_unsigned(uint32_t());  // generates u64
+-#endif
+-
+-  const ForGE128Vectors<TestZipUpper> upper_signed;
+-#if HWY_TARGET != HWY_RVV
+-  upper_signed(int8_t());
+-#endif
+-  upper_signed(int16_t());
+-#if HWY_CAP_INTEGER64
+-  upper_signed(int32_t());  // generates i64
+-#endif
+-
+-  // No float - concatenating f32 does not result in a f64
++  printf("\n");
+ }
+ 
+-class TestSpecialShuffle32 {
+- public:
+-  template <class T, class D>
+-  HWY_NOINLINE void operator()(T /*unused*/, D d) {
+-    const auto v = Iota(d, 0);
+-
+-#define VERIFY_LANES_32(d, v, i3, i2, i1, i0) \
+-  VerifyLanes32((d), (v), (i3), (i2), (i1), (i0), __FILE__, __LINE__)
++// Compressed to nibbles
++void PrintCompress32x8Tables() {
++  constexpr size_t N = 8;  // AVX2
++  for (uint64_t code = 0; code < 1ull << N; ++code) {
++    std::array<uint32_t, N> indices{0};
++    size_t pos = 0;
++    for (size_t i = 0; i < N; ++i) {
++      if (code & (1ull << i)) {
++        indices[pos++] = i;
++      }
++    }
+ 
+-    VERIFY_LANES_32(d, Shuffle2301(v), 2, 3, 0, 1);
+-    VERIFY_LANES_32(d, Shuffle1032(v), 1, 0, 3, 2);
+-    VERIFY_LANES_32(d, Shuffle0321(v), 0, 3, 2, 1);
+-    VERIFY_LANES_32(d, Shuffle2103(v), 2, 1, 0, 3);
+-    VERIFY_LANES_32(d, Shuffle0123(v), 0, 1, 2, 3);
++    // Convert to nibbles
++    uint64_t packed = 0;
++    for (size_t i = 0; i < N; ++i) {
++      HWY_ASSERT(indices[i] < 16);
++      packed += indices[i] << (i * 4);
++    }
+ 
+-#undef VERIFY_LANES_32
++    HWY_ASSERT(packed < (1ull << 32));
++    printf("0x%08x,", static_cast<uint32_t>(packed));
+   }
++  printf("\n");
++}
+ 
+- private:
+-  template <class D, class V>
+-  HWY_NOINLINE void VerifyLanes32(D d, V v, const int i3, const int i2,
+-                                  const int i1, const int i0,
+-                                  const char* filename, const int line) {
+-    using T = typename D::T;
+-    const size_t N = Lanes(d);
+-    auto lanes = AllocateAligned<T>(N);
+-    Store(v, d, lanes.get());
+-    const std::string name = TypeName(lanes[0], N);
+-    constexpr size_t kBlockN = 16 / sizeof(T);
+-    for (int block = 0; block < static_cast<int>(N); block += kBlockN) {
+-      AssertEqual(T(block + i3), lanes[block + 3], name, filename, line);
+-      AssertEqual(T(block + i2), lanes[block + 2], name, filename, line);
+-      AssertEqual(T(block + i1), lanes[block + 1], name, filename, line);
+-      AssertEqual(T(block + i0), lanes[block + 0], name, filename, line);
++// Pairs of 32-bit lane indices
++void PrintCompress64x4Tables() {
++  constexpr size_t N = 4;  // AVX2
++  for (uint64_t code = 0; code < 1ull << N; ++code) {
++    std::array<uint32_t, N> indices{0};
++    size_t pos = 0;
++    for (size_t i = 0; i < N; ++i) {
++      if (code & (1ull << i)) {
++        indices[pos++] = i;
++      }
+     }
+-  }
+-};
+ 
+-class TestSpecialShuffle64 {
+- public:
+-  template <class T, class D>
+-  HWY_NOINLINE void operator()(T /*unused*/, D d) {
+-    const auto v = Iota(d, 0);
+-    VerifyLanes64(d, Shuffle01(v), 0, 1, __FILE__, __LINE__);
+-  }
+-
+- private:
+-  template <class D, class V>
+-  HWY_NOINLINE void VerifyLanes64(D d, V v, const int i1, const int i0,
+-                                  const char* filename, const int line) {
+-    using T = typename D::T;
+-    const size_t N = Lanes(d);
+-    auto lanes = AllocateAligned<T>(N);
+-    Store(v, d, lanes.get());
+-    const std::string name = TypeName(lanes[0], N);
+-    constexpr size_t kBlockN = 16 / sizeof(T);
+-    for (int block = 0; block < static_cast<int>(N); block += kBlockN) {
+-      AssertEqual(T(block + i1), lanes[block + 1], name, filename, line);
+-      AssertEqual(T(block + i0), lanes[block + 0], name, filename, line);
++    for (size_t i = 0; i < N; ++i) {
++      printf("%d,%d,", 2 * indices[i], 2 * indices[i] + 1);
+     }
+   }
+-};
+-
+-HWY_NOINLINE void TestAllSpecialShuffles() {
+-  const ForGE128Vectors<TestSpecialShuffle32> test32;
+-  test32(uint32_t());
+-  test32(int32_t());
+-  test32(float());
+-
+-#if HWY_CAP_INTEGER64
+-  const ForGE128Vectors<TestSpecialShuffle64> test64;
+-  test64(uint64_t());
+-  test64(int64_t());
+-#endif
+-
+-#if HWY_CAP_FLOAT64
+-  const ForGE128Vectors<TestSpecialShuffle64> test_d;
+-  test_d(double());
+-#endif
++  printf("\n");
+ }
+ 
+-struct TestConcatHalves {
+-  template <class T, class D>
+-  HWY_NOINLINE void operator()(T /*unused*/, D d) {
+-    // TODO(janwas): fix
+-#if HWY_TARGET != HWY_RVV
+-    // Construct inputs such that interleaved halves == iota.
+-    const auto expected = Iota(d, 1);
+-
+-    const size_t N = Lanes(d);
+-    auto lo = AllocateAligned<T>(N);
+-    auto hi = AllocateAligned<T>(N);
+-    size_t i;
+-    for (i = 0; i < N / 2; ++i) {
+-      lo[i] = static_cast<T>(1 + i);
+-      hi[i] = static_cast<T>(lo[i] + T(N) / 2);
+-    }
+-    for (; i < N; ++i) {
+-      lo[i] = hi[i] = 0;
++// 4-tuple of byte indices
++void PrintCompress32x4Tables() {
++  using T = uint32_t;
++  constexpr size_t N = 4;  // SSE4
++  for (uint64_t code = 0; code < 1ull << N; ++code) {
++    std::array<uint32_t, N> indices{0};
++    size_t pos = 0;
++    for (size_t i = 0; i < N; ++i) {
++      if (code & (1ull << i)) {
++        indices[pos++] = i;
++      }
+     }
+ 
+-    HWY_ASSERT_VEC_EQ(d, expected,
+-                      ConcatLowerLower(Load(d, hi.get()), Load(d, lo.get())));
+-
+-    // Same for high blocks.
+-    for (i = 0; i < N / 2; ++i) {
+-      lo[i] = hi[i] = 0;
++    for (size_t i = 0; i < N; ++i) {
++      for (size_t idx_byte = 0; idx_byte < sizeof(T); ++idx_byte) {
++        printf("%zu,", sizeof(T) * indices[i] + idx_byte);
++      }
+     }
+-    for (; i < N; ++i) {
+-      lo[i] = static_cast<T>(1 + i - N / 2);
+-      hi[i] = static_cast<T>(lo[i] + T(N) / 2);
++  }
++  printf("\n");
++}
++
++// 8-tuple of byte indices
++void PrintCompress64x2Tables() {
++  using T = uint64_t;
++  constexpr size_t N = 2;  // SSE4
++  for (uint64_t code = 0; code < 1ull << N; ++code) {
++    std::array<uint32_t, N> indices{0};
++    size_t pos = 0;
++    for (size_t i = 0; i < N; ++i) {
++      if (code & (1ull << i)) {
++        indices[pos++] = i;
++      }
+     }
+ 
+-    HWY_ASSERT_VEC_EQ(d, expected,
+-                      ConcatUpperUpper(Load(d, hi.get()), Load(d, lo.get())));
+-#else
+-    (void)d;
+-#endif
++    for (size_t i = 0; i < N; ++i) {
++      for (size_t idx_byte = 0; idx_byte < sizeof(T); ++idx_byte) {
++        printf("%zu,", sizeof(T) * indices[i] + idx_byte);
++      }
++    }
+   }
+-};
+-
+-HWY_NOINLINE void TestAllConcatHalves() {
+-  ForAllTypes(ForGE128Vectors<TestConcatHalves>());
++  printf("\n");
+ }
+-
+-struct TestConcatLowerUpper {
+-  template <class T, class D>
+-  HWY_NOINLINE void operator()(T /*unused*/, D d) {
+-    // TODO(janwas): fix
+-#if HWY_TARGET != HWY_RVV
+-    const size_t N = Lanes(d);
+-    // Middle part of Iota(1) == Iota(1 + N / 2).
+-    const auto lo = Iota(d, 1);
+-    const auto hi = Iota(d, 1 + N);
+-    HWY_ASSERT_VEC_EQ(d, Iota(d, 1 + N / 2), ConcatLowerUpper(hi, lo));
+-#else
+-    (void)d;
++}  // namespace detail
+ #endif
+-  }
+-};
+ 
+-HWY_NOINLINE void TestAllConcatLowerUpper() {
+-  ForAllTypes(ForGE128Vectors<TestConcatLowerUpper>());
+-}
++HWY_NOINLINE void TestAllCompress() {
++  // detail::PrintCompress32x8Tables();
++  // detail::PrintCompress64x4Tables();
++  // detail::PrintCompress32x4Tables();
++  // detail::PrintCompress64x2Tables();
++  // detail::PrintCompress16x8Tables();
+ 
+-struct TestConcatUpperLower {
+-  template <class T, class D>
+-  HWY_NOINLINE void operator()(T /*unused*/, D d) {
+-    const size_t N = Lanes(d);
+-    const auto lo = Iota(d, 1);
+-    const auto hi = Iota(d, 1 + N);
+-    auto expected = AllocateAligned<T>(N);
+-    size_t i = 0;
+-    for (; i < N / 2; ++i) {
+-      expected[i] = static_cast<T>(1 + i);
+-    }
+-    for (; i < N; ++i) {
+-      expected[i] = static_cast<T>(1 + i + N);
+-    }
+-    HWY_ASSERT_VEC_EQ(d, expected.get(), ConcatUpperLower(hi, lo));
+-  }
+-};
++  const ForPartialVectors<TestCompress> test;
+ 
+-HWY_NOINLINE void TestAllConcatUpperLower() {
+-  ForAllTypes(ForGE128Vectors<TestConcatUpperLower>());
+-}
++  test(uint16_t());
++  test(int16_t());
++#if HWY_CAP_FLOAT16
++  test(float16_t());
++#endif
+ 
+-struct TestOddEven {
+-  template <class T, class D>
+-  HWY_NOINLINE void operator()(T /*unused*/, D d) {
+-    const size_t N = Lanes(d);
+-    const auto even = Iota(d, 1);
+-    const auto odd = Iota(d, 1 + N);
+-    auto expected = AllocateAligned<T>(N);
+-    for (size_t i = 0; i < N; ++i) {
+-      expected[i] = static_cast<T>(1 + i + ((i & 1) ? N : 0));
+-    }
+-    HWY_ASSERT_VEC_EQ(d, expected.get(), OddEven(odd, even));
+-  }
+-};
++  test(uint32_t());
++  test(int32_t());
++  test(float());
+ 
+-HWY_NOINLINE void TestAllOddEven() {
+-  ForAllTypes(ForGE128Vectors<TestOddEven>());
++#if HWY_CAP_INTEGER64
++  test(uint64_t());
++  test(int64_t());
++#endif
++#if HWY_CAP_FLOAT64
++  test(double());
++#endif
+ }
+ 
+ // NOLINTNEXTLINE(google-readability-namespace-comments)
+@@ -626,19 +332,19 @@ HWY_NOINLINE void TestAllOddEven() {
+ HWY_AFTER_NAMESPACE();
+ 
+ #if HWY_ONCE
++
+ namespace hwy {
+ HWY_BEFORE_TEST(HwySwizzleTest);
+-HWY_EXPORT_AND_TEST_P(HwySwizzleTest, TestAllShiftBytes);
+-HWY_EXPORT_AND_TEST_P(HwySwizzleTest, TestAllShiftLanes);
+-HWY_EXPORT_AND_TEST_P(HwySwizzleTest, TestAllBroadcast);
+-HWY_EXPORT_AND_TEST_P(HwySwizzleTest, TestAllTableLookupBytes);
+-HWY_EXPORT_AND_TEST_P(HwySwizzleTest, TestAllTableLookupLanes);
+-HWY_EXPORT_AND_TEST_P(HwySwizzleTest, TestAllInterleave);
+-HWY_EXPORT_AND_TEST_P(HwySwizzleTest, TestAllZip);
+-HWY_EXPORT_AND_TEST_P(HwySwizzleTest, TestAllSpecialShuffles);
+-HWY_EXPORT_AND_TEST_P(HwySwizzleTest, TestAllConcatHalves);
+-HWY_EXPORT_AND_TEST_P(HwySwizzleTest, TestAllConcatLowerUpper);
+-HWY_EXPORT_AND_TEST_P(HwySwizzleTest, TestAllConcatUpperLower);
++HWY_EXPORT_AND_TEST_P(HwySwizzleTest, TestAllGetLane);
+ HWY_EXPORT_AND_TEST_P(HwySwizzleTest, TestAllOddEven);
++HWY_EXPORT_AND_TEST_P(HwySwizzleTest, TestAllTableLookupLanes);
++HWY_EXPORT_AND_TEST_P(HwySwizzleTest, TestAllCompress);
+ }  // namespace hwy
++
++// Ought not to be necessary, but without this, no tests run on RVV.
++int main(int argc, char **argv) {
++  ::testing::InitGoogleTest(&argc, argv);
++  return RUN_ALL_TESTS();
++}
++
+ #endif
+diff --git a/third_party/highway/hwy/tests/test_util-inl.h b/third_party/highway/hwy/tests/test_util-inl.h
+index f62ebeeddd014..db9775aafbcaa 100644
+--- a/third_party/highway/hwy/tests/test_util-inl.h
++++ b/third_party/highway/hwy/tests/test_util-inl.h
+@@ -25,14 +25,13 @@
+ 
+ #include <cstddef>
+ #include <string>
+-#include <utility>  // std::forward
++#include <utility>  // std::tuple
+ 
++#include "gtest/gtest.h"
+ #include "hwy/aligned_allocator.h"
+ #include "hwy/base.h"
+ #include "hwy/highway.h"
+ 
+-#include "gtest/gtest.h"
+-
+ namespace hwy {
+ 
+ // The maximum vector size used in tests when defining test data. DEPRECATED.
+@@ -196,6 +195,10 @@ static HWY_INLINE uint32_t Random32(RandomState* rng) {
+   return static_cast<uint32_t>((*rng)());
+ }
+ 
++static HWY_INLINE uint64_t Random64(RandomState* rng) {
++  return (*rng)();
++}
++
+ // Prevents the compiler from eliding the computations that led to "output".
+ // Works by indicating to the compiler that "output" is being read and modified.
+ // The +r constraint avoids unnecessary writes to memory, but only works for
+@@ -270,20 +273,41 @@ HWY_BEFORE_NAMESPACE();
+ namespace hwy {
+ namespace HWY_NAMESPACE {
+ 
++template <typename T, HWY_IF_LANE_SIZE(T, 1)>
++HWY_NOINLINE void PrintValue(T value) {
++  uint8_t byte;
++  CopyBytes<1>(&value, &byte);  // endian-safe: we ensured sizeof(T)=1.
++  fprintf(stderr, "0x%02X,", byte);
++}
++
++#if HWY_CAP_FLOAT16
++HWY_NOINLINE void PrintValue(float16_t value) {
++  uint16_t bits;
++  CopyBytes<2>(&value, &bits);
++  fprintf(stderr, "0x%02X,", bits);
++}
++#endif
++
++template <typename T, HWY_IF_NOT_LANE_SIZE(T, 1)>
++HWY_NOINLINE void PrintValue(T value) {
++  fprintf(stderr, "%g,", double(value));
++}
++
+ // Prints lanes around `lane`, in memory order.
+ template <class D>
+-HWY_NOINLINE void Print(const D d, const char* caption, const Vec<D> v,
+-                        intptr_t lane = 0) {
++HWY_NOINLINE void Print(const D d, const char* caption,
++                        const decltype(Zero(d)) v, intptr_t lane = 0,
++                        size_t max_lanes = 7) {
+   using T = TFromD<D>;
+   const size_t N = Lanes(d);
+   auto lanes = AllocateAligned<T>(N);
+   Store(v, d, lanes.get());
+-  const size_t begin = static_cast<size_t>(std::max<intptr_t>(0, lane - 2));
+-  const size_t end = std::min(begin + 7, N);
++  const size_t begin = static_cast<size_t>(HWY_MAX(0, lane - 2));
++  const size_t end = HWY_MIN(begin + max_lanes, N);
+   fprintf(stderr, "%s %s [%zu+ ->]:\n  ", TypeName(T(), N).c_str(), caption,
+           begin);
+   for (size_t i = begin; i < end; ++i) {
+-    fprintf(stderr, "%g,", double(lanes[i]));
++    PrintValue(lanes[i]);
+   }
+   if (begin >= end) fprintf(stderr, "(out of bounds)");
+   fprintf(stderr, "\n");
+@@ -324,12 +348,12 @@ MakeUnsigned<TF> ComputeUlpDelta(TF x, TF y) {
+   const TU ux = BitCast<TU>(x);
+   const TU uy = BitCast<TU>(y);
+   // Avoid unsigned->signed cast: 2's complement is only guaranteed by C++20.
+-  return std::max(ux, uy) - std::min(ux, uy);
++  return HWY_MAX(ux, uy) - HWY_MIN(ux, uy);
+ }
+ 
+ template <typename T, HWY_IF_NOT_FLOAT(T)>
+ HWY_NOINLINE bool IsEqual(const T expected, const T actual) {
+-  return expected == actual;
++  return memcmp(&expected, &actual, sizeof(T)) == 0;
+ }
+ 
+ template <typename T, HWY_IF_FLOAT(T)>
+@@ -396,29 +420,68 @@ HWY_NOINLINE void AssertVecEqual(D d, const TFromD<D>* expected, Vec<D> actual,
+   AssertVecEqual(d, LoadU(d, expected), actual, filename, line);
+ }
+ 
++// Only checks the valid mask elements (those whose index < Lanes(d)).
+ template <class D>
+ HWY_NOINLINE void AssertMaskEqual(D d, Mask<D> a, Mask<D> b,
+                                   const char* filename, int line) {
+   AssertVecEqual(d, VecFromMask(d, a), VecFromMask(d, b), filename, line);
+ 
+   const std::string type_name = TypeName(TFromD<D>(), Lanes(d));
+-  AssertEqual(CountTrue(a), CountTrue(b), type_name, filename, line, 0);
+-  AssertEqual(AllTrue(a), AllTrue(b), type_name, filename, line, 0);
+-  AssertEqual(AllFalse(a), AllFalse(b), type_name, filename, line, 0);
++  AssertEqual(CountTrue(d, a), CountTrue(d, b), type_name, filename, line, 0);
++  AssertEqual(AllTrue(d, a), AllTrue(d, b), type_name, filename, line, 0);
++  AssertEqual(AllFalse(d, a), AllFalse(d, b), type_name, filename, line, 0);
+ 
+-  // TODO(janwas): StoreMaskBits
++  // TODO(janwas): remove RVV once implemented (cast or vse1)
++#if HWY_TARGET != HWY_RVV && HWY_TARGET != HWY_SCALAR
++  const size_t N = Lanes(d);
++  const Repartition<uint8_t, D> d8;
++  const size_t N8 = Lanes(d8);
++  auto bits_a = AllocateAligned<uint8_t>(N8);
++  auto bits_b = AllocateAligned<uint8_t>(N8);
++  memset(bits_a.get(), 0, N8);
++  memset(bits_b.get(), 0, N8);
++  const size_t num_bytes_a = StoreMaskBits(d, a, bits_a.get());
++  const size_t num_bytes_b = StoreMaskBits(d, b, bits_b.get());
++  AssertEqual(num_bytes_a, num_bytes_b, type_name, filename, line, 0);
++  size_t i = 0;
++  // First check whole bytes (if that many elements are still valid)
++  for (; i < N / 8; ++i) {
++    if (bits_a[i] != bits_b[i]) {
++      fprintf(stderr, "Mismatch in byte %zu: %d != %d\n", i, bits_a[i],
++              bits_b[i]);
++      Print(d8, "expect", Load(d8, bits_a.get()), 0, N8);
++      Print(d8, "actual", Load(d8, bits_b.get()), 0, N8);
++      hwy::Abort(filename, line, "Masks not equal");
++    }
++  }
++  // Then the valid bit(s) in the last byte.
++  const size_t remainder = N % 8;
++  if (remainder != 0) {
++    const int mask = (1 << remainder) - 1;
++    const int valid_a = bits_a[i] & mask;
++    const int valid_b = bits_b[i] & mask;
++    if (valid_a != valid_b) {
++      fprintf(stderr, "Mismatch in last byte %zu: %d != %d\n", i, valid_a,
++              valid_b);
++      Print(d8, "expect", Load(d8, bits_a.get()), 0, N8);
++      Print(d8, "actual", Load(d8, bits_b.get()), 0, N8);
++      hwy::Abort(filename, line, "Masks not equal");
++    }
++  }
++#endif
+ }
+ 
++// Only sets valid elements (those whose index < Lanes(d)). This helps catch
++// tests that are not masking off the (undefined) upper mask elements.
+ template <class D>
+ HWY_NOINLINE Mask<D> MaskTrue(const D d) {
+-  const auto v0 = Zero(d);
+-  return Eq(v0, v0);
++  return FirstN(d, Lanes(d));
+ }
+ 
+ template <class D>
+ HWY_NOINLINE Mask<D> MaskFalse(const D d) {
+-  // Lt is only for signed types and we cannot yet cast mask types.
+-  return Eq(Zero(d), Set(d, 1));
++  const auto zero = Zero(RebindToSigned<D>());
++  return RebindMask(d, Lt(zero, zero));
+ }
+ 
+ #ifndef HWY_ASSERT_EQ
+@@ -439,14 +502,39 @@ HWY_NOINLINE Mask<D> MaskFalse(const D d) {
+ 
+ // Helpers for instantiating tests with combinations of lane types / counts.
+ 
+-// For all powers of two in [kMinLanes, N * kMinLanes] (so that recursion stops
+-// at N == 0)
+-template <typename T, size_t N, size_t kMinLanes, class Test>
++template <bool valid>
++struct CallTestIf {
++  template <typename T, size_t N, class Test>
++  static void Do() {
++    const Simd<T, N> d;
++    // Skip invalid fractions (e.g. 1/8th of u32x4).
++    if (Lanes(d) == 0) return;
++    Test()(T(), d);
++  }
++};
++// Avoids instantiating tests for invalid N (a smaller fraction than 1/8th).
++template <>
++struct CallTestIf<false> {
++  template <typename T, size_t N, class Test>
++  static void Do() {
++    // Should only happen with scalable vectors.
++    HWY_ASSERT(HWY_TARGET == HWY_RVV || HWY_TARGET == HWY_SVE ||
++               HWY_TARGET == HWY_SVE2);
++  }
++};
++
++// For all powers of two in [kMinLanes, kMul * kMinLanes] (so that recursion
++// stops at kMul == 0)
++template <typename T, size_t kMul, size_t kMinLanes, class Test>
+ struct ForeachSizeR {
+   static void Do() {
+-    static_assert(N != 0, "End of recursion");
+-    Test()(T(), Simd<T, N * kMinLanes>());
+-    ForeachSizeR<T, N / 2, kMinLanes, Test>::Do();
++    static_assert(kMul != 0, "End of recursion");
++    constexpr size_t N = kMul * kMinLanes;
++    constexpr bool kIsExact = N * sizeof(T) <= 16;
++    constexpr bool kIsRatio = N >= HWY_LANES(T) / 8;
++    CallTestIf<kIsExact || kIsRatio>::template Do<T, N, Test>();
++
++    ForeachSizeR<T, kMul / 2, kMinLanes, Test>::Do();
+   }
+ };
+ 
+@@ -458,60 +546,122 @@ struct ForeachSizeR<T, 0, kMinLanes, Test> {
+ 
+ // These adapters may be called directly, or via For*Types:
+ 
+-// Calls Test for all powers of two in [kMinLanes, HWY_LANES(T) / kDivLanes].
+-template <class Test, size_t kDivLanes = 1, size_t kMinLanes = 1>
++// Calls Test for all power of two N in [1, Lanes(d)]. This is the default
++// for ops that do not narrow nor widen their input, nor require 128 bits.
++template <class Test>
+ struct ForPartialVectors {
+   template <typename T>
+   void operator()(T /*unused*/) const {
+ #if HWY_TARGET == HWY_RVV
+-    // Only m1..8 for now, can ignore kMaxLanes because HWY_*_LANES are full.
+-    ForeachSizeR<T, 8 / kDivLanes, HWY_LANES(T), Test>::Do();
++    // Only m1..8 until we support fractional LMUL.
++    ForeachSizeR<T, 8, HWY_LANES(T), Test>::Do();
+ #else
+-    ForeachSizeR<T, HWY_LANES(T) / kDivLanes / kMinLanes, kMinLanes,
+-                 Test>::Do();
++    ForeachSizeR<T, HWY_LANES(T), 1, Test>::Do();
+ #endif
+   }
+ };
+ 
+-// Calls Test for all vectors that can be demoted log2(kFactor) times.
+-template <class Test, size_t kFactor>
+-struct ForDemoteVectors {
++// Calls Test for all power of two N in [16 / sizeof(T), Lanes(d)]. This is for
++// ops that require at least 128 bits, e.g. AES or 64x64 = 128 mul.
++template <class Test>
++struct ForGE128Vectors {
+   template <typename T>
+   void operator()(T /*unused*/) const {
+ #if HWY_TARGET == HWY_RVV
+-    // Only m1..8 for now.
+-    ForeachSizeR<T, 8 / kFactor, kFactor * HWY_LANES(T), Test>::Do();
++    ForeachSizeR<T, 8, HWY_LANES(T), Test>::Do();
+ #else
+-    ForeachSizeR<T, HWY_LANES(T), 1, Test>::Do();
++    ForeachSizeR<T, HWY_LANES(T) / (16 / sizeof(T)), (16 / sizeof(T)),
++                 Test>::Do();
+ #endif
+   }
+ };
+ 
+-// Calls Test for all powers of two in [128 bits, max bits].
++// Calls Test for all power of two N in [8 / sizeof(T), Lanes(d)]. This is for
++// ops that require at least 64 bits, e.g. casts.
+ template <class Test>
+-struct ForGE128Vectors {
++struct ForGE64Vectors {
+   template <typename T>
+   void operator()(T /*unused*/) const {
+ #if HWY_TARGET == HWY_RVV
+     ForeachSizeR<T, 8, HWY_LANES(T), Test>::Do();
+ #else
+-    ForeachSizeR<T, HWY_LANES(T) / (16 / sizeof(T)), (16 / sizeof(T)),
++    ForeachSizeR<T, HWY_LANES(T) / (8 / sizeof(T)), (8 / sizeof(T)),
+                  Test>::Do();
+-
+ #endif
+   }
+ };
+ 
+-// Calls Test for all vectors that can be expanded by kFactor.
++// Calls Test for all power of two N in [1, Lanes(d) / kFactor]. This is for
++// ops that widen their input, e.g. Combine (not supported by HWY_SCALAR).
+ template <class Test, size_t kFactor = 2>
+ struct ForExtendableVectors {
+   template <typename T>
+   void operator()(T /*unused*/) const {
+-#if HWY_TARGET == HWY_RVV
++#if HWY_TARGET == HWY_SCALAR
++    // not supported
++#elif HWY_TARGET == HWY_RVV
+     ForeachSizeR<T, 8 / kFactor, HWY_LANES(T), Test>::Do();
++    // TODO(janwas): also capped
++    // ForeachSizeR<T, (16 / sizeof(T)) / kFactor, 1, Test>::Do();
++#elif HWY_TARGET == HWY_SVE || HWY_TARGET == HWY_SVE2
++    // Capped
++    ForeachSizeR<T, (16 / sizeof(T)) / kFactor, 1, Test>::Do();
++    // Fractions
++    ForeachSizeR<T, 8 / kFactor, HWY_LANES(T) / 8, Test>::Do();
+ #else
+-    ForeachSizeR<T, HWY_LANES(T) / kFactor / (16 / sizeof(T)), (16 / sizeof(T)),
+-                 Test>::Do();
++    ForeachSizeR<T, HWY_LANES(T) / kFactor, 1, Test>::Do();
++#endif
++  }
++};
++
++// Calls Test for all N that can be promoted (not the same as Extendable because
++// HWY_SCALAR has one lane). Also used for ZipLower, but not ZipUpper.
++template <class Test, size_t kFactor = 2>
++struct ForPromoteVectors {
++  template <typename T>
++  void operator()(T /*unused*/) const {
++#if HWY_TARGET == HWY_SCALAR
++    ForeachSizeR<T, 1, 1, Test>::Do();
++#else
++    return ForExtendableVectors<Test, kFactor>()(T());
++#endif
++  }
++};
++
++// Calls Test for all power of two N in [kFactor, Lanes(d)]. This is for ops
++// that narrow their input, e.g. UpperHalf.
++template <class Test, size_t kFactor = 2>
++struct ForShrinkableVectors {
++  template <typename T>
++  void operator()(T /*unused*/) const {
++#if HWY_TARGET == HWY_RVV
++    // Only m1..8 until we support fractional LMUL.
++    ForeachSizeR<T, 8 / kFactor, kFactor * HWY_LANES(T), Test>::Do();
++#elif HWY_TARGET == HWY_SVE || HWY_TARGET == HWY_SVE2
++    ForeachSizeR<T, 8 / kFactor, kFactor * HWY_LANES(T) / 8, Test>::Do();
++#elif HWY_TARGET == HWY_SCALAR
++    // not supported
++#else
++    ForeachSizeR<T, HWY_LANES(T) / kFactor, kFactor, Test>::Do();
++#endif
++  }
++};
++
++// Calls Test for all N than can be demoted (not the same as Shrinkable because
++// HWY_SCALAR has one lane). Also used for LowerHalf, but not UpperHalf.
++template <class Test, size_t kFactor = 2>
++struct ForDemoteVectors {
++  template <typename T>
++  void operator()(T /*unused*/) const {
++#if HWY_TARGET == HWY_RVV
++    // Only m1..8 until we support fractional LMUL.
++    ForeachSizeR<T, 8 / kFactor, kFactor * HWY_LANES(T), Test>::Do();
++#elif HWY_TARGET == HWY_SVE || HWY_TARGET == HWY_SVE2
++    ForeachSizeR<T, 8 / kFactor, kFactor * HWY_LANES(T) / 8, Test>::Do();
++#elif HWY_TARGET == HWY_SCALAR
++    ForeachSizeR<T, 1, 1, Test>::Do();
++#else
++    ForeachSizeR<T, HWY_LANES(T) / kFactor, kFactor, Test>::Do();
+ #endif
+   }
+ };
+diff --git a/third_party/highway/hwy/tests/test_util_test.cc b/third_party/highway/hwy/tests/test_util_test.cc
+index b0f5edf52afe7..140e2d975f013 100644
+--- a/third_party/highway/hwy/tests/test_util_test.cc
++++ b/third_party/highway/hwy/tests/test_util_test.cc
+@@ -94,9 +94,17 @@ HWY_NOINLINE void TestAllEqual() {
+ HWY_AFTER_NAMESPACE();
+ 
+ #if HWY_ONCE
++
+ namespace hwy {
+ HWY_BEFORE_TEST(TestUtilTest);
+ HWY_EXPORT_AND_TEST_P(TestUtilTest, TestAllName);
+ HWY_EXPORT_AND_TEST_P(TestUtilTest, TestAllEqual);
+ }  // namespace hwy
++
++// Ought not to be necessary, but without this, no tests run on RVV.
++int main(int argc, char **argv) {
++  ::testing::InitGoogleTest(&argc, argv);
++  return RUN_ALL_TESTS();
++}
++
+ #endif
+diff --git a/third_party/highway/run_tests.sh b/third_party/highway/run_tests.sh
+old mode 100644
+new mode 100755
+diff --git a/third_party/highway/test.py b/third_party/highway/test.py
+deleted file mode 100644
+index f0e5da31acdb9..0000000000000
+--- a/third_party/highway/test.py
++++ /dev/null
+@@ -1,131 +0,0 @@
+-#!/usr/bin/env python3
+-"""Helper for running tests for all platforms.
+-
+-One-time setup:
+-sudo apt-get install npm
+-sudo npm install -g npm jsvu
+-
+-Usage:
+-third_party/highway/test.py [--test=memory_test]
+-
+-"""
+-
+-import os
+-import sys
+-import tarfile
+-import tempfile
+-import argparse
+-import subprocess
+-
+-
+-def run_subprocess(args, work_dir):
+-  """Runs subprocess and checks for success."""
+-  process = subprocess.Popen(args, cwd=work_dir)
+-  process.communicate()
+-  assert process.returncode == 0
+-
+-
+-def print_status(name):
+-  print("=" * 60, name)
+-
+-
+-def run_blaze_tests(work_dir, target, desired_config, config_name, blazerc,
+-                    config):
+-  """Builds and runs via blaze or returns 0 if skipped."""
+-  if desired_config is not None and desired_config != config_name:
+-    return 0
+-  print_status(config_name)
+-  default_config = ["-c", "opt", "--copt=-DHWY_COMPILE_ALL_ATTAINABLE"]
+-  args = ["blaze"] + blazerc + ["test", ":" + target] + config + default_config
+-  run_subprocess(args, work_dir)
+-  return 1
+-
+-
+-def run_wasm_tests(work_dir, target, desired_config, config_name, options):
+-  """Runs wasm via blaze/v8, or returns 0 if skipped."""
+-  if desired_config is not None and desired_config != config_name:
+-    return 0
+-  args = [options.v8, "--no-liftoff", "--experimental-wasm-simd"]
+-  # Otherwise v8 returns 0 even after compile failures!
+-  args.append("--no-wasm-async-compilation")
+-  if options.profile:
+-    args.append("--perf-basic-prof-only-functions")
+-
+-  num_tests_run = 0
+-  skipped = []
+-
+-  # Build (no blazerc to avoid failures caused by local .blazerc)
+-  run_subprocess([
+-      "blaze", "--blazerc=/dev/null", "build", "--config=wasm",
+-      "--features=wasm_simd", ":" + target
+-  ], work_dir)
+-
+-  path = "blaze-bin/third_party/highway/"
+-  TEST_SUFFIX = "_test"
+-  for test in os.listdir(path):
+-    # Only test binaries (avoids non-tar files)
+-    if not test.endswith(TEST_SUFFIX):
+-      continue
+-
+-    # Skip directories
+-    tar_pathname = os.path.join(path, test)
+-    if os.path.isdir(tar_pathname):
+-      continue
+-
+-    # Only the desired test (if given)
+-    if options.test is not None and options.test != test:
+-      skipped.append(test[0:-len(TEST_SUFFIX)])
+-      continue
+-
+-    with tempfile.TemporaryDirectory() as extract_dir:
+-      with tarfile.open(tar_pathname, mode="r:") as tar:
+-        tar.extractall(extract_dir)
+-
+-      test_args = args + [os.path.join(extract_dir, test) + ".js"]
+-      run_subprocess(test_args, extract_dir)
+-      num_tests_run += 1
+-
+-  print("Finished", num_tests_run, "; skipped", ",".join(skipped))
+-  assert (num_tests_run != 0)
+-  return 1
+-
+-
+-def main(args):
+-  parser = argparse.ArgumentParser(description="Run test(s)")
+-  parser.add_argument("--v8", help="Pathname to v8 (default ~/jsvu/v8)")
+-  parser.add_argument("--test", help="Which test to run (defaults to all)")
+-  parser.add_argument("--config", help="Which config to run (defaults to all)")
+-  parser.add_argument("--profile", action="store_true", help="Enable profiling")
+-  options = parser.parse_args(args)
+-  if options.v8 is None:
+-    options.v8 = os.path.join(os.getenv("HOME"), ".jsvu", "v8")
+-
+-  work_dir = os.path.dirname(os.path.realpath(__file__))
+-  target = "hwy_ops_tests" if options.test is None else options.test
+-
+-  num_config = 0
+-  num_config += run_blaze_tests(work_dir, target, options.config, "x86", [], [])
+-  num_config += run_blaze_tests(
+-      work_dir, target, options.config, "rvv",
+-      ["--blazerc=../../third_party/unsupported_toolchains/mpu/blazerc"],
+-      ["--config=mpu64_gcc"])
+-  num_config += run_blaze_tests(work_dir, target, options.config, "arm8", [],
+-                                ["--config=android_arm64"])
+-  num_config += run_blaze_tests(work_dir, target, options.config, "arm7", [], [
+-      "--config=android_arm", "--copt=-mfpu=neon-vfpv4",
+-      "--copt=-mfloat-abi=softfp"
+-  ])
+-  num_config += run_blaze_tests(work_dir, target, options.config, "msvc", [],
+-                                ["--config=msvc"])
+-  num_config += run_wasm_tests(work_dir, target, options.config, "wasm",
+-                               options)
+-
+-  if num_config == 0:
+-    print_status("ERROR: unknown --config=%s, omit to see valid names" %
+-                 (options.config,))
+-  else:
+-    print_status("done")
+-
+-
+-if __name__ == "__main__":
+-  main(sys.argv[1:])
+From 4e8f7069ac2b8078897ee1fff9e6b4ec3d247f9e Mon Sep 17 00:00:00 2001
+From: Makoto Kato <m_kato@ga2.so-net.ne.jp>
+Date: Sun, 15 Aug 2021 01:23:35 +0900
+Subject: [PATCH] Export some header files after upgrading highway.
+
+---
+ media/highway/moz.build | 3 +++
+ 1 file changed, 3 insertions(+)
+
+diff --git a/media/highway/moz.build b/media/highway/moz.build
+index b64522554d5e7..d4465797931e9 100644
+--- a/media/highway/moz.build
++++ b/media/highway/moz.build
+@@ -18,6 +18,8 @@ EXPORTS.hwy += [
+     "/third_party/highway/hwy/aligned_allocator.h",
+     "/third_party/highway/hwy/base.h",
+     "/third_party/highway/hwy/cache_control.h",
++    "/third_party/highway/hwy/detect_compiler_arch.h",
++    "/third_party/highway/hwy/detect_targets.h",
+     "/third_party/highway/hwy/foreach_target.h",
+     "/third_party/highway/hwy/highway.h",
+     "/third_party/highway/hwy/targets.h",
+@@ -25,6 +27,7 @@ EXPORTS.hwy += [
+ 
+ EXPORTS.hwy.ops += [
+     "/third_party/highway/hwy/ops/arm_neon-inl.h",
++    "/third_party/highway/hwy/ops/generic_ops-inl.h",
+     "/third_party/highway/hwy/ops/rvv-inl.h",
+     "/third_party/highway/hwy/ops/scalar-inl.h",
+     "/third_party/highway/hwy/ops/set_macros-inl.h",
